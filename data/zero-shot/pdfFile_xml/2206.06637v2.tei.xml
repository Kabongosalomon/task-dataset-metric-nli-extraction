<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Yu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-dilation</term>
					<term>receptive field</term>
					<term>spatial convolutional network</term>
					<term>temporal convolutional network</term>
					<term>temporal action segmentation !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal/spatial receptive fields of models play an important role in sequential/spatial tasks. Large receptive fields facilitate long-term relations, while small receptive fields help to capture the local details. Existing methods construct models with hand-designed receptive fields in layers. Can we effectively search for receptive field combinations to replace hand-designed patterns? To answer this question, we propose to find better receptive field combinations through a global-to-local search scheme. Our search scheme exploits both global search to find the coarse combinations and local search to get the refined receptive field combinations further. The global search finds possible coarse combinations other than human-designed patterns. On top of the global search, we propose an expectation-guided iterative local search scheme to refine combinations effectively. Our RF-Next models, plugging receptive field search to various models, boost the performance on many tasks, e.g., temporal action segmentation, object detection, instance segmentation, and speech synthesis. The source code is publicly available on http://mmcheng.net/rfnext.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Due to the strong representation ability, convolutional neural networks (CNN) have been widely used in spatially visual recognition tasks, e.g., object detection <ref type="bibr" target="#b1">[2]</ref>, saliency detection <ref type="bibr" target="#b2">[3]</ref>, instance segmentation <ref type="bibr" target="#b3">[4]</ref>, and semantic segmentation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, as well as sequential perception tasks, e.g., temporal action segmentation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and speech synthesis <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. CNN processes short/longterm features by stacking convolutional filters with different receptive fields. Spatial convolutional networks (SCN) for visual recognition tasks process local and global features to represent the texture and semantic information. Temporal convolutional networks (TCN) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref> are widely adopted in sequential tasks with their ability to capture both long-term and short-term information. Appropriate receptive fields in layers are crucial for both SCN and TCN as large receptive fields contribute to long-term dependencies while small receptive fields benefit the local details. State-of-the-art (SOTA) SCN <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref> and TCN <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> methods rely on human-designed receptive field combinations, i.e., dilation rate or pooling size in each layer, to make the trade-off between capturing long and short term dependencies. Questions have been raised: Are there other effective receptive field combinations that perform comparable or better than hand-designed patterns? Will the receptive field combinations vary among different datasets? To answer those questions, we propose to find the possible receptive field combinations in a coarse-to-fine scheme through the global-to-local search.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, unlike the existing network architecture search spaces <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref> that only contain several operation options within a layer, the available search space of receptive field combinations could be huge. Suppose a TCN/SCN has L convolutional layers and D possible receptive fields in each layer,  there are D L possible combinations, i.e., the MS-TCN <ref type="bibr" target="#b11">[12]</ref> for the long-sequence temporal action segmentation task, consisting of 40 layers and 1024 possible receptive fields in each layer, has 1024 40 possible receptive field combinations. Directly applying network architecture searching algorithms <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref> to such a huge search space is impractical. For example, conventional reward-based searching methods <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref> are unsuitable for CNN-based models with a huge search space. The model training and performance evaluation of each possible combination are too costly. Differentiable architecture searching methods (DARTS) <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> rely on shared big networks to save training time, thus only supporting several operators within a layer due to the constraint of model size. Moreover, they are heavily dependent on the initial combination and fail to find new combinations with a huge difference from the initial one. While our goal is to explore effective receptive field combinations other than human-designed patterns in the huge search space, those algorithms are either too costly or cannot support the large search space. To explore the effective receptive fields with a low cost, we exploit both a genetic-based global search to find the coarse arXiv:2206.06637v2 [cs.CV] 15 Jun 2022 receptive field combinations and an expectation-guided iterative (EGI) local search to get the refined combinations. Specifically, we follow common settings in many existing methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> to use dilation rates to determine layers' receptive fields. A genetic-based global search scheme is proposed to find coarse combinations within a sparsely sampled search space at an affordable cost. The global search discovers various combinations that achieve even better performance than human designings but have completely different patterns. Based on the global-searched coarse combinations, we propose the local search to determine fine-grained dilation rates. In local search, a convolutional weightsharing scheme enforces learned dilation coefficients to approximate the probability mass distribution for calculating the expectation of dilation rates. The expectation-guided searching transfers the discrete dilation rates into a distribution, allowing fine-grained dilation rates search. With an iterative searching process, the local search gradually finds more effective fine-grained receptive field combinations with a low cost. Models enhanced by our proposed global-to-local search scheme, namely RF-Next models, surpass human-designed structures with impressive performance gain on many tasks. In summary, we make two major contributions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>operators dilations</head><p>? The expectation-guided iterative local search scheme enables searching fine-grained receptive field combinations in the dense search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The global-to-local search discovers effective receptive field combinations with better performance than handdesigned patterns.</p><p>The conference version <ref type="bibr" target="#b0">[1]</ref> mainly explores the proposed receptive field searching scheme on the temporal action segmentation task. In this work, we improve and give more analysis of our proposed searching scheme, i.e., parallel receptive fields for multi-scale enhancement, searching cost analysis, generalization to different tasks, observations of receptive fields. We generalize our proposed receptive field searching method to multiple tasks on both temporal and spatial dimensions, i.e., speech synthesis, sequence modeling, instance segmentation, object detection, and semantic segmentation. To meet the multi-scale requirements of some tasks, we take advantage of the proposed expectation-guide search scheme to enable the searched structure with parallel multiple receptive fields and shared convolutional weights. We give observations on the receptive field requirements of multiple tasks based on the searched results: 1) Proper receptive fields of CNN are beneficial to many tasks. 2) The receptive field requirements of different parts of the network are quite different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Receptive Field in Networks</head><p>The effect of receptive fields has been widely studied <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b33">[34]</ref>. Although the theoretical receptive field could be huge, <ref type="bibr" target="#b28">[29]</ref> shows that the effective receptive field occupies a fraction of the theoretical receptive field. <ref type="bibr" target="#b29">[30]</ref> assumes that useful predictive information for a pixel comes from nearby locations rather than far pixels and proposes to gradually suppress the distant pixel values with the scale-sensitive regularization. <ref type="bibr" target="#b34">[35]</ref> enlarges the receptive field to improve the performance of the image superresolution task. <ref type="bibr" target="#b30">[31]</ref> observes that model depth must be congruent with the receptive field size for the image super-resolution task. <ref type="bibr" target="#b33">[34]</ref> forms continuous receptive fields with the help of Gaussian scale-space representation. Parallel receptive fields are proposed to enable a more flexible receptive field within a layer <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Inception net series <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref> explores the parallel asymmetric convolutions with different receptive fields to enhance the model representation ability. Atrous spatial pyramid modules <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b31">[32]</ref> prove the effectiveness of parallel receptive fields in semantic segmentation. OctConv <ref type="bibr" target="#b32">[33]</ref> decomposes the convolution to process two feature scales simultaneously. Some works model the connections of all positions to form arbitrary receptive fields theoretically <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>. Multi-head attention is presented by <ref type="bibr" target="#b38">[39]</ref> to model the relation between every two pixels with attention mechanisms. Similarly, <ref type="bibr" target="#b39">[40]</ref> utilizes the non-local operation to aggregate features at all positions. A graph-based global reasoning module <ref type="bibr" target="#b40">[41]</ref> is proposed to capture relations between arbitrary regions. Despite these methods exploring receptive fields' great potential, choosing effective receptive fields for different tasks is still an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sequential Tasks</head><p>Sequential tasks process data in the form of sequences, e.g., video stream and audio stream. As the sequence length could have a large variance for sequential tasks, models with a proper range of effective receptive fields are needed. In this work, we mainly tackle two sequential tasks with long sequences of data, i.e., temporal action segmentation and speech synthesis, which represent video and audio dimensions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Temporal Action Segmentation</head><p>Temporal action recognition segments the action of each video frame, playing an important role in computer vision applications such as clips tagging <ref type="bibr" target="#b41">[42]</ref>, video surveillance <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, and anomaly detection <ref type="bibr" target="#b44">[45]</ref>. While conventional works <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b48">[49]</ref> have continuously refreshed the recognition performance of short trimmed videos containing a single activity, segmenting each frame densely in long untrimmed videos remains challenging as those videos contain many activities with different temporal lengths. In this work, we do our major experiments on the temporal action segmentation task. Therefore, we give a thorough introduction to related works for the temporal action segmentation task.</p><p>Many approaches have been proposed for modeling dependencies for temporal action segmentation. Early works <ref type="bibr" target="#b49">[50]</ref>- <ref type="bibr" target="#b51">[52]</ref> mostly model the changing state of appearance and actions with sliding windows <ref type="bibr" target="#b52">[53]</ref>- <ref type="bibr" target="#b54">[55]</ref>. Thus they mainly focus on shortterm dependencies. Capturing both short-term and long-term dependencies then gradually becomes the focus of temporal action segmentation.</p><p>Sequential Model. Sequential models capture long-short term dependencies in an iterative form. Vo and Bobick <ref type="bibr" target="#b55">[56]</ref> apply the Bayes network to segment actions represented by a stochastic context-free grammar. Tang et al. <ref type="bibr" target="#b56">[57]</ref> use a hidden Markov model to model transitions between states and durations. Later, hidden Markov models are combined with context-free grammar <ref type="bibr" target="#b57">[58]</ref>, Gaussian mixture model <ref type="bibr" target="#b58">[59]</ref>, and recurrent networks <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> to model long-term action dependencies. Cheng et al. <ref type="bibr" target="#b61">[62]</ref> apply the sequence memorizer to capture long-range dependencies in visual words learned from the video. However, these sequential models are inflexible in parallelly modeling long-term dependencies and usually suffer from information forgetting <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>Multi-stream Architecture. Some researchers <ref type="bibr" target="#b62">[63]</ref>- <ref type="bibr" target="#b65">[66]</ref> utilize multi-stream models to model dependencies from long and short term. Richard and Gall employ <ref type="bibr" target="#b62">[63]</ref> dynamic programming to inference models composed of length model, language model, and action classifier. Singh et al. <ref type="bibr" target="#b63">[64]</ref> learn short video chunks representation with a two-stream network and pass these chunks to a bi-directional network to predict temporal action segmentation results sequentially. A three-stream architecture is proposed in <ref type="bibr" target="#b64">[65]</ref>, which contains egocentric cues, spatial and temporal streams. Tricornet <ref type="bibr" target="#b65">[66]</ref> utilizes a hybrid temporal convolutional and recurrent network to capture local motion and memorize long-term action dependencies. CoupledGAN <ref type="bibr" target="#b66">[67]</ref> uses a GAN model to utilize multi-modal data to better model human actions' evolution. Capturing long-short term information with multiple streams increases computational redundancy.</p><p>Temporal Convolutional Network. Recently, temporal convolutional networks (TCN) have been introduced to model dependencies of different ranges within a unified structure by adjusting receptive fields and can process long videos in parallel. <ref type="bibr">Lea et al. [11]</ref> propose the encoder-decoder style TCN for the temporal action segmentation to capture long-range temporal patterns and apply the dilated convolution to enlarge the receptive field. TDRN <ref type="bibr" target="#b67">[68]</ref> further introduces the deformable convolution to process the full-resolution residual stream and low-resolution pooled stream. MS-TCN <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref> utilizes multi-stage dilated TCNs with hand-designed dilation rate combinations to capture information from various temporal receptive fields. However, the adjustment of receptive fields still relies on human design, which may not be appropriate. Our proposed efficient receptive field combinations searching scheme can automatically discover more efficient structures, improving these TCN based methods.</p><p>Complementary Techniques. Instead of capturing long-term and short-term information, some works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b68">[69]</ref> further improve the temporal action segmentation performance with boundary refinement. Li et al. <ref type="bibr" target="#b68">[69]</ref> utilize an iterative training procedure with transcript refinement and soft boundary assignment. Wang et al. <ref type="bibr" target="#b12">[13]</ref> leverage semantic boundary information to refine the prediction results. Other researchers focus on temporal action segmentation under the weakly supervised <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b68">[69]</ref>, or unsupervised <ref type="bibr" target="#b69">[70]</ref> settings. These works still rely on the efficient TCN to model the action dependencies, thus complementing the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Speech Synthesis</head><p>Speech synthesis, also known as text to speech (TTS), aims to synthesize human-like natural speech from text <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>, which has thrived due to the strong feature representation ability of the neural networks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b72">[73]</ref>. The large difference in modality and feature-length between text and speech makes it hard to implement TTS in an end-to-end style <ref type="bibr" target="#b73">[74]</ref>. Therefore, common approaches for speech synthesis decompose this process into the following steps: 1) text-to-linguistic features transformation <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>. 2) transfer linguistic features or text to acoustic features <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b76">[77]</ref>- <ref type="bibr" target="#b78">[79]</ref>. 3) generate the final speech waveform using the acoustic features <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b72">[73]</ref>. During the process, features are transformed from the short-length text sequence to the long-length acoustic features and speech waveform, i.e., text with about 20 words results in a 5-second speech sequence of 80k sampling points. We tackle the challenge of transferring acoustic features to speech waveform, as this procedure requires a suitable receptive field to model the short/long-term dependencies in the waveform <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b73">[74]</ref>. Since we only focus on the receptive field of TTS models, we refer readers to the survey <ref type="bibr" target="#b73">[74]</ref> for more details of speech synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Spatial Tasks</head><p>Unlike the sequential tasks that mostly process one-dimension sequences, spatial tasks process images with two dimensions, i.e., height and width dimensions. To extract features of objects of various sizes in the scene, the model needs small receptive fields to detect small objects and large receptive fields to cover large objects or capture surrounding context information <ref type="bibr" target="#b15">[16]</ref>. We mainly tackle two popular vision tasks with the proposed receptive field search, i.e., object detection and instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Object Detection</head><p>Object detection aims to localize objects with bounding boxes and assign categories accordingly <ref type="bibr" target="#b79">[80]</ref>- <ref type="bibr" target="#b83">[84]</ref>. Common object detection methods can be divided into single-stage <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b84">[85]</ref>- <ref type="bibr" target="#b86">[87]</ref> and two-stage <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref> pipelines. More details about object detection can be referred to related surveys <ref type="bibr" target="#b89">[90]</ref>- <ref type="bibr" target="#b91">[92]</ref>. Singlestage detectors, e.g., SSD <ref type="bibr" target="#b84">[85]</ref>, YOLO <ref type="bibr" target="#b85">[86]</ref>, and CornerNet <ref type="bibr" target="#b86">[87]</ref>, require one inference to end-to-end localize and categorize objects, which are effective in latency but hard to cover all objects. Twostage methods, e.g., R-CNN <ref type="bibr" target="#b92">[93]</ref>, Faster-RCNN <ref type="bibr" target="#b1">[2]</ref>, and Cascade R-CNN <ref type="bibr" target="#b87">[88]</ref>, decompose object detection into region proposals generation and objects detection from the proposal, enhancing the detection quality at the cost of slow inference speed. Despite the difference, object detectors tend to enhance the multi-scale ability to handle objects of various sizes <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>. SSD <ref type="bibr" target="#b84">[85]</ref> merges features from multiple stages to detect objects. Faster-RCNN <ref type="bibr" target="#b1">[2]</ref> utilizes a feature pyramid network to aggregate features with multiple scales. Cascade R-CNN <ref type="bibr" target="#b87">[88]</ref> and HTC <ref type="bibr" target="#b88">[89]</ref> perform cascaded multi-stage feature fusion and refinement. We show that the proper receptive field settings for these methods can further improve the detection ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Instance Segmentation</head><p>Instance segmentation aims to assign category labels to each pixel of instances <ref type="bibr" target="#b93">[94]</ref>- <ref type="bibr" target="#b95">[96]</ref>, which is similar to object detection as they both require localizing objects. Therefore, common instance segmentation methods add the segmentation branch on object detectors to segment instances from the bounding boxes, i.e., Mask-RCNN <ref type="bibr" target="#b3">[4]</ref> extends Faster-RCNN by adding an object mask predicting branch. The multi-scale ability in object detectors, e.g., Cascade R-CNN <ref type="bibr" target="#b87">[88]</ref> and HTC <ref type="bibr" target="#b88">[89]</ref>, is naturally inherited to the instance segmentation. Some works focus on refining the boundary of segmentation masks <ref type="bibr" target="#b96">[97]</ref>- <ref type="bibr" target="#b100">[101]</ref>, which still rely on feature extractors with proper receptive fields. We observe that the receptive field search benefits the performance of instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Network Architecture Search</head><p>The genetic algorithm <ref type="bibr" target="#b101">[102]</ref> has achieved remarkable performance on a wide range of applications. Many genetic-based methods have been recently introduced for the neural networks architecture search of vision tasks <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b102">[103]</ref>, <ref type="bibr" target="#b103">[104]</ref>. An evolutionary coding scheme is proposed in Genetic CNN <ref type="bibr" target="#b23">[24]</ref> to encode the network architecture to a binary string. A hierarchical representation is presented by Liu et al. <ref type="bibr" target="#b25">[26]</ref> to constrain the search space. Real et al. <ref type="bibr" target="#b24">[25]</ref> regularize the evolution by an age property selection operation. Sun et al. <ref type="bibr" target="#b102">[103]</ref> introduce a variable-length encoding method for effective architecture designing. However, the genetic algorithm requires the training of each candidate, consuming too much computational cost when faced with a huge search space.</p><p>Differentiable architecture search <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b104">[105]</ref>- <ref type="bibr" target="#b107">[108]</ref> saves the training time by introducing a large network containing subnetworks with different searching options. The importance of searched blocks is determined by gradient backpropagation <ref type="bibr" target="#b108">[109]</ref>. However, these network architecture search methods are designed for finding a limited number of operations such as convolution, ReLU, batch normalization, short connection, etc. Thus, these methods cannot be directly used for receptive field search due to the different searching targets, e.g., they cannot handle the huge receptive field combinations search space. Fair DARTS <ref type="bibr" target="#b104">[105]</ref> solves the problem of performance collapse due to the unfair advantage in exclusive competition between different operators. The receptive field searching has no such problem as it contains the same operation. <ref type="bibr" target="#b105">[106]</ref> reduces the memory cost of supernet by randomly sampling a proportion of channels for operation search and bypassing the held out part in a shortcut. This approach is unsuitable for receptive field search as the shortcut does not belong to the receptive field search space. <ref type="bibr" target="#b106">[107]</ref> conducts channel number and feature resolution searching with a masking mechanism for feature map reuse. The masking mechanism for feature map reuse cannot be applied to the receptive field search represented by dilation rate because different dilation rates do not belong to each other and cannot be chosen with different masks. <ref type="bibr" target="#b107">[108]</ref> narrows the performance gap between models searched from a small dataset and evaluated on the large dataset. Theoretically, <ref type="bibr" target="#b107">[108]</ref> is orthogonal to receptive field search space. However, the high efficiency of the proposed local search allows to directly search on the large dataset instead of on a small dataset. In this paper, we propose a global search to handle the huge search space with sparse sampling. The expectation-guided iterative local search then transfers the sparse search space of receptive fields into the dense one for fine-level searching.</p><p>This differentiable search idea is further extended <ref type="bibr" target="#b109">[110]</ref> to deal with semantic segmentation <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b110">[111]</ref>, and other tasks beyond image classification <ref type="bibr" target="#b20">[21]</ref>. Auto-deeplab <ref type="bibr" target="#b22">[23]</ref> and DCNAS <ref type="bibr" target="#b110">[111]</ref> focus on searching the feature resolution for different stages of the semantic segmentation network. We show that our proposed receptive field searching scheme can find better receptive fields on these searched segmentation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>The pipeline of our proposed global-to-local search method has two components: (i) a genetic-based global search algorithm that produces coarse but competitive combinations of the receptive fields; (ii) an expectation-guided iterative local search scheme that locally refines the global-searched coarse structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Description</head><p>Our objective is to efficiently search for optimal receptive field combinations for the given dataset. The receptive field can be represented in multiple forms: the dilation rate, kernel size, pooling size, stride, and the stack number of layers. Our method is initially designed for temporal action segmentation. We mainly follow the MS-TCN <ref type="bibr" target="#b11">[12]</ref> to formulate the receptive fields using the combinations of dilation rates in layers and propose to evolve these combinations during the searching process. Other receptive field representations can also be applied to the proposed globalto-local search with minor adjustments. Though we conduct major experiments on the temporal action segmentation task, our receptive field searching method can easily be generalized to new tasks, as introduced in Sec. 3.4.</p><p>Suppose a TCN has L convolutional layers and D = {d 1 , d 2 , ..., d N } is the possible dilation-rates/receptive-fields in each layer. The combination of receptive fields is represented with</p><formula xml:id="formula_0">C = {c 1 , ..., c l , ..., c L }, where l ? [1, L]</formula><p>is the index of layers with dilated convolutions, and c l ? D is the receptive field of each layer. There are |D| L possible combinations of receptive fields, i.e., the possible receptive field combinations in MS-TCN <ref type="bibr" target="#b11">[12]</ref> is 1024 40 when dilation rates range from 1 to 1024. Directly searching for effective combinations in such a large search space is impractical. We thus decompose the searching process into the global and local search to find the combination in a coarse-to-fine manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Global Search</head><p>The global search aims to find possible coarse receptive field combinations in the huge search space that are very different from human-designed structures, focusing more on discovering new structures with large diversity to human-designings instead of performance. To guarantee the diversity of new structures, we utilize the random sparse sampling strategy and apply a generic algorithm with the random crossover and random mutation specifically designed for receptive fields.</p><p>Population initialization with gradually sparse sampling. The objective of the global search is to find the coarse receptive field combinations at an affordable cost. Therefore, we reduce the search space by sparsely sampling the dilation rates within layers. Multiple sparse discrete sampling strategies such as uniform sampling, gradually sparse sampling, and gradually dense sampling can be applied to sparse the search space. Because small receptive fields benefit the extraction of precise local details, and large receptive fields contribute to coarse long-term dependencies. A gradually sparse sampling scheme from small to large dilation rates is appropriate for common tasks, e.g., temporal action segmentation. Therefore, we formulate the receptive field space in global search as:</p><formula xml:id="formula_1">D g = {d i = k i , i ? [0, 1, ? ? ? T ]},<label>(1)</label></formula><p>where k is the controller of the search space sparsity, and T determines the largest receptive field. With the same maximum receptive field, |D g | |D|, the search space is greatly reduced. i.e., when set k = 2, and set the maximum receptive field to 1024 as in MS-TCN, the search space is reduced from 1024 40 to 11 <ref type="bibr" target="#b39">40</ref> . The population of receptive field combinations can be described as a group of candidate structures P = {C i , i ? [1, M ]}, where C i is the candidate structure in the global search space, and M is the number of individuals in the population.</p><p>However, the reduced space of receptive field combinations can still be huge, and unaffordable for a brute force search. We propose a genetic algorithm <ref type="bibr" target="#b101">[102]</ref> based method to find coarse combinations that are competitive or even better than human s Crossover Mutation Selection</p><formula xml:id="formula_2">E(C i ) Fig. 2.</formula><p>Illustration of one iteration in our genetic-based global search algorithm. Step1. Gradually sparse random sampling initial receptive field combinations; Step2. Crossover between segments of the receptive field combinations; Step3. Randomly mutating the receptive fields to generate new individuals; Step4. Selecting individuals for the next iteration based on estimated performance of models trained with early stopping strategy.</p><p>designing. We now detail the selection, crossover, and mutation process within our proposed global search method.</p><p>Selection according to early stopped training. We need to select samples from the population of receptive field combinations P for each iteration. The selection operation selects individuals to be kept in P based on the estimated performance of each structure C i , denoted by E(C i ):</p><formula xml:id="formula_3">E(C i ) = f (V |C i , ? n ),<label>(2)</label></formula><p>where f (?) is the task-specific evaluation metrics on the validation set V , e.g., frame-wise accuracy for temporal action segmentation, ? n is a model trained with n epochs. The major cost of the global search is the performance evaluation of candidate structures. The global search aims to find coarse structures and reasonable performance, allowing searched structures to have sub-optimal performance. Also, we observe the receptive field combinations play a key role in the model convergence, i.e., a model with good receptive fields converges much faster than a model equipped with bad receptive fields. To reduce evaluation cost, we choose to early stop the training of candidate structures when the trained models can roughly show the relative performance gap of different structures, e.g., training MS-TCN for 5 epochs can reflect the structure performance. The early stop training strategy substantially reduces the structure evaluation cost.</p><p>Crossover between segments of the receptive field combination. This operation generates new samples of receptive field combinations. Every two combinations in the population are exchanged to bear new patterns of the combination while maintaining the local structures. Each C i will be selected for the crossover operation with probability p(C i ):</p><formula xml:id="formula_4">p(C i ) = E(C i ) M i E(C i ) .<label>(3)</label></formula><p>Since the representation ability lies in the combination patterns, we want to reserve the local combination patterns during the crossover. Instead of randomly exchanging individual points, we choose to exchange random segments of the receptive field combination. Specifically, we randomly choose two anchors and exchange receptive field combination segments within two anchors to generate new samples.</p><p>Random receptive field mutation. The mutation operation avoids getting stuck in local optimal results by choosing an individual with pre-defined probability p m ? [0, 1] and randomly changes each value within the selected combination with pre-defined probability p s ? [0, 1]. To reduce the searching cost, we also apply the gradually sparse sampling strategy when choosing a new receptive field value. Crossover between random segments of every two selected receptive field combinations;</p><p>Randomly choose combinations with probability p m , and mutate the receptive fields with probability p s within sparse sampling search space to generate new individuals;</p><p>Training each individual with early stopped n epochs to save evaluation cost;</p><p>Selecting the top M individuals based on estimated performance in Equ. (2) as the new population P ; end for return P .</p><p>The global search process can be summarised as Algorithm (1), and the illustration of one iteration in global search is given in <ref type="figure">Fig. 2</ref>. With the coarse search space and the global search method, we can find receptive field combinations with different patterns than human-designed structures while having similar or even better performance. We further propose the local search to locally find the more efficient combinations on top of the globalsearched structures. We show in Tab. 5(b) that local search heavily relies on the initial structure, revealing the importance of global search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Expectation-Guided Iterative Local Search</head><p>The local search aims to find more efficient receptive field combinations at a fine-grained level at a low cost. A naive approach is to sample finer-grained dilation rates near the initial dilation rate searched by the global search and apply existing DARTS algorithms <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> to choose the proper one. However, even with the good initial structure provided by the global search, the available range of fine-grained dilation rates is still large. Existing search algorithms are designed to search sparse operators with several choices in each layer, thus can not handle dilation rates with hundreds of choices. While too sparsely sampling conflicts with our goal of searching for the finer-grained receptive fields. Also, DARTS methods search operators with different functionality <ref type="bibr" target="#b19">[20]</ref>, while the searching on receptive fields only contains one functional dimension. Different subsets in the dataset sometimes <ref type="figure">Fig. 3</ref>. The approximated probability mass function of dilation rates is determined by the multi-dilated convolutional layer with shared convolutional weights. di is the dilation rate, and ?i is the PMF in Equ. <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_5">d 0 d 1 d 2 d 3 d S ? 0 ? 1 ? 2 ? 3 ? S +</formula><p>prefer different searching options. Searching within a functional dimension enables us to determine dilation rates with the expectation of all subsets instead of choosing the option required by one majority subset. Therefore, we propose an expectation-guided iterative (EGI) local search scheme to determine the finer-level dilation rates on top of the global-searched structures. Suppose that the receptive field of a layer l is D l . For a dataset, once we get the probability mass distribution of dilation rates around D l , we can obtain the expected dilation rate with the weighted average of the dilation rates required by all subsets. However, the probability mass of dilation rates for the dataset is inaccessible. Therefore, we utilize a convolutional weightsharing scheme to enforce the learned importance coefficients of dilation rates to approximate the probability mass. To get the approximated probability mass function of dilation rates, we first evenly sample S dilation rates near the initial dilation rate D l within the range of [D l ? ?D l ]. The set of available dilation rates within this layer is</p><formula xml:id="formula_6">T l = {d i |i ? [1, S]}, where d i = D l ??D l +(i?1)?2?D l /(S?1)</formula><p>. ?D l is the finer controller of the search space that results in a more dense sampling than that of the global search. With the dilation rates set T l , we propose a multi-dilated layer composed of a shared convolutional weight and multiple branches with different dilation rates, as shown in <ref type="figure">Fig. 3</ref>. Each branch has a unique coefficient to determine the importance of the dilation rate. During the searching process, the coefficients are updated with the gradient backpropagation to reflect the receptive field requirements of the dataset. Existing DARTS schemes <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b109">[110]</ref> have separated operator weights in each branch. In contrast, our convolutional weight-sharing strategy forces the model to learn the approximated probability of receptive fields and ease the model convergence. Specifically, the dilation rates in the multi-dilated convolutional layer are set to T l . Apart from the shared convolutional ?, the multi-dilated layer contains coefficient</p><formula xml:id="formula_7">W = {w 1 , w 2 , ..., w i , i ? [1,</formula><p>S]} to determine the importance of the dilation rates. Both ? and W are learnable parameters and can be trained with gradient backpropagation. For each iteration, each value in W is reinitialized with the same initial value.</p><p>W is unbounded, thus cannot be directly used to determine the dilation rates probability. Therefore, we propose a normalization function to get the approximated probability mass function P M F (d i ) of dilation rates through normalizing w i :</p><formula xml:id="formula_8">P M F (d i ) = ? i = |w i | S i |w i | .<label>(4)</label></formula><p>With the probability mass function, given the input feature x, the output y of the multi-dilated convolutional layer can be written as follows:</p><formula xml:id="formula_9">y = S i ? i ?(x, d i , ?),<label>(5)</label></formula><p>where ?(x, d i , ?) is the convolutional operation with the shared convolutional weight ? and dilation rate d i . ? i is updated with gradient optimization. Once we get the probability mass function, the newly searched dilation rate D l is obtained with the expectation:</p><formula xml:id="formula_10">D l = di?T l P M F (d i ) ? d i .<label>(6)</label></formula><p>To reduce the computational cost during the local search process, we reduce the number of dilation rates in T l to 3 by default and apply the iterative search scheme to find the more suitable dilation rate based on the D l from the last iteration. The local search process can be summarised as Algorithm <ref type="bibr" target="#b1">(2)</ref>. Furthermore, <ref type="figure" target="#fig_3">Fig. 4</ref> visualizes the dilation rates changes during the local searching process.</p><p>Parallel receptive fields for multi-scale enhancement. The local search results in one dilation rate for each convolution. However, we observe that some spatial tasks, e.g., instance segmentation and object detection, require the parallel multi-scale ability to process small and large objects in the senses. Our expectation-guided local search scheme can provide the parallel multi-scale ability with different dilation rates and shared convolutional weights. Therefore, we extend the local-searched structure to the parallel TABLE 1 Performance of the global and local searching stages of our global-to-local searching method using MS-TCN <ref type="bibr" target="#b11">[12]</ref> as the baseline. The global search finds new receptive field combinations that are better than baseline. Local search further refines the global searched structures to achieve better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">RF-Next: Next Generation Receptive Field Models</head><p>Our global-to-local receptive field searching scheme is suitable for various models that utilize convolutions. Given an initial network structure, we apply the searching scheme to convolutions with kernel sizes larger than one. For easy implementation, we utilize dilation rates to represent the receptive field. The global search aims to find receptive field combinations beyond human knowledge, which is optional as many models have been manually tuned. The local search finds suitable fine-grained receptive fields with a small extra cost, and thus it can be easily applied to humandesigned models of various tasks. Enhanced by our receptive field search, these Next generation Receptive Field models, namely RF-Next models, show advantages on many tasks, e.g., object detection, instance segmentation, semantic segmentation, speech synthesis, and sequence modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS ON TEMPORAL ACTION SEG-MENTATION</head><p>Temporal action segmentation requires a relatively large range of receptive fields, which is suitable for verifying the effectiveness of our proposed global-to-local search. Therefore, we do our major experiments on the temporal action segmentation task. This section introduces the implementation details of our proposed globalto-local search scheme and shows the superiority of searched receptive field combinations over the human-designed patterns on the temporal action segmentation task. We also give an analysis of the search scheme and the property of searched structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Structure Searching and Training. Our proposed method is implemented with the PyTorch <ref type="bibr" target="#b113">[114]</ref>, and Jittor <ref type="bibr" target="#b114">[115]</ref> frameworks. Following existing works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, features are first extracted from videos using the I3D network <ref type="bibr" target="#b47">[48]</ref> and then passed to temporal action segmentation models to get the temporal segmentation. Since our proposed global-to-local search scheme is modelagnostic, the training settings for model evaluation, i.e., training epochs, optimizer, learning rate, batch size, keep the same with the cooperation methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>. In the global search stage, we set the total iterations N = 100, k = 2 in Equ. <ref type="formula" target="#formula_1">(1)</ref>, the initialized population size M = 50, and mutation probability p m =p s =0.2.</p><p>The T in Equ. (1) is set to 10, indicating the maximum dilation rate of the global search space is 1024. We observe that 5 epochs of training can reflect the structure performance, and therefore models are trained with 5 epochs for evaluation. In the EGI local search stage, ?D l and S are set to be 0.1D l and 3, respectively. We train the model for 30 epochs during local search, and each iteration contains 3 epochs.</p><p>Datasets. Following <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, we evaluate our proposed method on three popular temporal action segmentation datasets: Breakfast <ref type="bibr" target="#b112">[113]</ref>, 50Salads <ref type="bibr" target="#b111">[112]</ref>, and GTEA <ref type="bibr" target="#b51">[52]</ref>. The details of the three datasets are summarized in Tab. 2. As far as we know, the Breakfast dataset is the largest public dataset for the temporal action segmentation task, which has a larger number of categories and samples compared with the other two datasets. So we perform our ablations mainly on the Breakfast dataset if not otherwise stated. Following common settings <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, we perform 4-fold cross-validation for the Breakfast and GTEA datasets and 5-fold cross-validation for the 50Salads dataset.</p><p>Evaluation Metrics. We follow previous works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> to use the frame-wise accuracy (Acc), segmental edit score (Edit) <ref type="bibr" target="#b10">[11]</ref>, and segmental F1 score <ref type="bibr" target="#b115">[116]</ref> at the temporal intersection over union with thresholds 0.1, 0.25, 0.5 (F@0.1, F@0.25, F@0.5) as our evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Evaluation</head><p>Global2Local Search. Our proposed global-to-local search aims to find new combinations of receptive fields better than human designings. We mainly take MS-TCN <ref type="bibr" target="#b11">[12]</ref> as our baseline architecture to perform the global-to-local search. When testing the MS-TCN on the Breakfast dataset, we train all models with batch size 8 to save training time. The reproduced results shown in Tab. 1 indicate that a large batch size achieves much better  <ref type="table" target="#tab_0">3 1080 11 153 531 72 1245 20 42 432  6 11 37 99 527 29 14 1243 197 9  1 54 15 5 1 13 20 10 136 6  1 24 6 37 182 9 1053 85 241 23   4 5 3 26 449 9 274 64 3 25  12 462 23 1 1 3 1 11 2 34  23 2 129 2 13 220 2 102 7 144  2 3 29 4 2 2 482 73 34 18</ref> Baseline 50Salads BreakFast GTEA <ref type="figure">Fig. 5</ref>. Visualization of the global-to-local searched structures of three datasets with the MS-TCN baseline. Each row represents the dilations of one structure, which contains four stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 3</head><p>Cooperation with existing temporal action segmentation methods. We perform the whole search pipeline based on MS-TCN <ref type="bibr" target="#b11">[12]</ref>. Because of the limited computing resources, we only perform the EGI local search on MS-TCN++ <ref type="bibr" target="#b6">[7]</ref> and BCN <ref type="bibr" target="#b12">[13]</ref>, denoted by ?. SSTDA <ref type="bibr" target="#b7">[8]</ref> uses MS-TCN <ref type="bibr" target="#b11">[12]</ref> as a backbone, so we directly add our searched structure to SSTDA, denoted by ?. performance. Tab. 1 shows that global-to-local searched structures achieve considerable performance improvements than humandesigned baselines, i.e., the searched structure surpasses the reproduced baseline with 5.8% in terms of F@0.1. The global-to-local search focuses on receptive field combinations, thus cooperating with existing SOTA temporal action segmentation methods to improve their performance. As shown in Tab. 3, on the largescale BreakFast dataset, global-to-local search consistently boosts the performance of MS-TCN++ <ref type="bibr" target="#b6">[7]</ref>, BCN <ref type="bibr" target="#b12">[13]</ref>, and SSTDA <ref type="bibr" target="#b7">[8]</ref>.</p><p>Also, we give comparisons on two small-scale datasets, 50Salads and GTEA datasets in Tab. 4, proving the effectiveness of our proposed global-to-local search.</p><p>Global Search. Global search reduces the computational cost with the sparse search space and our proposed genetic-based searching scheme. <ref type="figure">Fig. 6</ref> shows the performance change of models during  with better performance. We also compare the EGI local search with some variants of DARTS, i.e., the early stop scheme <ref type="bibr" target="#b118">[119]</ref> and Fair DARTS <ref type="bibr" target="#b119">[120]</ref>, and our method outperform them with a clear margin. The early stop scheme improves the performance of DARTS that searched with more epochs, while Fair DARTS <ref type="bibr" target="#b119">[120]</ref> has no advantage on the receptive field search task. The early stop scheme <ref type="bibr" target="#b118">[119]</ref> stops the searching before the overfitting of DARTS. Fair DARTS <ref type="bibr" target="#b104">[105]</ref> solves the problem of performance collapse caused by the unfair advantage in exclusive competition between different operator paths, while the dilation search has no such problem because each path has the same operation. As shown in Tab. 5(c), EGI local search is insensitive to the number of sampling dilation rates S, as it searches dilation rates with the expectation. Tab. 5(b) shows that the EGI local search can boost the performance of randomly generated, humandesigned, and global-searched structures. Still, the performance of the local-searched structures is related to the initial structures, as local search focuses on searching for receptive fields within  a finer local search space. We visualize the searching process of the iterative local search in <ref type="figure" target="#fig_3">Fig. 4</ref>. The dilation rates for each layer gradually converge to a suitable state during the iterative searching process. Tab. 5(d) verifies different ways to get the approximated probability mass function P M F (d i ) from coefficient w. Equ. (4) is superior to the sigmoid and softmax functions because it maintains the probability distribution while the other two functions change the distribution non-linearly.</p><p>Searching Cost. We report the cost of our proposed global-tolocal search method. When cooperating with MS-TCN, the size of the receptive field combination search space is 1024 40 . The cost of searching on such a huge space is unaffordable when using existing search methods. Our proposed global-to-local search decomposes the searching process into the global and local search to find the combination in a coarse-to-fine manner. Since the main bottleneck of the search method is the GPU resources, we report the GPU hours of the proposed global-to-local search in Tab. 6. The global search requires more computational cost to find multiple new well-performed structures with different patterns than human-designed structures. The local search needs a small training cost to fine-tune the global-searched/human-designed structures in the dense but local search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Observations</head><p>In this section, we try to exploit the common knowledge contained in the global-to-local searched structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connections between Receptive Fields and Data.</head><p>We want to know if receptive field combinations vary among data. Therefore, we evaluate the generalization ability of the searched structures on the subsets of the same dataset and different datasets, respectively. Within the BreakFast dataset, we perform the global-to-local  search on one fold and then evaluate the searched structures on other folds. Tab. 9 shows almost no obvious performance gap on different folds, indicating that receptive field combinations almost have no difference within a dataset. However, when searching and evaluating structures across different datasets, different structures searched on different datasets have a large performance gap, as shown in Tab. 7. We can conclude that different data distributions will result in different receptive field combinations. We visualize the structures searched from different datasets in <ref type="figure">Fig. 5</ref>. The searched structures are based on both global and local search. Since the global search introduces the randomness of each structure, we cannot fairly compare these structures for different datasets. Still, we give a rough explanation based on the searched receptive field of each structure. The structures searched on the Breakfast and 50Salads datasets tend to have larger receptive fields, while the structure searched on the GTEA dataset has smaller receptive fields. The number of video frames shown in <ref type="table" target="#tab_0">Table 2</ref> positively correlates with receptive fields. We assume that the average video length of a dataset might influence the receptive fields of the structure. A video with more frames normally requires larger receptive fields to capture longterm relations. The structures searched on Breakfast and 50Salads datasets have similar average receptive fields, but the average video length of 50Salads is longer than Breakfast. We assume that understanding the content of cooking breakfast in the Breakfast dataset requires more long-range information than the preparing salads content in the 50Salads dataset. Analyzing different video contents might need features from different ranges of receptive fields. Our work mainly focuses on searching receptive fields, but fully explaining why the receptive field combination of a certain task on a certain dataset looks like the searched one is still an open question.</p><p>Receptive Fields for Different Stages. Our global-to-local search is based on MS-TCN. MS-TCN contains four stages, and all stages share the same receptive field combination in human design. The visualized searched structures shown in <ref type="figure">Fig. 5</ref> demonstrate that different stages have different receptive field combinations, which conflicts with human design. We further count the average receptive fields of each stage among all individuals. The range of performance and the average dilation rates of each stage are shown in <ref type="figure" target="#fig_4">Fig. 7</ref>. The average dilation rate in the first stage of MS-TCN tends to be large on high-performance structures. In contrast, the average dilation rate in the third stage of MS-TCN is relatively small on high-performance structures. We assume that the first stage of MS-TCN requires large receptive fields to get the longterm context for coarse prediction, while the following stages need small receptive fields to refine the results locally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RF-NEXT FOR MULTIPLE NETWORKS AND TASKS</head><p>This section shows that RF-Next models can be applied to multiple networks and tasks. We use prefixes RF to denote RF-Next models, and P means the parallel receptive field version.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Spatial tasks</head><p>We apply our proposed searching scheme to find proper receptive fields for spatial tasks, e.g., object detection, instance segmentation, and semantic segmentation. We observe that proper receptive fields significantly improve the performance of these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Object Detection</head><p>Object detection aims to assign bounding boxes and categories to objects of various sizes. We utilize the widely used Faster-RCNN <ref type="bibr" target="#b1">[2]</ref> method with a dilation rate of 1 for all convolutions. Faster-RCNN applies the feature pyramid network to aggregate features with multiple scales to handle objects of various sizes. However, the receptive fields for convolutions are ignored. Therefore, we search for dilation rates of convolutions with kernel sizes larger than one in the Faster-RCNN. Due to the extremely large training cost and the small initial dilation rates, we only use the highly efficient local search scheme.</p><p>Training and Searching. We employ the ResNet-50 <ref type="bibr" target="#b14">[15]</ref> and ResNet-101 <ref type="bibr" target="#b14">[15]</ref> that are pre-trained on the ImageNet <ref type="bibr" target="#b121">[122]</ref> dataset as backbones. We verify the effectiveness of local search on the COCO dataset <ref type="bibr" target="#b120">[121]</ref> and report the mean average precision (mAP) to evaluate the trained model. Following the official training scheme <ref type="bibr" target="#b1">[2]</ref>, the images are resized to 1333 ? 800 with a randomly horizontal flip, and the model is trained for 12 epochs, with a batch size of 16 on 4 GPUs. During the local search, we train the model for 12 epochs and update the structure in each epoch for the first 10 epochs. The ?D l is set to be 0.5D l . Since the weights of the first stage of ResNet in Faster-RCNN are frozen during training, we skip this stage during searching.</p><p>Performance and Observation. As shown in Tab. 8, RF-Next model improves the test mAP of Faster-RCNN with ResNet-50 by 1.4%. For the RF-ResNet-101 model, the test mAP is also improved by 1.5%. Theoretically, a network with a larger depth has a larger range of receptive fields. Still, effective receptive field settings have a similar performance gain on both shallow and deep models. We visualize the searched dilation rates of RF-ResNet-50/101 based Faster-RCNN as shown in <ref type="figure">Fig. 8</ref>. The shallow layers require relatively small dilation rates, while some deep layers have large dilation rates. Interestingly, the ResNet-101 based model requires larger dilation rates in stage 4 of the network than ResNet-50 based model. When utilizing the RF-Next with parallel receptive fields, the performance gain of ResNet-50 and ResNet-101 based models are 2.6% and 2.4% in test mAP, indicating that object detection task needs parallel multi-scale ability. The visualization and probability of each receptive field in the parallel RF-Next are shown in <ref type="figure" target="#fig_5">Fig. 9</ref>. By default, we utilize the number of sampling dilation rates S = 3 in Equ. <ref type="bibr" target="#b3">(4)</ref>. And we also explore using S = 2 for local search, as shown in Tab. 8. Using two/three branches achieve similar performance for parallel RF-Next, showing that using two branches for each layer provides sufficient multi-scale ability. The result is also consistent with the observation in Tab. 5(c) that the proposed expectation-guided search is insensitive to the number of sampling dilation rates.</p><p>We analyze the performance gain for objects of different sizes in Tab. 8. For ResNet-50 and ResNet-101 based models, the test mAP improvement for small, medium, and large objects are (1.5%, 2.2%, 4.1%) and (1.8%, 2.2%, 3.4%), respectively. The performance gain gradually increases with the increase of object sizes, showing that the default receptive field settings of Faster-RCNN are not large enough to capture large objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Instance Segmentation</head><p>Instance segmentation outputs the instances segmentation masks and categories, which is similar to the object detection task. To compare the receptive field requirements of object detection and instance segmentation, we use the widely used Mask-RCNN <ref type="bibr" target="#b3">[4]</ref> that extends the Faster-RCNN with a mask segmentation branch. Like the object detection, we apply the local search on convolutions whose kernel is larger than one.</p><p>Training and Searching. The ResNet-50 <ref type="bibr" target="#b14">[15]</ref> and ResNet-101 <ref type="bibr" target="#b14">[15]</ref> with ImageNet <ref type="bibr" target="#b121">[122]</ref> pre-training are used as backbones. We use the COCO dataset <ref type="bibr" target="#b120">[121]</ref> for object detection and report the mean average precision of bounding box (mAP bb ) and instance segmentation (mAP mk ) to evaluate the trained model. To fairly compare the searched structure on instance segmentation and object detection, the training scheme of Mask-RCNN is aligned with Faster-RCNN in both searching and re-training stages.</p><p>Performance and Observation. We give the performance comparison of searched structure and baseline in Tab. 10. Using the RF-Next with a single branch brings 1.3%/1.3% gain on test mAP mk and 1.5%/1.6% gain on test mAP bb of ResNet-50/101 based models. And the performance gains are further enlarged by the parallel RF-Next, i.e., 2.2%/2.0% gain on test mAP mk and 2.5%/2.4% gain on test mAP bb . We give the visualized dilation rates comparison between Faster-RCNN and Mask-RCNN as shown in <ref type="figure">Fig. 8</ref>. The dilation rates of these two tasks are very similar because of the large similarity between them. As shown in <ref type="figure" target="#fig_5">Fig. 9</ref>, the probability of dilation rates in the mask segmentation head of Mask-RCNN shows that the middle two layers require diverse receptive fields while the first and last layer requires a small receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Semantic Segmentation</head><p>Semantic segmentation task requires assigning each pixel of images with category labels. Receptive fields are vital for the dense pixel-level prediction of semantic segmentation. Deeplab series <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b31">[32]</ref> utilize convolution with dilation rates larger than one to enlarge the receptive fields, which becomes the default option for semantic segmentation networks <ref type="bibr" target="#b122">[123]</ref>, <ref type="bibr" target="#b123">[124]</ref>. However, possible better receptive fields than the human-designed ones have not been explored. We apply the Deeplab V3 <ref type="bibr" target="#b31">[32]</ref> network as the baseline method and conduct local search to find more effective receptive fields.</p><p>Training and Searching. We conduct experiments on both the PASCAL VOC <ref type="bibr" target="#b124">[125]</ref> dataset and the ADE20k dataset <ref type="bibr" target="#b125">[126]</ref> using the ImageNet pretrained ResNet-50 <ref type="bibr" target="#b14">[15]</ref> based Deeplab V3 as the baseline method. Following official training settings <ref type="bibr" target="#b31">[32]</ref>, models are trained for 20k and 80k iterations for PASCAL VOC and ADE20k, respectively, with a batch size of 16 on 4 GPUs. For both datasets, the images are randomly scaled at a ratio between 0.5 and 2.0 and randomly cropped to 512 ? 512. An auxiliary loss is applied at the output of the third stage of the ResNet backbone to ease convergence. To avoid the influence of auxiliary loss, we apply the local search to stage 4 and the decoder of the network. During the local search, we update the structure every 2k and 8k iterations for PASCAL VOC and ADE20k, respectively, and conduct 8 iterations of the local search in total. The ?D l is set to be 0.15D l for both datasets.</p><p>Performance and Observation. We utilize the mean intersection over union (mIoU) and the mean accuracy (mAcc) to evaluate the trained models. As shown in Tab. 11, the RF-Next brings the 1.6% and 0.8% gain in mIoU for PASCAL VOC and ADE20k datasets, respectively. The searched multi-branch structures achieve similar performance to the single branch competitors. We assume the atrous spatial pyramid pooling structure in Deeplab V3 already enhances the parallel multi-scale ability of the network. The visualization of searched receptive fields in <ref type="figure" target="#fig_1">Fig. 10</ref> indicates that larger receptive fields are required in stage 4 of the network.</p><p>As described above, we skip the local search in the first three stages of the network to avoid the side effect of auxiliary loss. We show in Tab. 11 that searching all convolutions achieves 76.3% mIoU, similar to the human-designed baseline. We observe that searching receptive fields for all convolutions results in a much smaller auxiliary loss value than human-designed baseline (0.102 VS. 0.191) and searching after stage 3 (0.102 VS. 0.189). Also, compared to searching after stage 3, searching in the early stages achieves the performance gain of 15.4% in mIoU for output in stage 3 of the network. Since the local search relies on the gradient backpropagation to find receptive fields, adding auxiliary loss makes the local search find better receptive fields for output in stage 3 instead of the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sequential tasks</head><p>Except for the temporal action segmentation task, we also conduct receptive field search on other sequential tasks, e.g., speech <ref type="bibr">TABLE 11</ref> Performance of local search on semantic segmentation with PASCAL VOC <ref type="bibr" target="#b124">[125]</ref> and ADE20k datasets <ref type="bibr" target="#b125">[126]</ref>. Local-P means the local-searched structure with parallel receptive fields as described in Sec. 3.3. ?means apply local search to all convolutions of the network. Local-S3 indicates the output in stage 3 of the network, where an auxiliary loss is added to ease convergence <ref type="bibr" target="#b31">[32]</ref>.  synthesis, P-MNIST digit classification, and polyphonic music modeling. We verify the effectiveness of global-to-local search on the polyphonic music modeling and P-MNIST digit classification task. Due to the high training cost of speech synthesis, we apply the local search on top of the human-designed structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Speech Synthesis</head><p>As described in Sec. 2.2.2, we focus on the procedure of transferring acoustic features to speech waveform in speech synthesis. We use WaveGlow <ref type="bibr" target="#b126">[127]</ref> as the baseline method that combines the advantage of Glow <ref type="bibr" target="#b127">[128]</ref> and WaveNet <ref type="bibr" target="#b27">[28]</ref>. WaveGlow network has 12 coupling layers, where each contains 8 layers of dilated convolutions with human-designed gradually expanded dilation rates. To save computational cost, we utilize the local search to find more effective dilation rates of these layers based on humandesigned structures.</p><p>Training and Searching. We conduct experiments on the widely used LJ speech <ref type="bibr" target="#b128">[129]</ref> dataset, including 13,100 audio clips with a total length of about 24 hours. Each sample is randomly cropped to 16,000 for training, and the sampling rate is 22,050Hz. The mel-spectrograms <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b129">[130]</ref>, generated through a short-time Fourier transform, are fed to the network for speech synthesis. The network is optimized by Adam optimizer with a learning rate of 1e-4 for 100 epochs. We train the model with 4 GPUs using the batch sizes of 20 and 48 during searching and training, respectively. During the local searching, the ?D l and S are set to be 0.6D l and 3, respectively. We train the model for 60 epochs during the local search and update the structure every 3 epochs.</p><p>Performance and Observation. We use three metrics, i.e., melcepstral distortion (MCD) <ref type="bibr" target="#b130">[131]</ref>, perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b131">[132]</ref> and log-likelihood ratio (LLR) <ref type="bibr" target="#b132">[133]</ref>, to evaluate the speech synthesis quality, as shown in Tab. 12. MCD measures the difference between two sequences of mel-cepstra of (a) (b) (c) <ref type="figure" target="#fig_1">Fig. 11</ref>. Visualization of the baseline structure (a), the global searched structure (b) and the global-to-local searched structure (c) of TCN on polyphonic music modeling task. speech, and a small MCD indicates the synthesized and natural speeches are close. Similarly, LLR measures the difference between two speeches. The structure with searched receptive field combinations achieves better performance than human-designed receptive fields in MCD and PESQ. PESQ assesses the voice quality, and a higher value between the synthesized and natural speech means better the synthesized speech quality. We calculated the PESQ under the narrowband of 8,000Hz. The speech synthesis results of the local-searched structure also have a better PESQ score, indicating that more proper receptive fields benefit the speech synthesis quality. We visualize the local searched and baseline receptive fields (dilation rates) of WaveGlow <ref type="bibr" target="#b126">[127]</ref> in <ref type="figure" target="#fig_1">Fig. 13</ref>. We observe that the maximum dilation rate of the human-designed structure is much larger than the searched structure, indicating that too large receptive fields may not be necessary for this task. Unlike the human-designed structure with the same receptive field combination for each coupling layer, the searched structure has small receptive fields on the shallow layers and larger receptive fields on the deeper layers. We assume that the speech synthesis task requires local features in the shallow layers, and the deeper layers are responsible for modeling the long-term dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Sequence Modeling using TCN</head><p>Bai et al. <ref type="bibr" target="#b133">[134]</ref> have verified the performance of TCN on several sequence modeling tasks. We further show the effectiveness of RF-Next models on two sequential tasks, i.e., polyphonic music modeling and P-MNIST digit classification. P-MNIST Classification. P-MNIST classification aims to classify the handwritten digit images in which the order of the pixels is disrupted. P-MNIST dataset <ref type="bibr" target="#b134">[135]</ref>, <ref type="bibr" target="#b135">[136]</ref> randomly permutes images in the MNIST dataset <ref type="bibr" target="#b136">[137]</ref> to 784 length sequences, which is widely used for long-term relation modeling <ref type="bibr" target="#b133">[134]</ref>, <ref type="bibr" target="#b137">[138]</ref>- <ref type="bibr" target="#b140">[141]</ref>. A TCN with 8 layers is used for P-MNIST classification <ref type="bibr" target="#b133">[134]</ref>, where each layer has a convolution with a kernel size of 7 and a channel number of 25. We apply the global-to-local search on the TCN to find more effective receptive fields. Following the settings in <ref type="bibr" target="#b133">[134]</ref>, the final model is trained for 20 epochs with an Adam optimizer. The initial learning rate is 2e-3 and is multiplied by 0.1 at the 10-th epoch. In P-MNIST, the order of the pixels is randomly permuted, and we fix the permutation order for all experiments. During the global search, we set the Base Ours <ref type="figure" target="#fig_1">Fig. 13</ref>. Visualization of the local searched dilation rates of WaveGlow <ref type="bibr" target="#b126">[127]</ref> for the speech synthesis task.  iterations N = 50 and the initial population size M = 25. The model is trained with 11 epochs for each sample. During the local search, the ?D l is set to 0.1D l , and the structure is trained for 15 epochs and updated every 3 epochs. The classification accuracy is used as the evaluation metric. As shown in Tab. 13, the global search improves the accuracy from 97.2% to 97.6%, and the local search further improves the performance to 97.8%. The visualized structure in <ref type="figure" target="#fig_1">Fig. 12</ref> shows the searched receptive fields are very different from the human-designed patterns.</p><p>Polyphonic Music Modeling. Polyphonic music modeling aims to predict the subsequent musical notes based on the history of the played notes. The polyphonic music modeling is conducted on the widely used Nottingham <ref type="bibr" target="#b141">[142]</ref>- <ref type="bibr" target="#b143">[144]</ref> dataset composed of 1200 British and American folk tunes. For polyphonic music modeling, we utilize a TCN with 4 layers, where each layer has two convolutions with a kernel size of 5 and a channel number of 150. Following <ref type="bibr" target="#b133">[134]</ref>, the model is trained for 100 epochs with an Adam optimizer. The initial learning rate is 1e-3, which is multiplied by 0.1 every 30 epochs. The dropout with the rate of 0.25 and the gradient clipping with the maximum norm of 0.2 is applied. We set the iterations N = 50 and the initial population size M = 25 for the global search, and each sample is trained with 30 epochs. For local search, the ?D l is set to 0.15D l , and the model is trained for 60 epochs and updated in every 10 epochs. We evaluate models using the negative-log-likelihood (NLL), as shown in Tab. 13. The NLL is improved from 2.97 to 2.73 by the global search, and the local search improves the performance to 2.69. The global and local-searched structures are shown in <ref type="figure" target="#fig_1">Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Receptive field search on Modern Networks</head><p>We apply the receptive field searching method on multiple networks, e.g., SOTA attention/convolution based networks, multiscale networks, and searched networks.  Receptive Field Search Improves SOTA Models. We show recent SOTA models, e.g., PVT <ref type="bibr" target="#b144">[145]</ref>, <ref type="bibr" target="#b145">[146]</ref> and Con-vNeXt <ref type="bibr" target="#b146">[147]</ref>, still benefit from our receptive field searching method. PVTv2 <ref type="bibr" target="#b144">[145]</ref>, <ref type="bibr" target="#b145">[146]</ref> is a self-attention based pyramid vision transformer, which uses both global self-attentions and depthwise convolutions. We apply the receptive field search to convolutions of PVTv2 for object detection, instance segmentation, and semantic segmentation tasks, achieving stable improvements over the strong PVTv2-B0 baseline. As shown in Tab. 14, the singlebranch RF-PVTv2 has the gain of 0.6% box mAP and 0.6% mask mAP for object detection and instance segmentation tasks. The parallel RF-PVTv2 further improves 0.3% box mAP and 0.3% mask mAP for these two tasks. Tab. 15 shows the searched singlebranch structure improves the baselines with 0.7% and 0.5% on Pascal VOC and ADE20K datasets. ConvNeXt <ref type="bibr" target="#b146">[147]</ref> is a SOTA convolutional model that outperforms many SOTA attention-based models. Even though ConvNeXt manually tunes the kernel size of convolutions to support a larger range of receptive fields, the receptive field search still further improves the performance on object detection and instance segmentation. As shown in Tab. 14,  <ref type="bibr" target="#b15">16</ref> Receptive field search improves hand-crafted multiple receptive fields models, i.e., Res2Net <ref type="bibr" target="#b15">[16]</ref> and HRNet <ref type="bibr" target="#b147">[148]</ref>, <ref type="bibr" target="#b148">[149]</ref>, for object detection and instance segmentation tasks on the COCO val dataset. The Cascade Mask RCNN method is used as the detector. the parallel version of searched structure has considerable gains of 0.5% box mAP and 0.6% mask mAP over the strong ConvNeXt-T model. The performance gains over these two strong SOTA models prove the effectiveness of our receptive field searching method.</p><p>Receptive Field Search Improves Multi-scale Models. We show the advantage of receptive field searching over popular handcrafted multi-scale models with multiple receptive fields, e.g., HR-Net <ref type="bibr" target="#b147">[148]</ref>, <ref type="bibr" target="#b148">[149]</ref> and Res2Net <ref type="bibr" target="#b15">[16]</ref>. HRNet parallel processes features of multiple resolutions to form the multi-scale representation. Res2Net constructs hierarchical residual-like connections within a block to enable fine-grained multiple receptive fields. Despite their good hand-crafted multi-scale ability, we show in Tab. 16 that searched receptive fields constantly improve their performance on object detection and instance segmentation tasks. For HRNet, the single/multiple-branch RF-HRNet improves 1.3%/2.1% box mAP on object detection and 1.2%/1.7% mask mAP on instance segmentation. The single/multiple-branch RF-Res2Net also improves the Res2Net with 0.6%/1.6% box mAP on object detection and 0.7%/1.5% mask mAP on instance segmentation. Therefore, our receptive field searching method can further improve the handcrafted multi-scale models with better receptive field combinations.</p><p>Comparison with Attention Mechanisms. As discussed in the related work, attention mechanisms theoretically can form arbitrary receptive fields <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>. However, we are not aware of the actual receptive field representation ability of attention mechanisms. Therefore, we propose to compare our receptive field search method with the non-local module [40] on the instance segmentation task. Following the implementation in <ref type="bibr" target="#b39">[40]</ref>, we insert the non-local blocks to each residual block in stage 4 of the ResNet50 backbone. As shown in Tab. 10, on the COCO testing set, the non-local based Mask-RCNN improves the Mask-RCNN baseline with 1.1 mask mAP and 1.3 box mAP. The Mask-RCNN with searched parallel receptive fields outperforms non-local based Mask-RCNN by 1.1 mask mAP 1.2 box mAP, showing that the searched receptive fields provide better representation than non-  <ref type="bibr" target="#b22">[23]</ref> and Cityscapes datasets <ref type="bibr" target="#b149">[150]</ref>. Local-P means the local-searched structure with parallel receptive fields as described in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P mIoU mAcc</head><p>Auto-deeplab <ref type="bibr">[</ref> local modules. Therefore, though the non-local module improves the model performance, it cannot provide effective receptive fields as strong as our searched receptive fields. Then, we apply the local search to the non-local based network, and the hyper-parameters for searching are kept consistent with the standard ResNet. We observe a further performance gain with both searched singlebranch receptive fields and parallel-branch receptive fields. The single-branch version has the gain of 0.4 mask mAP and 0.6 box mAP, and the parallel-branch version has the gain of 1.3 mask mAP and 1.4 box mAP. The receptive field searching scheme can further improve the performance of the non-local based model, indicating that the non-local module may not be able to cover all effective receptive fields even with its dense connections among pixels.</p><p>Searching Receptive Fields over the Searched Network. Autodeeplab <ref type="bibr" target="#b22">[23]</ref> searches the feature resolution for different stages of the semantic segmentation network. To verify if it is possible to adjust the receptive fields of Auto-deeplab further, we conduct the local search on top of the Auto-deeplab. We follow the implementation of Auto-deeplab <ref type="bibr" target="#b22">[23]</ref> to search on the Cityscapes <ref type="bibr" target="#b149">[150]</ref> dataset. As shown in Tab. 17, both single/multiple-branch RF-Auto-deeplab brings performance gain over Auto-deeplab in mIoU. Therefore, our receptive field searching scheme can further benefit the semantic segmentation model with searched feature resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose a global-to-local search scheme to search for effective receptive field combinations at a coarse-to-fine scheme. The global search discovers effective receptive field combinations with better performance than hand designings but completely different patterns. The expectation-guided iterative local search scheme enables searching fine-grained receptive field combinations in the dense search space. RF-Next models, enhanced with receptive field search scheme, can be plugged into multiple tasks, e.g., action segmentation, sequence modeling <ref type="bibr" target="#b133">[134]</ref>, <ref type="bibr" target="#b150">[151]</ref>, segmentation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b151">[152]</ref>- <ref type="bibr" target="#b153">[154]</ref>, object detection <ref type="bibr" target="#b154">[155]</ref>, <ref type="bibr" target="#b155">[156]</ref> methods to boost the performance further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>S. Gao, Z.Y Li, Q, Han, and M.M. Cheng are with the TMCC, CS, Nankai University, Tianjin 300350, China. ? L Wang is the with National Laboratory of Pattern Recognition. ? M.M. Cheng (cmm@nankai.edu.cn) is the corresponding author. ? A preliminary version of this work has been presented in the CVPR 2021 [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Search space comparison between searching for network architecture and receptive field combinations. Left: Network architecture search mostly searches for several operations with different functions. Right: The search space of receptive field combinations is huge. The white, green, blue nodes, and orange shade represent the dilation rate candidates, the sparse search space in global search, one of the global searched results, and the local search space, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Global Search. Input: Iterations N , training epoch n, mutation probability p m , and population size M ; Gradually sparse random sampling initial receptive field combinations P ; for iter in [1, N ] do Selecting individuals for the crossover with the probability obtained with estimated performance in Equ. (3);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of receptive field combinations changes during the EGI local searching process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization of average dilation rates in each stage and the range of performance of global-searched structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Visualization and probability of each receptive field in the parallel searched structure of ResNet-50 based Faster-RCNN (a) and Mask-RCNN (b) when S=3 during searching. S2-S4 denotes stage 2 to stage 4 of the ResNet backbone. FPN, RPN, and MASK mean the feature pyramid network, region proposal network, and the mask segmentation head in Faster-RCNN and Mask-RCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Visualization of the local searched receptive fields of stage 4 and decoder in Deeplab V3 on the semantic segmentation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Visualization of the baseline structure (a), the global searched structure (b) and the global-to-local searched structure (c) of TCN on P-MNIST Classification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 2</head><label>2</label><figDesc>Expectation-Guided Iterative Local Search. Input: Iterations N , initial receptive fields D; Initialize model using given D; for iter in [1, N ] do Construct T l for each layer based on D and initialize W</figDesc><table><row><cell>with the same value;</cell></row><row><cell>Train model to get the P M F in Equ. (4);</cell></row><row><cell>Obtain new dilation rates through Equ. (6);</cell></row><row><cell>Update D;</cell></row><row><cell>end for</cell></row><row><cell>return local-searched D.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>BreakFast 50Salads GTEA F@0.1 F@0.25 F@0.5 Edit Acc F@0.1 F@0.25 F@0.5 Edit Acc F@0.1 F@0.25 F@0.5 Edit Acc MS-TCN<ref type="bibr" target="#b11">[12]</ref> Details of three temporal action segmentation datasets. #Cls and #Vid are the numbers of classes and videos, respectively. #Frame is the average frame of videos.</figDesc><table><row><cell></cell><cell>52.6</cell><cell>48.1</cell><cell>37.9</cell><cell cols="2">61.7 66.3</cell><cell>76.3</cell><cell>74.0</cell><cell>64.5</cell><cell>67.9 80.7</cell><cell>87.5</cell><cell>85.4</cell><cell>74.6</cell><cell>81.4 79.2</cell></row><row><cell>Reproduce</cell><cell>69.1</cell><cell>63.7</cell><cell>50.1</cell><cell cols="2">69.9 67.3</cell><cell>78.8</cell><cell>75.3</cell><cell>64.4</cell><cell>71.4 77.8</cell><cell>87.1</cell><cell>83.6</cell><cell>70.4</cell><cell>81.1 75.5</cell></row><row><cell>Global</cell><cell>72.2</cell><cell>66.0</cell><cell>51.5</cell><cell cols="2">71.0 69.2</cell><cell>79.3</cell><cell>76.5</cell><cell>68.1</cell><cell>71.9 81.2</cell><cell>89.1</cell><cell>87.1</cell><cell>74.4</cell><cell>84.2 78.6</cell></row><row><cell>Global+Local</cell><cell>74.9</cell><cell>69.0</cell><cell>55.2</cell><cell cols="2">73.3 70.7</cell><cell>80.3</cell><cell>78.0</cell><cell>69.8</cell><cell>73.4 82.2</cell><cell>89.9</cell><cell>87.3</cell><cell>75.8</cell><cell>84.6 78.5</cell></row><row><cell></cell><cell>#Cls</cell><cell>#Vid</cell><cell cols="2">#Frame</cell><cell></cell><cell>Scene</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GTEA [52]</cell><cell>11</cell><cell>28</cell><cell>1115</cell><cell></cell><cell cols="2">daily activities</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50Salads [112]</cell><cell>17</cell><cell>50</cell><cell cols="2">11552</cell><cell cols="2">preparing salads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BreakFast [113]</cell><cell>48</cell><cell>1712</cell><cell>2097</cell><cell></cell><cell cols="2">cooking breakfast</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>version by keeping the dilation rates in T l instead of merging them after the last iteration of searching. The parallel version only has |T l | extra parameters compared with the single branch version. The parallel structures have significant improvement over the single branch structures in instance segmentation and object detection tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Comparison with existing temporal action segmentation methods on the 50Salads and GTEA datasets.</figDesc><table><row><cell>50Salads</cell><cell>F@0.1</cell><cell>F@0.25</cell><cell>F@0.5</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>Spatial CNN [117]</cell><cell>32.3</cell><cell>27.1</cell><cell>18.9</cell><cell>24.8</cell><cell>54.9</cell></row><row><cell>Bi-LSTM [64]</cell><cell>62.6</cell><cell>58.3</cell><cell>47.0</cell><cell>55.6</cell><cell>55.7</cell></row><row><cell>Dilated TCN [11]</cell><cell>52.2</cell><cell>47.6</cell><cell>37.4</cell><cell>43.1</cell><cell>59.3</cell></row><row><cell>ST-CNN [117]</cell><cell>55.9</cell><cell>49.6</cell><cell>37.1</cell><cell>45.9</cell><cell>59.4</cell></row><row><cell>TUnet [118]</cell><cell>59.3</cell><cell>55.6</cell><cell>44.8</cell><cell>50.6</cell><cell>60.6</cell></row><row><cell>ED-TCN [11]</cell><cell>68.0</cell><cell>63.9</cell><cell>52.6</cell><cell>59.8</cell><cell>64.7</cell></row><row><cell>TResNet [15]</cell><cell>69.2</cell><cell>65.0</cell><cell>54.4</cell><cell>60.5</cell><cell>66.0</cell></row><row><cell>TricorNet [66]</cell><cell>70.1</cell><cell>67.2</cell><cell>56.6</cell><cell>62.8</cell><cell>67.5</cell></row><row><cell>TRN [68]</cell><cell>70.2</cell><cell>65.4</cell><cell>56.3</cell><cell>63.7</cell><cell>66.9</cell></row><row><cell>TDRN [68]</cell><cell>72.9</cell><cell>68.5</cell><cell>57.2</cell><cell>66.0</cell><cell>68.1</cell></row><row><cell>MS-TCN++ [7]</cell><cell>80.7</cell><cell>78.5</cell><cell>70.1</cell><cell>74.3</cell><cell>83.7</cell></row><row><cell>MS-TCN [12]</cell><cell>76.3</cell><cell>74.0</cell><cell>64.5</cell><cell>67.9</cell><cell>80.7</cell></row><row><cell>RF-MS-TCN</cell><cell>80.3</cell><cell>78.0</cell><cell>69.8</cell><cell>73.4</cell><cell>82.2</cell></row><row><cell>BCN [13]</cell><cell>82.3</cell><cell>81.3</cell><cell>74.0</cell><cell>74.3</cell><cell>84.4</cell></row><row><cell>RF-BCN</cell><cell>85.8</cell><cell>83.6</cell><cell>76.5</cell><cell>78.1</cell><cell>85.5</cell></row><row><cell>GTEA</cell><cell>F@0.1</cell><cell>F@0.25</cell><cell>F@0.5</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>Spatial CNN [117]</cell><cell>41.8</cell><cell>36.0</cell><cell>25.1</cell><cell>-</cell><cell>54.1</cell></row><row><cell>Bi-LSTM [64]</cell><cell>66.5</cell><cell>59.0</cell><cell>43.6</cell><cell>-</cell><cell>55.5</cell></row><row><cell>Dilated TCN [11]</cell><cell>58.8</cell><cell>52.2</cell><cell>42.2</cell><cell>-</cell><cell>58.3</cell></row><row><cell>ST-CNN [117]</cell><cell>58.7</cell><cell>54.4</cell><cell>41.9</cell><cell>-</cell><cell>60.6</cell></row><row><cell>TUnet [118]</cell><cell>67.1</cell><cell>63.7</cell><cell>51.9</cell><cell>60.3</cell><cell>59.9</cell></row><row><cell>ED-TCN [11]</cell><cell>72.2</cell><cell>69.3</cell><cell>56.0</cell><cell>-</cell><cell>64.0</cell></row><row><cell>TResNet [15]</cell><cell>74.1</cell><cell>69.9</cell><cell>57.6</cell><cell>64.4</cell><cell>65.8</cell></row><row><cell>TricorNet [66]</cell><cell>76.0</cell><cell>71.1</cell><cell>59.2</cell><cell>-</cell><cell>64.8</cell></row><row><cell>TRN [68]</cell><cell>77.4</cell><cell>71.3</cell><cell>59.1</cell><cell>72.2</cell><cell>67.8</cell></row><row><cell>TDRN [68]</cell><cell>79.2</cell><cell>74.4</cell><cell>62.7</cell><cell>74.1</cell><cell>70.1</cell></row><row><cell>MS-TCN++ [7]</cell><cell>88.7</cell><cell>87.4</cell><cell>73.5</cell><cell>83.0</cell><cell>78.2</cell></row><row><cell>MS-TCN [12]</cell><cell>87.5</cell><cell>85.4</cell><cell>74.6</cell><cell>81.4</cell><cell>79.2</cell></row><row><cell>Reproduce</cell><cell>87.1</cell><cell>83.6</cell><cell>70.4</cell><cell>81.1</cell><cell>75.5</cell></row><row><cell>RF-MS-TCN</cell><cell>89.9</cell><cell>87.3</cell><cell>75.8</cell><cell>84.6</cell><cell>78.5</cell></row><row><cell>BCN [13]</cell><cell>88.5</cell><cell>87.1</cell><cell>77.3</cell><cell>84.4</cell><cell>79.8</cell></row><row><cell>RF-BCN</cell><cell>92.1</cell><cell>90.2</cell><cell>79.2</cell><cell>87.2</cell><cell>80.6</cell></row><row><cell cols="6">the global searching process. Compared with the random search,</cell></row><row><cell cols="6">the genetic-based global search convergences faster. The standard</cell></row><row><cell cols="6">division of model performance searched by genetic-based search</cell></row><row><cell cols="6">is smaller than the random search, showing the stability of our</cell></row><row><cell cols="6">proposed search scheme. The visualized well-performed global-</cell></row><row><cell cols="6">searched structures in Fig. 5 prove that the global search discovers</cell></row><row><cell cols="6">various structures completely different from human-designed pat-</cell></row><row><cell cols="6">terns. Tab. 5(b) also shows that the local search heavily relies on</cell></row><row><cell cols="5">global-searched structures to achieve better performance.</cell><cell></cell></row></table><note>Local Search. Based on the global-searched structures, our pro- posed EGI local search aims to fine-tune the receptive field in a finer search space. We compare the DARTS [20] method and EGI local search based on the global searched structures, as shown in Tab. 5(a). Compared with the DARTS method that only supports several search options, the EGI local search iteratively finds the accurate dilations in a dense space, obtaining structures</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Ablation about the proposed EGI local search.(a) Performance of EGI local search and DARTS-related methods.</figDesc><table><row><cell>BreakFast</cell><cell></cell><cell>F@0.1</cell><cell cols="2">F@0.25</cell><cell>F@0.5</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>DARTS [20]</cell><cell></cell><cell>73.4</cell><cell>67.3</cell><cell></cell><cell>53.1</cell><cell>72.7</cell><cell>69.5</cell></row><row><cell cols="2">+Early Stop [119]</cell><cell>73.8</cell><cell>67.6</cell><cell></cell><cell>52.8</cell><cell>72.8</cell><cell>69.3</cell></row><row><cell cols="2">DARTS+Early Stop</cell><cell>73.8</cell><cell>67.6</cell><cell></cell><cell>52.8</cell><cell>72.8</cell><cell>69.3</cell></row><row><cell cols="2">+ Fair DARTS [120]</cell><cell>73.3</cell><cell>67.5</cell><cell></cell><cell>52.9</cell><cell>71.9</cell><cell>69.9</cell></row><row><cell>Ours</cell><cell></cell><cell>74.9</cell><cell>69.0</cell><cell></cell><cell>55.2</cell><cell>73.3</cell><cell>70.7</cell></row><row><cell cols="7">(b) Performance of EGI local search initialized by different structures.</cell></row><row><cell>BreakFast</cell><cell cols="2">F@0.1</cell><cell>F@0.25</cell><cell></cell><cell>F@0.5</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>random</cell><cell cols="2">67.7</cell><cell>61.8</cell><cell></cell><cell>48.3</cell><cell>68.4</cell><cell>67.0</cell></row><row><cell>random + local</cell><cell cols="2">73.6</cell><cell>67.8</cell><cell></cell><cell>53.7</cell><cell>72.3</cell><cell>69.9</cell></row><row><cell>baseline [12]</cell><cell cols="2">69.1</cell><cell>63.7</cell><cell></cell><cell>50.1</cell><cell>71.0</cell><cell>69.2</cell></row><row><cell>baseline + local</cell><cell cols="2">74.1</cell><cell>68.5</cell><cell></cell><cell>55.3</cell><cell>72.3</cell><cell>70.2</cell></row><row><cell>global</cell><cell cols="2">72.2</cell><cell>66.0</cell><cell></cell><cell>51.8</cell><cell>71.5</cell><cell>69.4</cell></row><row><cell>global + local</cell><cell cols="2">74.9</cell><cell>69.0</cell><cell></cell><cell>55.2</cell><cell>73.3</cell><cell>70.7</cell></row><row><cell cols="7">(c) Performance of EGI local search initialized by different structures.</cell></row><row><cell>BreakFast</cell><cell>F@0.1</cell><cell cols="2">F@0.25</cell><cell cols="2">F@0.5</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>S = 2</cell><cell>74.8</cell><cell cols="2">68.9</cell><cell cols="2">55.0</cell><cell>73.4</cell><cell>70.4</cell></row><row><cell>S = 3</cell><cell>74.9</cell><cell cols="2">69.0</cell><cell cols="2">55.2</cell><cell>73.3</cell><cell>70.7</cell></row><row><cell>S = 4</cell><cell>74.9</cell><cell cols="2">68.8</cell><cell cols="2">55.1</cell><cell>73.3</cell><cell>70.9</cell></row><row><cell>BreakFast</cell><cell>F@0.1</cell><cell cols="2">F@0.25</cell><cell cols="2">F@0.5</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>sigmoid</cell><cell>72.7</cell><cell cols="2">66.9</cell><cell cols="2">52.7</cell><cell>71.8</cell><cell>69.4</cell></row><row><cell>softmax</cell><cell>73.2</cell><cell cols="2">67.2</cell><cell cols="2">52.0</cell><cell>71.6</cell><cell>69.7</cell></row><row><cell>Equ. (4)</cell><cell>74.9</cell><cell cols="2">69.0</cell><cell cols="2">55.2</cell><cell>73.3</cell><cell>70.7</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE 6</cell><cell></cell><cell></cell></row><row><cell cols="7">GPU hours of the global and local search on different temporal action</cell></row><row><cell cols="7">segmentation datasets using the RTX 2080Ti GPU based on the</cell></row><row><cell></cell><cell></cell><cell cols="3">MS-TCN method.</cell><cell></cell></row><row><cell cols="2">GPU Hours</cell><cell cols="2">BreakFast</cell><cell cols="2">50Salads</cell><cell>GTEA</cell></row><row><cell cols="2">Global Search</cell><cell cols="2">144h</cell><cell></cell><cell>9h</cell><cell>1h</cell></row><row><cell cols="2">Local Search</cell><cell></cell><cell>2.2h</cell><cell></cell><cell>0.15h</cell><cell>0.05h</cell></row><row><cell cols="2">MS-TCN Training</cell><cell></cell><cell>2.0h</cell><cell></cell><cell>0.14h</cell><cell>0.05h</cell></row></table><note>(d) Ablation of possible probability mass functions in EGI local search.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7</head><label>7</label><figDesc>Cross-validation performance (F@0.1) of searched structures among the fold 1 of different datasets. Arch-dataset indicates the structure is searched on which dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MS-TCN</cell><cell>Arch-50Salads</cell><cell>Arch-GTEA</cell><cell>Arch-BF</cell></row><row><cell></cell><cell>50Salads</cell><cell>67.1</cell><cell></cell><cell>75.4</cell><cell>68.8</cell><cell>72.6</cell></row><row><cell></cell><cell>GTEA</cell><cell>83.8</cell><cell></cell><cell>82.4</cell><cell>88.9</cell><cell>85.6</cell></row><row><cell></cell><cell>BF</cell><cell>69.9</cell><cell></cell><cell>75.1</cell><cell>72.5</cell><cell>76.4</cell></row><row><cell></cell><cell>350</cell><cell>stage1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>stage2</cell><cell></cell><cell></cell></row><row><cell>Dilation rate</cell><cell>250</cell><cell>stage3 stage4</cell><cell></cell><cell></cell></row><row><cell></cell><cell>150</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell>0~50</cell><cell cols="3">50~59 59~60 60~61 61~100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">The range of performance</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8</head><label>8</label><figDesc>Performance of local search on object detection with COCO<ref type="bibr" target="#b120">[121]</ref> dataset using Faster-RCNN as the baseline method. Local-P means the local-searched structure with parallel receptive fields as described in Sec. 3.3. -R50 and -R101 denote using ResNet-50 and ResNet-101 as backbones, respectively. S indicates using S branches in local search as shown in Equ.<ref type="bibr" target="#b3">(4)</ref>. mAPs mAPm mAP l mAP mAP50 mAP75 mAPs mAPm mAP l</figDesc><table><row><cell cols="5">validation set mAP mAP50 mAP75 Faster-RCNN-R50 [2] P 37.4 58.3 40.6 22.0</cell><cell>41.1</cell><cell>48.1</cell><cell>37.8</cell><cell>59.0</cell><cell cols="2">test set 41.0 22.1</cell><cell>40.8</cell><cell>46.4</cell></row><row><cell>+RF (S = 3)</cell><cell>39.3</cell><cell>60.6</cell><cell>42.9</cell><cell>23.5</cell><cell>43.0</cell><cell>51.0</cell><cell>39.2</cell><cell>60.9</cell><cell>42.6</cell><cell>22.6</cell><cell>41.8</cell><cell>48.9</cell></row><row><cell>+RF (S = 3)</cell><cell>40.2</cell><cell>61.7</cell><cell>43.8</cell><cell>23.5</cell><cell>43.9</cell><cell>52.2</cell><cell>40.4</cell><cell>62.1</cell><cell>44.0</cell><cell>23.6</cell><cell>43.0</cell><cell>50.5</cell></row><row><cell>+RF (S = 2)</cell><cell>39.1</cell><cell>60.5</cell><cell>42.3</cell><cell>23.2</cell><cell>42.8</cell><cell>50.3</cell><cell>39.1</cell><cell>60.8</cell><cell>42.3</cell><cell>22.7</cell><cell>41.7</cell><cell>48.7</cell></row><row><cell>+RF (S = 2)</cell><cell>40.0</cell><cell>61.4</cell><cell>43.8</cell><cell>23.9</cell><cell>43.6</cell><cell>51.9</cell><cell>40.3</cell><cell>62.1</cell><cell>43.9</cell><cell>23.7</cell><cell>43.0</cell><cell>50.4</cell></row><row><cell>Faster-RCNN-R101 [2]</cell><cell>39.4</cell><cell>60.1</cell><cell>43.1</cell><cell>22.4</cell><cell>43.7</cell><cell>51.1</cell><cell>39.7</cell><cell>60.7</cell><cell>43.2</cell><cell>22.5</cell><cell>42.9</cell><cell>49.9</cell></row><row><cell>+RF (S = 3)</cell><cell>41.1</cell><cell>62.4</cell><cell>44.7</cell><cell>24.5</cell><cell>45.1</cell><cell>53.9</cell><cell>41.2</cell><cell>62.8</cell><cell>44.9</cell><cell>23.6</cell><cell>44.1</cell><cell>52.0</cell></row><row><cell>+RF (S = 3)</cell><cell>42.0</cell><cell>63.2</cell><cell>45.7</cell><cell>25.0</cell><cell>45.9</cell><cell>55.4</cell><cell>42.1</cell><cell>63.8</cell><cell>45.8</cell><cell>24.3</cell><cell>45.1</cell><cell>53.3</cell></row><row><cell>OB-R50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IN-R50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OB-R101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IN-R101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S2</cell><cell></cell><cell></cell><cell></cell><cell>S3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>S4</cell><cell>FPN</cell><cell>RPN</cell><cell>MASK</cell></row></table><note>Fig. 8. Visualization of the local-searched structures of Faster-RCNN [2] for object detection (OB) and Mask-RCNN [4] for instance segmentation (IN). S2-S4 denotes stage 2 to stage 4 of the ResNet backbone. FPN, RPN, and MASK mean the feature pyramid network, region proposal network, and the mask segmentation head in Faster-RCNN and Mask-RCNN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9</head><label>9</label><figDesc>Cross-validation performance (F@0.1) of searched structures among different folds of the BreakFast dataset. Arch-n means the structure is searched on fold n.</figDesc><table><row><cell>BreakFast</cell><cell>Arch-1</cell><cell>Arch-2</cell><cell>Arch-3</cell><cell>Arch-4</cell></row><row><cell>fold1</cell><cell>76.4</cell><cell>76.3</cell><cell>76.2</cell><cell>75.7</cell></row><row><cell>fold2</cell><cell>74.1</cell><cell>75.3</cell><cell>75.1</cell><cell>74.6</cell></row><row><cell>fold3</cell><cell>76.1</cell><cell>76.6</cell><cell>76.1</cell><cell>75.4</cell></row><row><cell>fold4</cell><cell>71.7</cell><cell>72.1</cell><cell>72.0</cell><cell>71.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10</head><label>10</label><figDesc>Performance of local search on instance segmentation with COCO<ref type="bibr" target="#b120">[121]</ref> dataset using Mask-RCNN as the baseline method. Local-P means the local-searched structure with parallel receptive fields as described in Sec. 3.3. -R50 and -R101 denote using ResNet-50 and ResNet-101 as backbones, respectively. R50-NonLocal means adding the non-local block<ref type="bibr" target="#b39">[40]</ref> to each residual block in stage 4 of the ResNet-50 backbone. S indicates using S branches in local search as shown in Equ.<ref type="bibr" target="#b3">(4)</ref>.</figDesc><table><row><cell></cell><cell cols="4">P mAP bb mAP bb 50 mAP bb 75 mAP bb s</cell><cell cols="2">mAP bb m mAP bb l</cell><cell cols="4">mAP mk mAP mk 50 mAP mk 75 mAP mk s</cell><cell>mAP mk m</cell><cell>mAP mk l</cell></row><row><cell>validation set:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask-RCNN-R50 [4]</cell><cell>38.2</cell><cell>59.0</cell><cell>41.7</cell><cell>22.4</cell><cell>41.6</cell><cell>49.2</cell><cell>34.7</cell><cell>55.9</cell><cell>37.0</cell><cell>16.5</cell><cell>37.4</cell><cell>50.1</cell></row><row><cell>+RF (S = 3)</cell><cell>39.7</cell><cell>60.7</cell><cell>43.4</cell><cell>23.7</cell><cell>43.3</cell><cell>51.3</cell><cell>36.0</cell><cell>57.8</cell><cell>38.4</cell><cell>17.0</cell><cell>39.1</cell><cell>52.2</cell></row><row><cell>+RF (S = 3)</cell><cell>41.0</cell><cell>62.3</cell><cell>44.9</cell><cell>24.2</cell><cell>44.7</cell><cell>53.2</cell><cell>37.1</cell><cell>59.1</cell><cell>39.5</cell><cell>18.4</cell><cell>39.9</cell><cell>53.6</cell></row><row><cell>+RF (S = 2)</cell><cell>39.6</cell><cell>60.8</cell><cell>43.2</cell><cell>23.3</cell><cell>43.1</cell><cell>51.4</cell><cell>35.9</cell><cell>57.6</cell><cell>38.3</cell><cell>16.4</cell><cell>38.9</cell><cell>52.2</cell></row><row><cell>+RF (S = 2)</cell><cell>40.7</cell><cell>62.1</cell><cell>44.5</cell><cell>24.4</cell><cell>44.3</cell><cell>52.9</cell><cell>36.8</cell><cell>58.9</cell><cell>39.3</cell><cell>18.0</cell><cell>39.7</cell><cell>53.6</cell></row><row><cell>Mask-RCNN-R101 [4]</cell><cell>40.0</cell><cell>60.5</cell><cell>44.0</cell><cell>22.6</cell><cell>44.0</cell><cell>52.6</cell><cell>36.1</cell><cell>57.5</cell><cell>38.6</cell><cell>18.8</cell><cell>39.7</cell><cell>49.5</cell></row><row><cell>+RF (S = 3)</cell><cell>41.7</cell><cell>62.8</cell><cell>45.7</cell><cell>24.8</cell><cell>45.4</cell><cell>55.4</cell><cell>37.4</cell><cell>59.6</cell><cell>40.0</cell><cell>18.5</cell><cell>40.6</cell><cell>55.1</cell></row><row><cell>+RF (S = 3)</cell><cell>42.7</cell><cell>64.0</cell><cell>46.7</cell><cell>25.6</cell><cell>46.8</cell><cell>55.8</cell><cell>38.4</cell><cell>60.9</cell><cell>41.3</cell><cell>18.5</cell><cell>42.0</cell><cell>55.7</cell></row><row><cell>R50-NonLocal [40]</cell><cell>39.4</cell><cell>60.8</cell><cell>43.4</cell><cell>23.2</cell><cell>43.2</cell><cell>50.5</cell><cell>35.5</cell><cell>57.1</cell><cell>37.9</cell><cell>17.2</cell><cell>38.5</cell><cell>51.3</cell></row><row><cell>+RF (S = 2)</cell><cell>40.2</cell><cell>61.5</cell><cell>43.9</cell><cell>23.9</cell><cell>43.9</cell><cell>52.2</cell><cell>36.4</cell><cell>58.5</cell><cell>38.7</cell><cell>17.6</cell><cell>39.3</cell><cell>52.3</cell></row><row><cell>+RF (S = 2)</cell><cell>40.8</cell><cell>62.2</cell><cell>44.6</cell><cell>24.4</cell><cell>44.5</cell><cell>53.0</cell><cell>36.9</cell><cell>59.0</cell><cell>39.6</cell><cell>18.0</cell><cell>39.8</cell><cell>53.8</cell></row><row><cell>test set:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask-RCNN-R50 [4]</cell><cell>38.5</cell><cell>59.5</cell><cell>41.8</cell><cell>22.3</cell><cell>41.6</cell><cell>47.4</cell><cell>34.9</cell><cell>56.4</cell><cell>37.2</cell><cell>18.9</cell><cell>37.5</cell><cell>44.6</cell></row><row><cell>+RF (S = 3)</cell><cell>40.0</cell><cell>61.4</cell><cell>43.6</cell><cell>23.2</cell><cell>42.6</cell><cell>49.8</cell><cell>36.2</cell><cell>58.5</cell><cell>38.5</cell><cell>19.9</cell><cell>38.6</cell><cell>46.8</cell></row><row><cell>+RF (S = 3)</cell><cell>41.0</cell><cell>62.5</cell><cell>45.0</cell><cell>24.0</cell><cell>43.8</cell><cell>51.3</cell><cell>37.1</cell><cell>59.5</cell><cell>39.9</cell><cell>20.5</cell><cell>39.7</cell><cell>48.2</cell></row><row><cell>+RF (S = 2)</cell><cell>39.8</cell><cell>61.2</cell><cell>43.4</cell><cell>23.2</cell><cell>42.3</cell><cell>49.4</cell><cell>36.1</cell><cell>58.2</cell><cell>38.6</cell><cell>19.8</cell><cell>38.4</cell><cell>46.5</cell></row><row><cell>+RF (S = 2)</cell><cell>41.0</cell><cell>62.5</cell><cell>44.7</cell><cell>24.1</cell><cell>43.7</cell><cell>51.4</cell><cell>37.1</cell><cell>59.5</cell><cell>39.7</cell><cell>20.7</cell><cell>39.6</cell><cell>48.3</cell></row><row><cell>Mask-RCNN-R101 [4]</cell><cell>40.4</cell><cell>61.2</cell><cell>44.1</cell><cell>23.1</cell><cell>43.5</cell><cell>50.8</cell><cell>36.5</cell><cell>58.3</cell><cell>38.9</cell><cell>19.6</cell><cell>39.2</cell><cell>47.8</cell></row><row><cell>+RF (S = 3)</cell><cell>42.0</cell><cell>63.4</cell><cell>45.9</cell><cell>24.2</cell><cell>45.0</cell><cell>53.1</cell><cell>37.8</cell><cell>60.5</cell><cell>40.4</cell><cell>20.5</cell><cell>40.4</cell><cell>49.7</cell></row><row><cell>+RF (S = 3)</cell><cell>42.8</cell><cell>64.1</cell><cell>46.9</cell><cell>24.7</cell><cell>46.1</cell><cell>54.2</cell><cell>38.5</cell><cell>61.3</cell><cell>41.3</cell><cell>21.0</cell><cell>41.4</cell><cell>50.7</cell></row><row><cell>R50-NonLocal [40]</cell><cell>39.8</cell><cell>61.4</cell><cell>43.3</cell><cell>23.2</cell><cell>42.8</cell><cell>49.1</cell><cell>36.0</cell><cell>58.2</cell><cell>38.3</cell><cell>19.7</cell><cell>38.6</cell><cell>46.3</cell></row><row><cell>+RF (S = 2)</cell><cell>40.4</cell><cell>61.9</cell><cell>44.0</cell><cell>23.2</cell><cell>42.8</cell><cell>50.4</cell><cell>36.4</cell><cell>58.7</cell><cell>38.9</cell><cell>19.7</cell><cell>38.6</cell><cell>47.2</cell></row><row><cell>+RF (S = 2)</cell><cell>41.2</cell><cell>62.9</cell><cell>45.0</cell><cell>23.7</cell><cell>43.7</cell><cell>51.7</cell><cell>37.3</cell><cell>59.8</cell><cell>39.9</cell><cell>20.3</cell><cell>39.7</cell><cell>48.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 12</head><label>12</label><figDesc>Speech synthesis performance of WaveGlow<ref type="bibr" target="#b126">[127]</ref> based on local-searched structure on the LJ speech dataset<ref type="bibr" target="#b128">[129]</ref>.</figDesc><table><row><cell></cell><cell>MCD? [131]</cell><cell>LLR? [133]</cell><cell>PESQ? [132]</cell></row><row><cell>WaveGlow [127]</cell><cell>5.79</cell><cell>1.29</cell><cell>1.52</cell></row><row><cell>RF-WaveGlow</cell><cell>5.59</cell><cell>0.71</cell><cell>1.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 13</head><label>13</label><figDesc></figDesc><table><row><cell cols="4">Performance of global-to-local search on TCN [134]. We evaluate the</cell></row><row><cell cols="4">performance of the polyphonic music modeling and P-MNIST digit</cell></row><row><cell cols="2">classification task.</cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell cols="3">baseline global global+local</cell></row><row><cell>Permuted MNIST (accuracy?)</cell><cell>97.2</cell><cell>97.6</cell><cell>97.8</cell></row><row><cell>Music Nottingham (NLL?)</cell><cell>2.97</cell><cell>2.73</cell><cell>2.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 14</head><label>14</label><figDesc>Receptive field search improves SOTA attention/convolution models for object detection and instance segmentation tasks on the COCO val dataset. Following the official implementation of PVT<ref type="bibr" target="#b144">[145]</ref>,<ref type="bibr" target="#b145">[146]</ref> and ConvNeXt<ref type="bibr" target="#b146">[147]</ref>, PVTv2-B0 and ConvNeXt-T adopt the Mask RCNN detector and Cascade Mask RCNN detector, respectively.</figDesc><table><row><cell>Object det.</cell><cell cols="2">P mAP mAP50 mAP75 mAPs mAPm mAP l</cell></row><row><cell>PVTv2-B0</cell><cell>38.2 60.5</cell><cell>40.7 22.9 40.9 49.6</cell></row><row><cell>RF-PVT</cell><cell>38.8 60.9</cell><cell>41.8 23.6 41.2 50.8</cell></row><row><cell>RF-PVT</cell><cell>39.1 60.8</cell><cell>42.7 23.3 41.8 51.4</cell></row><row><cell>ConvNeXt-T</cell><cell>50.4 69.1</cell><cell>54.8 33.9 54.5 65.1</cell></row><row><cell>RF-ConvNeXt</cell><cell>50.6 69.2</cell><cell>54.8 34.1 54.0 65.5</cell></row><row><cell>RF-ConvNeXt</cell><cell>50.9 69.5</cell><cell>55.5 34.3 54.6 65.8</cell></row><row><cell>Instance seg.</cell><cell cols="2">mAP mAP50 mAP75 mAPs mAPm mAP l</cell></row><row><cell>PVTv2-B0</cell><cell>36.2 57.8</cell><cell>38.6 18.0 38.4 51.9</cell></row><row><cell>RF-PVT</cell><cell>36.8 58.4</cell><cell>39.5 18.7 39.0 52.7</cell></row><row><cell>RF-PVT</cell><cell>37.1 58.5</cell><cell>40.0 17.8 39.3 53.7</cell></row><row><cell>ConvNeXt-T</cell><cell>43.7 66.5</cell><cell>47.3 24.2 47.1 62.1</cell></row><row><cell>RF-ConvNeXt</cell><cell>44.0 66.8</cell><cell>47.5 24.8 47.0 62.1</cell></row><row><cell>RF-ConvNeXt</cell><cell>44.3 67.3</cell><cell>47.8 24.7 47.4 62.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 15</head><label>15</label><figDesc>Performance of receptive field search on semantic segmentation using PVTv2-B0 backbone with the Semantic FPN method<ref type="bibr" target="#b145">[146]</ref>.</figDesc><table><row><cell>P</cell><cell cols="2">Pascal VOC [125] mIoU mAcc</cell><cell cols="2">ADE20K [126] mIoU mAcc</cell></row><row><cell>PVTv2-B0</cell><cell>73.7</cell><cell>85.0</cell><cell>37.5</cell><cell>48.3</cell></row><row><cell>RF-PVT</cell><cell>74.4</cell><cell>86.0</cell><cell>38.0</cell><cell>48.6</cell></row><row><cell>RF-PVT</cell><cell>74.4</cell><cell>85.9</cell><cell>37.8</cell><cell>48.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Object det.P mAP mAP50 mAP75 mAPs mAPm mAP l</figDesc><table><row><cell>Res2Net-101</cell><cell>46.3 64.4</cell><cell>50.5 27.2 50.3 60.5</cell></row><row><cell>RF-Res2Net</cell><cell>46.9 65.8</cell><cell>51.2 28.4 50.7 62.1</cell></row><row><cell>RF-Res2Net</cell><cell>47.9 66.6</cell><cell>52.2 29.7 51.9 62.8</cell></row><row><cell>HRNetV2p-W18</cell><cell>41.6 58.7</cell><cell>45.4 23.5 44.7 54.9</cell></row><row><cell>RF-HRNet</cell><cell>42.9 60.8</cell><cell>46.7 25.9 46.2 54.8</cell></row><row><cell>RF-HRNet</cell><cell>43.7 61.9</cell><cell>47.7 26.5 47.3 56.7</cell></row><row><cell>Instance seg.</cell><cell cols="2">mAP mAP50 mAP75 mAPs mAPm mAP l</cell></row><row><cell>Res2Net-101</cell><cell>40.0 61.7</cell><cell>43.3 22.2 43.8 54.1</cell></row><row><cell>RF-Res2Net</cell><cell>40.7 63.2</cell><cell>43.9 20.4 44.0 59.0</cell></row><row><cell>RF-Res2Net</cell><cell>41.5 64.0</cell><cell>44.9 21.3 44.6 59.5</cell></row><row><cell>HRNetV2p-W18</cell><cell>36.4 56.3</cell><cell>39.3 19.1 39.1 49.5</cell></row><row><cell>RF-HRNet</cell><cell>37.6 58.3</cell><cell>40.4 19.0 40.2 53.9</cell></row><row><cell>RF-HRNet</cell><cell>38.1 59.3</cell><cell>41.0 19.4 40.7 55.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 17</head><label>17</label><figDesc>Performance of local search on semantic segmentation with Auto-deeplab</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was supported by the Major Project for New Generation of AI under Grant No. 2018AAA0100400, NSFC (61922046), and S&amp;T innovation project from Chinese Ministry of Education.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global2local: Efficient structure search for video action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A highly efficient model to study the semantics of salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large-scale unsupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03149</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ms-tcn++: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abufarha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Action segmentation with joint self-supervised temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9454" to="9463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Waveflow: A compact flowbased model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>ser. Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural speech synthesis with transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Boundary-aware cascade networks for temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sct: Set constrained temporal transformer for set supervised action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="501" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Representative batch normalization with feature calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Set-constrained viterbi for set-supervised action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving action segmentation via graph-based temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="24" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4905" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised scale-regularized linear convolutionary filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lauze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Receptive field size versus model depth for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1669" to="1682" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3435" to="3444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep continuous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large receptive field networks for highscale image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Androutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="763" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A multimodal database for affect recognition and implicit tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Introduction to the special section on video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="745" to="746" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A system for video surveillance and monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duggins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tolliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Enomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VSAM final report</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2112" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="568" to="576" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Modeling actions through state changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2579" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Recognition of complex events: Exploiting temporal dynamics between underlying concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2235" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2641" to="2648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1250" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A hybrid rnn-hmm approach for weakly supervised temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Temporal sequence modeling for video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2227" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A multistream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1961" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">First person action recognition using deep learned descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2620" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Tricornet: A hybrid temporal convolutional and recurrent network for video action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07818</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial network for continuous fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gammulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="6742" to="6751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unsupervised learning and segmentation of complex activities from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8368" to="8376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">An introduction to text-to-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dutoit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Text-to-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Flowavenet : A generative flow for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<editor>Research, K. Chaudhuri and R. Salakhutdinov</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3370" to="3378" />
		</imprint>
	</monogr>
	<note>ser. Proceedings of Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">A survey on neural speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15561</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep voice: Realtime neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">?</forename><surname>Ar?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Simultaneous modeling of spectrum, pitch and duration in hmmbased speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masuko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Statistical parametric speech synthesis using deep neural networks,&quot; in ICASSP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="7962" to="7966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Tacotron: Towards end-toend speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02334,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="454" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Mobiledets: Searching for object detection architectures for mobile accelerators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="3825" to="3834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">End-toend object detection with fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">858</biblScope>
			<biblScope unit="page" from="15" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Opanas: One-shot path aggregation network architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="195" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2019.2956516</idno>
		<ptr target="http://dx.doi.org/10.1109/tpami.2019.29565163" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Object detection in 20 years: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05055</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">A survey of modern deep learning based object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S A</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11892</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="318" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A2-fpn: Attention aggregation based feature pyramid network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">352</biblScope>
			<biblScope unit="page" from="15" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Dct-mask: Discrete cosine transform mask representation for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="8720" to="8729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Deeply shape-guided cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="8278" to="8288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Boundary-preserving mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Boundary-aware instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5696" to="5704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Polytransform: Deep polygon transformer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9131" to="9140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Segfix: Model-agnostic boundary refinement for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="489" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Look closer to segment better: Boundary patch refinement for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">935</biblScope>
			<biblScope unit="page" from="13" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">An introduction to genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Automatically designing cnn architectures using the genetic algorithm for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Nsga-net: neural architecture search using multi-objective genetic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="419" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Fair darts: Eliminating unfair advantages in differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="965" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Dcnas: Densely connected neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Jittor: a novel deep learning framework with meta-operators and unified graph execution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal cnns for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">Darts+: Improved differentiable architecture search with early stopping</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">The lj speech dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://keithito.com/LJ-Speech-Dataset/" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Mel-cepstral distance measure for objective speech quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kubichek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACRIM</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Objective measures of speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Quackenbush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Barnwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Clements</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Prentice-Hall</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Architectural complexity measures of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1822" to="1830" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Fullcapacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="4880" to="4888" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Recurrent batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Zoneout: Regularizing rnns by randomly preserving hidden activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Tunable efficient unitary neural networks (eunn) and their application to rnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dubcek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peurifoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skirlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Solja?i?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1733" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6026</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<idno>2022. 14</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno>2022. 14</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Probabilistic forecasting with temporal convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">399</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Highly efficient salient object detection with 100k parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Vecroad: Pointbased iterative graph exploration for road graphs extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Pyramid constrained self-attention network for fast video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-P</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">876</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Deep hough transform for semantic line detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
