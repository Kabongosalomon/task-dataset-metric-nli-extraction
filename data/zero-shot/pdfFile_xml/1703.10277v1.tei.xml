<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Instance Segmentation via Deep Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
						</author>
						<title level="a" type="main">Semantic Instance Segmentation via Deep Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new method for semantic instance segmentation, by first computing how likely two pixels are to belong to the same object, and then by grouping similar pixels together. Our similarity metric is based on a deep, fully convolutional embedding model. Our grouping method is based on selecting all points that are sufficiently similar to a set of "seed points', chosen from a deep, fully convolutional scoring model. We show competitive results on the Pascal VOC instance segmentation benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic instance segmentation is the problem of identifying individual instances of objects and their categories (such as person and car) in an image. It differs from object detection in that the output is a mask representing the shape of each object, rather than just a bounding box. It differs from semantic segmentation in that our goal is not just to classify each pixel with a label (or as background), but also to distinguish individual instances of the same class. Thus, the label space is unbounded in size (e.g., we may have the labels "person-1", "person-2" and "car-1", assuming there are two people and one car). This problem has many practical applications in domains such as self-driving cars, robotics, photo editing, etc.</p><p>A common approach to this problem (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref>) is first to use some mechanism to predict object bounding boxes (e.g., by running a class-level object detector, or by using a class agnostic box proposal method such as EdgeBoxes), and then to run segmentation and classification within each proposed box. However, this can fail if there is more than one instance inside of the box. Also, intuitively it feels more "natural" to first detect the mask representing each object, and then derive a bounding box from this, if needed. (Note that boxes are a good approximation to the shape of certain classes, such as cars and pedestrians, but they are a poor approximation for many other classes, such as articulated people, "wirey" objects like chairs, or * Google Inc., USA ? UCLA non-axis-aligned objects like ships seen from the air.) Recently there has been a move towards "box-free" methods, that try to directly predict the mask for each object (e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref>). The most common approach to this is to modify the Faster RCNN architecture <ref type="bibr" target="#b22">[23]</ref> so that at each point, it predicts a "centeredness" score (the probability the current pixel is the center of an object instance), a binary object mask, and a class label (rather than predicting the usual "objectness" score, bounding box, and class label). However, this approach requires that the entire object instance fits within the receptive field of the unit that is making the prediction. This can be difficult for elongated structures, that might span many pixels in the image. In addition, for some object categories, the notion of a "center" is not well defined.</p><p>In this paper, we take a different approach. Our key idea is that we can produce instance segmentations by computing the likelihood that two pixels belong to the same object instance and subsequently use these likelihoods to group similar pixels together. This is similar to most approaches to unsupervised image segmentation (e.g. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8]</ref>), which group pixels together to form segments or "super-pixels". However, unlike the unsupervised case, we have a welldefined notion of what a "correct" segment is, namely the spatial extent of the entire object. This avoids ambiguities such as whether to treat parts of an object (e.g., the shirt and pants of a person) as separate segments, which plagues evaluation of unsupervised methods.</p><p>We propose to learn the similarity metric using a deep embedding model. This is similar to other approaches, such as FaceNet <ref type="bibr" target="#b24">[25]</ref>, which learn how likely two bounding boxes are to belong to the same instance (person), except we learn to predict the similarity of pixels, taking into account their local context.</p><p>Another difference from unsupervised image segmentation is that we do not use spectral or graph based partitioning methods, because computing the pairwise similarity of all the pixels is too expensive. Instead, we use compute the distance (in embedding space) to a set of K "seed points"; this can be implemented as tensor multiplication. To find these seed points, we learn a separate model that predicts how likely a pixel is to make a good seed; we call this the  <ref type="figure">Figure 1</ref>. Given an image, our model predicts the embedding vector of each pixel (top head of the network) as well as the classification score of the mask each pixel will generate if picked as a seed (bottom head of the network). We derive the seediness scores from the classification scores and use them to choose which seed points in the image to sample. Each seed point generates a mask based on the embedding vectors; each mask is then associated with a class label and confidence score. In this figure, pink color corresponds to the "cow" class and blue to the "background" class.</p><p>"seediness" score of each pixel. This is analogous to the "centeredness" score used in prior methods, except we do not need to identify object centers; instead, the seediness score is a measure of the "typicality" of a pixel with respect to the other pixels in this instance. We show that in practice we only have to take the top 100 seeds to obtain good coverage of nearly all of the objects in an image. Our method obtains a mAP score (at an IoU of 0.5) of 62.21 % on the Pascal VOC 2012 instance segmentation benchmark <ref type="bibr" target="#b6">[7]</ref>. This puts us in fourth place, behind <ref type="bibr" target="#b15">[16]</ref> (66.7%), <ref type="bibr" target="#b14">[15]</ref> (65.7%), and <ref type="bibr" target="#b5">[6]</ref> (63.5%). However, these are all proposal-based methods. (The previous fourth place method was the "proposal free" approach of <ref type="bibr" target="#b16">[17]</ref>, who obtained 58.7%.) Although not state of the art on this particular benchmark, our results are still competitive. Furthermore, we believe our approach may work particularly well on other datasets, with "spindly" objects with unusual shapes, although we leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The most common approach to instance segmentation is first to predict object bounding boxes, and then to predict the mask inside of each such box using one or more steps. For example, the MNC method of <ref type="bibr" target="#b5">[6]</ref>, which won the COCO 2015 instance segmentation competition, was based on this approach. In particular, they modified the Faster RCNN method <ref type="bibr" target="#b22">[23]</ref> as follows: after detecting the top K boxes, and predicting their class labels, they extract features for each box using ROI pooling applied to the feature map and use this to predict a binary mask representing the shape of the corresponding object instance. The locations of the boxes can be refined, based on the predicted mask, and the process repeated. <ref type="bibr" target="#b5">[6]</ref> iterated this process twice, and the R2-</p><formula xml:id="formula_0">A1 A2 A3 B1 B2 B3 C1 C2 C3 A1 A2 A3 B1 B2 B3 C1 C2 C3 A1 A2 A3 B1 B2 B3 C1 C2 C3</formula><p>Sigmoid cross entropy loss on the similarity between pairs of embedding vectors <ref type="figure">Figure 2</ref>. We compute the embedding loss by sampling the embedding vector of K pixels within each instance, resulting in a total of N K embedding vectors where N is the number of instances in image. We compute the similarity between every pair of embedding vectors ?(ep, eq) as described in Eq. 1. We want to learn a metric that returns a similarity of 1 for a pair of embedding vectors that belong to the same instance, and 0 for a pair of embedding vectors that belong to different instances. Thus, we add a cross entropy loss on the similarity between pairs by setting the ground-truth values to 0 and 1 based on whether embedding vectors belong to the same instance or not.</p><p>IOS method in <ref type="bibr" target="#b15">[16]</ref> iterated the process a variable number of times. The fully convolutional instance segmentation method of <ref type="bibr" target="#b14">[15]</ref> won the COCO instance segmentation challenge in 2016. It leverages the position sensitive score maps used in the mask proposal mechanism of <ref type="bibr" target="#b4">[5]</ref>. At each location, it predicts a mask, as well as a category likelihood. DeepMask <ref type="bibr" target="#b19">[20]</ref> and SharpMask <ref type="bibr" target="#b20">[21]</ref> use a similar approach, without the position-sensitive score maps. Note that most of these sliding window methods used an image pyramid to handle objects of multiple sizes. Recently, <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13]</ref> proposed to use a feature pyramid network instead of recomputing features on an image pyramid.</p><p>The above methods all operate in parallel across the whole image. Alternatively, we can run sequentially, extracting one object instance at a time. <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b21">[22]</ref> are examples of this approach. In particular, they extract features using a CNN and then use an RNN to "emit" a binary mask (corresponding to an object instance) at each step. The hidden state of the RNN is image-shaped and keeps track of which locations in the image have already been segmented.</p><p>The main drawback of this approach is that it is slow. Also, it is troublesome to scale to large images, since the hidden state sequence can consume a lot of memory.</p><p>An another way to derive a variable number of objects (regions) is to use the watershed algorithm. The method of <ref type="bibr" target="#b1">[2]</ref> predicts a (discretized) energy value for each pixel. This energy surface is then partitioned into object instances using the watershed algorithm. The process is repeated independently for each class. The input to the network is a 2d vector per pixel, representing the direction away from the pixel's nearest boundary; these vectors are predicted given an RGB input image. The overall approach shows state of the art results on the CityScapes dataset. (A similar approach was used in <ref type="bibr" target="#b27">[28]</ref>, except they used template matching to find the instances, rather than watershed; also, they relied on depth data during training.)</p><p>Kirrilov et al. <ref type="bibr" target="#b13">[14]</ref> also use the watershed algorithm to find candidate regions, but they apply it to an instanceaware edge boundary map rather than an energy function. They extract about 3000 instances (superpixels), which become candidate objects. They then group (and apply semantic labels) to each such region by solving a Multi-Cut integer linear programming optimization problem.</p><p>Although the watershed algorithm has some appeal, these techniques cannot group disconnected regions into a single instance (e.g., if the object is partitioned into two pieces by an occluder, such as the horse in Figure6 who is occluded by its rider). Therefore, we consider more general clustering algorithms. Newel and Deng <ref type="bibr" target="#b18">[19]</ref> predict an objectness score for each pixel, as well as a one-dimensional embedding for each pixel. They first threshold on the objectness heatmap to produce a binary mask. They then compute a 1d histogram of the embeddings of all the pixels inside the mask, perform NMS to find modes, and then assign each pixel in the mask to its closest centroid.</p><p>Our approach is related to <ref type="bibr" target="#b18">[19]</ref>, but differs in the following important ways: (1) we use a different loss function when learning the pixel similarity metric; (2) we use a different way of creating masks, based on identifying the basins of attraction in similarity space, rather than using a greedy clustering method; (3) we learn a D-dimensional embedding per pixel, instead of using 1d embeddings. As a consequence, our results are much better: we get a mAP r (at an IoU of 0.5) of 62.21 % on PASCAL VOC 2012 validation, whereas <ref type="bibr" target="#b18">[19]</ref> gets 35.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In the following sections, we describe our method in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of our approach</head><p>We start by taking a model that was pretrained for semantic segmentation (see Section 3.5 for details), and then modify it to perform instance segmentation by adding two different kinds of output "heads". The first output head produces embedding vectors for each pixel. Ideally, embedding vectors which are similar are more likely to belong to the same object instance. In the second head, the model predicts a class label for the mask centered at each pixel, as well as a confidence score that this pixel would make a good "seed" for creating a mask. We sketch the overall model in <ref type="figure">Figure 1</ref>, and give the details below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Embedding model</head><p>We start by learning an embedding space, so that pixels that correspond to the same object instance are close, and pixels that correspond to different objects (including the background) are far. The embedding head of our network takes as input a feature map from a convolutional feature extractor (see Section 3.5). It outputs a [h, w, d] tensor (as shown in <ref type="figure">Figure 1)</ref>, where h is the image height, w is the image width and d is the embedding space dimension (we use 64 dimensional embeddings in our experiments). Thus each pixel p in image is represented by a d-dimensional embedding vector e p .</p><p>Given the embedding vectors, we can compute the similarity between pixels p and q as follows:</p><formula xml:id="formula_1">?(p, q) = 2 1 + exp(||e p ? e q || 2 2 )<label>(1)</label></formula><p>We see that for pairs of pixels that are close in embedding space, we have ?(p, q) = 2 1+e 0 = 1, and for pairs of pixels that are far in embedding space, we have ?(p, q) = 2 1+e ? = 0.</p><p>We train the network by minimizing the following loss:</p><formula xml:id="formula_2">L e = ? 1 |S| p,q?S w pq 1 {yp=yq} log(?(p, q)) + 1 {yp =yq} log(1 ? ?(p, q))</formula><p>where S is the set of pixels that we choose, y p is the instance label of pixel p, and w pq is the weight of the loss on the similarity between p and q. The weights w pq are set to values inversely proportional to the size of the instances p and q belong to, so the loss will not become biased towards the larger examples. We normalize weights so that p,q w pq = 1. During training, we choose the set of pixels S by randomly sampling K points for each object instance in the image. For each pair of points, we compute the target label, which is 1 if they are from the same instance, and 0 otherwise, as shown in <ref type="figure">Figure 2</ref>. We then minimize the crossentropy loss for the |S| 2 set of points. Our overall procedure is closely related to the N-pairs loss used in <ref type="bibr" target="#b26">[27]</ref> for metric learning. <ref type="figure" target="#fig_0">Figure 3</ref> illustrates the learned embedding for few example images. We randomly project the 64d vectors to RGB space and then visualize the resulting false-color image. We see that instances of the same class (e.g., the two people or the two motorbikes) get mapped to different parts of embedding space, as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Creating masks</head><p>Once we have an embedding space, and hence a pairwise similarity metric, we create a set of masks in the following way. We pick a "seed" pixel p, and then "grow" the seed by finding all the other pixels q that have a similarity with p greater than a threshold ? : m(p, ? ) = {q : ?(p, q) ? ? }. Ideally, all the pixels in the mask belong to the same object as the seed p. By varying ? , we can detect objects of different sizes. In our experiments, we use ? ? {0.25, 0.5, 0.75}. (We also use a multi-scale representation of the image as input, as we discuss in Section 3.6.)</p><p>We can implement this method efficiently as follows. First we compute a tensor A of size [h, w, d] (where h is the height of the image, w is the width, and d is the emebdding dimension) representing the embedding vector for every pixel. Next we compute a second tensor B of size [k, d], representing the embedding vector of the K seed points. We can compute the distance of each vector in A to each vector in B using A 2 + B 2 ? 2A B. We can then select all the pixels that are sufficiently similar to each of the seeds by thresholding this distance matrix. <ref type="figure">Fig. 4</ref> shows examples of mask growing from randomly picked seed pixel. The seed pixel is noted by a red mark in the image. The similarity between every pixel and the <ref type="figure">Figure 4</ref>. We visualize the similarity of each pixel and a randomly chosen seed pixel in each image. The randomly chosen seed pixel is shown by a red mark in the image. The brighter the pixels the higher the similarity. seed pixel is computed based on Eq. 1 and shown in each picture. One can threshold the similarity values to generate a binary mask.</p><p>However, we still need a mechanism to choose the seeds. We propose to learn a "seediness" heatmap S p , which tells us how likely it is that a mask grown from p will be a good mask (one that overlaps with some ground truth mask by more than an IoU threshold). We discuss how to compute the seediness scores in Section 3.4.</p><p>Once we have the seediness heatmap, we can greedily pick seed points based on its modes (peaks). However, we also want to encourage spatial diversity in the locations of our seed points, so that we ensure our proposals have high coverage of the right set of masks. For this reason, we also compute the distance (in embedding space) between each point and all previously selected points and choose one that <ref type="figure">Figure 5</ref>. Visualization of sampled seed pixels. Our method leverages the learned distance metric and masks classification scores to sample high-quality seed pixels that result in instance segments that have high recall and precision.</p><p>is far from the already picked points (similar to the heuristic used by Kmeans++ initialization <ref type="bibr" target="#b0">[1]</ref>). More precisely, at step t of our algorithm, we select a seed point as follows:</p><formula xml:id="formula_3">p t = arg max p ?p1:t?1 [log(S p ) + ? log(D(p, p 1:t?1 )] where D(p, p 1:t?1 ) = min q?p1:t?1 ||e p ? e q || 2 2</formula><p>Selecting seed pixels with high seediness score guarantees high precision, and selecting diverse seed points guarantees high recall. Note that our sampling strategy is different from the non-maximum suppression algorithm. In NMS, points that are close in x-y image coordinate space are suppressed, while in our algorithm, we encourage diversity in the embedding space.</p><p>Once we have selected a seed, we select the best threshold ? , and then we can convert it into a mask, m t = m(p t , ? ), as we described above. Finally, we attach a confidence score s t and a class label c t to the mask. To do this, we leverage a semantic-segmentation model which associates a predicted class label with every pixel, as we explain in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Classification and seediness model</head><p>The mask classification head of our network takes as input a feature map from a convolutional feature extractor (see Section 3.5) and outputs a [h, w, C + 1] tensor as shown in <ref type="figure">Fig. 1</ref>, where C is the number of classes, and label 0 represents the background. In contrast to semantic segmentation, where the pixels themselves are classified, here we classify the mask that each pixel will generate if chosen as a seed. For example, assume a pixel falls inside an instance of a horse. Semantic segmentation will produce a high score for class "horse" at that pixel. However, our method might instead predict background, if the given pixel is not a good seed point for generating a horse mask. We show examples of mask classification heatmaps in <ref type="figure">Fig. 6</ref>. We see that most pixels inside the object make good seeds, but pixels near the boundaries (e.g., on the legs of the cows) are not so good.</p><p>We train the model to emulate this behavior as follows. For each image, we select K = 10 random points per object instance and grow a mask around each one. Let m(p, ? ) be the mask generated from pixel p, for a given similarity threshold ? . If the proposed mask overlaps with one of the ground truth masks by more than some fixed IoU threshold, we consider this a "good" proposal; we then copy the label from the ground truth mask and assign it to pixel p. If the generated mask does not sufficiently overlap with any of the ground truth masks, we consider this a "bad" proposal and assign the background label to pixel p. We then convert the assigned labels to one-hot form, and train using softmax cross-entropy loss, for each of the K chosen points per object instance. The classification model is fully convolutional, but we only evaluate the loss at N K points, where N is the number of instances. Thus our overall loss function has the form</p><formula xml:id="formula_4">L cls = ? 1 |S| p?S C c=0 y pc log C pc</formula><p>where C pc is the probability that the mask generated from seed pixel p belongs to class c. To handle objects of different sizes, we train a different classification model for each value of ? ; in our experiments, we use T = {0.25, 0.5, 0.75, 0.9}. Specifically, let C ? pc represent the probability that pixel p is a good seed for an instance class c when using similarity threshold ? .</p><p>We now define the "seediness" of pixel p to be</p><formula xml:id="formula_5">S p = max ? ?T C max c=1</formula><p>C ? pc (Note that the max is computed over the object classes and not on the background class.) Thus we see that the seediness tensor is computed from the classification tensor.</p><p>To understand why this is reasonable, suppose the background score at pixel p is very high, say 0.95. Then the max over foreground classes is going to be smaller than 0.05 (due to the sum-to-one constraint on C pc ), which means this is not a valid pixel to generate a mask. But if the max value is, say, 0.6, this means this is a good seed pixel, since it will grow into an instance of foreground class with probability 0.6.</p><p>Once we have chosen the best seed according to S p , we can find the corresponding best threshold ? , and label c by computing (? p , c p ) = arg max ? ?T ,c?1:C C ? pc <ref type="figure">Figure 6</ref>. Visualization of mask classification scores. The color at each pixel identifies the label of the mask that will be chosen if that pixel is chosen as a seed point. The pixels that are more likely to generate background masks are colored white. Darker colors correspond to pixels that will generate poor quality foreground masks. Brighter colors correspond to pixels that generate high quality masks.</p><p>The corresponding confidence score is given by</p><formula xml:id="formula_6">s p = C ?p p,cp</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Shared full image convolutional features</head><p>Our base feature extractor is based on the DeepLab v2 model <ref type="bibr" target="#b2">[3]</ref>, which in turn is based on resnet-101 <ref type="bibr" target="#b10">[11]</ref>. We pre-train this for semantic segmentation on COCO, as is standard practice for methods that compete on PASCAL VOC, and then "chop off" the final layer. The Deeplab v2 model is fully convolutional and runs on an image of size [2h, 2w, 3] outputs a [ h 4 , w 4 , 2048] sized feature map which is the input to both the embedding model and the classification/seediness model.</p><p>We can jointly train both output "heads", and backprop into the shared "body", by defining the loss</p><formula xml:id="formula_7">L = L e + ?L cls</formula><p>where L e is the embedding loss, L cls is the classification loss, and ? is a balancing coefficient. In our experiments, we initially set ? to 0, to give the embedding model time to learn proper embeddings. Once we have a reasonable similarity metric, we can start to learn the mask classification loss. We gradually increase ? to a maximum value of 0.2 (this was chosen empirically based on the validation set). Both losses get back-propagated into the same feature extractor. However, since we train on Pascal VOC, which is a small dataset, we set the learning rate of the shared features to be smaller than for the two output heads, so the original features (learned on COCO) do not change much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Handling scale</head><p>To handle objects of multiple scales, we compute an image pyramid at 4 scales (0.25, 0.5, 1.0, 2.0), and then run it through our feature extractor These feature maps are then rescaled to the same size, and averaged. Finally, the result is fed into the two different heads, as explained above. (In the future, we would like to investigate more efficient methods, such as those discussed in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>. that avoid having to run the base model 4 times.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, we discuss our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We follow the experimental protocol that is used by previous state of the art methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref>. In particular, we train on the PASCAL VOC 2012 training set, with additional instance mask annotation from <ref type="bibr" target="#b8">[9]</ref>, and we evaluate on the PASCAL VOC 2012 validation set.</p><p>After training the model, we compute a precision-recall curve for each class, based on all the test data. This requires a definition of what we mean by a true and false positive. We follow standard practice and say that a predicted mask that has an intersection over union IoU with a true mask above some threshold ? (e.g. 50%) is a true positive, unless the true mask is already detected, in which case the detection is a false positive. We provide results for three IoU thresholds: {0.5, 0.6, 0.7} similar to previous work. We then compute the area under the PR curve, known as the "average precision" or AP r score <ref type="bibr" target="#b8">[9]</ref>. Finally, we average this over classes, to get the mean average precision or mAP r score.</p><p>We can also evaluate the quality of our method as a "class agnostic" region proposal generator. There are two main ways to measure quality in this case. The first is to plot the recall (at a fixed IoU) vs the number of proposals. The second is to plot the recall vs the IoU threshold for a fixed number of proposals; the area under this curve is known as the "average recall" or AR <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Preprocessing</head><p>We use the following data augmentation components during training:</p><p>Random Rotation: We rotate the training images by a random degree in the range of [?10, 10].</p><p>Random Resize: We resize the input image during the training phase with a random ratio in the range of [0.7, 1.5].</p><p>Random Crop: We randomly crop the images during the training phase. At each step, we randomly crop 100 windows. We randomly sample one image weighted towards  - -  <ref type="table">Table 2</ref>. We find out that the best performing ? parameter for sampling seed points is 0.3. In the table, we compare the mAPr performance for different values of ?.  <ref type="table">Table 3</ref>. We analyze the performance of our model given the number of sampled seed points. cropped windows that have more object instances inside.</p><formula xml:id="formula_8">- - - - - - - - - - - - - - - - - - - 63.5 Li et al. [15] - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_9">- - - - - - - - - - - - - - - - - - - 41.5 Li et al. [15] - - - - - - - - - - - - - - - - - - - -</formula><p>Random Flip: We randomly flip our training images horizontally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Pascal VOC 2012</head><p>We first tried different values of ? (which trades off diversity with seediness when picking the next seed) to find the best performing seed sampling strategy. The results are shown for different values of ? in <ref type="table">Table 2</ref>. We also tried various sizes of embeddings from 2 to 256; 64 was best (on a validations set). We furthermore analyze the performance of our model given different number of mask proposals (number of sampled seed points) in <ref type="table">Table 3</ref>. Our model reaches a mAP r performance of 59.7 by only proposing 10 regions. In <ref type="table">Table 3</ref>, we also show the class agnostic average recall for different number of mask proposals. <ref type="figure" target="#fig_1">Figure 7</ref> shows some qualitative results, and <ref type="table" target="#tab_1">Table 1</ref> shows our quantitative results. In terms of mAP performance, we rank 4th at 0.5 IoU threshold, 2nd at 0.6 IoU threshold, and tied 3rd at 0.7 IoU threshold. So our method is competitive, if not state of the art.</p><p>Regarding performance for individual classes, we see that we do very well on large objects, such as trains, dogs, and motorbikes, but we do very poorly in the bicycle category. (See <ref type="figure">Fig. 6</ref> for some examples.) This is also true of the other methods. The reason is that there is a large discrepancy between the quality of the segmentation masks in the training set of <ref type="bibr" target="#b8">[9]</ref> compared to the PASCAL test set. In particular, the training set uses a coarse mask covering the entire bicycle, whereas the test set segments out each spoke of the wheels individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>We have proposed a novel approach to the semantic instance segmentation problem, and obtained promising preliminary results on the PASCAL VOC dataset. In the future, we would like to evaluate our method on COCO and Cityscapes. We would also like to devise a way to perform region growing in a differentiable way so that we can perform end-to-end training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of the embedding vectors by randomly projecting the 64 dimensional vectors into RGB space. The learned metric will move different instances of the same object category to different locations in embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 .</head><label>7</label><figDesc>Example instance segmentation results from our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.5 61.0 63.0 65.2 66.7 68.4 69.8 70.8 71.9 77.4 82.2 83.6 mAP r with ? = 0.2 59.7 60.7 60.7 60.9 61.2 61.3 61.1 61.1 61.0 61.0 61.4 61.5 61.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Per-class instance-level segmentation comparison using APr metric over 20 classes at 0.5, 0.6 and 0.7 IoU on the PASCAL VOC 2012 validation set. All numbers are in %.</figDesc><table><row><cell>IoU score</cell><cell>Method</cell><cell>plane</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>motor</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>average</cell></row><row><cell></cell><cell>SDS [10]</cell><cell cols="20">58.8 0.5 60.1 34.4 29.5 60.6 40.0 73.6 6.5 52.4 31.7 62.0 49.1 45.6 47.9 22.6 43.5 26.9 66.2 66.1</cell><cell>43.8</cell></row><row><cell>0.5</cell><cell>Chen et al. [4]</cell><cell cols="20">63.6 0.3 61.5 43.9 33.8 67.3 46.9 74.4 8.6 52.3 31.3 63.5 48.8 47.9 48.3 26.3 40.1 33.5 66.7 67.8</cell><cell>46.3</cell></row><row><cell></cell><cell>PFN [17]</cell><cell cols="20">76.4 15.6 74.2 54.1 26.3 73.8 31.4 92.1 17.4 73.7 48.1 82.2 81.7 72.0 48.4 23.7 57.7 64.4 88.9 72.3</cell><cell>58.7</cell></row><row><cell></cell><cell>MNC [6]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>mAP r 61.4 61.7 62.1 61.6 61.6 61.5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.1</cell></row><row><cell></cell><cell>R2-IOS [16]</cell><cell cols="20">54.5 0.3 73.2 34.3 38.4 71.1 54.0 76.9 6.0 63.3 13.1 67.0 26.9 39.2 33.2 25.4 44.8 45.4 81.5 74.6</cell><cell>46.2</cell></row><row><cell cols="3">Assoc. Embedding [19] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.0</cell></row><row><cell></cell><cell>Ours</cell><cell cols="20">53.0 0.0 51.8 24.9 21.9 69.2 40.1 76.6 4.1 43.1 21.1 74.4 44.7 54.3 40.3 7.5 40.5 39.6 69.5 52.6</cell><cell>41.5</cell></row><row><cell>?</cell><cell cols="6">0.1 0.2 0.3 0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. symposium on discrete algorithms</title>
		<meeting>symposium on discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-instance object segmentation with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3470" to="3478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient Graph-Based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">FastMask: Segment multi-scale object candidates in one shot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">InstanceCut: from edges to instances with Multi-Cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07709</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reversible recursive instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Associative Embedding:End-to-End learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-End instance segmentation with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
