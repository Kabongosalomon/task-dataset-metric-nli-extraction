<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Siamese Network for Dense Unsupervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<email>chenkai@pjlab.org.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ntu.edu.sgpangjiangmiao</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dense Siamese Network for Dense Unsupervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents Dense Siamese Network (DenseSiam), a simple unsupervised learning framework for dense prediction tasks. It learns visual representations by maximizing the similarity between two views of one image with two types of consistency, i.e., pixel consistency and region consistency. Concretely, DenseSiam first maximizes the pixel level spatial consistency according to the exact location correspondence in the overlapped area. It also extracts a batch of region embeddings that correspond to some sub-regions in the overlapped area to be contrasted for region consistency. In contrast to previous methods that require negative pixel pairs, momentum encoders or heuristic masks, DenseSiam benefits from the simple Siamese network and optimizes the consistency of different granularities. It also proves that the simple location correspondence and interacted region embeddings are effective enough to learn the similarity. We apply DenseSiam on ImageNet and obtain competitive improvements on various downstream tasks. We also show that only with some extra task-specific losses, the simple framework can directly conduct dense prediction tasks. On an existing unsupervised semantic segmentation benchmark, it surpasses state-of-the-art segmentation methods by 2.1 mIoU with 28% training costs. Code and models are released at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dense prediction tasks, such as image segmentation and object detection, are fundamental computer vision tasks with many real-world applications. Beyond conventional supervised learning methods, recent research interests grow in unsupervised learning to train networks from large-scale unlabeled datasets. These methods either learn representations as pre-trained weights then fine-tune on downstream tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b55">56]</ref> or directly learn for specific tasks <ref type="bibr" target="#b10">[11]</ref>.</p><p>In recent years, unsupervised pre-training has attracted a lot of attention. Much effort has been geared to learn global representation for image classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. As the global average pooling in these methods discards spatial information, it has been observed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b55">56]</ref> that the learned representations are sub-optimal for dense uses. Naturally, some attempts conduct similarity The pair of images are first processed by the same encoder network and a convolutional projector. Then a predictor is applied on one side, and a stop-gradient operation on the other side. A grid sampling method is used to extract dense predictions in the overlapped area. Dens-eSiam can perform unsupervised semantic segmentation by simply adding a segmentation loss with pseudo labels produced by the projector.</p><p>learning at pixel-level <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref> or region-level <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> and maintain the main structures in global ones to learn representations for dense prediction tasks.</p><p>Despite the remarkable progress in that field, the development in unsupervised learning for specific tasks is relatively slow-moving. For example, solutions in unsupervised semantic segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref> rely more on clustering methods (such as k-means) that are also derived from unsupervised image classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28]</ref>. When reflecting on these similar tasks from the perspective of unsupervised learning, we observe these tasks share the inherent goal of maximizing the similarity of dense predictions (either labels or embeddings) across images but differ in the task-specific training objectives.</p><p>In this paper, we propose Dense Siamese Network (DenseSiam) to unanimously solve these dense unsupervised learning tasks within a single framework ( <ref type="figure" target="#fig_0">Fig. 1</ref>). It learns visual representations by maximizing two types of consistency, i.e., pixel consistency and region consistency, with methods dubbed as PixSim and RegionSim, respectively. The encoder here can be directly fine-tuned for various downstream dense prediction tasks after unsupervised pre-training. By adding an extra segmentation loss to the projector and regarding the argmaxed prediction of projector as pseudo labels, the encoder and projector is capable of learning class-wise representations for unsupervised semantic segmentation.</p><p>Specifically, PixSim learns to maximize the pixel-level spatial consistency between the grid sampled predictions. Its training objective is constrained under the exact location correspondence. In addition, the projected feature maps are multiplied with the features from encoder to generate a batch of region embeddings on each image, where each region embedding corresponds to a sub-region in the overlapped area. RegionSim then conducts contrastive learning between pairs of region embeddings and optimizes them to be consistent.</p><p>In contrast to previous unsupervised pre-training methods for dense prediction tasks, DenseSiam benefits from the simple Siamese network <ref type="bibr" target="#b9">[10]</ref> that does not have negative pixel pairs <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b54">55]</ref> and momentum encoders <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b54">55]</ref>. Uniquely, DenseSiam optimizes the consistency of different granularities. The optimization for each granularity is simple as it neither requires heuristic masks <ref type="bibr" target="#b24">[25]</ref> nor manual regions crops <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. <ref type="table">Table 1</ref>: Comparison of unsupervised dense representation learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Base Pixel Region Extra Components Correspondence DenseCL <ref type="bibr" target="#b47">[48]</ref> MoCo v2 <ref type="bibr" target="#b8">[9]</ref> ? ? ? feature similarity PixPro <ref type="bibr" target="#b54">[55]</ref> BYOL <ref type="bibr" target="#b19">[20]</ref> ? ? ? coordinate distance VaDER <ref type="bibr" target="#b37">[38]</ref> MoCo v2 ? ? ? location ReSim <ref type="bibr" target="#b51">[52]</ref> MoCo v2 ? ? ? dense region crops SCRL <ref type="bibr" target="#b39">[40]</ref> BYOL <ref type="bibr" target="#b19">[20]</ref> ? ? ? sampled region crops DetCo <ref type="bibr" target="#b52">[53]</ref> MoCo v2 ? ? ? image patches SoCo <ref type="bibr" target="#b48">[49]</ref> BYOL ? ? selective search <ref type="bibr" target="#b44">[45]</ref> sampled region crops DetCon <ref type="bibr" target="#b24">[25]</ref> SimCLR <ref type="bibr" target="#b7">[8]</ref> ? ? FH masks <ref type="bibr" target="#b16">[17]</ref> heuristic masks CAST <ref type="bibr" target="#b42">[43]</ref> MoCo Extensive experiments show that DenseSiam is capable of learning strong representation for dense prediction tasks. DenseSiam obtains nontrivial 0.4 AP mask , 0.7 mIoU, and 1.7 AP improvement in comparison with SimSiam when transferring its representation on COCO instance segmentation, Cityscapes semantic segmentation, and PASCAL VOC detection, respectively. For unsupervised semantic segmentation, DenseSiam makes the first attempt to discard clustering <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref> while surpassing previous state-of-the-art method <ref type="bibr" target="#b10">[11]</ref> by 2.1 mIoU with only ?28% of the original training costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Siamese Network. Siamese network <ref type="bibr" target="#b2">[3]</ref> was proposed for comparing entities. They have many applications including object tracking <ref type="bibr" target="#b1">[2]</ref>, face verification <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref>, and one-shot recognition <ref type="bibr" target="#b28">[29]</ref>. In conventional use cases, Siamese Network takes different images as input and outputs either a global embedding of each image for comparison <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> or outputs feature map of each image for cross-correlation <ref type="bibr" target="#b1">[2]</ref>. DenseSiam uses the Siamese architecture to output pixel embeddings and maximizes similarity between embeddings of the same pixel from two views of an image to pre-train dense representations with strong transferability in dense prediction tasks. Unsupervised Representation Learning. Representative unsupervised representation learning methods include contrastive learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b50">51]</ref> and clustering-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b56">57]</ref>. Notably, Siamese network has become a common structure in recent attempts <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref>, despite their different motivations and solutions to avoid the feature 'collapsing' problem. SimSiam <ref type="bibr" target="#b9">[10]</ref> studies the minimum core architecture in these methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref> and shows that a simple Siamese network with stopping gradient can avoid feature 'collapsing' and yield strong representations.</p><p>Given the different natures of image-level representation learning and dense prediction tasks, more recent attempts <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref> pre-train dense representations specially designed for dense prediction tasks. Most of these methods conduct similarity learning with pixels <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref>, manually cropped patches of image or features <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref>, and regional features segmented by saliency map <ref type="bibr" target="#b42">[43]</ref> or segmentation masks <ref type="bibr" target="#b24">[25]</ref> obtained in unsupervised manners. These methods still need a momentum encoder <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55]</ref> or negative pixel samples <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref>, although they <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref> have been proven unnecessary <ref type="bibr" target="#b9">[10]</ref> in image-level representation learning.</p><p>DenseSiam only needs a Siamese network with a stop gradient operation to obtain strong dense representations, without momentum encoder <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55]</ref> nor negative pixel pairs <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref>  <ref type="table">(Table 1</ref>). It conducts contrastive learning among regional features inside an image emerged via pixel similarity learning. Thanks for the unique design, DenseSiam performs region-level similarity learning without saliency maps <ref type="bibr" target="#b42">[43]</ref>, region crops <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>, nor heuristic masks <ref type="bibr" target="#b24">[25]</ref>. Unsupervised Semantic Segmentation. Unsupervised semantic segmentation aims to predict labels for each pixel without annotations. There are a few attempts that introduce heuristic masks and conduct similarity learning between segments (regions) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b46">47]</ref> with object-centric images. Most methods focus on natural scene images and exploit the assumption that semantic information should be invariant to photometric transformations and equivariant to geometric transformations no matter how the model predicts the labels, which is inherently consistent with the goal of unsupervised representation learning. However, these methods still rely on clustering <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref> to predict the per-pixel labels and maximize the consistency of cluster assignments in different views of the image, which is cumbersome and difficult to be used for large-scale data.</p><p>DenseSiam conducts unsupervised semantic segmentation by adding a classbalanced cross entropy loss without clustering, significantly reduces the training costs and makes it scalable for large-scale data. RegionSim further boosts the segmentation accuracy by maximizing the consistency between regional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dense Siamese Network</head><p>DenseSiam, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, is a generic unsupervised learning framework for dense prediction tasks. The framework is inspired by SimSiam <ref type="bibr" target="#b9">[10]</ref> but differs in its formulation tailored for learning dense representation (Sec. 3.1). In particular, it conducts dense similarity learning by PixSim (Sec. 3.2), which aims at maximizing pixel-level spatial consistency between grid sampled predictions. Based on the region embeddings derived from per-pixel predictions inferred in PixSim, DenseSiam further performs region-level contrastive learning through RegionSim (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Siamese Dense Prediction</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, DenseSiam takes two randomly augmented views x 1 and x 2 of an image x as inputs. The two views are processed by an encoder network f and a projector network g. The encoder network can either be a backbone network like ResNet-50 <ref type="bibr" target="#b23">[24]</ref> or a combination of networks such as ResNet-50 with FPN <ref type="bibr" target="#b29">[30]</ref>. As the encoder network f does not use global average pooling (GAP), the output of of an image x as inputs. The two views are processed by an encoder network f without global average pooling (GAP), followed by a projector network g. The predictor network h transforms the dense prediction of the projector of one view. Then GridSample module samples the same pixels inside the intersected regions of the two views and interpolates their feature grids. The similarity between feature grids from two views are maximized by PixSim. Then DenseSiam multiplies the features of encoder and the dense predictions by projector g in the overlapped area to obtain region embeddings of each image. These region embeddings are processed by a new projector g ? and new predictor h ? for region level similarity learning.</p><p>f is a dense feature map and is later processed by the projector g, which consists of three 1 ? 1 convolutional layers followed by Batch Normalization (BN) and ReLU activation. Following the design in SimSiam <ref type="bibr" target="#b9">[10]</ref>, the last BN layer in g does not use learnable affine transformation <ref type="bibr" target="#b26">[27]</ref>. The projector g projects the per-pixel embeddings to either a feature space for representation learning or to a labels space for segmentation. Given the dense prediction from g, noted as z 1 ? g(f (x 1 )), the predictor network h transforms the output of one view and matches it to another view <ref type="bibr" target="#b9">[10]</ref>. Its output is denoted as p 1 ? h(g(f (x 1 ))).</p><p>The encoder and projector essentially form a Siamese architecture, which outputs two dense predictions (z 1 and z 2 ) for two views of an image, respectively. DenseSiam conducts similarity learning of different granularities using the Siamese dense predictions with the assistance of predictor <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>. After unsupervised pre-training, only the encoder network is fine-tuned for downstream dense prediction tasks, where the projector is only used during pre-training to improve the representation quality <ref type="bibr" target="#b7">[8]</ref>. When directly learning the dense prediction tasks, the encoder and projector are combined to tackle the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PixSim</head><p>Dense Similarity Learning. We formulate the dense similarity learning to maximize the similarity of dense predictions inside the overlapping regions between p 1 and z 2 . Specifically, given the relative coordinates of the intersected region in x 1 and x 2 , we uniformly sample K ? K point grids inside the intersected region from both views as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. These points in two views have exactly the same coordinates when they are mapped to the original image x; thus, their predictions should be consistent to those in another view. Assuming the sampled feature grids are z ? 1 ? gridsample(z 1 ) and p ? 1 ? gridsample(p 1 ), PixSim minimizes the distance of the feature grids by a symmetrical loss <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>:</p><formula xml:id="formula_0">L dense = 1 2 D(p ? 1 , stopgrad(z ? 2 )) + 1 2 D(p ? 2 , stopgrad(z ? 1 )),<label>(1)</label></formula><p>where stopgrad is the stop-gradient operation to prevent feature 'collapsing' <ref type="bibr" target="#b9">[10]</ref> and D is a distance function that can have many forms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. Two representative distance functions are negative cosine similarity <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref> </p><formula xml:id="formula_1">D(p ? 1 , z ? 2 )= ? p ? 1 ?p ? 1 ? 2 ? z ? 2 ?z ? 2 ? 2 ,<label>(2)</label></formula><p>and cross-entropy similarity</p><formula xml:id="formula_2">D(p ? 1 , z ? 2 )= ? softmax(p ? 1 )? log softmax(z ? 2 ).<label>(3)</label></formula><p>Advantages to Image-Level Similarity Learning. Previous formulation of image-level representation learning faces two issues when transferring their representations for dense prediction tasks. First, it learns global embeddings for each image during similarity learning, where most of the spatial information is discarded during GAP. Hence, the spatial consistency is not guaranteed in its goal of representation learning, while dense prediction tasks like semantic segmentation and object detection rely on the spatial information of each pixel. Second, it is common that p 1 and z 2 contain different contents (see examples of x 1 and x 2 in <ref type="figure" target="#fig_1">Fig. 2</ref>) after heavy data augmentations like random cropping and resizing, but the global embeddings that encode these different contents are forced to be close to each other during training. This forcefully makes the embeddings of different pixel groups to be close to each other, breaking the regional consistency and is thus undesirable for dense prediction tasks. PixSim resolves the above-mentioned issues by conducting pixel-wise similarity learning inside the intersected regions of two views, where the embeddings at identical locations of different views have different values due to augmentations <ref type="bibr" target="#b7">[8]</ref> are forced to be similar to each other in training. Such a process learns dense representations that are invariant to data augmentations, which are more favorable dense representations for dense prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RegionSim</head><p>When using cross-entropy with softmax as the training objective, PixSim implicitly groups similar pixels together into the same regions/segments. The consistency of such regions can be further maximized by the proposed RegionSim to learn better dense representation. The connection between PixSim and Re-gionSim is seamless -region-level similarity learning can be achieved without heuristic masks <ref type="bibr" target="#b24">[25]</ref>, saliency map <ref type="bibr" target="#b42">[43]</ref>, or manually cropping multiple regional features <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> as in previous studies. Region Embedding Generation. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, DenseSiam first obtains the feature grids of the intersected regions of each view by grid sampling from the features produced by the encoder f . This restricts RegionSim inside the intersected regions to ensure that the region embeddings to encode the similar contents. For simplicity, the region embeddings e 1 ? R N ?C are obtained by the summation of multiplication between the masks and features as</p><formula xml:id="formula_3">e 1 = K?K i,j z ? 1 [i, j] ? gridsample(f (x 1 ))[i, j],<label>(4)</label></formula><p>where N and C represent the number of sub-regions and the number of feature channels, respectively. The process yields an embedding for each segment of each pseudo category, which will then be used for contrastive learning to increase the consistency between these region embeddings.</p><p>Region Similarity Learning. After obtaining the region embeddings, Region-Sim transforms them by a projector network g ? , which is a three-layer MLP head <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref>. The output of g ? is then transformed by h ? for contrasting or matching with another view. RegionSim assumes each region embedding of a sub-region to be consistent with the embedding of the sub-region in another view. Therefore, Eq. 1 can be directly applied to these region embeddings to conduct region-level similarity learning. Meanwhile, we also enforce the region embedding to have low similarities with the embeddings of other region embeddings to make the feature space more compact. Consequently, by denoting the two outputs as u 1 ? h ? (g ? (e 1 )) and v 2 ? g ? (e 2 ), RegionSim can also minimize the symmetrized loss function</p><formula xml:id="formula_4">L region = 1 2 L c (u 1 , v 2 )+ 1 2 L c (u 2 , v 1 ).<label>(5)</label></formula><p>The contrastive loss function <ref type="bibr" target="#b34">[35]</ref> L c can be written as</p><formula xml:id="formula_5">L c (u 1 , v 2 )= ? N s log exp(u s 1 ? v s 2 ) N s ? exp(u s 1 ? v s ? 2 ) .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning Objective</head><p>When conducting unsupervised representation learning, DenseSiam also maintains a global branch to conduct image-level similarity learning to enhance the global consistency of representations, besides PixSim and RegionSim shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The architecture of the global branch remains the same as SimSiam <ref type="bibr" target="#b9">[10]</ref>. The numbers of channels in the projectors and predictors of PixSim and Region-Sim are set to 512 for efficiency. Denoting the loss of the global branch as L sim , DenseSiam optimizes the following loss</p><formula xml:id="formula_6">L=L sim + ? 1 L dense + ? 2 L region ,<label>(7)</label></formula><p>where ? 1 and ? 2 are loss weights of L dense for PixSim and L region for Region-Sim, respectively. The encoder f is used for fine-tuning in downstream dense prediction tasks after pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Unsupervised Semantic Segmentation</head><p>The proposed framework is appealing in that it can be readily extended to address dense prediction tasks such as unsupervised semantic segmentation, by simply adding a task-specific losses and layers, without needing the offline and cumbersome clustering process <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref>. Formally, when using cross-entropy similarity (Eq. 3) in PixSim, the softmax output softmax(z) can be regarded as the probabilities of belonging to each of N pseudo-categories. In such a case, the projector g predicts a label for each pixel, which aligns with the formulation for unsupervised semantic segmentation.</p><p>DenseSiam uses ResNet with a simplified FPN <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref> as the encoder g. The output channel N of PixSim is modified to match the number of classes in the dataset (e.g., 27 on COCO stuff-thing dataset <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref>). Our experiment shows that a direct change in the number of output channels in PixSim without any further modification can already yield reasonable performance without feature 'collapsing'. Following previous method <ref type="bibr" target="#b10">[11]</ref> that encourages the prediction scores to have a lower entropy (i.e., more like one-hot scores), we add a cross entropy loss, denoted as L seg , when applying PixSim for unsupervised semantic segmentation with the pseudo labels obtained by argmax(z 1 ).</p><p>We also observe that the small number of categories undermines the training stability, which is also a common issue in clustering-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref>. To solve this issue, DenseSiam introduces another set of projector and predictor in PixSim to keep a large number of pseudo categories following the over-clustering strategy <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref>. This head only conducts similarity learning using Eq. 1, noted as L aux . Therefore, the overall loss of DenseSiam for unsupervised semantic segmentation is calculated as</p><formula xml:id="formula_7">L=? 1 L dense + ? 2 L region + ? 3 L seg + ? 4 L aux .<label>(8)</label></formula><p>We use ? 4 = log N log N +log Naux and ? 1 = log Naux log N +log Naux to prevent the auxiliary loss from overwhelming the gradients because it uses a larger number N aux of pseudo categories <ref type="bibr" target="#b10">[11]</ref>. Note that RegionSim is only used in training to enhance the region consistency of dense labels predicted by PixSim. Only the encoder f and projector g are combined to form a segmentation model at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets. For unsupervised pre-training, we compare with other methods on ImageNet-1k <ref type="bibr" target="#b40">[41]</ref> (IN1k) dataset and conduct ablation studies on COCO <ref type="bibr" target="#b30">[31]</ref> dataset as it is smaller. COCO and IN1k are two large-scale datasets that contain ?118K and ?1.28 million images, respectively. Though having more images, IN1k is highly curated and it mainly consists of object-centric images. Thus, the data is usually used for image classification. In contrast, COCO contains more diverse scenes in the real world and it has more objects (?7 objects vs. ?1 in IN1k) in one image. Hence, it is mainly used for dense prediction tasks like object detection, semantic, instance, and panoptic segmentation.</p><p>For unsupervised semantic segmentation, the model is trained and evaluated on curated subsets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref> of COCO train2017 split and val2017 split, respectively. The training and validation set contain 49,629 images and 2,175 images, respectively. The semantic segmentation annotations include 80 thing categories and 91 stuff categories <ref type="bibr" target="#b3">[4]</ref>. We follow previous methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref> to merge these categories to form 27 (15 'stuff' and 12 'things') categories. Training Setup. We closely follow the pre-training settings of SimSiam <ref type="bibr" target="#b9">[10]</ref> when conducting unsupervised pre-training experiments of DenseSiam. Specifically, we use a learning rate of lr?BatchSize/256 following the linear scaling strategy <ref type="bibr" target="#b18">[19]</ref>, with a base lr = 0.05. The batch size is 512 by default. We use the cosine decay learning rate schedule <ref type="bibr" target="#b32">[33]</ref> and SGD optimizer, where the weight decay is 0.0001 and the SGD momentum is 0.9. We pre-train DenseSiam for 800 epochs on COCO and for 200 epochs on IN1k. ResNet-50 <ref type="bibr" target="#b23">[24]</ref> is used as default.</p><p>For unsupervised semantic segmentation, for a fair comparison, we follow PiCIE <ref type="bibr" target="#b10">[11]</ref> to adopt the backbone pre-trained on IN1k in a supervised manner. The backbone is then fine-tuned with DenseSiam for unsupervised semantic segmentation using the same base learning rate, weight decay, and SGD momentum as those used in unsupervised representation learning. The batch size is 256 by default. We empirically find the constant learning schedule works better during training. The model is trained for 10 epochs. ResNet-18 <ref type="bibr" target="#b23">[24]</ref> is used as default for a fair comparison with previous methods <ref type="bibr" target="#b10">[11]</ref>. Evaluation Protocol of Transfer Learning. For unsupervised pre-training, we evaluate the transfer learning performance of the pre-trained representations following Wang et al <ref type="bibr" target="#b47">[48]</ref>. We select different dense prediction tasks to comprehensively evaluate the transferability of the dense representation, including semantic segmentation, object detection, and instance segmentation. Challenging and popular dataset with representative algorithms in each task is selected.</p><p>When evaluating on semantic segmentation, we fine-tune a FCN [32] model and evaluate it with the Cityscapes dataset <ref type="bibr" target="#b12">[13]</ref>. The model is trained on the train fine set (2975 images) for 40k iterations and is tested on the val set. Strictly following the settings in MMSegmentation <ref type="bibr" target="#b11">[12]</ref>, we use FCN-D8 with a crop size of 769, a batch size of 16, and synchronized batch normalization. Results are averaged over five trials.</p><p>When evaluating on object detection, we fine-tune a Faster R-CNN <ref type="bibr" target="#b38">[39]</ref> with C4-backbone by 24k iterations on VOC 2007 trainval + 2012 train set and is tested in VOC 2007 test set. Results are reported as an average over five trials.</p><p>We also evaluate the representation on object detection and instance segmentation on COCO dataset <ref type="bibr" target="#b30">[31]</ref>. We fine-tune a Mask R-CNN with FPN <ref type="bibr" target="#b29">[30]</ref> with standard multi-scale 1x schedule <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50]</ref> on COCO train2017 split and evaluate it on COCO val2017 split. We apply synchronized batch normalization in backbone, neck, and RoI heads during training <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b47">48]</ref>. Evaluation of Unsupervised Semantic Segmentation. Since the model is trained without labels, a mapping between the model's label space and the ground truth categories needs be established. Therefore, we first let the model predicts on each image on the validation set, then we calculate the confusion matrix between the predicted labels and the ground truth classes. We use linear assignment to build a one-to-one mapping between the predicted labels and ground truth classes by taking the confusion matrix as the assignment cost. Then we calculate mean IoU over all classes based on the obtained mapping <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref>. To more comprehensively understand the model's behavior, we also report mean IoU of stuff and things classes, noted as mIoU St and mIoU Th , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Transfer Learning Results</head><p>The comparison on transfer learning in dense prediction tasks between Dens-eSiam and previous unsupervised representation learning methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52]</ref> is shown in <ref type="table" target="#tab_1">Table 2</ref>. The results of scratch, supervised, MoCo v2 <ref type="bibr" target="#b8">[9]</ref>, DenseCL <ref type="bibr" target="#b47">[48]</ref>, SimCLR <ref type="bibr" target="#b7">[8]</ref>, and BYOL <ref type="bibr" target="#b19">[20]</ref> are reported from DenseCL <ref type="bibr" target="#b47">[48]</ref>. For fair comparison, we fine-tune the model of ReSim-C4 <ref type="bibr" target="#b51">[52]</ref> using the similar setting (Sec. 5.1). The model checkpoint is released by the paper authors 1 . We report the results of SimSiam <ref type="bibr" target="#b9">[10]</ref>, DetCo <ref type="bibr" target="#b52">[53]</ref>, and PixPro <ref type="bibr" target="#b54">[55]</ref> based on our re-implementation. COCO Instance Segmentation and Detection. As shown in the first two columns of   <ref type="bibr" target="#b7">[8]</ref>, BYOL <ref type="bibr" target="#b19">[20]</ref> and Sim-Siam <ref type="bibr" target="#b9">[10]</ref> show considerable performance gap against dense unsupervised methods including DenseSiam, DenseCL, and ReSim, indicating that pre-training with image-level similarity learning is sub-optimal for dense prediction tasks.</p><p>PASCAL VOC Object Detection. We further compare DenseSiam with previous methods on PASCAL VOC <ref type="bibr" target="#b15">[16]</ref> object detection. We report the original metric AP 50 (AP calculated with IoU threshold 0.5) of VOC and further report COCO-style AP <ref type="bibr" target="#b49">[50]</ref> and AP 75 , which are stricter criteria in evaluating the detection performance. DenseSiam show a large improvement of 1.8 AP in comparison with SimSiam <ref type="bibr" target="#b9">[10]</ref>. Notably, DenseSiam exhibits considerable improvements on AP 75 than AP 50 , suggesting the effectiveness of DenseSiam in learning accurate spatial information. Its improvement over SimSiam is more than that of DetCo <ref type="bibr" target="#b52">[53]</ref> (0.8 AP) over MoCo v2, and is on-par with those of DenseCL <ref type="bibr" target="#b47">[48]</ref> and ReSim <ref type="bibr" target="#b51">[52]</ref> over MoCo v2 <ref type="bibr" target="#b8">[9]</ref>.</p><p>DenseCL, ReSim, and PixPro are 0.2, 0.2, and 1.0 AP better than Dens-eSiam, respectively. This phenomenon contradicts the results in the benchmarks of COCO dataset, although their improvements over their image-level counterparts are consistent across benchmarks. We hypothesize that the different backbones used in the two benchmarks leads to this phenomenon, where ResNet-50-C4 backbone <ref type="bibr" target="#b38">[39]</ref> is used on PASCAL VOC but ResNet-50-FPN <ref type="bibr" target="#b29">[30]</ref> is used on COCO. The hypothesis also explains the inferior performance of DetCo <ref type="bibr" target="#b52">[53]</ref>: <ref type="table">Table 3</ref>: Unsupervised Semantic Segmentation. The model is trained and tested on the curated COCO dataset <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref>. ' + aux.' denotes PiCIE or Dens-eSiam is trained with an auxiliary head DetCo conducts contrastive learning on pyramid features but only one feature scale is used when fine-tuning Faster R-CNN on PASCAL VOC dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Unsupervised Segmentation Results</head><p>In <ref type="table">Table 3</ref> we compare DenseSiam with previous state-of-the-art methods in unsupervised semantic segmentation. DenseSiam achieves new state-of-the-art performance of 16.4 mIoU, surpassing PiCIE by 2 mIoU over all classes. Imbalanced performance is observed in previous methods between thing and stuff classes, e.g., PiCIE <ref type="bibr" target="#b10">[11]</ref> and IIC <ref type="bibr" target="#b27">[28]</ref> surpass Modified DeepClustering <ref type="bibr" target="#b4">[5]</ref> on thing classes but fall behind on stuff classes. In contrast, DenseSiam consistently outperforms previous best results on both thing and stuff classes by more than 2 mIoU. The results reveal that clustering is unnecessary, whereas clustering is indispensable in previous unsupervised segmentation methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref>. We also calculate the training costs of PiCIE and DenseSiam by measuring the GPU hours used to train the model with similar batch size and backbone. Because PiCIE needs clustering to obtain pseudo labels before each training epoch, its training time is comprised of the time of label clustering, data loading, and model's forward and backward passes. In contrast, DenseSiam does not rely on clustering. In the setting of single GPU training, DenseSiam saves ?72% training costs in comparison with PiCIE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>Unsupervised Pre-training. We ablate the key components in DenseSiam as shown in <ref type="table" target="#tab_4">Table 4</ref>. The baselines in <ref type="table" target="#tab_4">Table 4a</ref>-e are SimSiam (53.5 AP). i) Effective grid number K in PixSim: We study the effective number K used in grid sampling in PixSim in <ref type="table" target="#tab_4">Table 4a</ref>. The greater the number, the more feature grids will be sampled from the intersected regions and will be used for similarity learning. With zero grid number DenseSiam degenerates to the SimSiam baseline where no pixel similarity learning is performed. The results in <ref type="table" target="#tab_4">Table 4a</ref> shows that 7?7 feature grids is sufficient to improve the per-pixel consistency of dense representation. We also compare the training memory used in PixSim. The comparison shows that PixSim only brings 0.1% ? 0.3% extra memory cost in comparison with SimSiam. ii) Loss weight of PixSim: We further study the loss weight ? 1 of L dense used for per-pixel similarity learning. The comparative results in <ref type="table" target="#tab_4">Table 4b</ref> show that with ? 1 = 1 we achieve the best performance, which is equal to the loss weight of the image-level similarity learning. iii) Loss weight of RegionSim: We study the loss weight ? 2 of RegionSim as shown in <ref type="table" target="#tab_4">Table 4c</ref>. We find that 0.1 works best and large value of ? 2 leads decreased performance. iv) Orders of grid sample, projector, and predictor : As DenseSiam consists of encoder, projector, and predictor, we study the optimal position where the grid sampler should be introduced. The results in <ref type="table" target="#tab_4">Table 4d</ref> show that it is necessary to put the grid sample module after the projector, but the predictor can perform equally well when it is before or behind the grid sample module. v) Regions to focus in global branch: We also study the regions that should be focused in global branch in <ref type="table" target="#tab_4">Table 4e</ref>, as an image-level similarity learning branch (SimSiam) is kept to facilitate training. The abbreviations 'global' and 'in.' indicate whether the similarity learning is conducted with the whole image or the intersected regions between two views. To maximize per-pixel consistency, PixSim is always conducted with the intersected regions. We find that the focusing on the whole image when conducting image-level similarity learning is always important in both SimSiam and DenseSiam. Only using the intersected regions </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>Data Augmentation. We use the same data augmentation techniques as those used in previous methods for a fair comparison. i) Unsupervised pre-training: We use existing augmentation modules in Py-Torch <ref type="bibr" target="#b36">[37]</ref> and describe them using the same notations as the following. for a fair comparison. The pre-trained backbone are then fine-tuned using the same evaluation protocol of transfer learning as described in Sec. 5.1. Exploitation of Correspondence. Previous methods explore different strategies to build and optimize dense correspondence between two views. DenseCL <ref type="bibr" target="#b47">[48]</ref> compares feature similarity to build the correspondence between pixels. Pix-Pro <ref type="bibr" target="#b54">[55]</ref> connects pixels by the distance of their coordinates in the original image. There are also attempts <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b51">52]</ref> to use manual crops and maximize the similarity between similar crops in different views. DenseSiam uses the location and regions for similarity learning of different granularities. For a fair comparison, we study these strategies under the same architecture of PixSim with a switch between strategies. Specifically, we keep the same architecture and symmetrized loss of PixSim and implement these strategies strictly following their official code releases. To study feature similarity used in DenseCL <ref type="bibr" target="#b47">[48]</ref>, we calculate feature similarities between pixels and link the most similar pixels between views. To study pixel distance used in PixPro <ref type="bibr" target="#b54">[55]</ref>, we calculate the coordinate distances of pixels and link the pixels having the close   locations. To study candidate regions used in ReSim <ref type="bibr" target="#b51">[52]</ref>, we use anchors in the intersected regions generated by sliding windows. <ref type="table" target="#tab_7">Table A1</ref> show that using feature similarity <ref type="bibr" target="#b47">[48]</ref> significantly decreases the performance. This is because PixSim does not use negative pixel pairs, a prerequisite for the strategy to work. Using pixel coordinates only brings marginal improvements. Using location correspondence in PixSim is much simpler and more effective than candidate regions. DenseSiam yields the best results by exploiting the correspondence built with both location and region embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Analysis</head><p>Grid Sampling Strategies. We tried a hard example mining strategy proposed in PointRend. Specifically, it first over-generates candidate points by randomly sampling kN points (k &gt; 1) from a uniform distribution. Then it estimates the similarities by Eq.3 between the embeddings of these points from both views. Finally, it selects the most dissimilar ?N (? ? [0, 1]) points from the kN candidates and sample the remaining (1 ? ?)N points from a uniform distribution.</p><p>The results in <ref type="table" target="#tab_1">Table A2</ref> shows that the strategy marginally improves the performance and only training on hard examples degrades the results. More strategies can be explored in future research. Visualization of masks. When using cross-entropy similarity, softmax(z ? 1 ) can be treated as a segmentation map, in which the pseudo categories of each pixel can be obtained by argmax over the channel dimension <ref type="bibr" target="#b9">[10]</ref>. We visualize the pixels' pseudo categories by different colors in the figures below. The results show that the pixels are grouped into different pseudo categories without supervision, and the features gathered by these masks thus contain region-level information, which are then forced to have consistency across views in RegionSim.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Dense Siamese Network (Dens-eSiam) unanimously solves unsupervised pretraining and unsupervised semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Pipeline of DenseSiam. DenseSiam takes two randomly augmented views</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Grid Sample module when grid size K=3. Pixels at the same location in the original image should have similar predictions when they are transformed into two views. The Grid Sample module samples these pixels and interpolates their predictions in two views. The predictions of one view are learned to match those in another view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>biased (k = 3, ? = 0.75) 55.0 80.7 60.8 heavily biased (k = 10, ? = 1.0) 54.3 80.3 60.0 Fig. A1: Visualization of pseudo categories of pixels produced by PixSim in unsupervised representation learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 ,</head><label>2</label><figDesc>DenseSiam outperforms SimSiam by 0.4 AP mask on COCO instance segmentation. The improvements over SimSiam is on-par with that of DenseCL and DetCo over MoCo v2 (0.4 vs. 0.3 AP mask ) and is better than ReSim (0.4 vs. 0 AP mask ). This further verifies the effectiveness of DenseSiam. Notably, DenseSiam outperforms ReSim, DetCo, DenseCL, and PixPro by 0.7, 0.4, 0.4, and 0.2 AP mask , respectively. The results in COCO object detection is consistent with that in instance segmentation.Cityscapes Semantic Segmentation. We compare DenseSiam with previous methods on semantic segmentation on Cityscapes dataset<ref type="bibr" target="#b12">[13]</ref> and report the mean IoU (mIoU) of fine-tuned models. DenseSiam surpasses SimSiam by 0.7</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Transfer Learning. All unsupervised methods are either based on 200-epoch pre-training in ImageNet ('IN1k'). COCO instance segmentation and COCO detection: Mask R-CNN [23] (1? schedule) fine-tuned in COCO train2017; Cityscapes: FCN fine-tuned on Cityscapes dataset [13]; VOC 07+12 detection: Faster R-CNN fine-tuned on VOC 2007 trainval + 2012 train, evaluated on VOC 2007 test; COCO train2017, evaluated on COCO val2017. All Mask R-CNN are with FPN<ref type="bibr" target="#b29">[30]</ref>. All Faster R-CNN models are with the C4backbone<ref type="bibr" target="#b17">[18]</ref>. All VOC and Cityscapes results are averaged over 5 trials</figDesc><table><row><cell></cell><cell cols="3">COCO Instance Seg.</cell><cell cols="3">COCO Detection</cell><cell cols="4">Cityscapes VOC 07+12 Detection</cell></row><row><cell>Pre-train</cell><cell cols="2">AP mask AP mask 50</cell><cell>AP mask 75</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell><cell>mIoU</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell></row><row><cell>scratch</cell><cell>29.9</cell><cell>47.9</cell><cell>32.0</cell><cell>32.8</cell><cell>50.9</cell><cell>35.3</cell><cell>63.5</cell><cell>32.8</cell><cell>59.0</cell><cell>31.6</cell></row><row><cell>supervised</cell><cell>35.9</cell><cell>56.6</cell><cell>38.6</cell><cell>39.7</cell><cell>59.5</cell><cell>43.3</cell><cell>73.7</cell><cell>54.2</cell><cell>81.6</cell><cell>59.8</cell></row><row><cell>BYOL [20]</cell><cell>34.9</cell><cell>55.3</cell><cell>37.5</cell><cell>38.4</cell><cell>57.9</cell><cell>41.9</cell><cell>71.6</cell><cell>51.9</cell><cell>81.0</cell><cell>56.5</cell></row><row><cell>SimCLR [8]</cell><cell>34.8</cell><cell>55.2</cell><cell>37.2</cell><cell>38.5</cell><cell>58.0</cell><cell>42.0</cell><cell>73.1</cell><cell>51.5</cell><cell>79.4</cell><cell>55.6</cell></row><row><cell>MoCo v2 [9]</cell><cell>36.1</cell><cell>56.9</cell><cell>38.7</cell><cell>39.8</cell><cell>59.8</cell><cell>43.6</cell><cell>74.5</cell><cell>57.0</cell><cell>82.4</cell><cell>63.6</cell></row><row><cell cols="2">SimSiam [10] 36.4</cell><cell>57.4</cell><cell>38.8</cell><cell>40.4</cell><cell>60.4</cell><cell>44.1</cell><cell>76.3</cell><cell>56.7</cell><cell>82.3</cell><cell>63.4</cell></row><row><cell>ReSim [52]</cell><cell>36.1</cell><cell>56.7</cell><cell>38.8</cell><cell>40.0</cell><cell>59.7</cell><cell>44.3</cell><cell>76.8</cell><cell>58.7</cell><cell>83.1</cell><cell>66.3</cell></row><row><cell>DetCo [53]</cell><cell>36.4</cell><cell>57.0</cell><cell>38.9</cell><cell>40.1</cell><cell>60.3</cell><cell>43.9</cell><cell>76.5</cell><cell>57.8</cell><cell>82.6</cell><cell>64.2</cell></row><row><cell cols="2">DenseCL [48] 36.4</cell><cell>57.0</cell><cell>39.2</cell><cell>40.3</cell><cell>59.9</cell><cell>44.3</cell><cell>75.7</cell><cell>58.7</cell><cell>82.8</cell><cell>65.2</cell></row><row><cell>PixPro [55]</cell><cell>36.6</cell><cell>57.3</cell><cell>39.1</cell><cell>40.5</cell><cell>60.1</cell><cell>44.3</cell><cell>76.3</cell><cell cols="3">59.5 83.4 66.9</cell></row><row><cell>DenseSiam</cell><cell>36.8</cell><cell>57.6</cell><cell>39.8</cell><cell cols="3">40.8 60.7 44.6</cell><cell>77.0</cell><cell>58.5</cell><cell>82.9</cell><cell>65.3</cell></row><row><cell cols="11">mIoU and outperforms previous dense representation learning methods ReSim,</cell></row><row><cell cols="11">DetCo, DenseCL, and PixPro by 0.2, 0.5, 1.3, and 0.7 mIoU, respectively. No-</cell></row><row><cell cols="8">tably, image-level unsupervised method such as SimCLR</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies in unsupervised pre-trainings. All unsupervised representations are based on 800-epoch pre-training on COCO train2017. The representations are fine-tuned with Faster R-CNN [39] (C4-backbone) in VOC 2007 trainval + 2012 train and evaluated on VOC 2007 test. 'Mem.' indicates memory cost measured by Gigabyte (GB). The results of fine-tuning are averaged over 5 trials. Best settings are bolded and used as default settings</figDesc><table><row><cell cols="2">(a) The effective number K</cell><cell cols="3">(b) The effective loss</cell><cell cols="3">(c) The effective loss</cell></row><row><cell>in PixSim</cell><cell></cell><cell cols="2">weight of PixSim</cell><cell></cell><cell cols="3">weight of RegionSim</cell></row><row><cell cols="2">Gird Number AP AP50 AP75 Mem.</cell><cell>?1</cell><cell cols="2">AP AP50 AP75</cell><cell></cell><cell>?2</cell><cell>AP AP50 AP75</cell></row><row><cell>0</cell><cell>53.5 79.7 59.3 8.06</cell><cell>0</cell><cell cols="2">53.5 79.7 59.3</cell><cell></cell><cell>0</cell><cell>53.5 79.7 59.3</cell></row><row><cell>1</cell><cell>28.8 53.8 27.1 8.11</cell><cell cols="3">0.1 53.3 79.6 58.8</cell><cell></cell><cell cols="2">0.01 55.3 81.0 61.3</cell></row><row><cell>3</cell><cell>53.9 80.0 59.4 8.12</cell><cell cols="3">0.3 53.5 79.9 58.8</cell><cell></cell><cell cols="2">0.05 55.0 80.9 60.6</cell></row><row><cell>7</cell><cell>54.9 80.8 60.9 8.18</cell><cell cols="3">0.5 54.0 80.2 60.0</cell><cell></cell><cell cols="2">0.1 55.5 81.1 61.5</cell></row><row><cell>9</cell><cell>54.6 80.7 60.6 8.21</cell><cell cols="3">0.7 54.0 79.8 59.8</cell><cell></cell><cell cols="2">0.2 55.3 81.0 61.0</cell></row><row><cell>14</cell><cell>54.7 80.6 60.8 8.28</cell><cell cols="3">1.0 54.9 80.8 60.9</cell><cell></cell><cell cols="2">0.5 55.3 80.9 61.1</cell></row><row><cell cols="2">(d) The effective order of</cell><cell cols="4">(e) Regions to be focused in</cell><cell cols="2">(f) Suitable start</cell></row><row><cell cols="2">grid sample, projector, and</cell><cell cols="4">global branch. 'in.' indicates the</cell><cell cols="2">epoch of RegionSim</cell></row><row><cell cols="2">predictor in PixSim</cell><cell cols="2">intersected regions</cell><cell></cell><cell></cell><cell cols="2">Start epoch AP AP50 AP75</cell></row><row><cell>Order</cell><cell>AP AP50 AP75</cell><cell cols="4">Image-level Pixel-level AP AP50 AP75</cell><cell></cell><cell>never</cell><cell>54.9 80.8 60.9</cell></row><row><cell>-</cell><cell>53.5 79.7 59.3</cell><cell>global</cell><cell>N/A</cell><cell cols="2">53.5 79.7 59.3</cell><cell></cell><cell>0.4</cell><cell>37.7 64.8 38.4</cell></row><row><cell cols="2">grid. + proj. + pred. 53.4 79.6 59.0</cell><cell>in.</cell><cell>N/A</cell><cell cols="2">0.0 0.0 0.0</cell><cell></cell><cell>0.5</cell><cell>55.5 81.1 61.5</cell></row><row><cell cols="2">proj. + grid. + pred. 54.7 80.6 60.4</cell><cell>global</cell><cell>in.</cell><cell cols="2">54.9 80.8 60.9</cell><cell></cell><cell>0.6</cell><cell>55.0 80.8 60.4</cell></row><row><cell cols="2">proj. + pred. + grid. 54.9 80.8 60.9</cell><cell>in.</cell><cell>in.</cell><cell cols="2">35.6 61.5 35.6</cell><cell cols="2">w/o PixSim 52.8 79.2 58.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study in unsupervised segmentation. 'Aux.' indicates auxiliary head PixSim Aux. CE Region. mIoU mIoU St mIoU Th We also study the effectiveness of the components in DenseSiam for unsupervised semantic segmentation. Directly applying PixSim without any further modification yields 10.1 mIoU, which already surpasses many previous methods<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref> (9.8 and 6.7 mIoU). Adding CE loss further improves the performance by making the feature space more compact and discriminative. After adding the auxiliary head, the model already surpasses the previous state-of-the-art method PiCIE. RegionSim further brings 1.4 mIoU of improvement.6 ConclusionDifferent dense prediction tasks essentially shares the similar goal of optimizing the spatial consistency between views of images. DenseSiam exploits such a property and unanimously solves unsupervised dense representation learning and unsupervised semantic segmentation within a Siamese architecture. Dens-eSiam optimizes similarity between dense predictions at pixel level by PixSim and at region level by RegionSim, with neither negative pixel pairs, momentum encoder, manual region crops, nor heuristic masks, which are all unnecessary as revealed by DenseSiam to obtain a strong dense representation for downstream tasks. Its unsupervised semantic segmentation performance also achieves the new state-of-the-art. Acknowledgements. This study is supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). The work is also suported by Singapore MOE AcRF Tier 2 (MOE-T2EP20120-0001) and NTU NAP Grant. Jiangmiao Pang and Kai Chen are partially supported by the Shanghai Committee of Science and Technology, China (Grant No. 20DZ1100800).</figDesc><table><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>10.1</cell><cell>19.0</cell><cell>17.7</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>11.1</cell><cell>20.4</cell><cell>22.3</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>15.0</cell><cell>24.8</cell><cell>23.4</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>16.4</cell><cell>24.5</cell><cell>29.5</cell></row><row><cell cols="7">will lead to feature 'collapsing' (second row with zero accuracy in downstream</cell></row><row><cell cols="7">tasks) in SimSiam since it makes some learning shortcuts for the model.</cell></row><row><cell cols="7">vi) Start point of RegionSim: The start point of RegionSim based on PixSim</cell></row><row><cell cols="7">(54.9) matters as shown in Table 4f. The label prediction quality is not accurate</cell></row><row><cell cols="7">and may lead to a wrong optimization direction if RegionSim is applied at a</cell></row><row><cell cols="7">wrong time. Starting RegionSim at the middle point of training yields the best</cell></row><row><cell cols="7">performance. We further try only using RegionSim without PixSim (last row in</cell></row><row><cell cols="7">Table 4f). Only adding RegionSim degrades fine-tuning results on by 2.7 AP,</cell></row><row><cell cols="7">this also implies that RegionSim needs PixSim to produce meaningful groups.</cell></row><row><cell cols="5">Unsupervised Semantic Segmentation.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>We apply the same augmentations as those used in PiCIE<ref type="bibr" target="#b10">[11]</ref> in unsupervised semantic segmentation for a fair comparison. The augmentations include color jittering, gray scale, blurring, cropping, and flipping. The color jittering augmentations consists of jittering brightness, contrast, saturation, and hue. These jittering transformations are randomly applied with probabilities of 0.8 with strength of 0.3, 0.3, 0.3, and 0.1, respectively. The gray scale augmentation is randomly applied with a probability of 0.2. Random crop is used with scale in [0.5,1.0]. Different from those augmentations used in unsupervised pre-training, the augmentations are sampled first and replayed with similar parameters in each epoch following PiCIE<ref type="bibr" target="#b10">[11]</ref>. Transfer Learning Results of PixPro. InTable 2, we compare DenseSiam with PixPro<ref type="bibr" target="#b54">[55]</ref>. Since PixPro<ref type="bibr" target="#b54">[55]</ref> only reports the transfer learning results of 100-epoch or 400-epoch pre-training in ImageNet (IN1k), we conduct 200epoch pre-training using the official code 2</figDesc><table><row><cell>Specif-</cell></row><row><cell>ically, for geometric augmentation, we use RandomResizedCrop with scale in</cell></row><row><cell>[0.2,1.0] and RandomHorizontalFlip. For color augmentations we use ColorJitter</cell></row><row><cell>and RandomGrayscale with probabilities of 0.8 and 0.2, respectively. The jitter-</cell></row><row><cell>ing strength of brightness, contrast, saturation, and hue are 0.4, 0.4, 0.4, and 0.1</cell></row><row><cell>in ColorJitter, respectively. Blurring augmentation [8] is also applied using a</cell></row><row><cell>Gaussian kernel with std in [0.1, 2.0]. These hyper-parameters are the same as</cell></row><row><cell>those adopted in previous methods [8-10, 20, 48].</cell></row><row><cell>i) Unsupervised semantic segmentation:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A1 :</head><label>A1</label><figDesc>Analysis: Strategies for building visual correspondence</figDesc><table><row><cell>Strategies</cell><cell cols="2">AP AP50 AP75</cell></row><row><cell>SimSiam</cell><cell>53.5 79.7</cell><cell>59.3</cell></row><row><cell>feature similarity [48]</cell><cell>36.3 63.5</cell><cell>36.1</cell></row><row><cell>pixel distance [55]</cell><cell>53.7 79.3</cell><cell>58.5</cell></row><row><cell>candidate regions [40, 52]</cell><cell>54.7 79.9</cell><cell>60.5</cell></row><row><cell>location</cell><cell>54.9 80.8</cell><cell>60.9</cell></row><row><cell cols="3">location + region embeddings 55.5 81.1 61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A2 :</head><label>A2</label><figDesc>Analysis: Strategies for grid sampling</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Tete-Xiao/ReSim</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/zdaxie/PixPro</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>Wallach, H.M., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E.B., Garnett, R.</editor>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">COCO-Stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS (2020) 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>In: ICML (2020) 1, 3, 4, 5, 6, 7, 10</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">PiCIE: Unsupervised semantic segmentation using invariance and equivariance in clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2021) 1, 2, 3, 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">UP-DETR: unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent -A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">?</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>In: NeurIPS (2020) 1, 3, 4, 5, 6, 7, 10</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rethinking ImageNet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient visual pretraining with contrastive detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>In: ICCV (2021) 2, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Segsort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>In: ICCV (2019) 2, 3, 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Siamese neural networks for oneshot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepusps: Deep robust unsupervised saliency prediction via self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Autoregressive unsupervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Autodiff Workshop</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of dense visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Benmalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatially consistent representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 2, 3, 4</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">CASTing your model: Learning to localize improves self-supervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unsupervised semantic segmentation by contrasting object mask proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2021) 2, 3, 4, 9</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aligning pretraining for detection via object-level contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Region similarity representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">DetCo: Unsupervised contrastive learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>In: ICCV (2021) 2, 3, 4, 7</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised object-level representation learning from scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2021) 2, 3, 4, 10, 11</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Instance localization for self-supervised detection pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
