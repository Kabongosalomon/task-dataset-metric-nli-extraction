<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reading Text in the Wild with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
						</author>
						<title level="a" type="main">Reading Text in the Wild with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we present an end-to-end system for text spotting -localising and recognising text in natural scene images -and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data.</p><p>Analysing the stages of our pipeline, we show stateof-the-art performance throughout. We perform rigorous experiments across a number of standard end-toend text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all previous methods. Finally, we demonstrate a real-world application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text, as the physical incarnation of language, is one of the basic tools for preserving and communicating information. Much of the modern world is designed to be interpreted through the use of labels and other textual cues, and so text finds itself scattered throughout many images and videos. Through the use of text spotting, an important part of the semantic content of visual media can be decoded and used, for example, for understanding, annotating, and retrieving the billions of consumer photos produced every day.</p><p>Traditionally, text recognition has been focussed on document images, where OCR techniques are well suited to digitise planar, paper-based documents. However, when applied to natural scene images, these document OCR techniques fail as they are tuned to the largely black-and-white, line-based environment of printed documents. The text that occurs in natural scene images is hugely variable in appearance and layout, being drawn from a large number of fonts and styles, suffering from inconsistent lighting, occlusions, orientations, noise, and, in addition, the presence of background objects causes spurious false-positive detections. This places text spotting as a separate, far more challenging problem than document OCR.</p><p>The increase of powerful computer vision techniques and the overwhelming increase in the volume of images produced over the last decade has seen a rapid development of text spotting methods. To efficiently perform text spotting, the majority of methods follow the intuitive process of splitting the task in two: text detection followed by word recognition <ref type="bibr" target="#b8">[11]</ref>. Text detection involves generating candidate character or word region detections, while word recognition takes these proposals and infers the words depicted.</p><p>In this paper we advance text spotting methods, making a number of key contributions as part of this. arXiv:1412.1842v1 [cs.CV] 4 Dec 2014 <ref type="figure">Fig. 1</ref> The end-to-end text spotting pipeline proposed. a) A combination of region proposal methods extracts many word bounding box proposals. b) Proposals are filtered with a random forest classifier reducing number of false-positive detections. c) A CNN is used to perform bounding box regression for refining the proposals. d) A CNN performs text recognition on each of the refined proposals. e) Detections are merged based on proximity and recognition results and assigned a score. f) Thresholding the detections results in the final text spotting result.</p><p>Our main contribution is a novel text recognition method -this is in the form of a deep convolutional neural network (CNN) <ref type="bibr" target="#b34">[34]</ref> which takes the whole word image as input to the network. Evidence is gradually pooled from across the image to perform classification of the word across a huge dictionary, such as the 90kword dictionary evaluated in this paper. Remarkably, our model is trained purely on synthetic data, without incurring the cost of human labelling. We also propose an incremental learning method to successfully train a model with such a large number of classes. Our recognition framework is exceptionally powerful, substantially outperforming previous state of the art on real-world scene text recognition, without using any real-world labelled training data.</p><p>Our second contribution is a novel detection strategy for text spotting: the use of fast region proposal methods to perform word detection. We use a combination of an object-agnostic region proposal method and a sliding window detector. This gives very high recall coverage of individual word bounding boxes, resulting in around 98% word recall on both ICDAR 2003 and Street View Text datasets with a manageable number of proposals. False-negative candidate word bounding boxes are filtered with a stronger random forest classifier and the remaining proposals adjusted using a CNN trained to regress the bounding box coordinates.</p><p>Our third contribution is the application of our pipeline for large-scale visual search of text in video. In a fraction of a second we are able to retrieve images and videos from a huge corpus that contain the visual rendering of a user given text query, at very high precision.</p><p>We expose the performance of each part of the pipeline in experiments, showing that we can maintain the high recall of the initial proposal stage while gradually boosting precision as more complex models and higher order information is incorporated. The recall of the detection stage is shown to be significantly higher than that of previous text detection methods, and the accuracy of the word recognition stage higher than all previous methods. The result is an end-to-end text spotting system that outperforms all previous methods by a large margin. We demonstrate this for the annotation task (localising and recognising text in images) across a large range of standard text spotting datasets, as well as in a retrieval scenario (retrieving a ranked list of images that contain the text of a query string) for standard datasets. In addition, the use of our framework for retrieval is further demonstrated in a real-world application -being used to instantly search through thousands of hours of archived news footage for a user-given text query.</p><p>The following section gives an overview of our pipeline. We then review a selection of related work in Section 3. Sections 4-7 present the stages of our pipeline. We extensively test all elements of our pipeline in Section 8 and include the details of datasets and the experimental setup. Finally, Section 9 summarises and concludes.</p><p>Our word recognition framework appeared previously as a tech report <ref type="bibr" target="#b29">[30]</ref> and at the NIPS 2014 Deep Learning and Representation Learning Workshop, along with some other non-dictionary based variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the Approach</head><p>The stages of our approach are as follows: word bounding box proposal generation (Section 4), proposal filtering and adjustments (Section 5), text recognition (Section 6) and final merging for the specific task (Section 7). The full process is illustrated in <ref type="figure">Fig. 1</ref>.</p><p>Our process loosely follows the detection/recognition separation -a word detection stage followed by a word recognition stage. However, these two stages are not wholly distinct, as we use the information gained from word recognition to merge and rank detection results at the end, leading to a stronger holistic text spotting system.</p><p>The detection stage of our pipeline is based on weakbut-fast detection methods to generate word boundingbox proposals. This draws on the success of the R-CNN object detection framework of Girshick et al . <ref type="bibr" target="#b21">[24]</ref> where region proposals are mapped to a fixed size for CNN recognition. The use of region proposals avoids the computational complexity of evaluating an expensive classifier with exhaustive multi-scale, multi-aspect-ratio sliding window search. We use a combination of Edge Box proposals <ref type="bibr" target="#b66">[62]</ref> and a trained aggregate channel features detector <ref type="bibr" target="#b10">[13]</ref> to generate candidate word bounding boxes. Due to the large number of false-positive proposals, we then use a random forest classifier to filter the number of proposals to a manageable size -this is a stronger classifier than those found in the proposal algorithms. Finally, inspired by the success of bounding box regression in DPM <ref type="bibr" target="#b17">[20]</ref> and R-CNN <ref type="bibr" target="#b21">[24]</ref>, we regress more accurate bounding boxes from the seeds of the proposal algorithms which greatly improves the average overlap ratio of positive detections with groundtruth. However, unlike the linear regressors of <ref type="bibr" target="#b17">[20,</ref><ref type="bibr" target="#b21">24]</ref> we train a CNN specifically for regression. We discuss these design choices in each section.</p><p>The second stage of our framework produces a text recognition result for each proposal generated from the detection stage. We take a whole-word approach to recognition, providing the entire cropped region of the word as input to a deep convolutional neural network. We present a dictionary model which poses the recognition task as a multi-way classification task across a dictionary of 90k possible words. Due to the mammoth training data requirements of classification tasks of this scale, these models are trained purely from synthetic data. Our synthetic data engine is capable of rendering sufficiently realistic and variable word image samples that the models trained on this data translate to the domain of real-world word images giving state-of-theart recognition accuracy.</p><p>Finally, we use the information gleaned from recognition to update our detection results with multiple rounds of non-maximal suppression and bounding box regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>In this section we review the contributions of works most related to ours. These focus solely on text detection <ref type="bibr" target="#b3">[6,</ref><ref type="bibr" target="#b7">10,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b64">60,</ref><ref type="bibr" target="#b65">61]</ref>, text recognition <ref type="bibr" target="#b1">[4,</ref><ref type="bibr" target="#b4">7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b48">45,</ref><ref type="bibr" target="#b53">50,</ref><ref type="bibr" target="#b62">59]</ref>, or on combining both in end-to-end systems <ref type="bibr">[5, 27, 31, 40-44, 48, 49, 56-58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Detection Methods</head><p>Text detection methods tackle the first task of the standard text spotting pipeline <ref type="bibr" target="#b8">[11]</ref>: producing segmentations or bounding boxes of words in natural scene images. Detecting instances of words in noisy and cluttered images is a highly non-trivial task, and the methods developed to solve this are based on either character regions <ref type="bibr">[10, 17, 29, 41-44, 60, 61]</ref> or sliding windows <ref type="bibr" target="#b3">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">48,</ref><ref type="bibr" target="#b52">49,</ref><ref type="bibr" target="#b59">56,</ref><ref type="bibr" target="#b60">57]</ref>.</p><p>Character region methods aim to segment pixels into characters, and then group characters into words. Epshtein et al . <ref type="bibr" target="#b14">[17]</ref> find regions of the input image which have constant stroke width -the distance between two parallel edges -by taking the stroke width transform (SWT). Intuitively characters are regions of similar stroke width, so clustering pixels together forms characters, and characters are grouped together into words based on geometric heuristics. In <ref type="bibr" target="#b47">[44]</ref>, Neumann and Matas revisit the notion of characters represented as strokes and use gradient filters to detect oriented strokes in place of the SWT. Rather than regions of constant stroke width, Neumann and Matas <ref type="bibr" target="#b43">[41]</ref><ref type="bibr" target="#b45">[42]</ref><ref type="bibr" target="#b46">[43]</ref> use Extremal Regions <ref type="bibr" target="#b39">[38]</ref> as character regions. Huang et al . <ref type="bibr" target="#b28">[29]</ref> expand on the use of Maximally Stable Extremal Regions by incorporating a strong CNN classifier to efficiently prune the trees of Extremal Regions leading to less false-positive detections.</p><p>Sliding window methods approach text detection as a classical object detection task. Wang et al . <ref type="bibr" target="#b59">[56]</ref> use a random ferns <ref type="bibr" target="#b49">[46]</ref> classifier trained on HOG features <ref type="bibr" target="#b17">[20]</ref> in a sliding window scenario to find characters in an image. These are grouped into words using a pictorial structures framework <ref type="bibr" target="#b16">[19]</ref> for a small fixed lexicon. Wang &amp; Wu et al . <ref type="bibr" target="#b60">[57]</ref> show that CNNs trained for character classification can be used as effective sliding window classifiers. In some of our earlier work <ref type="bibr" target="#b30">[31]</ref>, we use CNNs for text detection by training a text/notext classifier for sliding window evaluations, and also CNNs for character and bigram classification to perform word recognition. We showed that using feature sharing across all the CNNs for the different classification tasks resulted in stronger classifiers for text detection than training each classifier independently.</p><p>Unlike previous methods, our framework operates in a low-precision, high-recall mode -rather than using a single word location proposal, we carry a sufficiently high number of candidates through several stages of our pipeline. We use high recall region proposal methods and a filtering stage to further refine these. In fact, our "detection method" is only complete after performing full text recognition on each remaining proposal, as we then merge and rank the proposals based on the output of the recognition stage to give the final detections, complete with their recognition results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Recognition Methods</head><p>Text recognition aims at taking a cropped image of a single word and recognising the word depicted. While there are many previous works focussing on handwriting or historical document recognition <ref type="bibr" target="#b18">[21,</ref><ref type="bibr" target="#b20">23,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b53">50]</ref>, these methods don't generalise in function to generic scene text due to the highly variable foreground and background textures that are not present with documents.</p><p>For scene text recognition, methods can be split into two groups -character based recognition <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b4">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">48,</ref><ref type="bibr" target="#b52">49,</ref><ref type="bibr" target="#b59">[56]</ref><ref type="bibr" target="#b60">[57]</ref><ref type="bibr" target="#b61">[58]</ref><ref type="bibr" target="#b62">[59]</ref> and whole word based recognition <ref type="bibr" target="#b1">[4,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b48">45,</ref><ref type="bibr" target="#b54">51]</ref>.</p><p>Character based recognition relies on an individual character classifier for per-character recognition which is integrated across the word image to generate the full word recognition. In <ref type="bibr" target="#b62">[59]</ref>, Yao et al . learn a set of midlevel features, strokelets, by clustering sub-patches of characters. Characters are detected with Hough voting, with the characters identified by a random forest classifier acting on strokelet and HOG features.</p><p>The works of <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b4">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b60">57]</ref> all use CNNs as character classifiers. <ref type="bibr" target="#b4">[7]</ref> and <ref type="bibr" target="#b2">[5]</ref> over-segment the word image into potential character regions, either through unsupervised binarization techniques or with a supervised classifier. Alsharif et al . <ref type="bibr" target="#b2">[5]</ref> then use a complicated combination of segmentation-correction and character recognition CNNs together with an HMM with a fixed lexicon to generate the final recognition result. The Pho-toOCR system <ref type="bibr" target="#b4">[7]</ref> uses a neural network classifier acting on the HOG features of the segments as scores to find the best combination of segments using beam search. The beam search incorporates a strong N-gram language model, and the final beam search proposals are re-ranked with a further language model and shape model. Our own previous work <ref type="bibr" target="#b30">[31]</ref> uses a combination of a binary text/no-text classifier, a character classifier, and a bigram classifier densely computed across the word image as cues to a Viterbi scoring function in the context of a fixed lexicon.</p><p>As an alternative approach to word recognition other methods use whole word based recognition, pooling features from across the entire word sub-image before performing word classification. The works of Mishra et al . <ref type="bibr" target="#b40">[39]</ref> and Novikova et al . <ref type="bibr" target="#b48">[45]</ref> still rely on explicit character classifiers, but construct a graph to infer the word, pooling together the full word evidence. Goel et al . <ref type="bibr" target="#b22">[25]</ref> use whole word sub-image features to recognise words by comparing to simple black-and-white font-renderings of lexicon words. Rodriguez et al . <ref type="bibr" target="#b54">[51]</ref> use aggregated Fisher Vectors <ref type="bibr" target="#b50">[47]</ref> and a Structured SVM framework to create a joint word-image and text embedding.</p><p>Almazan et al . <ref type="bibr" target="#b1">[4]</ref> further explore the notion of word embeddings, creating a joint embedding space for word images and representations of word strings. This is extended in <ref type="bibr" target="#b26">[27]</ref> where Gordo makes explicit use of character level training data to learn mid-level features. This results in performance on par with <ref type="bibr" target="#b4">[7]</ref> but using only a small fraction of the amount of training data.</p><p>While not performing full scene text recognition, Goodfellow et al . <ref type="bibr" target="#b25">[26]</ref> had great success using a CNN with multiple position-sensitive character classifier outputs to perform street number recognition. This model was extended to CAPTCHA sequences up to 8 characters long where they demonstrated impressive performance using synthetic training data for a synthetic problem (where the generative model is known). In contrast, we show that synthetic training data can be used for a real-world data problem (where the generative model is unknown).</p><p>Our method for text recognition also follows a whole word image approach. Similarly to <ref type="bibr" target="#b25">[26]</ref>, we take the word image as input to a deep CNN, however we employ a dictionary classification model. Recognition is achieved by performing multi-way classification across the entire dictionary of potential words.</p><p>In the following sections we describe the details of each stage of our text spotting pipeline. The sections are presented in order of their use in the end-to-end system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposal Generation</head><p>The first stage of our end-to-end text spotting pipeline relies on the generation of word bounding boxes. This is word detection -in an ideal scenario we would be able to generate word bounding boxes with high recall and high precision, achieving this by extracting the maximum amount of information from each bounding box candidate possible. However, in practice a precision/recall tradeoff is required to reduce computational complexity. With this in mind we opt for a fast, high recall initial phase, using computationally cheap classifiers, and gradually incorporate more information and more complex models to improve precision by rejecting false-positive detections resulting in a cascade. To compute recall and precision in a detection scenario, a bounding box is said to be a true-positive detection if it has overlap with a groundtruth bounding box above a defined threshold. The overlap for bounding boxes b 1 and b 2 is defined as the ratio of intersection over union (IoU): |b1?b2| |b1?b2| . Though never applied to word detection before, region proposal methods have gained a lot of attention for generic object detection. Region proposal methods <ref type="bibr" target="#b0">[3,</ref><ref type="bibr" target="#b9">12,</ref><ref type="bibr" target="#b57">54,</ref><ref type="bibr" target="#b66">62]</ref> aim to generate object region proposals with high recall, but at the cost of a large number of falsepositive detections. Even so, this still reduces the search space drastically compared to sliding window evaluation of the subsequent stages of a detection pipeline. Effectively, region proposal methods can be viewed as a weak detector.</p><p>In this work we combine the results of two detection mechanisms -the Edge Boxes region proposal algorithm ( <ref type="bibr" target="#b66">[62]</ref>, Section 4.1) and a weak aggregate channel features detector ( <ref type="bibr" target="#b10">[13]</ref>, Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Edge Boxes</head><p>We use the formulation of Edge Boxes as described in <ref type="bibr" target="#b66">[62]</ref>. The key intuition behind Edge Boxes is that since objects are generally self contained, the number of contours wholly enclosed by a bounding box is indicative of the likelihood of the box containing an object. Edges tend to correspond to object boundaries, and so if edges are contained inside a bounding box this implies objects are contained within the bounding box, whereas edges which cross the border of the bounding box suggest there is an object that is not wholly contained by the bounding box.</p><p>The notion of an object being a collection of boundaries is especially true when the desired objects are words -collections of characters with sharp boundaries.</p><p>Following <ref type="bibr" target="#b66">[62]</ref>, we compute the edge response map using the Structured Edge detector <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b13">16]</ref> and perform Non-Maximal Suppression orthogonal to the edge responses, sparsifying the edge map. A candidate bounding box b is assigned a score s b based on the number of edges wholly contained by b, normalised by the perimeter of b. The full details can be found in <ref type="bibr" target="#b66">[62]</ref>.</p><p>The boxes b are evaluated in a sliding window manner, over multiple scales and aspect ratios, and given a score s b . Finally, the boxes are sorted by score and nonmaximal suppression is performed: a box is removed if its overlap with another box of higher score is more than a threshold. This results in a set of candidate bounding boxes for words B e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Aggregate Channel Feature Detector</head><p>Another method for generating candidate word bounding box proposals is by using a conventional trained detector. We use the aggregate channel features (ACF) detector framework of <ref type="bibr" target="#b10">[13]</ref> for its speed of computation. This is a conventional sliding window detector based on ACF features coupled with an AdaBoost classifier. ACF based detectors have been shown to work well on pedestrian detection and general object detection, and here we use the same framework for word detection.</p><p>For each image I a number of feature channels are computed, such that channel C = ?(I), where ? is the channel feature extraction function. We use channels similar to those in <ref type="bibr" target="#b11">[14]</ref>: normalised gradient magnitude, histogram of oriented gradients (6 channels), and the raw greyscale input. Each channel C is smoothed, divided into blocks and the pixels in each block are summed and smoothed again, resulting in aggregate channel features.</p><p>The ACF features are not scale invariant, so for multi-scale detection we need to extract features at many different scales -a feature pyramid. In a standard detection pipeline, the channel features for a particular scale s are computed by resampling the image and recomputing the channel features C s = ?(I s ) where C s are the channel features at scale s and I s = R(I, s) is the image resampled by s. Resampling and recomputing the features at every scale is computationally expensive. However, as shown in <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b11">14]</ref>, the channel features at scale s can be approximated by resampling the features at a different scale, such that C s ? R(C, s) ? s ?? ? , where ? ? is a channel specific power-law factor. Therefore, fast feature pyramids can be computed by evaluating C s = ?(R(I, s)) at only a single scale per octave (s ? {1, 1 2 , 1 4 , . . . }) and at intermediate scales,</p><formula xml:id="formula_0">C s is computed using C s = R(C s , s/s )(s/s ) ?? ? where s ? {1, 1 2 , 1 4 , . . . }.</formula><p>This results in much faster feature pyramid computation.</p><p>The sliding window classifier is an ensemble of weak decision trees, trained using AdaBoost <ref type="bibr" target="#b19">[22]</ref>, using the aggregate channel features. We evaluate the classifier on every block of aggregate channel features in our feature pyramid, and repeat this for multiple aspect ratios to account for different length words, giving a score for each box. Thresholding on score gives a set of word proposal bounding boxes from the detector, B d .</p><p>Discussion. We experimented with a number of region proposal algorithms. Some were too slow to be useful <ref type="bibr" target="#b0">[3,</ref><ref type="bibr" target="#b57">54]</ref>. A very fast method, BING <ref type="bibr" target="#b9">[12]</ref>, gives good recall when re-trained specifically for word detection but achieves a low overlap ratio for detections and poorer overall recall. We found Edge Boxes to give the best recall and overlap ratio.</p><p>It was also observed that independently, neither Edge Boxes nor the ACF detector achieve particularly high recall, 92% and 70% recall respectively (see Section 8.2 for experiments), but when the proposals are combined achieve 98% recall (recall is computed at 0.5 overlap). In contrast, combining BING, with a recall of 86% with the ACF detector gives a combined recall of only 92%. This suggests that the Edge Box and ACF detector methods are very complementary when used in conjunction, and so we compose the final set of candidate bounding boxes B = {B e ? B d }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Filtering &amp; Refinement</head><p>The proposal generation stage of Section 4 produces a set of candidate bounding boxes B. However, to achieve a high recall, thousands of bounding boxes are generated, most of which are false-positive. We therefore aim to use a stronger classifier to further filter these to a number that is computationally manageable for the more expensive full text recognition stage described in Section 5.1. We also observe that the overlap of many of the bounding boxes with the groundtruth is unsatisfactorily low, and therefore train a regressor to refine the location of the bounding boxes, as described in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word Classification</head><p>To reduce the number of false-positive word detections, we seek a classifier to perform word/no-word binary classification. For this we use a random forest classifier <ref type="bibr" target="#b5">[8]</ref> acting on HOG features <ref type="bibr" target="#b17">[20]</ref>.</p><p>For each bounding box proposal b ? B we resample the cropped image region to a fixed size and extract HOG features, resulting in a descriptor h. The descriptor is then classified with a random forest classifier, with decision stump nodes. The random forest classifies every proposal, and the proposals falling below a certain threshold are rejected, leaving a filtered set of bounding boxes B f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Bounding Box Regression</head><p>Although our proposal mechanism and filtering stage give very high recall, the overlap of these proposals can be quite poor.</p><p>While an overlap of 0.5 is usually acceptable for general object detection <ref type="bibr" target="#b15">[18]</ref>, for accurate text recognition this can be unsatisfactory. This is especially true when one edge of the bounding box is predicted accurately but not the other -e.g. if the height of the bounding box is computed perfectly, the width of the bounding box can be either double or half as wide as it should be and still achieve 0.5 overlap. This is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. Both predicted bounding boxes have 0.5 overlap with the groundtruth, but for text this can amount to only seeing half the word if the height is correctly computed, and so it would be impossible to recognise the correct word in the bottom example of <ref type="figure" target="#fig_0">Fig. 2</ref>. Note that both proposals contain text, so neither are filtered by the word/no-word classifier.</p><p>Due to the large number of region proposals, we could hope that there would be some proposals that would overlap with the groundtruth to a satisfactory degree, but we can encourage this by explicitly refining the coordinates of the proposed bounding boxes -we do this within a regression framework.</p><p>Our bounding box coordinate regressor takes each proposed bounding box b ? B f and produces an updated estimate of that proposal b * . A bounding box is parametrised by its top-left and bottom-right corners, such that bounding box b = (x 1 , y 1 , x 2 , y 2 ). The full image I is cropped to a rectangle centred on the region b, with the width and height inflated by a scale factor. The resulting image is resampled to a fixed size W ? H, giving I b , which is processed by the CNN to regress the four values of b * . We do not regress the absolute values of the bounding box coordinates directly, but rather encoded values. The top-left coordinate is encoded by the top-left quadrant of I b , and the bottom left coordinate by the bottom-left quadrant of I b as illustrated by <ref type="figure" target="#fig_1">Fig. 3</ref>. This normalises the coordinates to generally fall in the interval [0, 1], but allows the breaking of this interval if required.</p><p>In practice, we inflate the cropping region of each proposal by a factor of two. This gives the CNN enough context to predict a more accurate location of the proposal bounding box. The CNN is trained with example pairs of (I b , b gt ) to regress the groundtruth bounding box b gt from the sub-image I b cropped from I by the estimated bounding box b. This is done by minimising the L 2 loss between the encoded bounding boxes, i.e.</p><formula xml:id="formula_1">min ? b?Btrain g(I b ; ?) ? q(b gt ) 2 2 (1) over the network parameters ? on a training set B train ,</formula><p>where g is the CNN forward pass function and q is the bounding box coordinate encoder.</p><p>Discussion. The choice of which features and the classifier and regression methods to use was made through experimenting with a number of different choices. This included using a CNN for classification, with a dedicated classification CNN, and also by jointly training a single CNN to perform both classification and regression simultaneously with multi-task learning. However, the classification performance of the CNN was not significantly better than that of HOG with a random forest, but requires more computations and a GPU for processing. We therefore chose the random forest classifier to reduce the computational cost of our pipeline without impacting end results. The bounding box regression not only improves the overlap to aid text recognition for each individual sample, but also causes many proposals to converge on the same bounding box coordinates for a single instance of a word, therefore aiding the voting/merging mechanism described in Section 8.3 with duplicate detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Text Recognition</head><p>At this stage of our processing pipeline, a pool of accurate word bounding box proposals has been gener- ated as described in the previous sections. We now turn to the task of recognising words inside these proposed bounding boxes. To this end we use a deep CNN to perform classification across a pre-defined dictionary of words -dictionary encoding -which explicitly models natural language. The cropped image of each of the proposed bounding boxes is taken as input to the CNN, and the CNN produces a probability distribution over all the words in the dictionary. The word with the maximum probability can be taken as the recognition result.</p><p>The model, described fully in Section 6.2, can scale to a huge dictionary of 90k words, encompassing the majority of the commonly used English language (see Section 8.1 for details of the dictionary used). However, to achieve this, many training samples of every different possible word must be amassed. Such a training dataset does not exist, so we instead use synthetic training data, described in Section 6.1, to train our CNN. This synthetic data is so realistic that the CNN can be trained purely on the synthetic data but still applied to real world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Synthetic Training Data</head><p>This section describes our scene text rendering algorithm. As our CNN models take whole word images as input instead of individual character images, it is essential to have access to a training dataset of cropped word images that covers the whole language or at least a target lexicon. While there are some publicly available  <ref type="figure">Fig. 4 (a)</ref> The text generation process after font rendering, creating and colouring the image-layers, applying projective distortions, and after image blending. (b) Some randomly sampled data created by the synthetic text engine.</p><p>datasets from ICDAR <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b55">52]</ref>, the Street View Text (SVT) dataset <ref type="bibr" target="#b59">[56]</ref>, the IIIT-5k dataset <ref type="bibr" target="#b40">[39]</ref>, and others, the number of full word image samples is only in the thousands, and the vocabulary is very limited.</p><p>The lack of full word image samples has caused previous work to rely on character classifiers instead (as character data is plentiful), or this deficit in training data has been mitigated by mining for data or having access to large proprietary datasets <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. However, we wish to perform whole word image based recognition and move away from character recognition, and aim to do this in a scalable manner without requiring human labelled datasets.</p><p>Following the success of some synthetic character datasets <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b60">57]</ref>, we create a synthetic word data generator, capable of emulating the distribution of scene text images. This is a reasonable goal, considering that much of the text found in natural scenes is restricted to a limited set of computer-generated fonts, and only the physical rendering process (e.g. printing, painting) and the imaging process (e.g. camera, viewpoint, illumination, clutter) are not controlled by a computer algorithm. <ref type="figure">Fig. 4</ref> illustrates the generative process and some resulting synthetic data samples. These samples are composed of three separate image-layers -a background image-layer, foreground image-layer, and optional border/shadow image-layer -which are in the form of an image with an alpha channel. The synthetic data generation process is as follows: ) is dictated by a random process, and this creates an eclectic range of textures and compositions. The three image-layers are also blended together in a random manner, to give a single output image. 6. Noise -Elastic distortion similar to <ref type="bibr" target="#b56">[53]</ref>, Gaussian noise, blur, resampling noise, and JPEG compression artefacts are introduced to the image.</p><p>This process produces a wide range of synthetic data samples, being drawn from a multitude of random distributions, mimicking real-world samples of scene text images. The synthetic data is used in place of real-world data, and the labels are generated from a corpus or dictionary as desired. By creating training datasets many orders of magnitude larger than what has been available before, we are able to use data-hungry deep learning algorithms to train a richer, whole-word-based model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">CNN Model</head><p>This section describes our model for word recognition. We formulate recognition as a multi-class classification problem, with one class per word, where words w are constrained to be selected in a pre-defined dictionary W. While the dictionary W of a natural language may seem too large for this approach to be feasible, in practice an advanced English vocabulary, including different word forms, contains only around 90k words, which is large but manageable.</p><p>In detail, we propose to use a CNN classifier where each word w ? W in the lexicon corresponds to an output neuron. We use a CNN with five convolutional layers and three fully-connected layers, with the exact details described in Section 8.2. The final fully-connected layer performs classification across the dictionary of words, so has the same number of units as the size of the dictionary we wish to recognise.</p><p>The predicted word recognition result w * out of the set of all dictionary words W in a language L for a given input image x is given by w * = arg max w?W P (w|x, L).</p><p>(2)</p><p>Since P (w|x, L) can be written as</p><formula xml:id="formula_2">P (w|x, L) = P (w|x)P (w|L)P (x) P (x|L)P (w)<label>(3)</label></formula><p>and with the assumptions that x is independent of L and that prior to any knowledge of our language all words are equally probable, our scoring function reduces to w * = arg max w?W P (w|x)P (w|L).</p><p>The per-word output probability P (w|x) is modelled by the softmax output of the final fully-connected layer of the recognition CNN, and the language based word prior P (w|L) can be modelled by a lexicon or frequency counts. A schematic of the network is shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. One limitation of this CNN model is that the input x must be a fixed, pre-defined size. This is problematic for word images, as although the height of the image is always one character tall, the width of a word image is highly dependent on the number of characters in the word, which can range between one and 23 characters. To overcome this issue, we simply resample the word image to a fixed width and height. Although this does not preserve the aspect ratio, the horizontal frequency distortion of image features most likely provides the network with word-length cues. We also experimented with different padding regimes to preserve the aspect ratio, but found that the results are not quite as good as performing naive resampling.</p><p>To summarise, for each proposal bounding box b ? B f for image I we compute P (w|x b , L) by cropping the image to I b = c(b, I), resampling to fixed dimensions W ? H such that x b = R(I b , W, H), and compute P (w|x b ) with the text recognition CNN and multiply by P (w|L) (task dependent) to give a final probability distribution over words P (w|x b , L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Merging &amp; Ranking</head><p>At this point in the pipeline, we have a set of word bounding boxes for each image B f with their associated word probability distributions</p><formula xml:id="formula_4">P B f = {p b : b ? B f },</formula><p>where p b = P (w|b, I) = P (w|x b , L). However, this set of detections still contains a number of false-positive and duplicate detections of words, so a final merging and ranking of detections must be performed depending on the task at hand: text spotting or text based image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Text Spotting</head><p>The goal of text spotting is to localise and recognise the individual words in the image. Each word should be labelled by a bounding box enclosing the word and the bounding box should have an associated text label.</p><p>For this task, we assign each bounding box in b ? B f a label w b and score s b according to b's maximum word probability:</p><formula xml:id="formula_5">w b = arg max w?W P (w|b, I), s b = max w?W P (w|b, I)<label>(5)</label></formula><p>To cluster duplicate detections of the same word instance, we perform a greedy non maximum suppression (NMS) on detections with the same word label, aggregating the scores of suppressed proposals. This can be seen as positional voting for a particular word. Subsequently, we perform NMS to suppress non-maximal detections of different words with some overlap.</p><p>Our text recognition CNN is able to accurately recognise text in very loosely cropped word sub-images. Because of this, we find that some valid text spotting results have less than 0.5 overlap with groundtruth, but we require greater than 0.5 overlap for some applications (see Section 8.3).</p><p>To improve the overlap of detection results, we additionally perform multiple rounds of bounding box regression as in Section 5.2 and NMS as described above to further refine our detections. This can be seen as a recurrent regressor network. Each round of regression updates the prediction of the each word's localisation, giving the next round of regression an updated context window to perform the next regression, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. Performing NMS between each regression causes bounding boxes that have become similar after the latest round of regression to be grouped as a single detection. This generally causes the overlap of detections to converge on a higher, stable value with only a few rounds of recurrent regression.</p><p>The refined results, given by the tuple (b, w b , s b ), are ranked by their scores s b and a threshold determines the final text spotting result. For the direct comparison of scores across images, we normalise the scores of the results of each image by the maximum score for a detection in that image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Image Retrieval</head><p>For the task of text based image retrieval, we wish to retrieve the list of images which contain the given query words. Localisation of the query word is not required, only optional for giving evidence for retrieving that image. This is achieved by, at query time, assigning each image I a score s Q I for the query words Q = {q 1 , q 2 , . . .}, and sorting the images in the database I in descending order of score. It is also required that the score for all images can be computed fast enough to scale to databases of millions of images, allowing fast retrieval of visual content by text search. While retrieval is often performed for just a single query word (Q = {q}), we generalise our retrieval framework to be able to handle multiple query words.</p><p>We estimate the per-image probability distribution across word space P (w|I) by averaging the word probability distributions across all detections B f in an image</p><formula xml:id="formula_6">p I = P (w|I) = 1 |B f | b?B f p b .<label>(6)</label></formula><p>This distribution is computed offline for all I ? I. At query time, we can simply compute a score for each image s Q I representing the probability that the image I contains any of the query words Q. Assuming independence between the presence of query words</p><formula xml:id="formula_7">s Q I = q?Q P (q|I) = q?Q p I (q)<label>(7)</label></formula><p>where p I (q) is just a lookup of the probability of word q in the word distribution p I . These scores can be computed very quickly and efficiently by constructing an inverted index of p I ? I ? I. After a one-time, offline pre-processing to compute p I and assemble the inverted index, a query can be processed across a database of millions of images in less than a second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments</head><p>In this section we evaluate our pipeline on a number of standard text spotting and text based image retrieval benchmarks.</p><p>We introduce the various datasets used for evaluation in Section 8.1, give the exact implementation details and results of each part of our pipeline in Section 8.2, and finally present the results on text spotting and image retrieval benchmarks in Section 8.3 and Section 8.4 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Datasets</head><p>We evaluate our pipeline on an extensive number of datasets. Due to different levels of annotation, the datasets are used for a combination of text recognition, text spotting, and image retrieval evaluation. The datasets are summarised in <ref type="table" target="#tab_0">Table 1, Table 2, and Table 3</ref>. The smaller lexicons provided by some datasets are used to reduce the search space to just text contained within the lexicons.</p><p>The Synth dataset is generated by our synthetic data engine of Section 6.1. We generate 9 million 32 ? 100 images, with equal numbers of word samples from a 90k word dictionary. We use 900k of these for a testing dataset, 900k for validation, and the remaining for training. The 90k dictionary consists of the English dictionary from <ref type="figure" target="#fig_0">Hunspell [2]</ref>, a popular open source spell checking system. This dictionary consists of 50k root words, and we expand this to include all the prefixes and suffixes possible, as well as adding in the test dataset words from the ICDAR, SVT and IIIT datasets -90k words in total. This dataset is publicly available at http://www.robots.ox.ac.uk/~vgg/data/text/.</p><p>ICDAR 2003 (IC03) [1], ICDAR 2011 (IC11) <ref type="bibr" target="#b55">[52]</ref>, and ICDAR 2013 (IC13) <ref type="bibr" target="#b32">[33]</ref> are scene text recognition datasets consisting of 251, 255, and 233 full scene images respectively. The photos consist of a range of scenes and word level annotation is provided. Much of the test data is the same between the three datasets. For IC03, Wang <ref type="bibr" target="#b59">[56]</ref> defines per-image 50 word lexicons (IC03-50) and a lexicon of all test groundtruth words (IC03-Full). For IC11, <ref type="bibr" target="#b41">[40]</ref> defines a list of 538 query words to evaluate text based image retrieval.</p><p>The Street View Text (SVT) dataset <ref type="bibr" target="#b59">[56]</ref> consists of 249 high resolution images downloaded from Google StreetView of road-side scenes. This is a challenging dataset with a lot of noise, as well as suffering from many unannotated words. Per-image 50 word lexicons (SVT-50) are also provided.</p><p>The IIIT 5k-word dataset <ref type="bibr" target="#b40">[39]</ref> contains 3000 cropped word images of scene text and digital images obtained from Google image search. This is the largest dataset for natural image text recognition currently available. Each word image has an associated 50 word lexicon (IIIT5k-50) and 1k word lexicon (IIIT5k-1k).</p><p>IIIT Scene Text Retrieval (STR) <ref type="bibr" target="#b41">[40]</ref> is a text based image retrieval dataset also collected with Google image search. Each of the 50 query words has an associated list of 10-50 images that contain the query word. There are also a large number of distractor images with no text downloaded from Flickr. In total there are 10k images and word bounding box annotation is not provided.</p><p>The IIIT Sports-10k dataset <ref type="bibr" target="#b41">[40]</ref> is another text based image retrieval dataset constructed from frames of sports video. The images are low resolution and often noisy or blurred, with text generally located on ad-  vertisements and signboards, making this a challenging retrieval task. 10 query words are provided with 10k total images, without word bounding box annotations.</p><p>BBC News is a proprietary dataset of frames from the British Broadcasting Corporation (BBC) programmes that were broadcast between 2007 and 2012. Around 5000 hours of video (approximately 12 million frames) were processed to select 2.3 million keyframes at 1024? 768 resolution. The videos are taken from a range of different BBC programmes on news and current affairs, including the BBC's Evening News programme. Text is often present in the frames from artificially inserted labels, subtitles, news-ticker text, and general scene text. No labels or annotations are provided for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Implementation Details</head><p>We train a single model for each of the stages in our pipeline, and hyper parameters are selected using training datasets of ICDAR and SVT. Exactly the same pipeline, with the same models and hyper parameters are used for all datasets and experiments. This highlights the generalisability of our end-to-end framework to different datasets and tasks. The progression of de- tection recall and the number of proposals as the pipeline progresses can be seen in <ref type="figure" target="#fig_6">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Edge Boxes &amp; ACF Detector</head><p>The Edge Box detector has a number of hyper parameters, controlling the stride of evaluation and non maximal suppression. We use the default values of ? = 0.65 and ? = 0.75 (see <ref type="bibr" target="#b66">[62]</ref> for details of these parameters).</p><p>In practice, we saw little effect of changing these parameters in combined recall. For the ACF detector, we set the number of decision trees to be 32, 128, 512 for each round of bootstrapping. For feature aggregation, we use 4 ? 4 blocks smoothed with [1 2 1]/4 filter, with 8 scales per octave. As the detector is trained for a particular aspect ratio, we perform detection at multiple aspect ratios in the range [1, 1.2, 1.4, . . . , 3] to account for variable sized words. We train on 30k cropped 32 ? 100 positive word samples amalgamated from a number of training datasets as outlined in <ref type="bibr" target="#b30">[31]</ref>, and randomly sample negative patches from 11k images which do not contain text. <ref type="figure">Fig. 8</ref> shows the performance of our proposal generation stage. The recall at 0.5 overlap of groundtruth labelled words in the IC03 and SVT datasets is shown as a function of the number of proposal regions generated per image. The maximum recall achieved using Edge Boxes is 92%, and the maximum recall achieved by the ACF detector is around 70%. However, combining the proposals from each method increases the recall to 98% at 6k proposals and 97% at 11k proposals for IC03 and SVT respectively. The average maximum overlap of a particular proposal with a groundtruth bounding box is 0.82 on IC03 and 0.77 on SVT, suggesting the region proposal techniques produce some accurate detections amongst the thousands of false-positives.</p><p>This high recall and high overlap gives a good starting point to the rest of our pipeline, and has greatly reduced the search space of word detections from the tens of millions of possible bounding boxes to around 10k proposals per image.  <ref type="figure">Fig. 8</ref> The 0.5 overlap recall of different region proposal algorithms. The recall displayed in the legend for each method gives the maximum recall achieved. The curves are generated by decreasing the minimum score for a proposal to be valid, and terminate when no more proposals can be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">Random Forest Word Classifier</head><p>The random forest word/no-word binary classifier acts on cropped region proposals. These are resampled to a fixed 32 ? 100 size, and HOG features extracted with a cell size of 4, resulting in h ? R 8?25?36 , a 7200dimensional descriptor. The random forest classifier consists of 10 trees with a maximum depth of 64.</p><p>For training, region proposals are extracted as we describe in Section 4 on the training datasets of IC-DAR and SVT, with positive bounding box samples defined as having at least 0.5 overlap with groundtruth, and negative samples as less than 0.3 with groundtruth. Due to the abundance of negative samples, we randomly sample an equal number of negative samples to positive samples, giving 300k positive and 400k negative training samples.</p><p>Once trained, the result is a very effective falsepositive filter. We select an operating probability threshold of 0.5, giving 96.6% and 94.8% recall on IC03 and SVT positive proposal regions respectively. This filtering reduces the total number of region proposals to on average 650 (IC03) and 900 (SVT) proposals per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.3">Bounding Box Regressor</head><p>The bounding box regression CNN consists of four convolutional layers with stride 1 with {f ilter size, number of f ilters} of {5, 64}, {5, 128}, {3, 256}, {3, 512} for each layer from input respectively, followed by two fully-connected layers with 4k units and 4 units (one for each regression variable). All hidden layers are followed by rectified linear non-linearities, the inputs to convolutional layers are zero-padded to preserve dimensionality, and the convolutional layers are followed by 2 ? 2 max pooling. The fixed sized input to the CNN is a 32 ? 100 greyscale image which is zero centred by subtracting the image mean and normalised by dividing by the standard deviation.</p><p>The CNN is trained with stochastic gradient descent (SGD) with dropout <ref type="bibr" target="#b27">[28]</ref> on the fully-connected layers to reduce overfitting, minimising the L 2 distance between the estimated and groundtruth bounding boxes (Equation 1). We used 700k training examples of bounding box proposals with greater than 0.5 overlap with groundtruth computed on the ICDAR and SVT training datasets.</p><p>Before the regression, the average positive proposal region (with over 0.5 overlap with groundtruth) had an overlap of 0.61 and 0.60 on IC03 and SVT. The CNN improves this average positive overlap to 0.88 and 0.70 for IC03 and SVT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.4">Text Recognition CNN</head><p>The text recognition CNN consists of eight weight layers -five convolutional layers and three fully-connected layers. The convolutional layers have the following {f ilter size, number of f ilters}: {5, 64}, {5, 128}, {3, 256}, {3, 512}, {3, 512}. The first two fully-connected layers have 4k units and the final fully-connected layer has the same number of units as as number of words in the dictionary -90k words in our case. The final classification layer is followed by a softmax normalisation layer. Rectified linear non-linearities follow every hidden layer, and all but the fourth convolutional layers are followed by 2 ? 2 max pooling. The inputs to convolutional layers are zero-padded to preserve dimensionality. The fixed sized input to the CNN is a 32?100 greyscale image which is zero centred by subtracting the image mean and normalised by dividing by the standard deviation.</p><p>We train the network on Synth training data, backpropagating the standard multinomial logistic regression loss. Optimisation uses SGD with dropout regularisation of fully-connected layers, and we dynamically lower the learning rate as training progresses. With uniform sampling of classes in training data, we found the SGD batch size must be at least a fifth of the total number of classes in order for the network to train.</p><p>For very large numbers of classes (i.e. over 5k classes), the SGD batch size required to train effectively becomes large, slowing down training a lot. Therefore, for large dictionaries, we perform incremental training to avoid requiring a prohibitively large batch size. This involves initially training the network with 5k classes until partial convergence, after which an extra 5k classes are added. The original weights are copied for the previously trained classes, with the extra classification layer weights being randomly initialised. The network is then allowed to continue training, with the extra randomly initialised weights and classes causing a spike in training error, which is quickly trained away. This process of allowing partial convergence on a subset of the classes, before adding in more classes, is repeated until the full number of desired classes is reached.</p><p>At evaluation-time we do not do any data augmentation. If a lexicon is provided, we set the language prior P (w|L) to be equal probability for lexicon words, otherwise zero. In the absence of a lexicon, P (w|L) is calculated as the frequency of word w in a corpus (we use the opensubtitles.org English corpus) with power law normalisation. In total, this model contains around 500 million parameters and can process a word in 2.2ms on a GPU with a custom version of Caffe <ref type="bibr" target="#b31">[32]</ref>.</p><p>Recognition Results. We evaluate the accuracy of our text recognition model over a wide range of datasets and lexicon sizes. We follow the standard evaluation protocol by <ref type="bibr" target="#b59">[56]</ref> and perform recognition on the words containing only alphanumeric characters and at least three characters.</p><p>The results are shown in <ref type="table" target="#tab_2">Table 4</ref>, and highlight the exceptional performance of our deep CNN. Although we train on purely synthetic data, with no human annotation, our model obtains significant improvements on state-of-the-art accuracy across all standard datasets. On IC03-50, the recognition problem is largely solved with 98.7% accuracy -only 11 mistakes out of 860 test samples -and we significantly outperform the previous state-of-the-art <ref type="bibr" target="#b4">[7]</ref> on SVT-50 by 5% and IC13 by 3%. Compared to the ICDAR datasets, the SVT accuracy, without the constraints of a dataset-specific lexicon, is lower at 80.7%. This reflects the difficulty of the SVT dataset as image samples can be of very low quality, noisy, and with low contrast. The Synth dataset accuracy shows that our model really can recognise word samples consistently across the whole 90k dictionary. Synthetic Data Effects. As an additional experiment, we look into the contribution that the various stages of the synthetic data generation engine in Section 6.1 make to real-world recognition accuracy. We define two reduced recognition models (for speed of computation) with dictionaries covering just the IC03 and SVT full lexicons, denoted as DICT-IC03-Full and DICT-SVT-Full respectively, which are tested only on their respective datasets. We repeatedly train these models from  scratch, with the same training procedure, but with increasing levels of sophistication of synthetic data. <ref type="figure" target="#fig_9">Fig. 9</ref> shows how the test accuracy of these models increases as more sophisticated synthetic training data is used. The addition of random image-layer colouring causes a notable increase in performance (+44% on IC03 and +40% on SVT), as does the addition of natural image blending (+1% on IC03 and +6% on SVT). It is interesting to observe a much larger increase in accuracy through incorporating natural image blending on the SVT dataset compared to the IC03 dataset. This is most likely due to the fact that there are more varied and complex backgrounds to text in SVT compared to in IC03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Text Spotting</head><p>In the text spotting task, the goal is to localise and recognise the words in the test images. Unless otherwise stated, we follow the standard evaluation protocol by <ref type="bibr" target="#b59">[56]</ref> and ignore all words that contain alphanumeric characters and are not at least three characters long. A positive recognition result is only valid if the detection bounding box has at least 0.5 overlap (IoU) with the groundtruth. <ref type="table">Table 5</ref> shows the results of our text spotting pipeline compared to previous methods. We report the global F-measure over all images in the dataset. Across all datasets, our pipeline drastically outperforms all previ-  <ref type="table">Table 5</ref> Comparison to previous methods for end-to-end text spotting. Bold results outperform previous state-of-the-art methods. * Recognition is constrained to a dictionary of 50k words. + Evaluation protocol described in <ref type="bibr" target="#b47">[44]</ref>. ous methods. On SVT-50, we increase the state-of-theart by +20% to a P/R/F (precision/recall/F-measure) of 0.85/0.68/0.76 compared to 0.73/0.45/0.56 in <ref type="bibr" target="#b30">[31]</ref>. Similarly impressive improvements can be seen on IC03, where in all lexicon scenarios we improve F-measure by at least +10%, reaching a P/R/F of 0.96/0.85/0.90. Looking at the precision/recall curves in <ref type="figure" target="#fig_10">Fig. 10</ref>, we can see that our pipeline manages to maintain very high recall, and the recognition score of our text recognition system is a strong cue to the suitability of a detection.</p><p>We also give results across all datasets when no lexicon is given. As expected, the F-measure suffers from the lack of lexicon constraints, though is still significantly higher than other comparable work. It should  <ref type="table">Table 6</ref> Comparison to previous methods for text based image retrieval. We report mean average precision (mAP) for IC11, SVT, STR, and Sports, and also report top-n retrieval to compute precision at n (P@n) on Sports. Bold results outperform previous state-of-the-art methods. * Experiments were performed by Mishra et al. in <ref type="bibr" target="#b41">[40]</ref>, not by the original authors.</p><p>1.00/1.00/1.00</p><p>1.00/1.00/1.00</p><p>1.00/1.00/1.00</p><p>1.00/1.00/1.00 1.00/0.88/0.93 1.00/1.00/1.00 be noted that the SVT dataset is only partially annotated. This means that the precision (and therefore Fmeasure) is much lower than the true precision if fully annotated, since many words that are detected are not annotated and are therefore recorded as false-positives. We can however report recall on SVT-50 and SVT of 71% and 59% respectively.</p><p>Interestingly, when the overlap threshold is reduced to 0.3 (last row of <ref type="table">Table 5</ref>), we see a small improvement across ICDAR datasets and a large +8% improvement on SVT-50. This implies that our text recognition CNN is able to accurately recognise even loosely cropped detections. Ignoring the requirement of correctly recognising the words, i.e. performing purely word detection, we get an F-measure of 0.85 and 0.81 for IC03 and IC11.</p><p>Some example text spotting results are shown in <ref type="figure" target="#fig_11">Fig. 11</ref>. Since our pipeline does not rely on connected component based algorithms or explicit character recog-nition, we can detect and recognise disjoint, occluded and blurry words.</p><p>A common failure mode of our system is the missing of words due to the lack of suitable proposal regions, especially apparent for slanted or vertical text, something which is not explicitly modelled in our framework. Also the detection of sub-words or multiple words together can cause false-positive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Image Retrieval</head><p>We also apply our pipeline to the task of text based image retrieval. Given a text query, the images containing the query text must be returned.</p><p>This task is evaluated using the framework of <ref type="bibr" target="#b41">[40]</ref>, with the results shown in <ref type="table">Table 6</ref>. For each defined query, we retrieve a ranked list of all the images of the dataset and compute the average precision (AP) hollywood -P@100: 100% boris johnson -P@100: 100% vision -P@100: 93% <ref type="figure" target="#fig_0">Fig. 12</ref> The top two retrieval results for three queries on our BBC News datasethollywood, boris johnson, and vision. The frames and associated videos are retrieved from 5k hours of BBC video. We give the precision at 100 (P@100) for these queries, equivalent to the first page of results of our web application.</p><p>for each query, reporting the mean average precision (mAP) over all queries. We significantly outperform Mishra et al . across all datasets -we obtain an mAP on IC11 of 90.3%, compared to 65.3% from <ref type="bibr" target="#b41">[40]</ref>. Our method scales seamlessly to the larger Sports dataset, where our system achieves a precision at 20 images (P@20) of 92.5%, more than doubling that of 43.4% from <ref type="bibr" target="#b41">[40]</ref>. Mishra et al . <ref type="bibr" target="#b41">[40]</ref> also report retrieval results on SVT for released implementations of other text spotting algorithms. The method from Wang et al . <ref type="bibr" target="#b59">[56]</ref> achieves 21.3% mAP, the method from Neumann et al .</p><p>[43] acheives 23.3% mAP and the method proposed by <ref type="bibr" target="#b41">[40]</ref> itself achieves 56.2% mAP, compared to our own result of 86.3% mAP. However, as with the text spotting results for SVT, our retrieval results suffer from incomplete annotations on SVT and Sports datasets - <ref type="figure" target="#fig_1">Fig. 13</ref> shows how precision is hurt by this problem. The consequence is that the true mAP on SVT is higher than the reported mAP of 86.3%.</p><p>Depending on the image resolution, our algorithm takes approximately 5-20s to compute the end-to-end results per image on a single CPU core and single GPU. We analyse the time taken for each stage of our pipeline on the SVT dataset, which has an average image size of 1260 ? 860, showing the results in <ref type="table">Table 7</ref>. Since we reduce the number of proposals throughout the pipeline, we can allow the processing time per proposal to increase while keeping the total processing time for each stage stable. This affords us the use of more computa-  <ref type="table">Table 7</ref> The processing time for each stage of the pipeline evaluated on the SVT dataset on a single CPU core and single GPU. As the pipeline progresses from (a)-(e), the number of proposals is reduced (starting from all possible bounding boxes), allowing us to increase our computational budget per proposal while keeping the overall processing time for each stage comparable.</p><p>tionally complex features and classifiers as the pipeline progresses. Our method can be trivially parallelised, meaning we can process 1-2 images per second on a high-performance workstation with 16 physical CPU cores and 4 commodity GPUs.</p><p>The high precision and speed of our pipeline allows us to process huge datasets for practical search applications. We demonstrate this on a 5000 hour BBC News dataset. Building a search engine and front-end web application around our image retrieval pipeline allows a user to instantly search for visual occurrences of text within the huge video dataset. This works exceptionally well, with <ref type="figure" target="#fig_0">Fig. 12</ref> showing some example retrieval results from our visual search engine. While we do not have groundtruth annotations to quantify the retrieval performance on this dataset, we measure the precision at 100 (P@100) for the test queries in <ref type="figure" target="#fig_0">Fig. 12</ref>, showing a P@100 of 100% for the queries hollywood and boris johnson, and 93% for vision. These results demonstrate the scalable nature of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>In this work we have presented an end-to-end text reading pipeline -a detection and recognition system for text in natural scene images. This general system works remarkably well for both text spotting and image retrieval tasks, significantly improving the performance on both tasks over all previous methods, on all standard datasets, without any dataset-specific tuning. This is largely down to a very high recall proposal stage and a text recognition model that achieves superior accuracy to all previous systems. Our system is fast and scalable -we demonstrate seamless scalability from datasets of hundreds of images to being able to process datasets of millions of images for instant text based image retrieval without any perceivable degradation in accuracy. Additionally, the ability of our recognition model to be trained purely on synthetic data allows our system to be easily re-trained for recognition of other languages or scripts, without any human labelling effort.</p><p>We set a new benchmark for text spotting and image retrieval. Moving into the future, we hope to explore additional recognition models to allow the recognition of unknown words and arbitrary strings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>The shortcoming of using an overlap ratio for text detection of 0.5. The two examples of proposal bounding boxes (green solid box) have approximately 0.5 overlap with groundtruth (red dashed box). In the bottom case, a 0.5 overlap is not satisfactory to produce accurate text recognition results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>The bounding box regression encoding scheme showing the original proposal (red) and the adjusted proposal (green). The cropped input image shown is always centred on the original proposal, meaning the original proposal always has implied encoded coordinates of (0.5, 0.5, 0.5, 0.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 A</head><label>5</label><figDesc>schematic of the CNN used for text recognition by word classification. The dimensions of the featuremaps at each layer of the network are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>An example of the improvement in localisation of the word detection pharmacy through multiple rounds of recurrent regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7</head><label>7</label><figDesc>The recall and the average number of proposals per image at each stage of the pipeline on IC03. (a) Edge Box proposals, (b) ACF detector proposals, (c) Proposal filtering, (d) Bounding box regression, (e) Regression NMS round 1, (f) Regression NMS round 2, (g) Regression NMS round 3. The recall computed is detection recall across the dataset (i.e. ignoring the recognition label) at 0.5 overlap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(0.92 recall) SVT Edge Boxes (0.92 recall) IC03 ACF (0.70 recall) SVT ACF (0.74 recall) IC03 Combined (0.98 recall) SVT Combined (0.97 recall)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9</head><label>9</label><figDesc>The recognition accuracies of text recognition models trained on just the IC03 lexicon (DICT-03-Full) and just the SVT lexicon (DICT-SVT-Full), evaluated on IC03 and SVT respectively. The models are trained on purely synthetic data with increasing levels of sophistication of the synthetic data.(a) Black text rendered on a white background with a single font, Droid Sans. (b) Incorporating all of Google fonts. (c) Adding background, foreground, and border colouring. (d) Adding perspective distortions. (e) Adding noise, blur and elastic distortions. (f) Adding natural image blending -this gives an additional 6.2% accuracy on SVT. The final accuracies on IC03 and SVT are 98.1% and 87.0% respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10</head><label>10</label><figDesc>The precision/recall curves on (a) the IC03-50 dataset, (b) the IC03-Full dataset, and (c) the SVT-50 dataset. The lines of constant F-measure are shown at the maximum F-measure point of each curve. The results from<ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b60">57]</ref> were extracted from the papers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11</head><label>11</label><figDesc>Some example text spotting results from SVT-50 (top row) and IC11 (bottom row). Red dashed shows groundtruth and green shows correctly localised and recognised results. P/R/F figures are given above each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1 .</head><label>1</label><figDesc>Font rendering -a font is randomly selected from a catalogue of over 1400 fonts downloaded from Google</figDesc><table /><note>Fonts. The kerning, weight, underline, and other properties are varied randomly from arbitrarily de- fined distributions. The word is rendered on to the foreground image-layer's alpha channel with either a horizontal bottom text line or following a random curve. 2. Border/shadow rendering -an inset border, outset border, or shadow with a random width may be ren- dered from the foreground. 3. Base colouring -each of the three image-layers are filled with a different uniform colour sampled from clusters over natural images. The clusters are formed by k-means clustering the RGB components of each image of the training datasets of [36] into three clus- ters. 4. Projective distortion -the foreground and border/shadow image-layers are distorted with a random, full pro- jective transformation, simulating the 3D world. 5. Natural data blending -each of the image-layers are blended with a randomly-sampled crop of an im- age from the training datasets of ICDAR 2003 and SVT. The amount of blend and alpha blend mode (e.g. normal, add, multiply, burn, max, etc.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 Table 2 Table 3</head><label>123</label><figDesc>A description of the various text recognition datasets evaluated on. A description of the various text spotting datasets evaluated on. A description of the various text retrieval datasets evaluated on.</figDesc><table><row><cell></cell><cell>Text Recognition Datasets</cell><cell></cell><cell></cell></row><row><cell>Label</cell><cell>Description</cell><cell cols="2">Lex. size # images</cell></row><row><cell>Synth</cell><cell>Our synthetically generated test dataset.</cell><cell>90k</cell><cell>900k</cell></row><row><cell>IC03</cell><cell>ICDAR 2003 [1] test dataset.</cell><cell>-</cell><cell>860</cell></row><row><cell>IC03-50</cell><cell>ICDAR 2003 [1] test dataset with fixed lexicon.</cell><cell>50</cell><cell>860</cell></row><row><cell>IC03-Full</cell><cell>ICDAR 2003 [1] test dataset with fixed lexicon.</cell><cell>860</cell><cell>860</cell></row><row><cell>SVT</cell><cell>SVT [55] test dataset.</cell><cell>-</cell><cell>647</cell></row><row><cell>SVT-50</cell><cell>SVT [55] test dataset with fixed lexicon.</cell><cell>50</cell><cell>647</cell></row><row><cell>IC13</cell><cell>ICDAR 2013 [33] test dataset.</cell><cell>-</cell><cell>1015</cell></row><row><cell cols="2">IIIT5k-50 IIIT5k [39] test dataset with fixed lexicon.</cell><cell>50</cell><cell>3000</cell></row><row><cell cols="2">IIIT5k-1k IIIT5k [39] test dataset with fixed lexicon.</cell><cell>1000</cell><cell>3000</cell></row><row><cell></cell><cell>Text Spotting Datasets</cell><cell></cell><cell></cell></row><row><cell>Label</cell><cell>Description</cell><cell cols="2">Lex. size # images</cell></row><row><cell>IC03</cell><cell>ICDAR 2003 [1] test dataset.</cell><cell>-</cell><cell>251</cell></row><row><cell>IC03-50</cell><cell>ICDAR 2003 [1] test dataset with fixed lexicon.</cell><cell>50</cell><cell>251</cell></row><row><cell cols="2">IC03-Full ICDAR 2003 [1] test dataset with fixed lexicon.</cell><cell>860</cell><cell>251</cell></row><row><cell>SVT</cell><cell>SVT [55] test dataset.</cell><cell>-</cell><cell>249</cell></row><row><cell>SVT-50</cell><cell>SVT [55] test dataset with fixed lexicon.</cell><cell>50</cell><cell>249</cell></row><row><cell>IC11</cell><cell>ICDAR 2011 [52] test dataset.</cell><cell>-</cell><cell>255</cell></row><row><cell>IC13</cell><cell>ICDAR 2013 [33] test dataset.</cell><cell>-</cell><cell>233</cell></row><row><cell></cell><cell>Text Retrieval Datasets</cell><cell></cell><cell></cell></row><row><cell>Label</cell><cell>Description</cell><cell cols="2"># queries # images</cell></row><row><cell>IC11</cell><cell>ICDAR 2011 [52] test dataset.</cell><cell>538</cell><cell>255</cell></row><row><cell>SVT</cell><cell>SVT [55] test dataset.</cell><cell>427</cell><cell>249</cell></row><row><cell>STR</cell><cell>IIIT STR [40] text retrieval dataset.</cell><cell>50</cell><cell>10k</cell></row><row><cell>Sports</cell><cell>IIIT Sports-10k [40] text retrieval dataset.</cell><cell>10</cell><cell>10k</cell></row><row><cell cols="2">BBC News A dataset of keyframes from BBC News video.</cell><cell>-</cell><cell>2.3m</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Comparison to previous methods for text recognition accuracy -where the groundtruth cropped word image is given as input. The ICDAR 2013 results given are case-insensitive. Bold results outperform previous state-of-the-art methods. The baseline method is from a commercially available document OCR system.</figDesc><table><row><cell></cell><cell></cell><cell cols="7">Cropped Word Recognition Accuracy (%)</cell><cell></cell></row><row><cell>Model</cell><cell cols="9">Synth IC03-50 IC03-Full IC03 SVT-50 SVT IC13 IIIT5k-50 IIIT5k-1k</cell></row><row><cell>Baseline (ABBYY) [56, 59]</cell><cell>-</cell><cell>56.0</cell><cell>55.0</cell><cell>-</cell><cell>35.0</cell><cell>-</cell><cell>-</cell><cell>24.3</cell><cell>-</cell></row><row><cell>Wang [56]</cell><cell>-</cell><cell>76.0</cell><cell>62.0</cell><cell>-</cell><cell>57.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mishra [39]</cell><cell>-</cell><cell>81.8</cell><cell>67.8</cell><cell>-</cell><cell>73.2</cell><cell>-</cell><cell>-</cell><cell>64.1</cell><cell>57.5</cell></row><row><cell>Novikova [45]</cell><cell>-</cell><cell>82.8</cell><cell>-</cell><cell>-</cell><cell>72.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang &amp; Wu [57]</cell><cell>-</cell><cell>90.0</cell><cell>84.0</cell><cell>-</cell><cell>70.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Goel [25]</cell><cell>-</cell><cell>89.7</cell><cell>-</cell><cell>-</cell><cell>77.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PhotoOCR [7]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.4</cell><cell cols="2">78.0 87.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Alsharif [5]</cell><cell>-</cell><cell>93.1</cell><cell>88.6</cell><cell>85.1 *</cell><cell>74.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Almazan [4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.2</cell><cell>-</cell><cell>-</cell><cell>91.2</cell><cell>82.1</cell></row><row><cell>Yao [59]</cell><cell>-</cell><cell>88.5</cell><cell>80.3</cell><cell>-</cell><cell>75.9</cell><cell>-</cell><cell>-</cell><cell>80.2</cell><cell>69.3</cell></row><row><cell>Jaderberg [31]</cell><cell>-</cell><cell>96.2</cell><cell>91.5</cell><cell>-</cell><cell>86.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Gordo [27]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.7</cell><cell>-</cell><cell>-</cell><cell>93.3</cell><cell>86.6</cell></row><row><cell>Proposed</cell><cell>95.2</cell><cell>98.7</cell><cell>98.6</cell><cell>93.3</cell><cell cols="3">95.4 80.7 90.8</cell><cell>97.1</cell><cell>92.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Recognition is constrained to a dictionary of 50k</cell></row><row><cell>words.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="7">End-to-End Text Spotting (F-measure %)</cell><cell></cell></row><row><cell>Model</cell><cell cols="9">IC03-50 IC03-Full IC03 IC03 + SVT-50 SVT IC11 IC11 + IC13</cell></row><row><cell>Neumann [42]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>41</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang [56]</cell><cell>68</cell><cell>51</cell><cell>-</cell><cell>-</cell><cell>38</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang &amp; Wu [57]</cell><cell>72</cell><cell>67</cell><cell>-</cell><cell>-</cell><cell>46</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Neumann [44]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45</cell><cell>-</cell></row><row><cell>Alsharif [5]</cell><cell>77</cell><cell>70</cell><cell>63 *</cell><cell>-</cell><cell>48</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Jaderberg [31]</cell><cell>80</cell><cell>75</cell><cell>-</cell><cell>-</cell><cell>56</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Proposed</cell><cell>90</cell><cell>86</cell><cell>78</cell><cell>72</cell><cell>76</cell><cell>53</cell><cell>76</cell><cell>69</cell><cell>76</cell></row><row><cell>Proposed (0.3 IoU)</cell><cell>91</cell><cell>87</cell><cell>79</cell><cell>73</cell><cell>82</cell><cell>57</cell><cell>77</cell><cell>70</cell><cell>77</cell></row></table><note>*</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the EPSRC and ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. We thank the BBC and in particular Rob Cooper for access to data and video processing resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Word spotting and recognition with embedded attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forn?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-End Text Recognition with Hybrid HMM Maxout Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detection of artificial and scene text in images and video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anthimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pho-toOCR: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Character recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>VISAPP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust text detection in natural images with edge-enhanced maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Image Processing (ICIP)</title>
		<meeting>International Conference on Image essing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2609" to="2612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">366</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bing: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<title level="m">Fast feature pyramids for object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The fastest pedestrian detector in the west</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1841" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5549</idno>
		<title level="m">Fast edge detection using structured forests</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grishick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hmmbased word spotting in handwritten documents using subword models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Frinken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern recognition (icpr)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3416" to="3419" />
		</imprint>
	</monogr>
	<note>20th international conference on</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Additive logistic regression: a statistical view of boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="407" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A novel word spotting method based on recurrent neural networks. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Frinken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="224" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Whole is greater than sum of parts: Recognizing scene text words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICDAR</publisher>
			<biblScope unit="page" from="398" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">SVT: apartments Sports: castrol</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">We show examples of the top two results for the query apartments on the SVT dataset and the query castrol on the Sports dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fig</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>All retrieved images contain the query word (green box), but some of the results have incomplete annotations. and so although the query word is present the result is labelled as incorrect. This leads to a reported AP of 0.5 instead of 1.0 for the SVT query, and a reported P@2 of 0</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6082</idno>
		<title level="m">Multi-digit number recognition from street view imagery using deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Supervised mid-level features for word image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust scene text detection with convolution neural network induced mser trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<title level="m">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>de las</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Heras</surname></persName>
		</author>
		<title level="m">ICDAR 2013 robust reading competition. In: ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ICDAR 2005 text locating competition results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="80" to="84" />
		</imprint>
	</monogr>
	<note type="report_type">Proceedings</note>
	<note>Document Analysis and Recognition</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Icdar 2003 robust reading competitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Word spotting: A new approach to indexing handwriting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="631" to="637" />
		</imprint>
	</monogr>
	<note>Proceedings CVPR&apos;96</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust wide baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="127" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image retrieval using textual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3040" to="3047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="770" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Text localization in real-world images using efficiently pruned exhaustive search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="687" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scene text localization and recognition with oriented stroke detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Large-lexicon attribute-consistent text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="752" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast keypoint recognition in ten lines of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozuysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Largescale image retrieval with compressed fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Using text-spotting to query the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IROS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Large scale mining and retrieval of visual data in a multimodal context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Zurich</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Word spotting for historical documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJDAR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="139" to="152" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Label embedding for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Rodriguez-Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">ICDAR 2011 robust reading competition challenge 2: Reading text in scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1491" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="958" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3304" to="3308" />
		</imprint>
	</monogr>
	<note>In: ICPR</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Toward integrated scene text reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Weinman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feild</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2013.126</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="375" to="387" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<title level="m">IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4042" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Text string detection from natural scenes by structure-based partition and grouping. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2594" to="2605" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Robust text detection in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1301.2628</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
