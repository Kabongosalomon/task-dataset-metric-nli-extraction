<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdaCoF: Adaptive Collaboration of Flows for Video Frame Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeoh</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Young</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Pak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Ban</surname></persName>
							<email>ban@add.re.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Agency for Defense Development</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
							<email>syleee@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AdaCoF: Adaptive Collaboration of Flows for Video Frame Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video frame interpolation is one of the most challenging tasks in video processing research. Recently, many studies based on deep learning have been suggested. Most of these methods focus on finding locations with useful information to estimate each output pixel using their own frame warping operations. However, many of them have Degrees of Freedom (DoF) limitations and fail to deal with the complex motions found in real world videos. To solve this problem, we propose a new warping module named Adaptive Collaboration of Flows (AdaCoF). Our method estimates both kernel weights and offset vectors for each target pixel to synthesize the output frame. AdaCoF is one of the most generalized warping modules compared to other approaches, and covers most of them as special cases of it. Therefore, it can deal with a significantly wide domain of complex motions. To further improve our framework and synthesize more realistic outputs, we introduce dualframe adversarial loss which is applicable only to video frame interpolation tasks. The experimental results show that our method outperforms the state-of-the-art methods for both fixed training set environments and the Middlebury benchmark. Our source code is available at https:// github.com/HyeongminLEE/AdaCoF-pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Synthesizing the intermediate frame when consecutive frames have been provided is one of the main research topics in the video processing area. Using a frame interpolation algorithm, we can obtain slow-motion videos from ordinary videos without using professional high-speed cameras. In addition, we can freely convert the frame rates of the videos so it can be applied to the video coding system. To interpolate the intermediate frame of a video requires an understanding of motion, unlike image pixel interpolation. Unfortunately, real world videos contain not only simple motions, but also large and complex ones, making the task significantly more difficult. Most of the approaches define video frame interpolation as a problem of finding reference locations in input frames which include information for estimating each output pixel value. This can be seen as a motion estimation process, because the task involves tracking the path of the target pixel. Therefore, each algorithm covers its own motion domain, and this area is directly related to the performance. To handle motion in real world videos, we need a generalized operation that can refer to any number of pixels in any location in the input frames. However, most of the existing approaches have a variety of limitations in Degrees of Freedom (DoF).</p><p>One is the kernel-based approach <ref type="figure" target="#fig_0">(Figure 1 (a)</ref>) <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35]</ref>, which adaptively estimates the large-sized kernel for each pixel and synthesizes the intermediate frame by convolving the kernels with the input. This approach finds the proper reference location by assigning large weights to the pixels of interest. However, it does not refer to any location, as it cannot deal with large motions beyond the kernel size. It is not efficient to keep the large size of the kernel even though the motion is small. The second approach is the flow-based approach <ref type="figure" target="#fig_0">(Figure 1 (b)</ref>) <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b27">27]</ref>, which estimates the flow vector directly pointing to the reference location for each output pixel. However, it cannot refer to any number of pixels because only one location is referred to in each input frame. Therefore, it is not suitable for complex motions and the result may suffer from lack of information when the input frame is of low-quality. Recently, methods of combining kernel-based and flow-based approaches are proposed to compensate for each other's limitations <ref type="figure" target="#fig_0">(Figure 1</ref> (c)) <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b2">3]</ref>. They multiply the kernels with the location pointed to by the flow vector. Therefore, they can refer to any location plus some additional neighboring pixels. However, this approach is not much different from the flow based approach as it uses significantly fewer reference points than the kernel-based one. In addition, there is room for improvement in terms of DoF because the shape of the kernel is a fixed square.</p><p>In this paper, we propose an operation that refers to any number of pixels and any location called Adaptive Collaboration of Flows (AdaCoF). To synthesize a target pixel, we estimate multiple flows, called offset vectors, pointing to the reference locations and sample them. Then the target pixel is obtained by linearly combining the sampled values. Our method is inspired by deformable convolution (Def-Conv) <ref type="bibr" target="#b7">[8]</ref>, but AdaCoF is significantly different from it in some points. First, DefConv has a shared weight for all positions, and it is not suitable for video because there are various motions in each position of a frame. Therefore, we allow the weights to be spatially adaptive. Second, AdaCoF is used as an independent module for frame warping, not for feature extraction as DefConv. Therefore, we obtain the weights as the outputs of a neural network, instead of training them as learnable parameters. Third, we add dilation for the starting point of the offset vectors to enforce the them to search a wider area. Lastly, we add an occlusion mask to utilize only one of the two input frames when one of the reference pixels is occluded. As shown in <ref type="figure" target="#fig_0">Figure 1 (d)</ref>, it can refer to any number within any location in the input frames, because the sizes and shapes of the kernels are not fixed. Therefore, our method has the highest DoF compared to most of the other competitive algorithms, and therefore can deal with various complex motions in real world videos. To make the synthesized frames more realistic, we further train a discriminator to detect the generated frame given the output and one of the input frames. Then we train the generator to maximize the entropy of the discriminator using dual-frame adversarial loss. Experimental results on various benchmarks show the effectiveness of AdaCoF over the latest state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most of the classic video frame interpolation methods estimate the dense flow maps using optical flow algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46]</ref> and warp the input frames <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50]</ref>. Therefore, the performance of these approaches largely depends on optical flow algorithms. Also, optical flow based approaches have limitations in many cases, such as occlusions, large motion, and brightness changes. Although there are some approaches without using external optical flow modules <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b29">29]</ref>, they still have difficulty in dealing with these problems. Meyer et al. <ref type="bibr" target="#b32">[32]</ref> regard video frames as linear combinations of wavelets with different directions and frequencies. This approach interpolates each wavelet's phase and magnitude. This method makes notable progress in both performance and running time. Their recent work also applies deep learning to this approach <ref type="bibr" target="#b31">[31]</ref>. However, it still has limitations for large motions of high frequency components.</p><p>Recent work has demonstrated the success of applying deep learning in the field of computer vision <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b41">41]</ref>, which, in turn, inspires various deep learning based frame interpolation methods. As all we require for training neural networks are three consecutive video frames, learning based approaches are appropriate for this task. Long et al. <ref type="bibr" target="#b28">[28]</ref> propose a CNN architecture that uses two input frames and directly estimates the intermediate frame. However, this type of approach often leads to blurry results. Some other methods focus on where to find the output pixel from the input frames, instead of directly estimating the image. This paradigm is based on the fact that at least one input frame contains the output pixel, even in the case of occlusion. Niklaus et al. <ref type="bibr" target="#b34">[34]</ref> estimate a kernel for each location and obtains the output pixel by convolving it over input patches. Each kernel samples the proper input pixels by combining them selectively. However, this requires a lot of memory and estimating large kernels for every pixel is computationally expensive. Niklaus et al. <ref type="bibr" target="#b35">[35]</ref> solve this problem by estimating each kernel from the outer product of two vectors. However, this approach cannot handle motions larger than the kernel size and it is still wasteful to estimate large kernels for small motions. Liu et al. <ref type="bibr" target="#b27">[27]</ref> estimate a flow map that consists of vectors directly pointing to reference locations. They sample the proper pixels according to the flow map. However, as they assume that the forward and backward flows are the same, it is difficult to handle complex motions. Jiang et al. <ref type="bibr" target="#b20">[20]</ref> propose a similar algorithm, but they estimate the forward and backward flows separately. They also improve the flow computation stage by defining the warping loss. However, it could be risky to get only one pixel value from each frame, especially when the input patches are of poor quality. To solve these problems, Reda et al. <ref type="bibr" target="#b38">[38]</ref> and Bao et al. <ref type="bibr" target="#b2">[3]</ref> combine kernel and flow map based approaches. They multiply small-sized kernels with the locations pointed by the flow vectors. However, the reference points are still limited in a small area because the kernels maintain their square shape, which results in low DoF.</p><p>There are some approaches that use additional information to solve problems in video frame interpolation. Niklaus et al. <ref type="bibr" target="#b33">[33]</ref> exploit the context informations extracted from ResNet-18 <ref type="bibr" target="#b18">[18]</ref> to enable the informative interpolation and succeed in obtaining high-quality results. In addition, Bao et al. <ref type="bibr" target="#b1">[2]</ref> use depth maps estimated from hourglass architecture <ref type="bibr" target="#b5">[6]</ref> to solve the occlusion problems. Lastly, Liu et al. <ref type="bibr" target="#b26">[26]</ref> obtain better performance with cycle consistency loss and additional edge maps. These approaches can be independently applied to many other algorithms, including our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video Frame Interpolation</head><p>Given consecutive video frames I n and I n+1 , where n ? Z is a frame index, our goal is to find the intermediate frame I out . All the information required to produce I out can be obtained from I n and I n+1 . Therefore, all we have to do is find the relations between them. We regard the relation as a warping operation T from I n and I n+1 to I out . For the forward and backward warping operations T f and T b , we can consider I out as a combination of T f (I n ) and T b (I n+1 ) as follows.</p><formula xml:id="formula_0">I out = T f (I n ) + T b (I n+1 )<label>(1)</label></formula><p>The frame interpolation task results in a problem of how the spatial transform T can be found. We employ a new operation called Adaptive Collaboration of Flows (AdaCoF) for T , which convolve the input image with adaptive kernel weights and offset vectors for each output pixel. Occlusion reasoning. Let both the input and output image sizes be M ? N . In the case of occlusion, the target pixel will not be visible in one of the input images. Therefore we define occlusion map V ? [0, 1] M ?N and modify Equation (1) as follows.</p><formula xml:id="formula_1">I out = V T f (I n ) + (J ? V ) T b (I n+1 ),<label>(2)</label></formula><p>where is a pixel-wise multiplication and J is an M ? N matrix of ones. For the target pixel (i, j), V (i, j) = 1 implies that the pixel is visible only in I n and V (i, j) = 0 implies that it is visible only in I n+1 . </p><formula xml:id="formula_2">(a) d = 0 (b) d = 1 (c) d = 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptive Collaboration of Flows</head><p>Let the frame warped from I be?. When we define T as a classic convolution, we can write? as follows.</p><formula xml:id="formula_3">I(i, j) = F ?1 k=0 F ?1 l=0 W k,l I(i + k, j + l),<label>(3)</label></formula><p>where F is the kernel size and W k,l are the kernel weights.</p><p>The input image I is considered to be padded so that the original input and output size are equal. Deformable convolution <ref type="bibr" target="#b7">[8]</ref> adds offset vectors ?p k,l = (? k,l , ? k,l ) to the classic convolution as follows.</p><formula xml:id="formula_4">I(i, j) = F ?1 k=0 F ?1 l=0 W k,l I(i + k + ? k,l , j + l + ? k,l ) (4)</formula><p>AdaCoF, unlike the classic deformable convolutions, does not share the kernel weights over the different pixels. Therefore the notation for the kernel weights W k,l should be written as follows.</p><formula xml:id="formula_5">I(i, j) = F ?1 k=0 F ?1 l=0 W k,l (i, j)I(i + k + ? k,l , j + l + ? k,l )<label>(5)</label></formula><p>The offset values ? k,l and ? k,l may not be integer values. In other words, (? k,l , ? k,l ) could point to an arbitrary location, not only the grid point. Therefore, the pixel value of I for any location has to be defined. We use bilinear interpolation to obtain the values of non-grid location as DCNs <ref type="bibr" target="#b7">[8]</ref>. It also makes the module differentiable; therefore, the whole network can be trained end-to-end. Dilation. We found that dilating the starting point of the offset vectors helps AdaCoF to explore wider area as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Therefore, we add dilation term d ? {0, 1, 2, ...} to the operation as follows. </p><formula xml:id="formula_6">I(i, j) = F ?1 k=0 F ?1 l=0 W k,l (i, j)I(i + dk + ? k,l , j + dl + ? k,l )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>We design a fully convolutional neural network which estimates the kernel weights W k,l , offset vectors (? k,l , ? k,l ), and occlusion map V . Therefore, any video frames size can be used as the input. Furthermore, because each module of the neural network is differentiable, it is end-to-end trainable. Our neural network starts with the U-Net architecture, which consists of encoder, decoder, and skip connections <ref type="bibr" target="#b39">[39]</ref>. Each processing unit basically contains 3 ? 3 convolution and ReLU activation. For the encoder part, we use average pooling to extract the features. And for the decoder part, we use bilinear interpolation for the upsampling. After the U-Net architecture, the seven sub-networks finally estimate the outputs (W k,l , ? k,l , ? k,l for each frame and V ). We use sigmoid activation for V to satisfy V ? [0, 1] M ?N . Moreover, as the weights W k,l for each pixel have to be non-negative and must add up to 1, softmax layers are used for the constraints. More specific architectures of the network are described in <ref type="figure" target="#fig_2">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Functions</head><p>Loss Function. First, we have to reduce a difference between the model output I out and ground truth I gt . We use 1 norm for the loss as follows.</p><formula xml:id="formula_7">L 1 = I out ? I gt 1<label>(7)</label></formula><p>The 2 norm can be used, but it is known that the 2 norm-based optimization leads to blurry results in most of the image synthesis tasks <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b43">43]</ref>. Following Liu et al. <ref type="bibr" target="#b27">[27]</ref>, we use the Charbonnier Function ?(x) = (x 2 + 2 ) 1/2 for optimizing 1 norm, where = 0.001. Perceptual Loss. Perceptual loss has been found to be effective in producing visually more realistic outputs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b51">51]</ref>. We add the perceptual loss with the feature extractor F from conv4 3 of ImageNet pretrained VGG16 network.</p><formula xml:id="formula_8">L vgg = F(I out ) ? F(I gt ) 2<label>(8)</label></formula><p>Dual-Frame Adversarial Loss. It is known that training the networks with adversarial loss <ref type="bibr" target="#b15">[15]</ref> can lead to results of higher quality and sharpness, instead of increasing mean squared error <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b4">5]</ref>. This could be applied to video frame interpolation tasks. However, simply applying it to the single output frame does not consider the temporal consistency and leads to a disparate result compared to the input frames. What we want is to make the synthesized frame appear natural among the adjacent frames, not the other real images. Therefore, we concatenate the generated frame and one of the input frames in the temporal order and train the discriminator C to distinguish which of the two is the generated frame with the following loss.</p><p>?L C = log(C([I n , I out ]))+log(1?C([I out , I n+1 ])), where [?] is concatenation. Then we train the main network to maximize the uncertainty, i.e., entropy, of the discriminator with the following loss. This idea is inspired by some prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>.</p><formula xml:id="formula_10">L adv =C([I n , I out ]) log(C([I n , I out ])) + C([I out , I n+1 ]) log(C([I out , I n+1 ]))<label>(10)</label></formula><p>Thus, the network is intended to generate an output that is realistic compared to the adjacent input frames.</p><p>We finally combine above losses to compose two versions of objective function: distortion-oriented loss (L d ) and perception-oriented loss (L p ) as follows.</p><formula xml:id="formula_11">L d = L 1 ,<label>(11)</label></formula><formula xml:id="formula_12">L p = ? 1 L 1 + ? vgg L vgg + ? adv L adv ,<label>(12)</label></formula><p>For the perception-oriented version, we first train the network with L d then fine-tune it with L p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Learning Strategy. We train our neural network using AdaMax optimizer <ref type="bibr" target="#b22">[22]</ref>, where ? 1 = 0.9, ? 2 = 0.999. The learning rate is initially 0.001 and decays half every 20 epochs. The batch size is 4 and the network is trained for 50 epochs. Training Dataset. We use Vimeo90K <ref type="bibr" target="#b49">[49]</ref> dataset for training. It contains 51,312 triplets of 256 ? 448 video frames. To augment the dataset, we randomly crop 256 ? 256 patches from the original images. We also eliminate the biases due to the priors by flipping horizontally, vertically and swapping the order of frames for the probability 0.5. Computational issue. Our approach is implemented using PyTorch <ref type="bibr" target="#b36">[36]</ref>. To implement the AdaCoF layer, we used CUDA and cuDNN <ref type="bibr" target="#b6">[7]</ref> for the parallel processing. We set the kernel size 5 ? 5 and all the weights, offsets and occlusion map require 0.94 GB of memory for a 1080p video frame. It is about 70% demand compared to   Niklaus et al. <ref type="bibr" target="#b35">[35]</ref>. Using RTX 2080 Ti GPU, it takes 0.21 seconds to synthesize a 1280 ? 720 frame.</p><p>Evaluation settings. The test datasets used for the experiments are the Middlebury dataset <ref type="bibr" target="#b0">[1]</ref>, some randomly sampled sequences from UCF101 <ref type="bibr" target="#b42">[42]</ref> and the DAVIS dataset <ref type="bibr" target="#b37">[37]</ref>. We evaluate each algorithm by measuring PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity) <ref type="bibr" target="#b45">[45]</ref> for all test datasets. For all the tables in this section, the red numbers mean the best performance and the blue numbers mean the second best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We analyze the contributions of each module in terms of five keywords: warping operation, perceptual loss, kernel size, dilation and adversarial loss. Warping Operation. To verify that higher DoF leads to better performance, we fixed the backbone network and replaced AdaCoF with some other warping operations of lower DoF. We train all versions of warping operation with L d and the kernel sizes are fixed to be 5 except for Ours-fb.</p><p>? Ours-fb: To compare AdaCoF with flow-based approaches, we set the kernel size to be 1. ? Ours-kb: SepConv <ref type="bibr" target="#b35">[35]</ref> is one of the most representative kernel-based approaches. However, because it does not contain an occlusion map, the comparison is not fair. Therefore, we train a new network of SepConv with an occlusion map. ? Ours-sdc: To compare our algorithm with kernel and flow combined approaches, we exploit Spatially Displaced Convolution (SDC) <ref type="bibr" target="#b38">[38]</ref> instead of AdaCoF. ? Ours-ws: One of the differences between deformable convolution and AdaCoF is that our algorithm does not share the weights over all locations of images. Therefore, we compare it with the weight shared version.   <ref type="table">Table 5</ref>: Evaluation result with fixed train dataset.</p><p>? Ours-woocc: AdaCoF without occlusion map. The intermediate frame is obtained by simply averaging the outputs from the forward and backward warping.</p><p>As shown in <ref type="table">Table 1</ref>, our warping operation outperforms the other ones with lower DoFs. Especially, we can find that the PSNR gap between Ours-sdc and Ours is larger than the gap between Ours-kb and Ours-sdc. It means that breaking the square-shaped kernels to be any shape is more crucial than allowing the kernels to move freely. Perceptual Loss. We add perceptual loss L vgg introduced in Section 3.4 without adversarial loss. We set ? vgg = 0.01. The row of Ours-vgg in <ref type="table">Table 1</ref> shows that the PSNR generally decreases and increases only for DAVIS datasets. This implies that the perceptual loss improves the robustness for hard sequences with large and complex motions. Kernel Size. We train the network with various kernel sizes F ? {1, 3, 5, 7, 9, 11} which means that F 2 offset vectors are used. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the larger kernel size generally leads to better performance and the PSNR saturates as F increases. Especially, the saturation is earlier for the UCF101 dataset because it contains relatively small motion and low-resolution sequences so that there is no room for the performance increase. Dilation. In Section 3.3, we add dilation to the AdaCoF operation to enforce the offset vectors to start from a wider area. We check the effect of dilation by training the network with F = 5 and d ? {0, 1, 2}. d = 0 means that the offset vectors start from the same location. <ref type="table" target="#tab_3">Table 3</ref> shows that the larger dilation generally leads to better results. As we can see from the 4 th -7 th columns of <ref type="figure" target="#fig_6">Figure 6</ref>, the offset vectors tend to spread more in the case of large motion. Therefore, dilation provides the effect of better initialization for them. <ref type="figure" target="#fig_6">Figure 6</ref> will be covered in more detail in Section 4.5.</p><p>Adversarial Loss. For the visually more convincing results, we first train the network with L d for 50 epochs and fine-tune it for 10 epochs with L p which is introduced in Section 3.4. We set ? 1 = 0.01, ? vgg = 1, ? adv = 0.005. For the comparison, we train the version of changing L adv to be WGAN-GP loss <ref type="bibr" target="#b17">[17]</ref> and TGAN loss <ref type="bibr" target="#b40">[40]</ref>. Then we visually compare them with the result of the proposed dualframe adversarial loss (Ours-L p ). According to <ref type="figure" target="#fig_4">Figure 4</ref>, fine-tuning the network with adversarial losses increase the sharpness of the results. However, WGAN-GP and TGAN loss cause some artifacts to the output image, while our loss preserves the structures of the frames.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Evaluation</head><p>We compare our method with simply overlapped results and several competing algorithms including Phase Based <ref type="bibr" target="#b32">[32]</ref>, MIND <ref type="bibr" target="#b28">[28]</ref>, SepConv <ref type="bibr" target="#b35">[35]</ref>, DVF <ref type="bibr" target="#b27">[27]</ref>, and SuperSlomo <ref type="bibr" target="#b20">[20]</ref>. We evaluate two versions of our algorithm. One is the basic version of F = 5, D = 1 (Ours) and the other is the version of F = 11, D = 2 (Ours +). For a fair comparison, we fix the training environment. We implement the competing algorithms and train them with the train dataset introduced in Section 4.1 commonly for 50 epochs. We measure PSNR and SSIM of each algorithm for the three test datasets. The results are shown in <ref type="table">Table 5</ref>. According to the table the kernel-based approach (SepConv) generally perform better than the flow-based ones (DVF, SuperSlomo). Finally, our method outperforms the other algorithms for all test datasets by a high margin. We also upload our result to Middlebury Benchmark <ref type="bibr" target="#b0">[1]</ref> and compare it with the other recent state-of-the-art algorithms. As reported in <ref type="table" target="#tab_5">Table 4</ref>, AdaCoF ranks 2 nd in both IE (Interpolation Error) and NIE (Normalized Interpolation Error) among all published methods in Middlebury website. In addition, CyclicGen <ref type="bibr" target="#b26">[26]</ref>, which ranks 1 st in IE, uses additional edge maps for sharper results and the cycle consistency loss is orthogonally applicable to our method. Also, DAIN <ref type="bibr" target="#b1">[2]</ref>, which ranks 1 st in NIE, use pre-trained optical flow estimator and depth maps while our method does not require any additional information. Lastly, our approach shows better performance for data with dynamic motions such as Basketball, Dumptruck and Evergreen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visual Comparison</head><p>Because the video frame interpolation task does not have a fixed answer, the evaluations based on PSNR and SSIM are not perfect by themselves. Therefore we quantitatively evaluate the methods by comparing each result. Especially, we check how our method and other state-of-the-art algorithms handle the two main obstacles which make motions complex in real world videos: large motion and occlusion.</p><p>Large motion. When the reference point is located far away, the search area has to be expanded accordingly. Therefore the large motion problem is one of the most challenging obstacles in video frame interpolation research. The first and second rows of <ref type="figure" target="#fig_5">Figure 5</ref> show the estimated results of various approaches including our method. The results of MIND, SepConv tend to be blurry and DVF, Super-Slomo suffer from some artifacts. Compared to the other competing algorithms, our approach better synthesizes fast moving objects. In addition, the perception-oriented Ada-CoF (Ours-L p ) mitigate the motion blurs of the objects.</p><p>Occlusion. Most of the objects in the intermediate frame appear in both adjacent frames. However, in case of occlusion, the object does not appear in one of the frames. Therefore, the appropriate frame has to be selected for each case, which makes the problem more difficult. In the third and fourth rows of <ref type="figure" target="#fig_5">Figure 5</ref>, a car causes occlusion in its front and back. Comparing the estimated images on occluded areas, our method handles the occlusion problems better than the other approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Offset Visualization</head><p>Our method estimates some parameters from the input images: the kernel weights W k,l , the offset vectors (? k,l , ? k,l ), and the occlusion map V . To check whether the parameters behave as intended, we visualize them in various ways. Further, because the network is trained by self-supervised learning, the visualizations can be obtained without any supervision. Therefore, they can be used for some other tasks in motion estimation research. Occlusion map. The third column of <ref type="figure" target="#fig_6">Figure 6</ref> shows the occlusion map V . To handle occlusion, the proper frame has to be selected in each case. For example, the pixels in the red area cannot be found in the second frame. Therefore the network decides to consider only the first frame, not the second one. The blue area can be explained in the same way for the second frame, and the green area means that there is no occlusion. Mean Flow map. The fourth and fifth columns of <ref type="figure" target="#fig_6">Figure 6</ref> show the weighted sum of the backward and forward offset vectors for each pixel. We call them Mean Flow F m and they can be calculated by the following equation.</p><p>?p k,l = (? k,l , ? k,l )</p><formula xml:id="formula_13">F m (i, j) = F ?1 k=0 F ?1 l=0 W k,l (i, j)?p k,l<label>(13)</label></formula><p>This means the overall tendency of the offset vectors. Therefore they might behave like a forward/backward optical flow and the figures prove it. This can be used as dense optical flow and can also be obtained from the other flowbased algorithms such as DVF and SuperSlomo.</p><p>Variance Flow map. The sixth and seventh columns of <ref type="figure" target="#fig_6">Figure 6</ref> are the weighted variance of the backward and forward offset vectors. We call them Variance Flow map F v and they can be calculated by the following equation.</p><formula xml:id="formula_15">F v (i, j) = F ?1 k=0 F ?1 l=0 W k,l (i, j)(F m (i, j) ? ?p k,l ) 2 (15)</formula><p>The large value for this map means that the offset vectors for the pixel are more spread out so that it can refer to more pixels. According to the figure, more challenging locations such as large motions and occluded areas have larger variance values. Therefore, it can be used as a kind of uncertainty map for some motion estimation tasks. Unlike Mean Flow map, it can only be obtained through our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we point out that the DoF of the warping operation to deal with various complex motions is one of the most critical factors in video frame interpolation. Then we propose a new operation called Adaptive Collaboration of Flows (AdaCoF). This method is the most generalized because all of the previous approaches are special versions of AdaCoF. The parameters needed for the AdaCoF operation are obtained from a fully convolutional network which is end-to-end trainable. Our experiments show that our method outperforms most of the competing algorithms even in several challenging cases such as those with large motion and occlusion. We visualize the network outputs to check whether they behave as intended and that the visualized maps are meaningful, so they can be used for other motion estimation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall description of the main streams and our method. The blue parts of each figure represent the reference points for generating the target pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the offset vectors of AdaCoF under various dilations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The neural network architecture. The model consists of three main parts: the U-Net, sub-networks, and Adaptive Collaboration of Flows (AdaCoF). The U-Net architecture extracts features from the input image. Then the sub-networks estimates the parameters needed for AdaCoF from the extracted features. The output's height and width of each sub-network are the same as that of the input. Each parameter group for an output pixel is obtained as a 1D vector along the channel axis. The AdaCoF part synthesizes the intermediate frame using the input frames and parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The result of adding adversarial losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visual comparison of sample sequences with large motions (1 st -2 nd row) and visual comparison of sample sequences with occlusion (3 rd -4 th row). There are occluded areas in the front and back of the car.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Various visualizations of the network outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.956 33.449 0.967 24.787 0.828 Ours-kb 34.762 0.972 34.689 0.973 25.802 0.854 Ours-ws 35.412 0.976 34.901 0.973 26.623 0.866 Ours-woocc 35.471 0.975 34.907 0.973 26.482 0.863</figDesc><table><row><cell></cell><cell>Middlebury</cell><cell>UCF101</cell><cell>DAVIS</cell></row><row><cell></cell><cell cols="3">PSNR SSIM PSNR SSIM PSNR SSIM</cell></row><row><cell cols="4">Ours-fb 32.879 0Ours-sdc 34.973 0.972 34.673 0.974 26.367 0.866</cell></row><row><cell>Ours-vgg</cell><cell cols="3">35.694 0.977 34.973 0.973 26.773 0.869</cell></row><row><cell>Ours</cell><cell cols="3">35.715 0.978 35.063 0.974 26.636 0.868</cell></row><row><cell cols="4">Table 1: Result of ablation study on warping operations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental result on kernel size F .</figDesc><table><row><cell>Middlebury</cell><cell>UCF101</cell><cell>DAVIS</cell></row><row><cell cols="3">PSNR SSIM PSNR SSIM PSNR SSIM</cell></row><row><cell cols="3">d = 0 35.489 0.977 35.032 0.974 26.710 0.870</cell></row><row><cell cols="3">d = 1 35.715 0.978 35.063 0.974 26.636 0.868</cell></row><row><cell cols="3">d = 2 35.876 0.980 35.099 0.974 26.910 0.870</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Experimental result on dilation d.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IENIE MDP-Flow2 [48] 5.83 0.87 2.89 0.59 3.47 0.62 3.66 1.24 5.20 0.94 10.20 0.98 6.13 1.09 7.36 0.70 7.75 0.78 DeepFlow [46] 5.97 0.86 2.98 0.62 3.88 0.74 3.62 0.86 5.39 0.99 11.00 1.04 5.91 1.02 7.14 0.63 7.80 0.96 SepConv [35] 5.61 0.83 2.52 0.54 3.56 0.67 4.17 1.07 5.41 1.03 10.20 0.99 5.47 0.96 6.88 0.68 6.63 0.70 SuperSlomo [20] 5.31 0.78 2.51 0.59 3.66 0.72 2.91 0.74 5.05 0.98 9.56 0.94 5.37 0.96 6.69 0.60 6.73 0.69</figDesc><table><row><cell></cell><cell>AVERAGE Mequon Schefflera</cell><cell>Urban</cell><cell>Teddy</cell><cell>Backyard Basketball Dumptruck Evergreen</cell></row><row><cell>CtxSyn [33]</cell><cell cols="4">5.28 0.82 2.24 0.50 2.96 0.55 4.32 1.42 4.21 0.87 9.59 0.95 5.22 0.94 7.02 0.68 6.66 0.67</cell></row><row><cell>CyclicGen [26]</cell><cell cols="4">4.20 0.73 2.26 0.64 3.19 0.67 2.76 0.72 4.97 0.95 8.00 0.91 3.36 0.87 4.55 0.53 4.48 0.52</cell></row><row><cell>TOF-M [49]</cell><cell cols="4">5.49 0.84 2.54 0.55 3.70 0.72 3.43 0.92 5.05 0.96 9.84 0.97 5.34 0.98 6.88 0.72 7.14 0.90</cell></row><row><cell>DAIN [2]</cell><cell cols="4">4.86 0.71 2.38 0.58 3.28 0.60 3.32 0.69 4.65 0.86 7.88 0.87 4.73 0.85 6.36 0.59 6.25 0.66</cell></row><row><cell>MEMC-Net [3]</cell><cell cols="4">5.00 0.74 2.39 0.59 3.36 0.64 3.37 0.80 4.84 0.88 8.55 0.88 4.70 0.85 6.40 0.64 6.37 0.63</cell></row><row><cell>AdaCoF (Ours)</cell><cell cols="4">4.75 0.73 2.41 0.60 3.10 0.59 3.48 0.84 4.84 0.92 8.68 0.90 4.13 0.84 5.77 0.58 5.60 0.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Evaluation results on the Middlebury benchmark.</figDesc><table><row><cell></cell><cell>Middlebury</cell><cell>UCF101</cell><cell>DAVIS</cell></row><row><cell></cell><cell cols="3">PSNR SSIM PSNR SSIM PSNR SSIM</cell></row><row><cell>Overlapping</cell><cell cols="3">27.968 0.879 30.445 0.935 21.922 0.740</cell></row><row><cell cols="4">Phase Based [32] 31.117 0.933 32.454 0.953 23.465 0.800</cell></row><row><cell>MIND [28]</cell><cell cols="3">31.346 0.943 32.437 0.963 25.570 0.852</cell></row><row><cell>SepConv [35]</cell><cell cols="3">35.521 0.977 34.735 0.973 26.258 0.861</cell></row><row><cell>DVF [27]</cell><cell cols="3">34.340 0.971 34.465 0.972 25.880 0.858</cell></row><row><cell cols="4">SuperSlomo [20] 34.234 0.972 34.055 0.970 25.699 0.858</cell></row><row><cell>Ours</cell><cell cols="3">35.715 0.978 35.063 0.974 26.636 0.868</cell></row><row><cell>Ours +</cell><cell cols="3">36.139 0.981 35.048 0.974 27.070 0.874</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Performance of optical flow techniques. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven S</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beauchemin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="43" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6228" to="6237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Singleimage depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emily L Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4414" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
		<idno>De- cember 2015. 2</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to linearize under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">C</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiple hypotheses bayesian frame rate upconversion by adaptive fusion of motion-compensated interpolations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1188" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep video frame interpolation using cyclic frame generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Lun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Tung</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gucan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Moving gradients: a path-based method for plausible image interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Chung</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Phasenet for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Autodiff Workshop: The Future of Gradient-based Machine Learning Software and Techniques</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sdc-net: Video prediction using spatially-displaced convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<editor>Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2830" to="2839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Optical flow guided tv-l 1 video interpolation and restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="273" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video enhancement with taskoriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-level video frame interpolation: Exploiting the interaction among different levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhefei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1235" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
