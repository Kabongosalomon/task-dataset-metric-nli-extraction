<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Anticipate Egocentric Actions by Imagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021">2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
						</author>
						<title level="a" type="main">Learning to Anticipate Egocentric Actions by Imagination</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING</title>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="page">1</biblScope>
							<date type="published" when="2021">2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Action Anticipation</term>
					<term>Action Prediction</term>
					<term>Egocen- tric videos</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anticipating actions before they are executed is crucial for a wide range of practical applications, including autonomous driving and robotics. In this paper, we study the egocentric action anticipation task, which predicts future action seconds before it is performed for egocentric videos. Previous approaches focus on summarizing the observed content and directly predicting future action based on past observations. We believe it would benefit the action anticipation if we could mine some cues to compensate for the missing information of the unobserved frames. We then propose to decompose the action anticipation into a series of future feature predictions. We imagine how the visual feature changes in the near future and then predicts future action labels based on these imagined representations. Differently, our ImagineRNN is optimized in a contrastive learning way instead of feature regression. We utilize a proxy task to train the ImagineRNN, i.e., selecting the correct future states from distractors. We further improve ImagineRNN by residual anticipation, i.e., changing its target to predicting the feature difference of adjacent frames instead of the frame content. This promotes the network to focus on our target, i.e., the future action, as the difference between adjacent frame features is more important for forecasting the future. Extensive experiments on two large-scale egocentric action datasets validate the effectiveness of our method. Our method significantly outperforms previous methods on both the seen test set and the unseen test set of the EPIC Kitchens Action Anticipation Challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Anticipating actions before they are executed is crucial for a wide range of practical applications, including autonomous driving and robotics. In this paper, we study the egocentric action anticipation task, which predicts future action seconds before it is performed for egocentric videos. Previous approaches focus on summarizing the observed content and directly predicting future action based on past observations. We believe it would benefit the action anticipation if we could mine some cues to compensate for the missing information of the unobserved frames. We then propose to decompose the action anticipation into a series of future feature predictions. We imagine how the visual feature changes in the near future and then predicts future action labels based on these imagined representations. Differently, our ImagineRNN is optimized in a contrastive learning way instead of feature regression. We utilize a proxy task to train the ImagineRNN, i.e., selecting the correct future states from distractors. We further improve ImagineRNN by residual anticipation, i.e., changing its target to predicting the feature difference of adjacent frames instead of the frame content. This promotes the network to focus on our target, i.e., the future action, as the difference between adjacent frame features is more important for forecasting the future. Extensive experiments on two large-scale egocentric action datasets validate the effectiveness of our method. Our method significantly outperforms previous methods on both the seen test set and the unseen test set of the EPIC Kitchens Action Anticipation Challenge.</p><p>Index Terms-Action Anticipation, Action Prediction, Egocentric videos</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A NTICIPATING the near future is a natural task that has drawn increasing research attention <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. It has a wide range of applications in the intelligent systems when it needs to react before an action gets executed. For instance, it is critical to anticipate if a car would stop or a pedestrian would cross the road in the autonomous driving task. The prediction is supposed to be seconds before the action is actually taken place, so that the autonomous vehicle could have time to react to avoid an accident. Under these circumstances, recent works are proposed to predict activities a few seconds in the future <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, which is practical for real-world applications.</p><p>In this paper, we focus on the problem of egocentric action anticipation defined in <ref type="bibr" target="#b3">[4]</ref>. Egocentric (First Person Vision) videos <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> offers an interesting scenario to study the action <ref type="bibr">(</ref>   <ref type="figure">Fig. 1</ref>. In the action anticipation task, the model needs to predict the future action that happens T seconds later. Predicting the intermediate future features <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> would benefit the action anticipation task. Our study focuses on how to learn a better imagined intermediate feature.</p><p>anticipation problem. Given an egocentric video sequence denoted as observed video, we aim to predict the future action that happens after a time period of T seconds, whereas the time T is known as the anticipation time. Most previous approaches <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> focus on summarizing the past observed frames, and then directly predict the future action that takes place T seconds later. These methods overlook the temporal gap between the past observations and the future action that is supposed to be predicted. However, frames in this temporal period are closer to the future, thus containing more useful evidence for the next action. If we could mine cues to compensate for missing information of unobserved frames, it would be easier for anticipation models to predict the future.</p><p>In this paper, we propose to tackle this issue by imagining the near future. First, we decompose the long-time action anticipation into a series of future feature predictions. We imagine how the visual feature changes in the very near future and then predict the future action labels based on these imagined representations. Specifically, we design the ImagineRNN to predict the next visual representation based on past observations in a step-wise manner. Since our target is to predict the future action, it is unnecessary to waste model capacity on resolving the stochasticity of frame appearance changes due to camera motion and shadows in egocentric videos. Thus in ImagineRNN, we only generate the visual representation instead of raw pixels. The final anticipation is built on both the observed content and visual representation that we imagined within the anticipation time T .</p><p>Recently, some works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> also propose to generate intermediate future frames or future content features using RNN or GAN architectures. Most of these works arXiv:2101.04924v2 [cs.CV] 19 Jan 2021 use regression loss functions (e.g., l 2 loss or cosine loss) or discriminator (justifying real or fake) to optimize their generator model. However, these optimization methods are too deterministic in training the generator model. There are only positive targets in these loss functions, leading to biased or sub-optimal optimization on the predicted future features. In addition, since actions are changed very quickly in egocentric videos, the predicted future states should be distinguishable in time sequences. Optimization with only positive targets would overlook the state changes in the future time period.</p><p>Our ImagineRNN differs from existing works in two aspects. First is that our ImagineRNN is optimized in the contrastive learning manner instead of feature regression. We propose a proxy task to train the ImagineRNN by selecting the correct future states from distractors. For the predicted future feature, we first build a set of candidates containing both the positive target (the ground truth future feature) and negative distractors (features from other time steps). Then we encourage the model to learn to identify the correct future state from candidates given the observed context. In this way, our ImagineRNN could essentially learn the change of future features. We found the new optimization method significantly improves the generalisability on the unseen test set.</p><p>Second, we further improve ImagineRNN by residual anticipation, i.e., changing its target to predicting the feature difference of adjacent frames, instead of the entire frame feature. Different from <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> that predict the entire optical flow frames or dynamic image, we only predict the feature changes between adjacent frames. The motivation is in threefolds. First, the difference between adjacent frame features is more important for forecasting the future. Predicting the video difference promotes the network to focus on the change of intermediate features, leading to better results on the future action anticipation. Second, it reduces the load of the ImagineRNN and thus saves the model capacity. In this way, the information the ImagineRNN has to predict is minimized, while the unchanged feature channels are directly carried forward. Third, the unchanged content plays a role of shortcut connection, avoiding the noise accumulation and the gradient vanishing. To the best of our knowledge, we are the first to forecast the difference of frames in generating future features.</p><p>We conduct extensive experiments on two large-scale egocentric video datasets EPIC-KITCHENS <ref type="bibr" target="#b4">[5]</ref> and EGTEA Gaze+ <ref type="bibr" target="#b11">[12]</ref>. Results from the leaderboard of the EPIC-KITCHENS action anticipation challenge clearly show our model beats other existing single models. To summarize, our contributions are summarized as follows:</p><p>? We propose ImagineRNN that breaks down the longtime action anticipation into a series of step-wise feature predictions of short periods, and then predicts the future action labels upon these imagined features. ? We reformulate the future feature prediction problem, and propose to optimize the ImagineRNN by picking the correct future states from lots of distractors, which essentially learns the change of future features compared to the traditional regression loss functions. ? We further replace the ImagineRNN's target by predicting the difference between adjacent frames, which helps the model focus on the feature change along time, leading to better anticipation performance. Experiments with different architectures validate the effectiveness of this change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video Understanding and Action Recognition</head><p>Deep learning methods have achieved promising performance on the video classification task. Simonyan et al. <ref type="bibr" target="#b12">[13]</ref> proposed Two-Stream to utilize both RGB frames and optical flow as the 2D CNN input to modeling appearance and motion, respectively. Temporal Segment Networks (TSN) <ref type="bibr" target="#b13">[14]</ref> extended the two-stream CNN by extracting features from multiple temporal segments. Tran et al. <ref type="bibr" target="#b14">[15]</ref> proposed a 3D CNN to learn the spatial-temporal information. Moreover, Recurrent Neural Networks (RNNs) are also effective in temporal modeling and have been found useful for video classification in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. More recently, some researchers study the egocentric action recognition problem <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Sudhakaran et al. <ref type="bibr" target="#b18">[19]</ref> proposed a Long Short-Term Attention model to focus on features from relevant spatial parts. Wang et al. <ref type="bibr" target="#b20">[21]</ref> proposed a Symbiotic Attention mechanism to enable the communications between motion features and object features in egocentric videos. Our method builds on these methods and uses TSN as a base framework to train CNNs for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Early Action Recognition</head><p>The early action recognition task <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b0">[1]</ref> is to recognize the ongoing action as early as possible from partial observations. In this task, the model is only allowed to observe a part of the action videos, and predict the action based on the video segment <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. This task is closed to our target, the action anticipating task. Differently from these works, in the egocentric anticipating task, the action should be recognized before it starts, so we cannot partially observe the action frames at the time of prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Action Anticipation</head><p>Predicting the near future has been widely studied recently <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Action anticipation is to predict an action before it occurs <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Previous works investigated different forms of action and activity anticipation <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. We share a similar idea with past works and use the recurrent neural networks to summarize the past observations <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Very recently, RULSTM <ref type="bibr" target="#b3">[4]</ref> consists of two LSTMs to anticipate actions from egocentric video, where one LSTM is used to summarize the past, and the other is used to predict future actions based on the past future directly. Miech et al. <ref type="bibr" target="#b2">[3]</ref> proposed to directly anticipate future action based on the combination of past visual inputs and past action recognition results. Concurrent to us, Sener et al. <ref type="bibr" target="#b40">[41]</ref> propose a multi-scale temporal aggregate method for action anticipation by relating recent to long-range observations. It computes recent and spanning representations pooled from snippets that are related via coupled attention mechanisms. The experiments shows great advantages brought by ensembles of multiple scales.</p><p>There are also some other interesting researches for the anticipation task. In <ref type="bibr" target="#b33">[34]</ref>, the authors study the problem of anticipating a sequence of activities within time horizons of up to 5 minutes, in contrast to other works that anticipate the next action within several seconds. <ref type="bibr" target="#b41">[42]</ref> studies anticipating future actions in the long-time period. Given an untrimmed video containing a long composite activity, the proposed topological affordance graphs could predict the actions that will likely occur in the future to complete it. Ego-OMG <ref type="bibr" target="#b42">[43]</ref> proposes to structure the long video clips into a discrete set of states, where each state represents the objects presently in contact or anticipated to soon be in contact.</p><p>Given past observation, it might have many possible future actions due to future uncertainty. Future uncertainty (alternative future) is important in the action anticipation task. Furnari et al. <ref type="bibr" target="#b31">[32]</ref> study how to explicitly incorporate the uncertainty in the loss functions. Canuto et al. <ref type="bibr" target="#b43">[44]</ref> propose to minimize the model uncertainty instead of maximizing its class probabilities, which could be used as the online decisionmaking criterion for action anticipation. In <ref type="bibr" target="#b44">[45]</ref>, both an action model and a length model are trained to capture the uncertainty of future activities. In this paper, we do not explicitly model the future uncertainty in our method. Given existing video data, we only optimize the model to predict the exact next future action that happens in the video. It is a limitation of our method. We hope to handle uncertainty in our future works. Some recent works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> propose to predict the optical flow frames or dynamic image in the future, which has a similar motivation with our designed residual anticipation, i.e., predicting low-entropy signals (the frame-feature difference) However, different from <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> that predict the entire optical flow frames or dynamic image, we only predict the feature changes between adjacent frames, which avoids wasting model capacity on resolving the stochasticity of frame changes due to camera motion in egocentric videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contrastive Learning</head><p>Contrastive learning aims at optimizing models by distinguishing similar and dissimilar data pairs. Recent works <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref> proposed to utilize contrastive learning for selfsupervised learning. Contrastive Predictive Coding (CPC) <ref type="bibr" target="#b47">[48]</ref> proposed to learn representation by encoding predictions over future observations from the past. MoCo <ref type="bibr" target="#b48">[49]</ref> designed a momentum encoder and maintained a queue of representations to conduct contrastive learning. SimCLR <ref type="bibr" target="#b45">[46]</ref> experiments with different combinations of data augmentation methods for paired samples in contrastive learning. Very recently, Han et al. <ref type="bibr" target="#b46">[47]</ref> proposed to introduce contrastive learning into the action recognition task. The model is optimized by a predictive attention mechanism over the compressed memories that predicts future representations based on recent observation. Different these methods, we focus on the action anticipation task rather than representation learning. We found the contrastive learning helps to learn the change of future features, which can be used to obtain better intermediate imaginary data in our ImagineRNN framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH A. Egocentric Action Anticipation</head><p>Task definition. In the EPIC-Kitchens anticipation challenge <ref type="bibr" target="#b4">[5]</ref>, the egocentric action anticipation task is defined to predict the future action one second before it happens. In a more general task definition <ref type="bibr" target="#b3">[4]</ref>, the video is input in an on-line fashion, with a short video snippet consumed every ? seconds, i.e., the video is divided into segments of length ?. For an action occurring at time ? s , the model should anticipate the action by observing the video frames before ? s ? T . In our framework, our model is allowed to observe the video segment of length (l ? T ) starting at time (? s ? l) and ending at time (? s ? T ). Following <ref type="bibr" target="#b3">[4]</ref>, we use the same task setting and set l = 3.5s and ? = 0.25s. We also validate our model under different anticipate time, i.e., T ? {1.75s, 1.5s, 1.25s, 1s, 0.75s, 0.5s, 0.25s}. Note that it is more general compared to the task defined in <ref type="bibr" target="#b4">[5]</ref>, which only validates the model under anticipate time T = 1. CNN pretraining. The input of our model is the frame-level feature provided by the pre-trained TSN model. In action anticipation, the anticipation targets (objects and actions) do not always appear in the input video, making it hard to learn good representations for CNN models in an end-to-end manner. To avoid over-fitting and make the CNN model more meaningful, we follow <ref type="bibr" target="#b3">[4]</ref> and pre-train the TSN model on the action recognition task. Then the pre-trained CNN weights are fixed during the following training on our action anticipation task. We pre-process the videos and obtain different modalities features by pre-trained CNN models, i.e., RGB frame features, optical flow frame features, and the object features. Encoder. We take a Long Short-Term Memory (LSTM) <ref type="bibr" target="#b49">[50]</ref> model as the temporal encoder. At each time step, the encoder takes as input the visual content that is being observed. Specifically, at each time-step t, we use the pre-trained TSN model to get the current frame feature f t . Then we input the feature f t to update the memory. The new encoding hidden state h E t+1 is obtained by updating the LSTM unit as follows:</p><formula xml:id="formula_0">h E t+1 = Encoder(f t , h E t ),<label>(1)</label></formula><p>where h E t is the hidden state from the previous forward. We initialize the hidden state as zeros. To save memory and avoid noises, we only input the frames several seconds before the action occurring time ? s . Following <ref type="bibr" target="#b3">[4]</ref>, we take the frames from (? s ? 4)s to (? s ? 2.5)s as the input for the encoder. Decoder. The decoder is an LSTM model that performs anticipation. It takes the observed information extracted from the EncodingRNN as the initial hidden states, and then recurrently takes the last observed frame as input. Based on the last output of the DecodingRNN, we use a fully-connected layer as the classifier for the action anticipation prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bridging the gap between past and future</head><p>In the egocentric action anticipation task, it is hard to train a meaningful model due to the clear gap between past observations and future action. We alleviate this issue by decomposing the long-time prediction into a series of shortterm forecasts. Then we design ImagineRNN to fill in the gap by producing the future visual representation. In this way, the long-time reasoning is simplified by predicting the action based on past observations and future imaginary data. Specifically, we break down the T seconds anticipation into several short-term anticipations with each lasting ? seconds (? &lt; T ). Given the visual feature f t at time t, the ImagineRNN is designed to generate the future visual featuref t+1 by,</p><formula xml:id="formula_1">h I t+1 = ImagineRNN(f t , h I t ),<label>(2)</label></formula><formula xml:id="formula_2">f t+1 = ?(h I t+1 ),<label>(3)</label></formula><p>where h I t is the hidden state of ImagineRNN at time step t. ?(?) is a transformation layer that maps the hidden state space to the visual feature space. The generated visual featuref t+1 is supposed to fill in the gap between the past and future. In the framework, we input the output of ImagineRNN to the decoder to predict future action. Thus the prediction of ImagineRNN should be consistent with the ground truth visual content. Next, we illustrate how we optimize the ImagineRNN model efficiently in the action anticipation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization of ImagineRNN</head><p>In egocentric videos, the action states usually change very quickly. Thus the predicted future from ImagineRNN should be substantially different along with the anticipation time. The commonly used regression loss functions, such as l 2 loss, can hardly optimize the ImagineRNN to perceive the changes of action states. Differently, we propose a more effective optimization for the ImagineRNN by introducing the contrastive learning task, where the model is asked to pick the correct future states from lots of distractors. We use Noise Contrastive Estimation (NCE) <ref type="bibr" target="#b50">[51]</ref> to encourage the predicted future featuref t+1 to be close to the ground truth future stat? f t+1 . Compared to the regression losses, NCE does not require to resolve the low-level stochasticity strictly. Specifically, for the imagined future featuref t at time t, the only positive target is the ground truth feature f t . We then build a set of candidates as distractors for the ground truth feature f t at time t.</p><p>Distractors. The distractors contain easy negatives and hard negatives. The easy negatives contain the frame features from the other videos instead of the target video. We use the framelevel features from the other videos in the same mini-batch as the easy negatives for simplicity in the calculation. These candidates are easy to distinguish since these frames usually look different from the current video.</p><p>The hard negatives contain the frames from the same video but at different time steps, f t where t = t. These candidates are hard to distinguish since they are very close to the ground truth frame feature f t . Distinguishing the hard negatives encourages ImagineRNN to generate essential intermediate features and capture the change of a series of future states.</p><p>Contrastive Learning. With the positive targets and these distractors, we can take the contrastive learning as a proxy task for better optimizing the ImagineRNN. Inspired by recent representation learning work <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, we first calculate the cosine similarity between the predicted feature and the candidates, v T jf t , where v j denotes the j-th distractors. Here we enforce all vectors to be L2-normalized feature embeddings, i.e., ||v j || = 1, ||f t || = 1, and ||f t || = 1. Thus we have the following objective function at the time step t,</p><formula xml:id="formula_3">L c = ? log exp(f t Tf t /? ) j exp(v j Tf t /? ) + exp(f t Tf t /? ) ,<label>(4)</label></formula><p>where ? is a temperature parameter that controls the concentration level of the distribution. Higher ? leads to a softer probability distribution. We set ? = 0.2 in our experiments. With Eqn. (4), we optimize the ImagineRNN with a crossentropy loss (negative log-likelihood), instead of the commonly used regression loss functions. During optimization, the loss function encourages the predicted featuref t to be close to ground truth target f t , and also push the predicted featuref t to be distinct from these distractors. Thus the trained ImagineRNN could catch the change of action states at different times, which is essential in action anticipation.</p><p>2) The future intention. In addition, following <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, we also take the future intentions as additional supervision. The future intention is the purpose of the currently observed actions (the next future action), which explains the visual changes that happen during the unseen temporal region T . The intuition behinds it is that the generated visual representation should also benefit the anticipation task. Specifically, we input the generated visual feature to the decoder for several time steps during the anticipation time period. The decoder's last hidden state is further input to the action classifier for recognizing the future action. Then we use the Cross-Entropy loss on the final action anticipation to optimize the ImagineRNN. Denote the Cross-Entropy loss of the classifier as L f , the final loss is the sum of the two losses,</p><formula xml:id="formula_4">L = L c + L f .<label>(5)</label></formula><p>D. Forecasting the difference between frames However, the visual features of adjacent frames would be close since the backgrounds in frames are the same. Directly predicting the visual feature of future frames might waste model capacity in generating the unchanged background information. In addition, ImagineRNN might not essentially learn the change during future frames. Thus we propose to improve ImagineRNN by explicitly force it to predict the feature difference of adjacent frames, instead of the entire frame feature.</p><p>Specifically, we optimize ImagineRNN by learning to produce the difference between the current visual feature and the next one. The output of ImagineRNN is to forecast future changes of the visual feature given the current observation. Thus we change Eqn. (3) to be,</p><formula xml:id="formula_5">f t+1 = ?(h I t+1 ) + f t .<label>(6)</label></formula><p>Since we are designed to predict a series of intermediate frame features before anticipating the future action, we repeatedly use Eqn. <ref type="bibr" target="#b5">(6)</ref> to generate a series of future frame features in an auto-regressive way. Suppose frame t to be the last observed frame, we can obtain the imagined featuref t+n of future frame t + n by,</p><formula xml:id="formula_6">f t+n = ?(h I t+n ) + ?(h I t+n?1 ) + ... + ?(h I t+1 ) + f t . (7)</formula><p>As can be seen in the above equation, predicting the difference sets up a shortcut connection between step-wise reconstructions, which helps ease the optimization of Imag-ineRNN and avoids noise accumulation during the autoregressive future feature generation in testing. In addition, Predicting the frame difference promotes the model to focus on the change of intermediate features, which might be the core of future action anticipation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We first discuss the experimental setups and then compare our method with the state-of-the-art methods on two largescale egocentric action datasets, EPIC-Kitchens and EGTEA Gaze+. Ablation studies and qualitative results are provided to show the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Datasets. We perform experiments on two large-scale datasets of egocentric videos: EPIC-Kitchens <ref type="bibr" target="#b4">[5]</ref> and EGTEA Gaze+ <ref type="bibr" target="#b11">[12]</ref>. EPIC-Kitchens is the largest dataset in firstperson vision so far. It consists of 55 hours of recordings capturing all daily activities in the kitchens. The activities performed are non-scripted, which makes the dataset very challenging and close to real-world data. This dataset is densely annotated with timestamps for each action so that it is ready for the action anticipation task. Actions in the EPIC-Kitchens dataset is annotated in the format of (verb, noun) pairs. The dataset contains 39, 596 action annotations, 125 verbs, and 352 nouns. We considered all unique (verb, noun) pairs in the public training set, thus obtaining 2, 513 unique actions. We use the same split as <ref type="bibr" target="#b3">[4]</ref> and split the public training set of EPIC-Kitchens (28, 472 action segments) into training (23, 493 segments) and validation (4, 979 segments) sets. EGTEA Gaze+ contains 19 verbs, 51 nouns and 106 unique actions. We report the average performance across the three official splits provided by the authors of the dataset. Evaluation Metrics. Following <ref type="bibr" target="#b3">[4]</ref>, we use the Top-k accuracy to evaluate our method. Under this evaluation metric, the prediction is deemed to be correct if the ground truth action falls in the top-k predictions. This metric is appropriate due to the uncertainty of future predictions <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b53">[54]</ref>. Many possible actions can follow an observation. We use the Top-5 accuracy as a class-agnostic measure. We also report Mean Top-5 Recall <ref type="bibr" target="#b31">[32]</ref> as a class aware metric. Top-5 recall for a given class c is defined as the fraction of samples of ground truth class c for which the class c is in the list of the top-5 anticipated actions. Mean Top-5 Recall averages Top-5 recall values over classes. In <ref type="bibr" target="#b31">[32]</ref>, Top-5 Recall is averaged over the provided list of many-shot verbs, nouns, and actions. Performances are evaluated for verb, noun, and action predictions. Following <ref type="bibr" target="#b3">[4]</ref>, in training the only targets are the action labels, and our model is optimized to predict the action prediction. In the testing, we obtain the predictions for verb and noun by the marginalization on action predictions. Implementation Details. We use Pytorch <ref type="bibr" target="#b54">[55]</ref> to implement our framework. For the pre-trained action recognition model, we use a BNInception CNN <ref type="bibr" target="#b55">[56]</ref> with the TSN framework to train the action recognition model. After pre-training, we resize the frame to 456 ? 256 pixels and input it to the CNN model. The output (1024-dimensional vectors) of the last global average pooling layer is used as the frame-level feature. The encoder, decoder, and the ImagineRNN are all single-layer LSTMs with 1024 hidden units. We use Stochastic Gradient Descent (SGD) to train the framework with a mini-batch size of 128 and a learning rate of 0.01 and momentum equal to 0.9. We train 100 epochs and apply early stopping at each training stage the same as <ref type="bibr" target="#b3">[4]</ref>. This is done by choosing the intermediate and final models' iterations, which obtain the best Top-5 action anticipation accuracy for the anticipation time T = 1s on the validation set. Following <ref type="bibr" target="#b3">[4]</ref>, we use the RGB frames, optical flow frames, and the object detection features as input for our model. We first train the model with each modality individually and then obtain the final prediction by a late fusion of the three models' predictions. In the following experiments, for fair comparisons with RULSTM, our model takes all the three modalities as input if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison to the state-of-the-art methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Methods</head><p>We compare our method with stateof-the-art action anticipation methods: Deep Multimodal Regressor (DMR) <ref type="bibr" target="#b56">[57]</ref>, Anticipation Temporal Segment Network (ATSN) of <ref type="bibr" target="#b4">[5]</ref>, Anticipation Temporal Segment Network trained with verb-noun Marginal Cross Entropy Loss (MCE) <ref type="bibr" target="#b31">[32]</ref>, and the Encoder-Decoder LSTM (ED) introduced in <ref type="bibr" target="#b30">[31]</ref>. We also compare with the early action recognition methods to the problem of egocentric action anticipation: Feedback Network LSTM (FN) <ref type="bibr" target="#b24">[25]</ref>, and an LSTM trained using the Exponential Anticipation Loss <ref type="bibr" target="#b57">[58]</ref> (EL). To compare with state-of-the-art action anticipation methods, we reproduced a vanilla version of Feature Mapping RNN <ref type="bibr" target="#b8">[9]</ref> without the kernalised RBF. For a fair comparison, we first train models with the three input modalities, i.e., RGB features, optical flow features, and the object features. Then we obtain the final prediction by a late fusion of the three models. Very recently, RULSTM <ref type="bibr" target="#b3">[4]</ref> is proposed by combining two LSTM to anticipate actions from egocentric video, where one LSTM is used to summarize the past, and the other is used to predict future actions based on the past future. We compare our method under both the standard anticipation setting (anticipation time T = 1s) and a more general anticipation setting (with variant anticipation time).</p><p>Results on the EPIC-KITCHENS test server. We compare our method with the state-of-the-art methods on the test server of EPIC-KITCHENS. <ref type="table" target="#tab_2">Table I and Table II</ref> report results obtained from the official EPIC-KITCHENS action anticipation challenge submission server. The official test server computes the performances on two test sets, i.e., the "seen" test, which includes the same scenes appearing in the training set (S1) and the "unseen" test set (S2), with kitchens not appearing in the training set. On both test sets, our method outperforms all previously reported results under all metrics. On the S1 (seen) test set <ref type="table" target="#tab_2">(Table I)</ref>, our method outperforms the previous method RULSTM by 1.25% on the Top-5 Action accuracy. On the S2 (unseen) test set <ref type="table" target="#tab_2">(Table II)</ref> where the videos are captured in new environments, our method significantly improves RULSTM in all metrics on Verb, Noun, and Action prediction. Note that we use the same input features with RULSTM, thus the comparison with RULSTM is a fair comparison, and the performance improvements over RULSTM are all from our algorithm instead of better features. These results demonstrate our method is better at anticipating the future action.</p><p>Results with Different Anticipation Time T . Our method can also be used to predict future action under different anticipation time. Since each time step ? in our method is 0.25s, we can evaluate the future anticipation every 0.25s. We compare our method with the state-of-the-art methods under different anticipation time T ? {2s, 1.75s, 1.5s, 1.25s, 1s, 0.75s, 0.5s, 0.25s }. The results are shown in <ref type="table" target="#tab_2">Table III</ref>. Note that some methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b56">[57]</ref> can anticipate actions only at a fixed anticipation time. We found the proposed method always outperforms the strong competitor RULSTM <ref type="bibr" target="#b3">[4]</ref> under all anticipation time T . Note that the results are reported on the validation set, where the models are selected by choosing the best validation performance, as used by RULSTM <ref type="bibr" target="#b3">[4]</ref>. As indicated in <ref type="bibr" target="#b3">[4]</ref>, the results on the test server are more important in evaluating compared to the validation results.</p><p>Results on the EGTEA Gaze+ dataset. We also conduct experiments on the EGTEA Gaze+ dataset. <ref type="table" target="#tab_6">Table V</ref> reports Top-5 action accuracy scores on EGTEA Gaze+ under different anticipation times. We use the same input modalities as RULSTM. Our method outperforms the compared methods under different anticipation time T . We also found the relative improvement is smaller on the EGTEA Gaze+ dataset compared to that on the EPIC-KITCHENS dataset. It might be because the EGTEA Gaze+ is relatively small in scale. It only consists 106 actions, which is far less than the 2, 513 actions in EPIC-KITCHENS. Thus the anticipation on the EPIC-KITCHENS dataset is more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>We conduct ablation studies to evaluate the effectiveness of the two components of our method. Effectiveness of ImagineRNN. Without our proposed Imag-ineRNN, the model is the baseline RULSTM. From <ref type="table" target="#tab_2">Table I</ref> and <ref type="table" target="#tab_2">Table II</ref>   Effectiveness of Contrastive Learning. The common used optimization for the ImagineRNN is the regression loss functions (l 2 loss). In <ref type="table" target="#tab_2">Table I and Table II</ref> Anticipation time: on the RGB modality. Similarly, we found our approach also suppresses the model (w/o diff) with optical flow data as inputs. These comparison results prove the effectiveness of forecasting the difference instead of directly generating the whole visual feature. We also validate the effectiveness of forecasting the difference with other architectures. We replace the basic architectures of our ImagineRNN and the encoder-decoder by Gated Recurrent Unit (GRU), instead of the previously used LSTM. The results are shown in <ref type="table" target="#tab_2">Table VII</ref>. It can be seen that our predicting the feature difference of adjacent frames still performs better with the GRU-based architecture. Effectiveness of Future Intention. We also show the comparison of results with and without the future intention optimization Eqn. <ref type="bibr" target="#b4">(5)</ref> in <ref type="table" target="#tab_2">Table IV</ref>. The ablation studies show a small improvement brought by future intention. Specifically, our final model outperforms the one without future intension on the RGB and flow modalities by about 0.4% in Top-1 accuracy and 0.2% in Top-5 accuracy. Ablation studies over different lengths of the past. We show the results over different lengths of observed past in <ref type="figure" target="#fig_1">Figure 3</ref>. Note the anticipation time T is 1s for all experiments. It can be seen from the figure that the performance is relatively low if the encoder period is too short (i.e., less than 2.25 seconds). As the encoding period gets longer, we found the performance gets steady. Inputting more observed frames did not lead to further performance improvement if the encoder period is longer than 2.5 seconds. The reason might be that actions usually change quickly in egocentric videos. Too early frames do not have strong correlations with the future action. Discussion on different optimization methods. We evaluate the models with different optimization methods on the test set of the EPIC-KITCHENS Action Anticipation Challenge. The results are shown in <ref type="table" target="#tab_2">Table VI</ref>, where "Con." indicates the contrastive learning loss, and "Adv." is the adversarial loss used in GAN <ref type="bibr" target="#b9">[10]</ref>. It can be seen that a combination of contrastive loss and l 2 loss does not outperform the one with the contrastive learning only. Besides, we add the adversarial loss in the model training, where the discriminator is a three-layer MLP. According to the validation results, we set the weight of the adversarial loss to be 0.01 in the overall loss function and the discriminator's learning rate to be 2 ? 10 ?6 . As shown in <ref type="table" target="#tab_2">Table VI</ref>, the model trained with the combination of the three loss functions performs worst among all candidates. Our model trained with contrastive learning performs best among all candidates. The reason might be that contrastive learning helps to learn the change of future features essentially, since it needs to distinguish the positive target from lots of distractors.</p><p>(frame features at other times).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>We show some qualitative results of our method in <ref type="figure" target="#fig_2">Fig 4</ref>. From left to right, the observations are getting closer to future action. The orange box and "GT" indicates the ground truth of the future action. We list the Top-5 action predictions of our results at the anticipation time T ? {2s, 1.5s, 1s, 0.5s}. Green indicates the prediction matches the ground truth. Taking the first row as an example, the anticipations become more and more accurate as time flows. It is consistent with our motivation that long-time modeling might involve lots of noise. It is interesting to see the model always predicts "Open fridges" when T is less than 2 seconds probably because the fridge shows up in the observations at T = 1.5s. The other action candidates, including "Take milk" and "Open drawer", are also likely to take place in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORKS</head><p>In this work, we decompose the action anticipation task into a series of future frame feature predictions. We first imagine how the future feature changes and then predict future action based on these imagined representations. We found that ImagineRNN optimized with contrastive learning is superior than the typical anticipation models. In addition, we further propose to improve ImagineRNN by predicting the feature difference of adjacent frames instead of the whole frame content. It helps promote the model to focus on the change of future states and avoid the noise accumulation during the autoregressive future feature generation. Extensive experimental results on different architectures validate the effectiveness of the proposed method.</p><p>In conclusion, we found it useful by decomposing action anticipation into lots of intermediate predictions. Focusing on the future state transition by contrastive learning and predicting future frames' difference improves the quality of intermediate predictions, leading to better results on the final action anticipation task.</p><p>In future works, we might further explore the uncertainty of future in the egocentric action anticipation task, which is a limitation of our current work. It might also be helpful in generating better intermediate future features by considering the hand tracking on the observed frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENT</head><p>Yu Wu is the recipient of the Google PhD Fellowship 2020. We acknowledge the Google PhD Fellowship Programme and ARC Discovery Project DP200100938 for funding this research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Previous methods directly predict based on observed video Feature of observed video Imagined intermediate Future action Feature of observed video Prediction (b) Our methods predict based on both observed video and imaginary data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Top-5 accuracies over different lengths of observed past for the encoder. The results are produced by our method with the RGB modality input. Note the anticipation time T is 1s for all experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results with anticipation time T = 2s, 1.5s, 1s, 0.5s. From left to right, the observations are getting closer to future action. We list the Top-5 action predictions of our model at each anticipation time. Orange indicates the ground truth, and green means our prediction matches the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Corresponding Author: Yi Yang) Y. Wu, L. Zhu, X. Wang, and Y. Yang are with the Australian Artificial Intelligence Institute, University of Technology Sydney, Ultimo 2007, NSW, Australia. (E-mail: yu.wu-3@student.uts.edu.au; linchao.zhu@uts.edu.au; xiaohan.wang-3@student.uts.edu.au; yi.yang@uts.edu.au).</figDesc><table /><note>F. Wu is with Zhejiang University, Hangzhou 310027, China (Email:wufei@cs.zju.edu.cn).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fig. 2. The framework of our method. ImagineRNN predicts the next visual representation based on past observations in a step-wise manner. The imaginary features are input to the decoder to improve the anticipation performance. We propose to better optimize the ImagineRNN with the contrastive learning task. We further improve the ImagineRNN by forecasting the features difference between frames, instead of generating the entire frame representations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Imagined future</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>visual feature</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Predicted feature</cell><cell>Feat 1</cell><cell>Feat 2</cell><cell>Feat 3</cell><cell>Feat 4</cell><cell>Feat 5</cell><cell>Feat 6</cell><cell>Feat 7</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell>+</cell><cell></cell></row><row><cell>Imagine RNN</cell><cell>IRNN</cell><cell>IRNN</cell><cell>IRNN</cell><cell>IRNN</cell><cell>IRNN</cell><cell>IRNN</cell><cell>IRNN</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Future action:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>wash pan</cell></row><row><cell>Observed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>video</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Encoder-decoder</cell><cell>Enc</cell><cell>Enc</cell><cell>Enc</cell><cell>Enc</cell><cell>Dec</cell><cell>Dec</cell><cell>Dec</cell><cell>Anticipation classifier</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I EGOCENTRIC</head><label>I</label><figDesc>ACTION ANTICIPATION RESULTS ON THE SEEN (S1) TEST SET OF THE EPIC-KITCHENS ACTION ANTICIPATION CHALLENGE [5] WITH ANTICIPATION TIME T = 1 SECOND. ALL VALUES ARE REPORTED AS PERCENTAGE (%).TABLE II EGOCENTRIC ACTION ANTICIPATION RESULTS ON THE UNSEEN (S2) TEST SET OF THE EPIC-KITCHENS ACTION ANTICIPATION CHALLENGE<ref type="bibr" target="#b4">[5]</ref> WITH ANTICIPATION TIME T = 1 SECOND. ALL VALUES ARE REPORTED AS PERCENTAGE (%).</figDesc><table><row><cell>Methods</cell><cell cols="2">Verb Top-1 Acc Top-5 Acc</cell><cell cols="2">Noun Top-1 Acc Top-5 Acc</cell><cell cols="2">Action Top-1 Acc Top-5 Acc</cell></row><row><cell>2SCNN [5]</cell><cell>29.76</cell><cell>76.03</cell><cell>15.15</cell><cell>38.56</cell><cell>04.32</cell><cell>15.21</cell></row><row><cell>ATSN [5]</cell><cell>31.81</cell><cell>76.56</cell><cell>16.22</cell><cell>42.15</cell><cell>06.00</cell><cell>28.21</cell></row><row><cell>ED [31]</cell><cell>29.35</cell><cell>74.49</cell><cell>16.07</cell><cell>38.83</cell><cell>08.08</cell><cell>18.19</cell></row><row><cell>MCE [32]</cell><cell>27.92</cell><cell>73.59</cell><cell>16.09</cell><cell>39.32</cell><cell>10.76</cell><cell>25.28</cell></row><row><cell>Transitional [3]</cell><cell>30.74</cell><cell>76.21</cell><cell>16.47</cell><cell>42.72</cell><cell>09.74</cell><cell>25.44</cell></row><row><cell>RULSTM [4]</cell><cell>33.04</cell><cell>79.55</cell><cell>22.78</cell><cell>50.95</cell><cell>14.39</cell><cell>33.73</cell></row><row><cell>Ours (l2 loss)</cell><cell>35.26</cell><cell>79.66</cell><cell>22.57</cell><cell>52.04</cell><cell>15.07</cell><cell>34.66</cell></row><row><cell>Ours (Contrastive)</cell><cell>35.44</cell><cell>79.72</cell><cell>22.79</cell><cell>52.09</cell><cell>14.66</cell><cell>34.98</cell></row><row><cell>Methods</cell><cell cols="2">Verb Top-1 Acc Top-5 Acc</cell><cell cols="2">Noun Top-1 Acc Top-5 Acc</cell><cell cols="2">Action Top-1 Acc Top-5 Acc</cell></row><row><cell>2SCNN [5]</cell><cell>25.23</cell><cell>68.66</cell><cell>09.97</cell><cell>27.38</cell><cell>02.29</cell><cell>09.35</cell></row><row><cell>ATSN [5]</cell><cell>25.30</cell><cell>68.32</cell><cell>10.41</cell><cell>29.50</cell><cell>02.39</cell><cell>06.63</cell></row><row><cell>ED [31]</cell><cell>22.52</cell><cell>62.65</cell><cell>07.81</cell><cell>21.42</cell><cell>02.65</cell><cell>07.57</cell></row><row><cell>MCE [32]</cell><cell>21.27</cell><cell>63.33</cell><cell>09.90</cell><cell>25.50</cell><cell>05.57</cell><cell>15.71</cell></row><row><cell>Transitional [3]</cell><cell>28.37</cell><cell>69.96</cell><cell>12.43</cell><cell>32.20</cell><cell>07.24</cell><cell>19.29</cell></row><row><cell>RULSTM [4]</cell><cell>27.01</cell><cell>69.55</cell><cell>15.19</cell><cell>34.38</cell><cell>08.16</cell><cell>21.10</cell></row><row><cell>Ours (l2 loss)</cell><cell>27.35</cell><cell>69.78</cell><cell>15.36</cell><cell>35.34</cell><cell>08.54</cell><cell>20.79</cell></row><row><cell>Ours (Contrastive)</cell><cell>29.33</cell><cell>70.67</cell><cell>15.50</cell><cell>35.78</cell><cell>09.25</cell><cell>22.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III ACTION</head><label>III</label><figDesc>ANTICIPATION RESULTS ON THE EPIC-KITCHENS VALIDATION SET UNDER DIFFERENT ANTICIPATION TIME T . THE PERFORMANCE IS MEASURED BY THE TOP-5 ACCURACY OF ACTION ANTICIPATION.</figDesc><table><row><cell>Methods</cell><cell cols="6">Top-5 Action Accuracy @ different T 1.5 1.25 1.0 0.75 0.5 0.25</cell></row><row><cell>DMR [57]</cell><cell>/</cell><cell>/</cell><cell>16.9</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>ATSN [5]</cell><cell>/</cell><cell>/</cell><cell>16.3</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>MCE [32]</cell><cell>/</cell><cell>/</cell><cell>26.1</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>FMRNN [9]</cell><cell>/</cell><cell>/</cell><cell>32.7</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>ED [31]</cell><cell cols="6">23.2 24.8 25.8 26.7 27.7 29.7</cell></row><row><cell>FN [25]</cell><cell cols="6">24.7 25.7 26.3 26.9 27.9 29.0</cell></row><row><cell>EL [58]</cell><cell cols="6">26.4 27.4 28.6 30.3 31.5 33.6</cell></row><row><cell cols="7">RULSTM [4] 32.2 33.4 35.3 36.3 37.3 39.0</cell></row><row><cell>Ours</cell><cell cols="6">32.5 33.6 35.6 36.7 38.5 39.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF THE ANTICIPATED ACTION ACCURACIES WITH DIFFERENT MODALITIES ON THE VALIDATION SET. "OBJ" INDICATES THE OBJECT FEATURES. "W/O INTENTION" IS THE MODEL OPTIMIZED WITHOUT THE FUTURE INTENTION (EQN. (5)). "W/O DIFF" INDICATES THE MODEL WITHOUT FORECASTING THE DIFFERENT.</figDesc><table><row><cell cols="2">Modality Method</cell><cell cols="2">Top-1 Acc Top-5 Acc</cell></row><row><cell></cell><cell>RULSTM [4]</cell><cell>13.05</cell><cell>30.83</cell></row><row><cell>RGB</cell><cell>Ours w/o intention Ours w/o diff</cell><cell>13.23 12.97</cell><cell>31.39 30.61</cell></row><row><cell></cell><cell>Ours</cell><cell>13.68</cell><cell>31.58</cell></row><row><cell></cell><cell>RULSTM [4]</cell><cell>08.77</cell><cell>21.42</cell></row><row><cell>Flow</cell><cell>Ours w/o intention Ours w/o diff</cell><cell>08.81 08.51</cell><cell>21.89 21.68</cell></row><row><cell></cell><cell>Ours</cell><cell>09.23</cell><cell>22.06</cell></row><row><cell></cell><cell>RULSTM [4]</cell><cell>10.04</cell><cell>29.89</cell></row><row><cell>Obj</cell><cell>Ours w/o intention Ours w/o diff</cell><cell>10.76 10.62</cell><cell>30.05 30.12</cell></row><row><cell></cell><cell>Ours</cell><cell>10.72</cell><cell>30.27</cell></row><row><cell></cell><cell>RULSTM [4]</cell><cell>15.00</cell><cell>35.24</cell></row><row><cell>Fusion</cell><cell>Ours w/o intention Ours w/o diff</cell><cell>15.04 14.91</cell><cell>35.17 34.98</cell></row><row><cell></cell><cell>Ours</cell><cell>15.23</cell><cell>35.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, we can see the results of our baseline model</figDesc><table><row><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>31.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>31.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-5 Acc</cell><cell>31.2 31.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.5</cell><cell>1.75</cell><cell>2</cell><cell>2.25</cell><cell>2.5</cell><cell>2.75</cell><cell>3</cell><cell>3.25</cell><cell>3.5</cell><cell>3.75</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Length of the observed video</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V ANTICIPATION</head><label>V</label><figDesc>RESULTS ON THE EGTEA GAZE+ DATASET.</figDesc><table><row><cell></cell><cell cols="4">Top-5 Action Accuracy% @ different T</cell></row><row><cell></cell><cell>1.0</cell><cell>0.75</cell><cell>0.5</cell><cell>0.25</cell></row><row><cell>DMR [57]</cell><cell>55.70</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>ATSN [5]</cell><cell>40.53</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>MCE [32]</cell><cell>56.29</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>ED [31]</cell><cell>50.22</cell><cell>51.86</cell><cell>49.99</cell><cell>49.17</cell></row><row><cell>FN [25]</cell><cell>60.12</cell><cell>62.03</cell><cell>63.96</cell><cell>66.45</cell></row><row><cell>RL [26]</cell><cell>62.56</cell><cell>64.65</cell><cell>67.35</cell><cell>70.42</cell></row><row><cell>EL [58]</cell><cell>64.62</cell><cell>66.89</cell><cell>69.60</cell><cell>72.38</cell></row><row><cell>RULSTM [4]</cell><cell>66.40</cell><cell>68.41</cell><cell>71.84</cell><cell>74.28</cell></row><row><cell>Ours</cell><cell>66.71</cell><cell>68.54</cell><cell>72.32</cell><cell>74.59</cell></row><row><cell cols="5">only achieve 33.73% in Top-5 accuracy on the seen (S1)</cell></row><row><cell cols="5">test set and 21.10% on the unseen (S2) test set. By adding</cell></row><row><cell cols="5">our ImagineRNN to the framework, we observed a clear</cell></row><row><cell cols="4">performance improvement on both test sets.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>, we show the comparison of different optimization methods on the test set of EPIC-KITCHENS. Ours (l 2 loss) indicates our models optimized by the l 2 loss, while Ours (Contrastive) is the model optimized in the Contrastive Learning way, i.e., picking the correct one from lots of distractors. With l 2 loss, our model achieves 34.66 on Top-5 Accuracy in the seen test set. In contrast, with Contrastive Learning, our method achieves 34.98% on Top-5 action accuracy. The improvement of Contrastive Learning is more clear in the unseen test set. With the proposed Contrastive Learning, the action anticipation result on the unseen set shows a 1.40% (22.19% versus 20.79%) improvement on the Top-5 action accuracy. The significant performance gap shows that contrastive learning is a better way to optimize ImagineRNN. It leads to a better generalisability across the various benchmarks. Effectiveness of Forecasting the Difference. InTable IV, we show the comparison of results with and without forecasting the difference. We conduct ablation studies on the RGB input, the flow input, and the fused modalities input. The results show a steady improvement by introducing to forecast the difference. Specifically, our method significantly outperforms the one (w/o diff) by 0.9 points on the Top-5 action accuracy</figDesc><table><row><cell></cell><cell></cell><cell>T=2s</cell><cell>T=1.5s</cell><cell>T=1s</cell><cell>T=0.5s</cell></row><row><cell></cell><cell></cell><cell>Take spoon</cell><cell>Open fridge</cell><cell>Open fridge</cell><cell>Open fridge</cell></row><row><cell>Future GT</cell><cell>Top-5 predictions:</cell><cell>Open fridge Take plate Put-down spoon</cell><cell>Open door Open drawer Rinse hand</cell><cell>Open door Open drawer Take milk</cell><cell>Take milk Open drawer Take container</cell></row><row><cell>Open fridge</cell><cell></cell><cell>Take spatula</cell><cell>Take milk</cell><cell>Put container</cell><cell>Put container</cell></row><row><cell></cell><cell></cell><cell>Close door</cell><cell>Close door</cell><cell>Pour salt</cell><cell>Close door</cell></row><row><cell>Future GT</cell><cell>Top-5 predictions:</cell><cell>Get inegar Pour salt Pour oil</cell><cell>Pour salt Put-down salt Pour oil</cell><cell>Put-down oil Close door Pour oil</cell><cell>Pour salt Take salt Put-down salt</cell></row><row><cell>Pour salt</cell><cell></cell><cell>Put-down sauce</cell><cell>Take salt</cell><cell>Get vinegar</cell><cell>Take oil</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF DIFFERENT OPTIMIZATIONS ON THE UNSEEN (S2) TEST SET OF THE EPIC-KITCHENS ACTION ANTICIPATION CHALLENGE. "CON." INDICATES THE CONTRASTIVE LEARNING LOSS. "ADV." INDICATES THE ADVERSARIAL LOSS USED IN GAN [10].TABLE VII ABLATION STUDIES OF PREDICTING THE FEATURE DIFFERENCE BETWEEN ADJACENT FRAMES WITH GRU-BASED ARCHITECTURE ON THE EPIC-KITCHENS ACTION ANTICIPATION VALIDATION SET.</figDesc><table><row><cell>Methods</cell><cell cols="6">Verb Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Noun Action</cell></row><row><cell>l 2</cell><cell>27.4</cell><cell>69.8</cell><cell>15.4</cell><cell>35.3</cell><cell>8.5</cell><cell>20.8</cell></row><row><cell>Con. + l 2 + Adv.</cell><cell>27.9</cell><cell>70.3</cell><cell>14.3</cell><cell>34.7</cell><cell>8.5</cell><cell>20.7</cell></row><row><cell>Con. + l 2</cell><cell>28.4</cell><cell>70.0</cell><cell>15.1</cell><cell>34.9</cell><cell>9.0</cell><cell>21.1</cell></row><row><cell>Con.</cell><cell>29.3</cell><cell>70.7</cell><cell>15.5</cell><cell>35.8</cell><cell>9.3</cell><cell>22.2</cell></row><row><cell>Methods</cell><cell cols="6">Verb Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Noun Action</cell></row><row><cell>Ours (GRU w/o diff)</cell><cell>32.9</cell><cell>78.7</cell><cell>22.0</cell><cell>49.2</cell><cell>13.3</cell><cell>32.3</cell></row><row><cell>Ours (GRU with diff)</cell><cell>33.7</cell><cell>79.7</cell><cell>22.7</cell><cell>50.2</cell><cell>14.0</cell><cell>33.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Part-activated deep reinforcement learning for action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="421" to="436" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Which way are you going? imitative decision learning for path forecasting in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Leveraging the present to anticipate the future in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR-W</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-stream deep neural networks for rgb-d egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3001" to="3015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6536" to="6545" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting future instance segmentation by forecasting convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="584" to="599" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action anticipation with rbf kernelized feature mapping rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting the future: A jointly learnt model for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gammulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5562" to="5571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action anticipation by predicting future dynamic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV-W</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">In the eye of beholder: Joint learning of gaze and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Youtube-8m: A large-scale video classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional multirate reconstruction for temporal modeling in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Epic-fusion: Audio-visual temporal binding for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lsta: Long short-term attention for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all we need: Nailing down object-centric attention for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Symbiotic attention for egocentric action recognition with object-centric alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Encouraging lstms to anticipate actions very early</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadegh Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Am I done? predicting action progress in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Becattini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling temporal structure with lstm for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A deep spatiotemporal perspective for understanding crowd behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3289" to="3297" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Peeking into the future: Predicting future person activities and locations in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5725" to="5734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Eidetic 3d lstm: A model for video prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Forecasting future action sequences with attention: a new approach to weakly supervised action forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Yan Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Red: Reinforced encoder-decoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Leveraging uncertainty to rethink loss functions and evaluation measures for egocentric action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV-W</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning latent global network for skeleton-based action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="959" to="970" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">When will you do what?-anticipating temporal occurrences of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What will happen next? forecasting player moves in sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nextactive-object prediction from egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="401" to="411" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint prediction of activity labels and starting times in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual forecasting by imitating dynamics in natural sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">First-person activity forecasting with online inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning driven visual path prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5892" to="5904" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal aggregate representations for long-range video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ego-topo: Environment affordances from egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Egocentric object manipulation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dessalene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maynord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devaraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fermuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03201,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Action anticipation for collaborative environments: The impact of contextual information and uncertainty-based prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L A</forename><surname>Samatelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Vassallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Uncertainty-aware anticipation of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV-W</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Memory-augmented dense predictive coding for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AISTATS</publisher>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3733" to="3742" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Progressive learning for person re-identification with one example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2872" to="2881" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="29" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<idno>2017. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for driver activity anticipation via sensory-fusion architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
