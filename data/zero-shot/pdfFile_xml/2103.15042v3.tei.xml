<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distilling Virtual Examples for Long-tailed Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Yin</forename><surname>He</surname></persName>
							<email>heyy@lamda.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distilling Virtual Examples for Long-tailed Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We tackle the long-tailed visual recognition problem from the knowledge distillation perspective by proposing a Distill the Virtual Examples (DiVE) method. Specifically, by treating the predictions of a teacher model as virtual examples, we prove that distilling from these virtual examples is equivalent to label distribution learning under certain constraints. We show that when the virtual example distribution becomes flatter than the original input distribution, the under-represented tail classes will receive significant improvements, which is crucial in long-tailed recognition. The proposed DiVE method can explicitly tune the virtual example distribution to become flat. Extensive experiments on three benchmark datasets, including the large-scale iNaturalist ones, justify that the proposed DiVE method can significantly outperform state-of-the-art methods. Furthermore, additional analyses and experiments verify the virtual example interpretation, and demonstrate the effectiveness of tailored designs in DiVE for long-tailed problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks have achieved remarkable success in various fields of computer vision, part of which should be attributed to rich and representative datasets. Manually-built datasets are often well-designed and roughly balanced, with sufficient samples for every category, e.g., ImageNet ILSVRC 2012 <ref type="bibr" target="#b23">[24]</ref>. In the real world, however, image data are often inherently long-tailed. A few categories (the "head" categories) contain most training images, while most categories (the "tail" ones) have only few samples. Some recently released datasets start to draw our attention to this practical setting, e.g., iNaturalist <ref type="bibr" target="#b3">[4]</ref> and LVIS <ref type="bibr" target="#b8">[9]</ref>. These datasets show a naturally long-tailed dis-tribution. Models trained on them are easily biased towards the head classes, while the tail categories often have much lower accuracy rates compared to the head ones. This bias is, of course, not welcome by researchers or practitioners.</p><p>Many attempts have been made to deal with long-tailed recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref>. In particular, resampling makes a more balanced distribution through undersampling <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5]</ref> the head classes or oversampling the tail classes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref>. Another direction, reweighting, is to assign higher costs for tail categories in novel loss functions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18]</ref>. Recent methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17]</ref> also decouple the training of the backbone network and the classifier part. These methods, however, never cross the category boundary. That is, resampling, reweighting, and decoupling all happen independently inside each category, and there is no interaction among different categories.</p><p>A simple but interesting experiment motivated us to utilize cross-category interactions for long-tailed recognition. The complete CIFAR-100 dataset is balanced, and a highly imbalanced subset of it (i.e., CIFAR-100-LT) is a widely used benchmark for long-tailed recognition <ref type="bibr" target="#b35">[36]</ref>. We use the entire CIFAR-100 training set to train a (teacher) network, and then use knowledge distillation <ref type="bibr" target="#b12">[13]</ref> to distill a student network on the long-tailed CIFAR-100-LT with imbalance factor 100. The student's test accuracy is 61.58%, which is significantly (more than 10 percentage points) higher than existing long-tail recognition methods (c.f . Table 1)! Then, what makes its accuracy so high aside from the teacher being trained using the entire training set (which is not available in our long-tailed setting for the student)? Our answer to this question is two-fold: virtual examples &amp; knowledge distillation, or in short, distilling the virtual examples.</p><p>In a dog vs. cat binary recognition problem, if the prediction for a dog image is (0.7, 0.3), we interpret this prediction as two virtual examples: 0.7 dog virtual example, plus 0.3 cat virtual example. This interpretation extends naturally to the multiclass case. If dog is a head category and cat is a tail category, the 0.3 cat virtual example will help recognize cats, even if the input image is in fact a dog. Empirically, we often divide the categories in a longtailed problem into three subsets based on the number of training images in a category: Many (or head), Medium, and Few (or tail). <ref type="figure" target="#fig_0">Fig. 1</ref> shows the average number of examples and virtual examples for these subsets of 4 different cases. The first ("INPUT") is the distribution for original input images in CIFAR-100-LT. The rest are the virtual example distributions for 3 models: cross entropy ("CE", i.e., regular CNN training without any long-tail-specific learning), "BSCE" <ref type="bibr" target="#b22">[23]</ref> (a long-tailed recognition method), and "FULL" (trained using CIFAR-100, as described above). Using the 3 models (i.e., their virtual example distributions in <ref type="figure" target="#fig_0">Fig. 1</ref>) as teachers, the three students' accuracy on CIFAR-100-LT are 39.20%, 43.25%, and 53.71%, respectively. <ref type="bibr" target="#b0">1</ref> That is, the more balanced the teacher's virtual example distribution, the higher the student's accuracy is.</p><p>These observations inspired us to propose a DiVE (distilling virtual examples) method, which has the following properties and contributions:</p><p>? Validity of the virtual example interpretation. In Sec. 3.1 and 3.2, we show that the virtual example interpretation is valid, which then allows us to utilize direct and explicit cross-category interactions. ? Necessity of a balanced virtual example distribution.</p><p>Comparing INPUT with CE, the virtual example distribution of CE is almost identical to the original example distribution INPUT. However, we prove in Sec. 3.3 that the virtual example distribution must be flatter so long as we want to remove the bias against tail categories. Comparing CE, BSCE and FULL, we indeed observe empirically that the flatter the teacher's virtual example distribution, the higher the student's accuracy is. ? To level and to distill the virtual example distribution (DiVE). Noticing that even FULL in <ref type="figure" target="#fig_0">Fig. 1</ref> is still longtailed, we propose methods to make the virtual example distribution balanced, and then distill from it, which directly and explicitly learns from the balanced virtual example distribution. As validated by experiments, the proposed DiVE method outperforms existing long-tailed recognition methods by large margins in various long-tailed recognition datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, long-tailed recognition has attracted lots of attention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref>, including in recognition and detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref>. We will briefly review previous methods on long-tailed recognition and knowledge distillation.</p><p>Resampling/reweighting: One classic way to deal with long-tail distribution is data resampling. The idea is to make the class distribution more balanced. It includes oversampling for minority categories <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref> and undersampling for majority categories <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5]</ref> or learning to sample <ref type="bibr" target="#b22">[23]</ref>. However, resampling can cause problems in deep learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, e.g., oversampling may lead to overfitting, while undersampling limits the generalization ability of neural networks. Another commonly used method is to reweigh the loss function <ref type="bibr" target="#b19">[20]</ref>. This series of methods assign minority category instances more costs which are always misclassified or not confident <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18]</ref>. And balanced softmax <ref type="bibr" target="#b22">[23]</ref> is proposed to replace the standard softmax transformation. These methods, however, all sacrifice the accuracy of the head to compensate for the tail.</p><p>Decoupled training: Recent works show that decoupling the representation and classifier learning improves the performance on long-tailed datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36]</ref> significantly. However, they did not take into account the underrepresented features of tail categories, which confines their improvements only to the classifier.</p><p>Knowledge transfer: To transfer knowledge from head to tail categories is another branch of methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31]</ref>. Specifically, <ref type="bibr" target="#b29">[30]</ref> designed a module to use the head classes to learn the parameters of tail classes through meta-learning. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref> transfer knowledge from head to tail through complex memory banks. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29]</ref> ensembled multiple experts' knowledge. Some methods explored self-and semi-supervised learning <ref type="bibr" target="#b32">[33]</ref>, but required much longer training time or extra training data. They are usually complex and hard to generalize on different tasks.</p><p>In short, existing methods either lack a mechanism that let head and tail categories interact with each other, or are too complex to generalize and utilize well. The proposed DiVE method, on the contrary, is a simple pipeline that utilizes knowledge distillation to distill from virtual examples.</p><p>Within this process, examples from different categories naturally interact with each other (i.e., head helps tail).</p><p>Knowledge distillation: Knowledge distillation (KD) is a technique to transfer knowledge across different models <ref type="bibr" target="#b12">[13]</ref>, which is most popular in model compression.</p><p>Since its inception in <ref type="bibr" target="#b12">[13]</ref>, knowledge distillation has attracted lots of attention <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. Recently, <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b20">21]</ref> invited KD to long-tailed problems. However, in DiVE, the key difference is that we have a different starting point by making the teacher's virtual example distribution to be flat.</p><p>Some works try to explain the mechanism behind KD. Specifically, <ref type="bibr" target="#b34">[35]</ref> treated KD as a learnable label smoother, while we provide another interpretation. We argue that knowledge distillation shares knowledge among different classes through virtual examples, which is very similar to deep label distribution learning (DLDL) <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Distilling the Virtual Examples</head><p>We call the proposed method DiVE (Distilling Virtual Examples), which has a relatively simple pipeline: A teacher model is first trained for the long-tailed task with any existing methods, then we use knowledge distillation to transfer knowledge from the teacher (the virtual examples) to a student model. The distilled student model is DiVE's output.</p><p>Since the proposed DiVE method hinges on distilling virtual examples, we first establish an equivalence between knowledge distillation and deep label distribution learning (Sec. 3.1), then explain in detail how the label distribution interpretation leads to virtual examples (Sec. 3.2) in our context, then why the virtual example distribution must be flat (Sec. 3.3), and finally how to generate a balanced virtual example distribution to distill in long-tailed tasks (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">KD ? DLDL</head><p>In a C-class classification problem, assume a training set of n examples D = {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x n , y n )}, where x i is the i-th training instance and y i ? {1, 2, . . . , C} is its groundtruth label. The one-hot encoding can turn y i into an equivalent vector y i = (y i,1 , y i,2 , . . . , y i,C ) ? R C , with the k-th component of y i defined by y i,k = 1 if k = y i , otherwise y i,k = 0. With a slight abuse of notations, we denote y k as the k-th component of y from now on. For a given training example x and its corresponding one-hot label y, suppose a teacher CNN model predicts t = (t 1 , t 2 , . . . , t C ) ? R C for it, which is obtained by transforming the logits z ? R C by a softmax function, as</p><formula xml:id="formula_0">t i = exp(z i ) C k=1 exp(z k ) .<label>(1)</label></formula><p>Knowledge distillation (KD) <ref type="bibr" target="#b12">[13]</ref> then uses knowledge hidden in t to help train a student network (which often has smaller capacity than the teacher network).</p><p>We can similarly denote the student network's prediction as s = (s 1 , s 2 , . . . , s C ). Then, the student's loss function is</p><formula xml:id="formula_1">L KD = (1 ? ?)L CE (y, s) + ?L KL (t, s) .<label>(2)</label></formula><p>The first term is the usual cross entropy (CE) loss between groundtruth labels and student predictions:</p><formula xml:id="formula_2">L CE (y, s) = ? C k=1 y k log s k .<label>(3)</label></formula><p>The second term encourages the student predictions to mimic the teachers' via minimizing their Kullback-Leibler (KL) divergence,</p><formula xml:id="formula_3">L KL (t, s) = C k=1 t k log t k s k .<label>(4)</label></formula><p>Note that a temperature parameter ? is often used in KD. When ? = 1, we need to compute t ? as</p><formula xml:id="formula_4">t ? i = exp(z i /? ) C k=1 exp(z k /? ) ,<label>(5)</label></formula><p>and similarly change s to s ? , and the second loss term becomes ? 2 L KL (t ? , s ? ). The hyperparameter ? ? [0, 1] balances these two loss terms, which is between 0 and 1. For now, we temporarily assume that ? = 1.</p><p>Note that y, t and s are all discrete distributions, and we use H(?) to denote the entropy. Let us defin?</p><formula xml:id="formula_5">t = (1 ? ?)y + ?t ,<label>(6)</label></formula><p>and use the well-known fact that L CE (x, y) = L KL (x, y) + H(x), then it is easy to derive that</p><formula xml:id="formula_6">L KD = (1 ? ?)L CE (y, s) + ?L KL (t, s) (7) = (1 ? ?)L CE (y, s) + ?L CE (t, s) ? ?H(t) (8) = L CE (1 ? ?)y + ?t, s ? ?H(t) (9) = L CE (t, s) ? ?H(t) (10) = L KL (t, s) + H(t) ? ?H(t) .<label>(11)</label></formula><p>On one hand, because y, t (and hencet) have zero gradients with respect to the student model's parameters, we immediately notice that L KL (t, s) (or L CE (t, s)) is an equivalent loss function for training the student model. On the other hand, L KL (t, s) is, as we will discuss in the next subsection, exactly the loss function of a DLDL <ref type="bibr" target="#b5">[6]</ref> model. Hence, we have proved that when the temperature ? = 1, knowledge distillation is equivalent to DLDL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">From label distributions to virtual examples</head><p>The label distribution learning (LDL) <ref type="bibr" target="#b6">[7]</ref> handles tasks where the groundtruth labels are in fact uncertain. For example, estimating the apparent age based on a facial image is difficult-two annotators may give different answers for the same picture, e.g., 25 and 27 years old. Hence, when the groundtruth label is 25, instead of using a one-hot encoding for 25, LDL generates a "label distribution" y as its label, with y 25 being the largest, and other labels around 25 have non-zeros values, too. For example, treating 0 to 100 as C = 101 classification labels, the LDL label may be: y 25 = 0.7, y 24 = y 26 = 0.1, y 23 = y 27 = 0.05, and y k = 0 if k &lt; 23 or k &gt; 27. Note that the LDL label y is a valid distribution: k y k = 1 and y k ? 0. The DLDL (deep LDL) method <ref type="bibr" target="#b5">[6]</ref> combines LDL with the deep learning paradigm, and uses a KL-based loss L KL (y, t) to calculate the loss for a training example when its prediction is t. Hence, when ? = 1, knowledge distillation is equivalent to DLDL with the groundtruth label distribution beingt.</p><p>In KD, although the groundtruth label is assumed to be certain, the teacher model's prediction may be wrong. For example, the groundtruth label is 7, but arg max k t k = 7 may hold true. KD can correct such errors using the L CE term, because it forces the prediction to match the groundtruth label. In DLDL, it was coerced throughtnowt 7 = (1 ? ?) ? 1 + ?t 7 , and so long as ? ? 0.5 we know for sure arg max ktk = 7 becauset 7 ? 0.5.</p><p>DLDL argues that a facial image with age 24 is in fact useful for classifying 25-year-old faces, because faces with "nearby" ages must share similar visual characteristics. Although the "nearby" concept does not apply to more general and long-tailed recognition problems, we now show that the teacher model's prediction in fact creates virtual examples that help long-tailed recognition, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>In this illustrative example, there are many dog images, but very few cat ones, therefore it is a long-tailed problem. The teacher model's prediction scores for all categories are nonzero due to the property of softmax. Hence, the original input image is replaced by virtual examples in 5 categories: 0.7 dog, 0.02 car, 0.07 rabbit, 0.01 ship and 0.2 cat in a KD process. Since a dog image's prediction has non-negligible mass (0.2) for cat, it means there are similarities between these categories, and the virtual cat example will be useful in learning the cat category, even though it is a dog image.</p><p>Note that the distillation loss is L CE (t, s) (Eqn. 10), which equals ?  It is worth noting that our Equations <ref type="formula" target="#formula_5">(6)</ref> and <ref type="formula" target="#formula_0">(11)</ref> are the same as those in <ref type="bibr" target="#b34">[35]</ref>. However, their meanings and goals are different. <ref type="bibr" target="#b34">[35]</ref> is motivated by label smoothing <ref type="bibr" target="#b25">[26]</ref> and is assuming high temperature ? such that t is close to uniform for every example. However, in long-tailed problems, we only want the distribution of all virtual examples to become flatter, but every t must still carry useful information to discriminate different categories and transfer to s. In fact, we mostly use a small temperature (e.g., ? = 1 or ? = 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The virtual example distribution must be flat</head><p>Now we further show that the virtual example distribution must be flat (or at least flatter than the original input image distribution).</p><p>We go back to the dog vs. cat binary recognition problem. What if the dataset is imbalanced? Suppose the dog has n head samples while the cat has n tail samples, and n head n tail . We use label smoothing to generate virtual examples from dog for cat. Each dog sample is converted to virtual cat example and 1 ? dog example ( &lt; 0.5). The number of virtual examples for cat and dog are n tail + n head ? and n head ? n head ? , respectively. We use the virtual example ratio between cat and dog to measure the flatness of the virtual example distribution.</p><p>This simple experiment was conducted on two classes randomly chosen from CIFAR-10, one with 5000 training samples, the other 500. We take "airplane" as head and "automobile" as tail, with results in <ref type="figure" target="#fig_2">Fig. 3</ref>. As the virtual example distribution goes flatter, performance of the tail increase significantly while the head is almost intact. The original label distribution learning requires correlations among different labels. But, in long-tailed recognition, <ref type="figure" target="#fig_2">Fig. 3</ref> shows that virtual examples from the head categories will help recognizing examples from the tail categories, even if these categories are not correlated. For more examples, please refer to our supplementary materials.</p><p>Hence, we easily obtain the following conclusion: in order to obtain a balanced model for a long-tailed task, the virtual example distribution must be much flatter than the input distribution. The tail categories must have significantly more virtual examples than their number of input images, and the trend is reversed for the head categories. Otherwise, tail categories will have low accuracy rates. <ref type="figure" target="#fig_0">Fig. 1</ref> clearly verifies this conclusion, in which "CE" is a failure case, while in "BSCE" and especially in "FULL", we observe better virtual example distributions.</p><p>Still, two difficulties exist. First, existing methods like BSCE often assign different weights to categories, but this kind of strategies have only limited effect on the virtual example distribution, because they can only affect the virtual example distribution in an indirect manner. <ref type="figure" target="#fig_3">Fig. 4</ref> shows two such examples (BSCE <ref type="bibr" target="#b22">[23]</ref> and LWS <ref type="bibr" target="#b16">[17]</ref>). For example, BSCE <ref type="bibr" target="#b22">[23]</ref> uses the following function to replace the softmax function in computing the soft logits s BSCE :</p><formula xml:id="formula_7">s BSCE i = n i exp(z i ) C k=1 n k exp(z k ) ,<label>(12)</label></formula><p>where n i is the number of training examples for category i. But, its virtual example distribution is still similar to that of the original input (c.f . <ref type="figure" target="#fig_0">Fig. 1</ref>). We need a direct and explicit way to obtain a flatter virtual example distribution. Second, we still do not know what level of "flatness" is beneficial for long-tailed recognition.  knowledge distillation, the teacher's virtual example distribution t is an explicit supervision signal, while we have various knobs to directly tune this distribution towards a flatter one. We also provide a rule-of-thumb for determining the level of flatness. To level the virtual example distribution, the temperature is in fact already a built-in weapon in KD. Eqn. (5) clearly tells us that when the temperature ? increases, the teacher signal t ? will be more and more balanced. As <ref type="bibr" target="#b34">[35]</ref> mentioned, when ? ? ?, t ? will become a uniform distribution. <ref type="figure" target="#fig_4">Fig. 5</ref> illustrates this trend when ? increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Level and distill virtual examples</head><p>A very large temperature (e.g., the last temperature in <ref type="figure" target="#fig_4">Fig. 5</ref> with ? = 20) does not deem useful in knowledge distillation, because then the supervision signal t ? will be roughly uniform and contains little information.</p><p>Thus, we resort to a classic trick (power normalization) to further adjust the virtual example distribution without in-creasing ? to an unreasonable range. The power normalization <ref type="bibr" target="#b21">[22]</ref> simply converts a nonnegative real number x to its power x p . For simplicity, we always set p = 0.5 in our experiments. That is, to level the teacher's virtual example distribution, we perform the following transformations:</p><formula xml:id="formula_8">t ? k ?? t ? k , ? 1 ? k ? C ,<label>(13)</label></formula><formula xml:id="formula_9">t ? i ?? t ? i k t ? k ? 1 ? i ? C .<label>(14)</label></formula><p>It is easy to find out that applying the power normalization with p amounts to multiplying 1 p to the temperature ? for the teacher's supervision signal t ? (and p = 0.5 means doubling ? for the teacher). The temperature for student s ? , however, remains unchanged.</p><p>Next, we introduce a rule-of-thumb to choose the temperature ? . We want the virtual example distribution to be more balanced. But we also want to keep it to be relatively small (e.g., ? &lt; 10). Hence, after training a teacher model, we will compute the virtual example distribution of t ? on the entire training set for different ? between 1 and 10, and with or without power normalization. We prefer distributions that are flat, specifically, whose average number of examples per category in the tail part is slightly higher than that in the head part. For example, applying this rule-ofthumb, we will choose ? = 6 in <ref type="figure" target="#fig_4">Fig. 5</ref>, then temperature for student s ? should be 3 if power normalization (p = 0.5) is used for the teacher t ? . Please note that computing the virtual example distributions does not involve any fine-tuning or training of networks, hence it is very efficient-we need to simply transform the vectors t ? using different ? values and then normalize them.</p><p>One final thing to note is that in a long-tailed setting, usingt is at best suboptimal, because the y in Eqn. (6) is distributed in a long-tailed fashion and we cannot utilize the adjusting capability of the temperature ? . Thus, in knowledge distillation we use the BSCE loss L CE (y, s BSCE ). The overall DiVE loss function is then</p><formula xml:id="formula_10">L DiVE (y, s BSCE ) = (1 ? ?)L CE (y, s BSCE ) + ?? 2 L KL (t ? , s ? ) .<label>(15)</label></formula><p>The first term is the BSCE loss, in which the student's soft logits s BSCE does not involve the temperature. The second term is the KD term, in which t ? uses a temperature ? and possibly followed by a power normalization (p = 0.5), while s ? only uses the temperature ? but does not apply the power normalization. With all necessary components ready, the simple DiVE pipeline is summarized in Algorithm 1.</p><p>Note that we choose BSCE as our teacher model because it has a good starting point for virtual example distribution (c.f . <ref type="figure" target="#fig_3">Fig. 4</ref>), and at the same time it is simple in implementation. But the teacher model can be trained by any other</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: The DiVE pipeline</head><p>Input: A long-tailed training set D. <ref type="bibr" target="#b0">1</ref> Use BSCE to train a teacher model on D; 2 Use the rule-of-thumb to determine ? and determine whether to use the power normalization; 3 Transform teacher's soft logits as t ? accordingly; 4 Train a DiVE model by minimizing Eqn. <ref type="bibr" target="#b14">(15)</ref>. method. Without special instructions, the teacher and the student use the same model architecture in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We now validate DiVE on various long-tailed datasets, with the empirical settings and implementation details in Sec </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and implementations</head><p>We conduct experiments on three major benchmarks to evaluate the effectiveness of our proposed DiVE.</p><p>CIFAR-100-LT. These long-tailed versions of CIFAR-100 <ref type="bibr" target="#b2">[3]</ref> follow an exponential decay in sample sizes across different classes with various imbalance factor ?. we use ? = 10, 50, 100 in our experiments. ResNet-32 <ref type="bibr" target="#b11">[12]</ref> is used as the backbone network. We use the same training recipe as <ref type="bibr" target="#b35">[36]</ref> with standard CIFAR data augmentation.</p><p>ImageNet-LT. They are long-tailed versions of Ima-geNet <ref type="bibr" target="#b18">[19]</ref>. We use ResNeXt-50 <ref type="bibr" target="#b31">[32]</ref> as the backbone in all experiments. For training strategies, we follow <ref type="bibr" target="#b16">[17]</ref>.</p><p>iNaturalist2018. iNaturalist2018 <ref type="bibr" target="#b3">[4]</ref> is a large-scale real-world datasets with severe long-tail problems. We select ResNet-50 <ref type="bibr" target="#b11">[12]</ref> as the backbone network and apply the similar training strategies with ImageNet-LT. 90 and 200 epochs results are reported.</p><p>For more details of the datasets and implementation, please refer to our supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation setups and comparison methods</head><p>After long-tailed training, we evaluate the models on the corresponding balanced validation/test dataset, and report the commonly used top-1 accuracy over all classes, denoted as "All". We also report the top-5 accuracy on iNatural-ist2018 to evaluate the robustness of the methods. To better understand the methods' abilities on categories with different number of examples, we follow <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> to split the categories into three subsets and report the average accuracy rates in these three subsets: Many-shot (&gt;100 images), Medium-shot (20 ? 100 images), and Few-shot (&lt;20 images), which are also called the head, medium and tail categories, respectively.</p><p>We compare DiVE with two groups of methods: ? Baseline methods. Networks trained with the standard cross-entropy loss and the focal loss <ref type="bibr" target="#b17">[18]</ref> are used as baselines in this group. Also, the balanced softmax method proposed in <ref type="bibr" target="#b22">[23]</ref> (which also trains all our teacher networks) is compared in all experiments. ? State-of-the-art methods. We also compare our DiVE method with recently proposed state-of-the-art methods, like De-confund-TDE <ref type="bibr" target="#b27">[28]</ref> ("TDE" in short) and RIDE <ref type="bibr" target="#b28">[29]</ref>. We further apply our DiVE to RIDE and get RIDE-DiVE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main results</head><p>We show our experimental results on the three datasets one by one, and finally RIDE-DiVE. For more experiments, please refer to our supplementary materials.</p><p>Results on CIFAR-100-LT. <ref type="table" target="#tab_0">Table 1</ref> shows the experimental results on long-tailed CIFAR-100 with various imbalance factors ranging from 10 to 100. The proposed DiVE method consistently achieves the best results under all imbalance factors, and it outperforms the state-of-the-art method De-confund-TDE <ref type="bibr" target="#b27">[28]</ref> by a large margin.</p><p>Although knowledge distillation is also used in the LFME method <ref type="bibr" target="#b30">[31]</ref>, DiVE utilizes the abundant head-class samples to produce virtual examples for tail classes, thus enjoys the benefit of information from the entire dataset. <ref type="table" target="#tab_0">Table 1</ref> clearly shows that DiVE outperforms LFME by a large margin (1.55 percentage points).</p><p>Results on ImageNet-LT. We further evaluate DiVE on the ImageNet-LT dataset, with results in <ref type="table" target="#tab_1">Table 2</ref>. We also report the average accuracy details of each category subsets.</p><p>DiVE almost obtains consistently higher accuracy rates than all compared methods in all comparisons (Many, Medium, Few, and "All").</p><p>DiVE also beats the teacher model BSCE in all three subsets, and its accuracy loss in the Many subset is less than 1%. On the contrary, the compared methods often lose accuracy in one of the subsets, and their accuracy loss in the Many subset is both consistent and significant. Results on iNaturalist2018. To verify the performance of DiVE in real world long-tailed circumstances, we conduct experiments on the iNaturalist2018 dataset. <ref type="table">Table 3</ref> shows the overall accuracy results computed using all categories. Following <ref type="bibr" target="#b16">[17]</ref>, besides 90 epochs, we train for more epochs (200 epochs) to get further improvement. We gain 2.58% and 1.54% on top-1/top-5 accuracy from that. In terms of top-1 accuracy, DiVE is at least 1 percentage point higher than all compared methods in both settings.</p><p>We further break the accuracy statistics into three groups, and the results are in <ref type="table" target="#tab_2">Table 4</ref>. BBN hurts the Many-shot subset a lot to enhance the Medium-shot and Few-shot, while LWS has very similar accuracy with the BSCE baseline in all subsets. On the contrary, DiVE outperforms BSCE consistently and significantly in all three subsets. DiVE's accuracy drop in the Many subset from the baseline CE method is also much smaller than other methods.</p><p>Results of RIDE-DiVE. Our DiVE can be easily deployed on any existing method. Following <ref type="bibr" target="#b28">[29]</ref>, we first use RIDE with 6 experts to generate the virtual examples, then distill to a 4 experts student model. Results are in <ref type="table">Table 5</ref>. It is worth noting that our teacher networks (using BSCE) is much inferior to the RIDE teacher in two datasets, but our student networks surpass both the teacher and RIDE, setting new state-of-arts for these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Analyses</head><p>In this final part, we analyze various components in our DiVE method, with results mainly presented in <ref type="table">Table 6</ref> and <ref type="figure" target="#fig_6">Fig. 6</ref>, all from experiments on CIFAR-100-LT (? = 100).</p><p>The first row "CE" is the baseline that only uses a normal cross entropy loss, and row #1 to #6 study a few variants of the DiVE method. The column "BSCE" means whether using the first loss term in Eqn. <ref type="bibr" target="#b14">(15)</ref> or not.t means using Eqn. (6), while t ? means not to factor in groundtruth labels. ? is the temperature value, and "power" means whether the power normalization is used or not. Hence, row #1 is DiVE with the temperature ? = 3 and power normalization.</p><p>Firstly, as Sec. 3.1 shows, KD ? DLDL. In row #3, we remove the first term in the right hand side of Eqn. <ref type="bibr" target="#b14">(15)</ref> and only use the DLDL loss. Without the BSCE loss term, DiVE's accuracy drops (but less than 1% in all three cases). However, cross compare row #3 with results in <ref type="table" target="#tab_0">Table 1</ref>, the DLDL-only version of DiVE is still better than most compared methods. This fact shows that distilling the virtual examples alone are very effective in long-tailed recognition. A close examination of <ref type="figure" target="#fig_6">Fig. 6</ref> shows that row #1 (DiVE) indeed leads to a balanced virtual example distribution, and the tail is slightly higher than the heads, which matches our rule-of-thumb.</p><p>In row #5, we factor the groundtruth label into the teacher's supervision signal, as directed by Eqn. (6) (? = 0.5 and ? = 1). The BSCE loss is removed, too. Its results are the worst in all DiVE variants, and in <ref type="figure" target="#fig_6">Fig. 6</ref> its distribution is the most imbalanced.</p><p>On top of row #5, we can add the power normalization (row #2) and a further temperature ? = 3 (row #6). As <ref type="figure" target="#fig_6">Fig. 6</ref> shows, both operations make the virtual examples distribute more evenly, at the same time their accuracy rates are also improved. Row #2 results are similar to DiVE without BSCE (row #3), which corroborates with the derivation in Sec. 3.1. The fact that all rows usingt are worse than row #1 (DiVE, which uses t ? ) supports our decision of not usingt (c.f . Sec. <ref type="bibr">3.4)</ref>.</p><p>Besides, the fact that row #4 has the lowest accuracy among variants that use t ? once again shows that a balanced virtual example distribution is important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed the DiVE method to distill the predictions of a teacher model (as the virtual examples) into a student model, which leverages interactions among different categories for long-tailed visual recognition: virtual examples from head categories will help recognize tail categories, even if these categories are not correlated. Therefore, employing knowledge distillation upon virtual examples was able to alleviate the extreme imbalance for longtailed data, particularly for the tail classes. Also, in order to further improve the distillation accuracy, we provided a rule-of-thumb for adjusting the distribution of the virtual examples towards flat. DiVE has achieved the best results on long-tailed benchmarks, including the large-scale iNaturalist. In the future, we attempt to extend our DiVE into handling the long-tailed detection problem.  <ref type="figure">Figure 7</ref>. Accuracy (mean value and plus / minus 1 standard deviation) in "airplane" vs. "dog" binary classification experiments. We take "airplane" as the head and "dog" as the tail categories. The numbers in each sub-figure title are the number of samples in the head and tail, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional experiments on the influence of virtual example distribution flatness</head><p>Due to the space limit of the main paper, we explain the experimental details of Sec 3.3 in this appendix. The virtual example ratio R between tail and head equals n tail +n head ? n head ?n head ? after smoothing, and R ? [ n tail n head , 2n tail n head +1) because we restrict that 0 ? &lt; 0.5. We sampled ? {0, 0.1, 0.2, 0.3, 0.4}, and each experiment was run for 5 times to compute the mean and standard deviation.</p><p>In this section, following Sec. 3.3 of our main paper, we conduct additional experiments under different settings, to further justify our conjecture that virtual example distribution must be flat. Specifically, we vary the categories and the imbalance factor. Results are in <ref type="figure">Fig. 7</ref>. In <ref type="figure">Fig. 7</ref>(a), we use "airplane" as the head and "dog" as the tail category, which are very dissimilar in appearance. But, similar to what <ref type="figure" target="#fig_2">Fig. 3</ref> in the main paper shows, the performance is improved significantly as the virtual example distribution gets flatter. Comparing <ref type="figure">Fig. 7(a)</ref>, <ref type="figure">Fig. 7(b)</ref> and <ref type="figure">Fig. 7(c)</ref>, under different imbalance factors or dataset size, all head accuracies are almost intact while the tail accuracies increase significantly as the virtual example distribution goes flatter.</p><p>All these observations are consistent with our conjecture that the virtual example distribution must be flat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>In this section, we describe the implementation details of our DiVE in different long-tailed datasets. The properties of all datasets used in our experiments are summarized in <ref type="table" target="#tab_3">Table 7</ref>.</p><p>On CIFAR-100-LT. CIFAR-100 contains 100 categories and 60,000 images (50,000 for training and 10,000 for validation). Following <ref type="bibr" target="#b35">[36]</ref>, we manually split the long-tailed versions of it with controllable degrees of data imbalance.</p><p>We follow the data augmentation strategy in <ref type="bibr" target="#b11">[12]</ref>: randomly crop a 32 ? 32 patch from the original image or its horizontal flip with 4 pixels padded on each side. ResNet-32 <ref type="bibr" target="#b11">[12]</ref> is used as the backbone network. Following <ref type="bibr" target="#b35">[36]</ref>, we use stochastic gradient descent (SGD) to optimize networks with momentum of 0.9, weight decay of 2 ? 10 ?4 for 200 epochs with batch size being 128. The initial learning rate is 0.1 with first 5 epochs being linear warm-up, then decayed at 120 th and 160 th epochs by 0.01. In the proposed DiVE method, we choose ? = 3 with the power normalization (p = 0.5), as well as ? = 0.5 in all experiments on CIFAR-100-LT.</p><p>On ImageNet-LT. It is a long-tailed version of Ima-geNet, first used by <ref type="bibr" target="#b18">[19]</ref>. It has 115.8K images from 1000 categories, with n max = 1280 and n min = 5.</p><p>To have fair comparisons, we use ResNeXt-50 <ref type="bibr" target="#b31">[32]</ref> as the backbone network in all experiments on ImageNet-LT. We use the same data augmentation strategy as that in <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b16">[17]</ref>. In detail, images are firstly resized by setting shorter side to 256, then we randomly take a 224 ? 224 crop from it or its horizontal flip, followed by color jittering. For training strategies, we follow <ref type="bibr" target="#b16">[17]</ref>. Both teacher and student networks are trained for 90 epochs with batch size 512. The initial learning rate is set to 0.2 and cosine decayed epoch by epoch. Mini-batch stochastic gradient descent (SGD) with momentum of 0.9, weight decay of 5 ? 10 ?4 is used as our optimizer. In this dataset, power normalization is not chosen in DiVE, and we set ? = 9, ? = 0.5.</p><p>On iNaturalist. The iNaturalist species classification datasets are large-scale real-world datasets with severe long-tail problems. iNaturalist2018 <ref type="bibr" target="#b3">[4]</ref> contains 437.5K images from 8,412 categories, with ? = 500. We adopt the official training and validation split in our experiments.</p><p>We use ResNet-50 <ref type="bibr" target="#b11">[12]</ref> as the backbone network across all experiments for iNaturalist2018. Standard data augmentation strategies proposed in <ref type="bibr" target="#b7">[8]</ref> are utilized. We train the teacher and student networks both for 90 epochs with batch size 256. The initial learning rate is set to 0.1, and decayed following the cosine decay schedule. The optimizer is the same as that used for ImageNet-LT. In DiVE, we set ? = 2 with the power normalization (p = 0.5) and ? = 0.5. Some methods reported results trained with 200 epochs, hence we also report DiVE results with 200 epochs.</p><p>Note that the training strategies of RIDE <ref type="bibr" target="#b28">[29]</ref> are slightly different from those standard long-tailed training strategies. So, when comparing with RIDE <ref type="bibr" target="#b28">[29]</ref>, we follow experimental settings in <ref type="bibr" target="#b28">[29]</ref>. For implementation details of RIDE-DiVE, we adopt BSCE to train a 6 experts RIDE in place of LDAM. Then we distill the virtual examples to each expert of a 4 experts student network using Eqn. <ref type="bibr" target="#b14">(15)</ref> in our main paper, and train the expert assignment module finally. We normalize the feature and classifier weights of student network for fair comparison.</p><p>In addition, we set ? to 0.75 in all experiments, and set ? = 3 in ImageNet-LT for RIDE-DiVE because the teacher networks provide more reliable predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on various shifted test label distributions</head><p>Recently, <ref type="bibr" target="#b13">[14]</ref> proposed a more realistic evaluation protocol, they evaluated models on a range of target label distributions, including two types, Forward and Backward. For the Forward type, the target label distribution becomes similar to the source label distribution when the imbalance factor increases. The order is flipped for the Backward type. Please refer to <ref type="bibr" target="#b13">[14]</ref> for more details.</p><p>Follow <ref type="bibr" target="#b13">[14]</ref>, we evaluate CE, BSCE and DiVE trained for 90 epochs on test time shifted ImageNet-LT, the results are in <ref type="table" target="#tab_4">Table 8</ref>. Here PC means injecting target label distribution information to the final output. Knowing the target label distribution or not, DiVE surpass CE and BSCE by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. t-SNE visualization</head><p>We use the t-SNE method to visualize the embedding space on CIFAR100-LT (? = 100). We aggregate the classes into ten groups, based on the order of the number of examples from head to tail, and sample one class from each group for visualization. Results are in <ref type="figure">Fig. 8</ref>. In CE (cross entropy), the feature embedding is dispersed for both head and tail, making it hard to distinguish classes of similar appearance (e.g., "mouse" and "squirrel"). DiVE enlarges the inter-class variance while reduces the intra-class variance for both head and tail (e.g., features of "mouse" and "squirrel" are more compact and easier to separate). And, RIDE-DiVE is better than both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Sample images visualization</head><p>In <ref type="figure">Fig. 9</ref>, we visualize some sample images in ImageNet-LT test set, comparing the predictions of CE, BSCE and DiVE. We choose samples on which DiVE's predictions are correct, to show how dose DiVE correct the predictions.</p><p>DiVE can correct the predictions not only to semantically "nearby" categories (e.g., the "Polyporus frondosus" example and the "Siberian husky" example in <ref type="figure">Fig. 9</ref>), but also to semantically "far" categories (e.g., the "pot" example and the "ski mask" example in <ref type="figure">Fig. 9</ref>).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(Virtual) example distribution of different models. Given a training set and a CNN model, we can compute the model's virtual example distribution on the training set by summing the contribution of all training examples to all categories. Note that through virtual examples, different categories naturally interact in every training example!</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>CFigure 2 .</head><label>2</label><figDesc>k=1t k log s k . For category k, there aret k virtual example(s), and the virtual example's loss is ? log s k if we collect virtual examples from all categories and perform a normal cross-entropy training. Hence, summing up the losses of all virtual examples, we obtain ? C k=1t k log s k , or equivalent to the distillation loss L CE (t, s). Hence, the virtual example interpretation is valid. For one category, if we sum the count of virtual ex-Illustration of virtual examples. An input dog example received prediction in the first row from the teacher model, and in effect creates virtual examples in all 5 categories: 0.7 dog example, 0.02 car example, 0.07 rabbit example, 0.01 ship example and 0.2 cat example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Accuracy (mean value and standard deviation) in a binary classification example. The accuracy becomes higher when the virtual example ratio between two classes grows (i.e., when the virtual example distribution becomes flatter). 'All' is the union of 'head' and 'tail'. amples contributed to this category by all training examples, we obtain the number of virtual examples for this category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Our answers to addressing both difficulties are pretty straightforward: Distill the Virtual Examples (DiVE). In Smoothed virtual example distributions of different teachers on ImageNet-LT (a long-tailed version of ImageNet), with the temperature ? = 2. The category id is sorted based on the number of examples in each category, from head to tail. Both BSCE<ref type="bibr" target="#b22">[23]</ref> and LWS<ref type="bibr" target="#b16">[17]</ref> generate flatter distributions than the baseline cross entropy method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The virtual example distribution becomes flatter when the temperature ? increases, illustrated on CIFAR-100-LT with imbalance factor 100. The model was trained with BSCE<ref type="bibr" target="#b22">[23]</ref>. Based on our rule-of-thumb, ? = 6 is a proper temperature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. 4.1, evaluation settings in Sec. 4.2, and main results in Sec. 4.3. Sec. 4.4 analyzes various aspects of DiVE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Virtual example distribution of different method id.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 Figure 9 .</head><label>89</label><figDesc>. t-SNE visualization of different models' embedding space on CIFAR100-LT (? = 100). Some sample images in ImageNet-LT test set with predictions from CE, BSCE and DiVE. Below each image are the predicted categories from CE, BSCE and DiVE on it. Categories in blue are "Many", categories in yellow are "Medium", while categories in red are "Few". DiVE's predictions are also ground-truth labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Top-1 accuracy (%) on CIFAR-100-LT. The " ?" symbol denotes results copied directly from<ref type="bibr" target="#b35">[36]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="3">Imbalance factor 100 50 10</cell></row><row><cell>CE</cell><cell cols="3">38.35 42.41 56.51</cell></row><row><cell>Focal  ? [18]</cell><cell cols="3">38.41 44.32 55.78</cell></row><row><cell>BSCE</cell><cell cols="3">42.39 47.60 58.38</cell></row><row><cell>LFME [31]</cell><cell>43.80</cell><cell>-</cell><cell>-</cell></row><row><cell>LDAM-DRW [1]</cell><cell cols="3">42.04 46.62 58.71</cell></row><row><cell>BBN [36]</cell><cell cols="3">42.56 47.02 59.12</cell></row><row><cell>Meta-learning [16]</cell><cell cols="3">44.70 50.08 59.59</cell></row><row><cell cols="4">LDAM-DRW+SSP [33] 43.43 47.11 58.91</cell></row><row><cell>TDE [28]</cell><cell cols="3">44.10 50.30 59.60</cell></row><row><cell>DiVE</cell><cell cols="3">45.35 51.13 62.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-art methods on ImageNet-LT. A " ?" symbol denotes results copied from<ref type="bibr" target="#b16">[17]</ref>, and a "*" symbol denotes results obtained by running author-provided code.</figDesc><table><row><cell>Methods</cell><cell cols="3">Many Medium</cell><cell>Few</cell><cell>All</cell></row><row><cell>CE</cell><cell cols="2">65.02</cell><cell>37.07</cell><cell cols="2">8.07 43.89</cell></row><row><cell>BSCE</cell><cell cols="2">60.92</cell><cell>47.97</cell><cell cols="2">29.79 50.48</cell></row><row><cell>OLTR  ? [19]</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>46.30</cell></row><row><cell cols="3">? -norm [17] 59.10</cell><cell>46.90</cell><cell cols="2">30.70 49.40</cell></row><row><cell>LWS [17]</cell><cell cols="2">60.20</cell><cell>47.20</cell><cell cols="2">30.30 49.90</cell></row><row><cell>TDE [28]</cell><cell cols="2">62.70</cell><cell>48.80</cell><cell cols="2">31.60 51.80</cell></row><row><cell>TDE  *</cell><cell cols="2">62.56</cell><cell>47.83</cell><cell cols="2">29.91 51.06</cell></row><row><cell>DiVE</cell><cell cols="2">64.06</cell><cell>50.41</cell><cell cols="2">31.46 53.10</cell></row><row><cell cols="6">Table 3. Results on the large-scale long-tailed iNaturalist2018</cell></row><row><cell cols="6">dataset. We present results when trained for 90 &amp; 200 epochs, ex-</cell></row><row><cell cols="6">cept for BBN [36] (which were trained 90 &amp; 180 epochs). BBN's</cell></row><row><cell cols="6">top-5 accuracy is from the author-released checkpoint. A " ?" sym-</cell></row><row><cell cols="5">bol denotes results copied directly from [1].</cell></row><row><cell>Methods</cell><cell></cell><cell cols="4">90 epochs top-1 top-5 top-1 top-5 200 epochs</cell></row><row><cell>CE</cell><cell></cell><cell cols="2">62.60 83.44</cell><cell>-</cell><cell>-</cell></row><row><cell>CB-Focal  ? [3]</cell><cell></cell><cell cols="2">61.12 81.03</cell><cell>-</cell><cell>-</cell></row><row><cell>BSCE</cell><cell></cell><cell cols="4">65.35 83.36 67.84 85.45</cell></row><row><cell cols="2">LDAM-DRW  ? [1]</cell><cell cols="2">68.00 85.18</cell><cell>-</cell><cell>-</cell></row><row><cell>BBN [36]</cell><cell></cell><cell cols="4">66.29 85.57 69.65 87.64</cell></row><row><cell cols="4">Meta-learning [16] 67.55 86.17</cell><cell>-</cell><cell>-</cell></row><row><cell>LWS [17]</cell><cell></cell><cell>65.90</cell><cell>-</cell><cell>69.50</cell><cell>-</cell></row><row><cell>cRT+SSP [33]</cell><cell></cell><cell>68.10</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DiVE</cell><cell></cell><cell cols="4">69.13 86.85 71.71 88.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Accuracy (%) on the three subsets of iNaturalist2018. The 90-epoch model was used in this table. BBN's results are from the author-released checkpoint.</figDesc><table><row><cell cols="2">Methods</cell><cell cols="4">Many Medium</cell><cell>Few</cell><cell>All</cell></row><row><cell>CE</cell><cell></cell><cell cols="2">73.08</cell><cell>63.74</cell><cell></cell><cell>58.41 62.60</cell></row><row><cell cols="2">BSCE</cell><cell cols="2">65.20</cell><cell>65.38</cell><cell></cell><cell>65.38 65.35</cell></row><row><cell cols="4">BBN [36] 49.49</cell><cell>70.87</cell><cell></cell><cell>65.31 66.43</cell></row><row><cell cols="4">LWS [17] 65.00</cell><cell>66.30</cell><cell></cell><cell>65.50 65.90</cell></row><row><cell cols="2">DiVE</cell><cell cols="2">70.63</cell><cell>70.01</cell><cell></cell><cell>67.58 69.13</cell></row><row><cell cols="7">Table 5. Accuracy (%) compared with RIDE [29] on three datasets.</cell></row><row><cell cols="7">ResNet-32, ResNeXt-50 and ResNet-50 are used as backbones,</cell></row><row><cell cols="7">respectively. "C100-LT" is short for CIFAR-100-LT (? = 100),</cell></row><row><cell cols="7">"IN-LT" for ImageNet-LT, and "iNat18" for iNaturalist2018. Ar-</cell></row><row><cell cols="7">rows indicate whether student's accuracy is higher or lower than</cell></row><row><cell cols="7">the teacher network. iNat18 is trained for 100 epochs.</cell></row><row><cell cols="2">Methods</cell><cell></cell><cell></cell><cell cols="3">C100-LT IN-LT</cell><cell>iNat18</cell></row><row><cell cols="2">RIDE teacher</cell><cell></cell><cell></cell><cell>50.20</cell><cell></cell><cell>57.50</cell><cell>72.80</cell></row><row><cell cols="2">RIDE [29]</cell><cell></cell><cell></cell><cell>49.10 ?</cell><cell></cell><cell>56.80 ? 72.60 ?</cell></row><row><cell cols="5">RIDE-DiVE teacher 51.07</cell><cell></cell><cell>55.60</cell><cell>68.79</cell></row><row><cell cols="2">RIDE-DiVE</cell><cell></cell><cell></cell><cell>51.66 ?</cell><cell></cell><cell>57.12 ? 73.44 ?</cell></row><row><cell cols="7">Table 6. Effects of balancing the virtual example distribution.</cell></row><row><cell cols="7">BSCEt/t ? ? power 100</cell><cell>50</cell><cell>10</cell></row><row><cell>CE</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">38.35 42.41 56.51</cell></row><row><cell># 1</cell><cell></cell><cell>t ?</cell><cell>3</cell><cell></cell><cell cols="2">45.35 51.13 62.00</cell></row><row><cell># 2</cell><cell cols="2">t</cell><cell>1</cell><cell></cell><cell cols="2">44.55 49.69 61.62</cell></row><row><cell># 3</cell><cell></cell><cell>t ?</cell><cell>3</cell><cell></cell><cell cols="2">44.50 50.20 61.28</cell></row><row><cell># 4</cell><cell></cell><cell>t ?</cell><cell>1</cell><cell></cell><cell cols="2">43.25 47.64 60.07</cell></row><row><cell># 5</cell><cell cols="2">t</cell><cell>1</cell><cell></cell><cell cols="2">41.59 47.10 59.10</cell></row><row><cell># 6</cell><cell cols="2">t</cell><cell>3</cell><cell></cell><cell cols="2">43.22 48.51 60.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>Properties of long-tailed datasets. For CIFAR-100-LT, we report results with different imbalance factors.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Classes Imbalance Factor</cell></row><row><cell>CIFAR-100-LT</cell><cell>100</cell><cell>10, 50, 100</cell></row><row><cell>ImageNet-LT</cell><cell>1,000</cell><cell>256</cell></row><row><cell>iNaturalist2018</cell><cell>8,142</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 .</head><label>8</label><figDesc>Top-1 accuracy over all classes on test time shifted ImageNet-LT. All models are trained for 90 epochs. 59.48 56.01 52.84 48.07 43.89 39.66 34.35 30.70 26.54 23.95 BSCE 59.46 58.51 56.64 54.94 52.50 50.48 48.24 5.29 43.18 40.89 39.31 DiVE 62.61 61.44 59.73 58.06 55.40 53.10 50.88 47.87 45.69 43.17 41.55 PC CE 61.91 59.80 56.60 54.39 51.39 49.33 47.71 46.20 45.57 45.03 45.41 PC BSCE 63.31 61.32 58.16 55.72 52.55 50.48 48.73 47.48 46.81 46.74 47.09 PC DiVE 65.82 63.56 60.70 58.38 55.17 53.10 51.39 49.97 49.42 49.15 49.29</figDesc><table><row><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell>Forward</cell><cell></cell><cell></cell><cell>Uniform</cell><cell></cell><cell></cell><cell>Backward</cell><cell></cell></row><row><cell cols="2">Imbalance factor</cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell></row><row><cell>CE</cell><cell cols="2">61.67 apple</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>bowl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>chair</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>dolphin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>lamp</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>mouse</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>plain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>rose</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>squirrel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The temperature ? = 1 is used inFig. 1and in these experiments. Setting ? = 3 leads to a more balanced virtual example distribution, and the accuracy is 61.58% for "FULL". More details about the temperature and distillation will be provided in Sec. 3.1.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SMOTE: Synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">C4.5, class imbalance, and cost sensitivity: Why under-sampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on learning from imbalanced datasets II</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep label distribution learning with label ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Wei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1734" to="1748" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. Intelligent Computing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edwardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Disentangling label distribution for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungju</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghee</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buru</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00321</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking classbalanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Long-tail learning via logit adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07314</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Background splitting: Finding rare classes in a sea of background</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fait</forename><surname>Ravi Teja Mullapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayvon</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8043" to="8052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving the Fisher Kernel for Large-Scale Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6314</biblScope>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Balanced meta-softmax for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11662" to="11671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Longtailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Long-tailed recognition by routing diverse distribution-aware experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01809</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst. 30</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking the value of labels for improving class-imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BBN: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inflated episodic memory with region self-attention for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4344" to="4353" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
