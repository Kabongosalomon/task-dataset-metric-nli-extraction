<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoxuan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In document-level event extraction (DEE) task, event arguments always scatter across sentences (across-sentence issue) and multiple events may lie in one document (multi-event issue). In this paper, we argue that the relation information of event arguments is of great significance for addressing the above two issues, and propose a new DEE framework which can model the relation dependencies, called Relation-augmented Document-level Event Extraction (ReDEE). More specifically, this framework features a novel and tailored transformer, named as Relation-augmented Attention Transformer (RAAT). RAAT is scalable to capture multi-scale and multi-amount argument relations. To further leverage relation information, we introduce a separate event relation prediction task and adopt multitask learning method to explicitly enhance event extraction performance. Extensive experiments demonstrate the effectiveness of the proposed method, which can achieve state-ofthe-art performance on two public datasets. Our code is available at https://github. com/TencentYoutuResearch/RAAT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event extraction (EE) task aims to detect the event from texts and then extracts corresponding arguments as different roles, so as to provide a structural information for massive downstream applications, such as recommendation <ref type="bibr" target="#b9">(Gao et al., 2016;</ref><ref type="bibr" target="#b16">Liu et al., 2017)</ref>, knowledge graph construction <ref type="bibr" target="#b25">(Wu et al., 2019;</ref><ref type="bibr" target="#b1">Bosselut et al., 2021)</ref> and intelligent question answering <ref type="bibr" target="#b2">(Boyd-Graber and B?rschinger, 2020;</ref>.</p><p>Most of the previous methods focus on sentencelevel event extraction (SEE) <ref type="bibr" target="#b0">(Ahn, 2006;</ref><ref type="bibr" target="#b15">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b13">Li et al., 2013;</ref><ref type="bibr" target="#b4">Chen et al., 2015;</ref><ref type="bibr" target="#b21">Nguyen et al., 2016;</ref><ref type="bibr" target="#b23">Sha et al., 2018;</ref><ref type="bibr" target="#b28">Yan et al., 2019;</ref><ref type="bibr" target="#b8">Du and Cardie, 2020</ref>; Li * These authors contributed equally to this work <ref type="bibr" target="#b22">Paolini et al., 2021;</ref><ref type="bibr" target="#b20">Lu et al., 2021)</ref>, extracting events from a single sentence. However, SEE is mostly inconsistent with actual situations. For example, event arguments may scatter across different sentences. As illustrated in <ref type="figure">Figure 1</ref>, the event argument [ORG1] of event role Pledger is mentioned in Sentence 4 and the corresponding argument [ORG2] of event role Pledgee is in Sentence 5 and 6. We call this across-sentence issue. Another situation involves the multi-event issue, which means that multiple events may exist in the same document. As seen in the example in <ref type="figure">Figure 1</ref>, where two event records coincide, we should recognize that they may partially share common arguments.</p><p>Recently, document-level event extraction (DEE) attracts great attention from both academic and industrial communities, and is regarded as a promising direction to tackle the above issues <ref type="bibr" target="#b34">Zheng et al., 2019;</ref><ref type="bibr" target="#b27">Xu et al., 2021b;</ref><ref type="bibr" target="#b30">Yang et al., 2021;</ref><ref type="bibr" target="#b35">Zhu et al., 2021)</ref>. However, by our observation, we discover that the relations between event arguments have patterns which are an important indicator to guide the event extraction. This information is neglected by existing DEE methods. Intuitively, the relation information could build long-range relationship knowledge of event roles among multiple sentences, which could relieve the across-sentence issue. For multi-event issue, shared arguments within one document could be distinguished to different roles based on the different prior relation knowledge. As illustrated in <ref type="figure">Figure 1</ref>, [ORG1] and [ORG2] have a prior relation pattern of Pledger and Pledgee, as well as <ref type="bibr">[ORG1]</ref> and <ref type="bibr">[SHARE1]</ref> for the relation pattern between Pledger and its Pledged Shares. Therefore, the relation information could increase the DEE accuracy if it is well modeled.</p><p>In this paper, we propose a novel DEE framework, called Relation-augmented Document-level Event Extraction (ReDEE), which is able to model <ref type="figure">Figure 1</ref>: An example document for the event type of Equity Pledge, including selected sentences that are involved in multiple event records and where the event arguments scatter across sentences. We can observe that the relations between these entity mentions have intuitive patterns that could be leveraged to enhance the event extraction task. More information of entity color and complete event-related relations can be found in Appendix A.2. the relation information between arguments by designing a tailored transformer structure. This structure can cover multi-scale and multi-amount relations and is general for different relation modeling situations. We name the structure as Relationaugmented Attention Transformer (RAAT). To fully leverage the relation information, we introduce a relation prediction task into the ReDEE framework and adopt multi-task learning method to optimize the event extraction task. We conduct extensive experiments on two public datasets. The results demonstrate the effectiveness of modeling the relation information, as well as our proposed framework and method.</p><p>In summary, our contributions are as follows:</p><p>? We propose a Relation-augmented Documentlevel Event Extraction (ReDEE) framework. It is the first time that relation information is implemented in the document-level event extraction field.</p><p>? We design a novel Relation-augmented Attention Transformer (RAAT). This network is general to cover multi-scale and multi-amount relations in DEE.</p><p>? We conduct extensive experiments and the results demonstrate that our method outper-form the baselines and achieve state-of-the-art performance by 1.6% and 2.8% F1 absolute increasing on two datasets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentence-level Event Extraction</head><p>Previously, most of the related works focus on sentence-level event extraction. For example, a neural pipeline model is proposed to identify triggers first and then extracts roles and arguments <ref type="bibr" target="#b4">(Chen et al., 2015)</ref>. Then a joint model is created to extract triggers and arguments simultaneously via multi-task learning <ref type="bibr" target="#b21">(Nguyen et al., 2016;</ref><ref type="bibr" target="#b23">Sha et al., 2018)</ref>. To utilize more knowledge, some studies propose to leverage document contexts , pre-trained language models <ref type="bibr" target="#b31">(Yang et al., 2019)</ref>, and explicit external knowledge <ref type="bibr" target="#b17">(Liu et al., 2019a;</ref><ref type="bibr" target="#b24">Tong et al., 2020)</ref>. However, these sentence-level models fail to extract multiple qualified events spanning across sentences, while document-level event extraction is a more common need in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document-level Event Extraction</head><p>Recently, DEE has attracted a great attention from both academic and industrial communities. At first, the event is identified from a central sentence and other arguments are extracted from neighboring sentences separately . Later, an innovative end-to-end model Doc2EDAG, is proposed <ref type="bibr" target="#b34">(Zheng et al., 2019)</ref>, which can generate event records via an entity-based directed acyclic graph to fulfill the document-level event extraction effectively. Based on Doc2EDAG, there are some variants appearing. For instance, GIT <ref type="bibr" target="#b27">(Xu et al., 2021b</ref>) designs a heterogeneous graph interaction network to capture global interaction information among different sentences and entity mentions. It also introduces a specific Tracker module to memorize the already extracted event arguments for assisting record generation during next iterations. DE-PPN <ref type="bibr" target="#b30">(Yang et al., 2021)</ref> is a multi-granularity model that can generate event records via limiting the number of record queries. Not long ago, a pruned complete graph-based non-autoregressive model PTPCG was proposed to speedup the record decoding and get competitive overall evaluation results <ref type="bibr" target="#b35">(Zhu et al., 2021)</ref>. In summary, although those existing works target for solving across-sentence and multi-event issues of the DEE task from various perspectives, to our best knowledge, we conduct a pioneer investigation on relation modeling towards this research field in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Trigger-aware Event Extraction</head><p>Previously a lot of works <ref type="bibr" target="#b10">((Ji and Grishman, 2008;</ref><ref type="bibr" target="#b15">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b13">Li et al., 2013;</ref><ref type="bibr" target="#b4">Chen et al., 2015;</ref><ref type="bibr" target="#b21">Nguyen et al., 2016;</ref>) deal with event extraction in two stages: firstly, trigger words are detected, which are usually nouns or verbs that clearly express event occurrences. And secondly, event arguments, the main attributes of events, are extracted by modeling relationships between triggers and themselves. In our work, we unify task as a whole to avoid error propagation between sub-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Firstly, we clarify several key concepts in event extraction tasks. 1) entity: a real world object, such as person, organization, location, etc.2) entity mention: a text span in document referring to an entity object. 3) event role: an attribute corresponding a pre-defined field in an event. 4) event argument: an entity playing a specific event role. 5) event record: a record expressing an event itself, including a series of event arguments. In document-level event extraction task, one doc-ument can contain multiple event records, and an event record may miss a small set of event arguments. Further more, a entity can have multiple event mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we introduce the proposed architecture first and then the key components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture Overview</head><p>End-to-end training methods for DEE usually involve a pipeline paradigm, including three subtasks: named entity recognition, event role prediction and event argument extraction. In this paper, we propose the Relation-augmented Documentlevel Event Extraction (ReDEE) framework coordinated with the paradigm. Our framework features leverage the relation dependency information in both encoding and decoding stages. Moreover, a relation prediction task is added into the framework to fully utilize the relation knowledge and enhance the event extraction task. More specifically, shown in <ref type="figure" target="#fig_0">Figure 2</ref>, there are four key components in our ReDEE framework: Entity Extraction and Representation(EER), Document Relation Extraction(DRE), Entity and Sentence Encoding(ESE), and Event Record Generation(ERG). In the following, we would introduce the detailed definition of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Extraction and Representation</head><p>We treat the component of entity extraction as a sequence labeling task. Given a document D with multiple sentences {s 1 , s 2 , ..., s i }, we use a native transformer encoder to represent the token sequence. Specifically, we use the BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> encoder pre-trained in Roberta setting <ref type="bibr" target="#b19">(Liu et al., 2019)</ref>. Then we use the Conditional Random Field(CRF) <ref type="bibr" target="#b11">(Lafferty et al., 2001)</ref> to classify token representations into labels of named entities. We adopt the classical BIOSE sequence labeling scheme. The labels are predicted by the following calculation:? ne = CRF (T rans(D)). Then all the intermediate embeddings of extracted entity mentions and sentences are concatenate into a matrix M ne+s ? R (j+i)?de by max-pooling operation on each sentence and entity mention span, where j and i are the numbers of entity mentions and sentences, and d e is the dimension of embeddings. The loss function for named entity recognition is denoted:</p><formula xml:id="formula_0">Lne = ? s i ?D logP (yi|si)<label>(1)</label></formula><p>where s i denotes the i th sequence sentence in document, and y i is the corresponding ground truth label sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Document Relation Extraction</head><p>The DRE component takes the document text (D) and entities ({e 1 , e 2 , ..., e j }) extracted in the previous step as inputs, and outputs the relation pairs among entities, in a form of</p><formula xml:id="formula_1">triples ({[e h 1 , e t 1 , r 1 ], [e h 2 , e t 2 , r 2 ], ..., [e h k , e t k , r k ]}). [e h</formula><p>k , e t k , r k ] means the head entity, the tail entity and the relationship of the k th triple respectively.</p><p>An important aspect is how to define and collect the relations from data. Here we assume that every two arguments in an event record can be connected by a relation. For example, Pledger and Pledgee in the EquityPledge event could have a relation named as Pledge2Pledgee, and the order of head and tail entities is determined by the pre-order of event arguments <ref type="bibr" target="#b34">(Zheng et al., 2019)</ref>. In this way, every event record with n arguments could create C 2 n relation samples. Note that this method to build relations is general to event extraction tasks from various domains, and the supervised relation information just comes from event record data itself, without any extra human labeling work. We do statistics for the relation types for ChiFinAnn dataset.  To predict the argument relations in this step, we adopt the structured self attention network <ref type="bibr" target="#b26">(Xu et al., 2021a)</ref> which is the latest method for document-level relation extraction. However, different from previous work using multi-class binary cross-entropy loss, we use normal cross-entropy loss to predict only one label for each entity pair. The relation type is inferred by this function:</p><formula xml:id="formula_2">yi,j = argmax(e T i Wrej)<label>(2)</label></formula><p>where e i , e j ? R d denote entity embedding from encoder module of DRE and d is the dimension of embeddings. W r ? R d?c?d denotes biaffine matrix trained by DRE task and c is the total number of relations. And the loss function for optimize the relation prediction task is denoted: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Entity and Sentence Encoding</head><p>Now we have embeddings of entity mentions and sentences from EER component and a list of predicted triple relations from DRE component. Then this component encodes data mentioned above and output embeddings effectively integrated with relation information. In this subsection, we would introduce the method that translates triple relations to calculable matrices and the novel RAAT structure for encoding all the above data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Entity and Sentence Dependency</head><p>First, we introduce a mechanism: entity and sentence dependency, which not only includes relation triples, but also describes links among sentences and entities beyond triples.</p><p>Co-relation and Co-reference are defined to represent entity-entity dependency. For the former one, two entities have a Co-relation dependency between them if they belong to a predicted relation triple. Entity pairs are considered having different Co-relation if their involved triples have different relations. Co-reference shows dependency between entity mentions pointing to same entities. That is, if an entity has several mentions existing across document, then each two of them has Coreference dependency. However, in the case that  We use Co-existence to describe dependency between entities and sentences where entity mentions come from. To be more specific, the entity mention together with its belonged sentence has Coexistence. For remaining entity-entity and entitysentence pairs without any dependency mentioned above, we uniformly treat them as NA dependency. <ref type="table" target="#tab_3">Table 2</ref> shows the complete dependency mechanism. Co-relation differs from NA, Co-reference, and Co-existence in that it has several sub-types, with number equaling to that of relation types defined in document relation extraction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">RAAT</head><p>In order to effectively encode entity and sentence dependencies, we design the RAAT which takes advantage of a calculable matrix representing dependencies and integrates it into attention computation. According to the architecture shown in <ref type="figure" target="#fig_1">Figure 3</ref>, RAAT is inherited from native transformer but has a distinct attention computation module which is made up of two parts: self-attention and relationaugmented attention computation.</p><p>Given a document shown as D = {s 1 , s 2 , ...s j }, all entity mentions in this document as E m = {e m 1 , e m 2 , ..., e m t }, where e m i denotes entity mentions with the superscript m denotes mention, and the subscript i denotes index, and a list of triples {[e h 1 , e t 1 , r 1 ], [e h 2 , e t 2 , r 2 ], ..., [e h k , e t k , r k ]}, we build a matrix T ? R c?(t+j)?(t+j) where c for the number of dependencies, and t and j for the number of sentences and entity mentions respectively. T is comprised of c matrices with same dimensions R ? R (t+j)?(t+j) , and each R represents one type of dependency r ? {Co ? relation k , Co ? ref erence, Co?existence, N A}, k = 1, 2, ...N , N as the number of relation types. For element within T , t k,i,j represent the dependency between node i and node j . Specifically, t k,i,j = 1 if they have the k th dependency, otherwise, t k,i,j = 0.</p><p>Here, node k ? {e m 1 , e m 2 , ..., e m t , s 1 , s 2 , ...s j } can be either entity mention or sentence. However, T would be giant and sparse if we use the above strategy. To squeeze T and decrease training parameters, we cluster Co-relation dependency based on the type of head entity in relation triple. For example, Pledger2Pledgee and Pledger2PledgedShares are clustered as one Co-relation dependency, and two matrice R a and R b corresponding to them are merged into one matrix. As a result, we finally get T ? R (3+H)?(t+j)?(t+j) where H denotes the number of head entity type in Co-relation, and 3 covers NA, Co-reference, and Co-existence. Let X ? R (t+j)?d as input embeddings of attention module, W rq , W rk , W q , W k , W v ? R d?d , M ? R (3+H)?d?d as weight matrices, we compute relation-augmented attention in the following steps:</p><formula xml:id="formula_3">Qr = XWrq, Kr = XW rk (4) Sa = 3+H i=1 QrM [i, :, :]K T r ? T [i, :, :] ? d + biasi (5)</formula><p>where S a denotes score matrix of relationaugmented attention, ? denotes element-wise multiplication. We compute self attention score and combine it with S a in the following way:</p><formula xml:id="formula_4">Q = XWq, K = XW k , Wv = XWv (6) S b = QK T ? d (7) O = (Sa + S b )V<label>(8)</label></formula><p>where O is the output of attention module. Similar to the structure of native transformer, RAAT has multiple identical blocks stacking up layer by layer. Furthermore, T is extensive since the number of Co-relation can be selected. RAAT can be adaptive to the change of input length, which is equivalent to the total number of sentences and entity mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Event Record Generation</head><p>With the outputs from previous component, the embeddings of entities and sentences, this ERG component actually includes two sub-modules: event type classifier and event record decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Event Type Classifier</head><p>Given the embeddings of sentences, we adopt several binary classifiers on every event type to predict whether the corresponding event is identified or not. If there is any classifier identifying an event type, the following event record decoder would be activated to iteratively generate every argument for the corresponding event type. The loss function to optimize this classifier is as the following:</p><formula xml:id="formula_5">L pred = ? i log(P (yi|S))<label>(9)</label></formula><p>where y i denotes the label of the i th event type, y i = 1 if there exists event record with event type i, otherwise, y i = 0. S denotes input embeddings of sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Event Record Decoder</head><p>To iteratively generate every argument for a specific event type, we refer to the entity-based directed acyclic graph (EDAG) method <ref type="bibr" target="#b34">(Zheng et al., 2019)</ref>. EDAG is a sequence of iterations with the length equaling to number of roles for certain event type. The objective of each iteration is to predict event argument of certain event role. Inputs of each iteration are come up with entities and sentences embeddings. And the predicted arguments of outputs will be a part of inputs for next iteration. However, different from EDAG, we substitute its vanilla transformer part with our proposed RAAT structure (i.e. RAAT-2 as shown in <ref type="figure" target="#fig_0">Figure 2</ref>). More specifically, the EDAG method uses a memory structure to record extracted arguments and adds role type representation to predict current-iteration arguments. However, this procedure hardly captures dependency between entities both in memory and argument candidates and sentences. In our method, RAAT structure can connect entities in memory and candidate arguments via relation triples extracted by the DRE component, and it can construct a structure to represent dependencies. In detail, before predicting event argument for current iteration, Matrix T is constructed in the way shown above so that dependency is integrated into attention computation. After extracting the argument, it is added into memory, meanwhile, a new T is generated to adapt next iteration prediction. Therefore, the RAAT can strengthen the relation signal for attention computation. The RAAT-2 has the same structure with RAAT-1 but independent parameters. The formal definition of loss function for event recorder decoder is:</p><formula xml:id="formula_6">La = ? v?V D e log(P (ye|(v, s)))<label>(10)</label></formula><p>where V D denotes node set in event records graph, v denotes extracted event arguments of event record by far, s denotes embedding of sentences and event argument candidates, and y e denotes label of argument candidate e in current step. y e = 1 means e is the ground truth argument corresponding to current step event role, otherwise, y e = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Model Training</head><p>To train the above four components, we leverage the multi-task learning method <ref type="bibr" target="#b6">(Collobert and Weston, 2008)</ref> and integrate the four corresponding loss functions together as the following: <ref type="bibr">11)</ref> where the ? i pre-set to balance the weight among the four components.</p><formula xml:id="formula_7">L = ?1Lne + ?2L dre + ?3L pred + ?4La<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we report the experimental results to prove the effectiveness of our proposed method. In summary, the experiments could answer the following three questions:</p><p>? To what degree does the ReDEE model outperform the baseline DEE methods?</p><p>? How well does ReDEE overcome acrosssentence and multi-event issues?</p><p>? In what level does the each key component of ReDEE contribute to the final performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>DEE is a relatively new task and there are only a few datasets published. In our experiments we adopt two public Chinese datasets, i.e. ChiFinAnn <ref type="bibr" target="#b34">(Zheng et al., 2019)</ref> and DuEE-fin <ref type="bibr" target="#b14">(Li, 2021)</ref>. ChiFinAnn includes 32,040 documents with 5 types of events, involving in equity-related activities for the financial domain. Statistics show that about 30% of the documents contain multiple event records. We randomly split the dataset into train/dev/test sets in the ratio of 8/1/1. Readers can refer to the original paper for details.</p><p>DuEE-fin is also from the financial domain with around 11,900 documents in total. The dataset is downloaded from an online competition website * . Since there is no ground truth publicly available for the test set, we can only submit our extracted results to the website as a black-box online evaluation. Compared to ChiFinAnn, there are two * https://aistudio.baidu.com/aistudio/competition/detail/46 differences. The DuEE-fin dataset has 13 different event types and its test set includes a large size of document samples that do not have any event records, which both make it more complicated. We get the distribution information of the dataset from Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines and Metrics</head><p>Five different baseline models are taken into consideration: 1) DCFEE , the first model proposed to solve DEE task. 2) Doc2EDAG <ref type="bibr" target="#b34">(Zheng et al., 2019)</ref>, proposed an end-to-end model which transforms DEE as directly filling event tables with entity-based path expending. 3) DE-PPN <ref type="bibr" target="#b30">(Yang et al., 2021</ref>), a pipeline model firstly introducing the non-autoregressive mechanism. 4) GIT <ref type="bibr" target="#b27">(Xu et al., 2021b)</ref>, a model using heterogeneous graph interaction network as encoder and maintaining a global tracker during the decoding process. 5) PTPCG <ref type="bibr" target="#b35">(Zhu et al., 2021)</ref>, a light-weighted and latest DEE model.</p><p>For evaluation metrics, we use precision, recall, and F1 score at the entity argument level for fair comparison with baselines. The overall "Avg" in the result tables denotes the micro average value of precision, recall, and F1 score. We conduct several offline evaluations for ChiFinAnn, but only an online test for DuEE-fin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Settings</head><p>In our implementation, for text processing, we consistently set the maximum sentence number and the maximum sentence length as 128 and 64 separately. We use BERT encoder in the EER component for fine-tuning and Roberta-chinese-wwm <ref type="bibr" target="#b32">(Yiming et al., 2020)</ref> as the pre-trained model. Both RAAT-1 and RAAT-2 have four layers of identical blocks. More training details can be found in Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results and Analysis</head><p>Overall Performance <ref type="table">Table 3</ref> shows the comparison between baselines and our ReDEE model on the ChiFinAnn dataset. The ReDEE can achieve the state-of-the-art performance in terms of micro average recall and F1 scores on almost every type of events (i.e. EF, ER, EU, EO, EP), consistent with the Avg. results increased by 1.5% and 1.6% respectively. Our model also performs competitively well on precision results.  <ref type="table">Table 3</ref>: Comparison of event extraction between baselines and our ReDEE model on the ChiFinAnn dataset. The missing parts are caused by the inaccessibility of baseline codes. ?: results from <ref type="bibr" target="#b27">(Xu et al., 2021b)</ref>; ?: results from <ref type="bibr" target="#b30">(Yang et al., 2021)</ref>; ?: results from <ref type="bibr" target="#b35">(Zhu et al., 2021)</ref>.   <ref type="table">Table 5</ref>: F1 scores on four sets growing with average number of sentences involved in event records. ?: results from <ref type="bibr" target="#b27">(Xu et al., 2021b</ref>).</p><p>DuEE-fin and its online testing. Seeing from former results, our model outperforms in a great leap by increasing 6.7% on F1 score. For the online testing evaluation, our model has a distinct growth of 2.8% on F1 score than the baselines. This experiment demonstrates our model could achieve a superior performance than existing methods.  <ref type="table" target="#tab_9">Table 6</ref> shows the comparison results of all baselines and ReDEE. We find ReDEE performs much better in the multi-event scenario and outperforms baseline models dramatically in all five event types, improving ranging from 1.9% to 3.2% F1 scores. The results suggest that our relation modeling method is more effective to overcome the multi-event issue than existing baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Study</head><p>To probe the impact of RAAT structure for different components in ReDEE, we conduct ablation studies on whether to use RAAT or vanilla transformer. In this experiment, we implement tests on three variants: 1) -RAAT-1 substitutes the RAAT in the ESE component with vanilla transformer. 2) -RAAT-2 substitutes the RAAT in the event record generation module with vanilla transformer. 3) -RAAT-1&amp;2 substitutes the RAATs in both the above places with vanilla transformers, so that our model degrades to only import a relation extraction task via multi-task learning.</p><p>The results in <ref type="table" target="#tab_10">Table 7</ref> indicate that both two RAATs have positive influence on our model. Especially on ChiFinAnn, RAAT-2 makes more con-DE-PPN? 82.1 63.5 89.1 70.5 79.7 66.7 80.6 69.6 88.0 73.2 -   tribution than RAAT-1, with a decrease of 0.7% versus 0.4% in F1 scores once been substituted. After replacing both two RAATs, the value of relation extraction task becomes more weak and the model encounters a 1.5% drop in F1 score. When it comes to DuEE-fin, a similar phenomenon can be observed that both the RAATs can contribute positively to our model.</p><formula xml:id="formula_8">- - PTPCG? - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we investigate a challenging task of event extraction at document level, towards the across-sentence and multi-event issues. We propose to model the relation information between event arguments and design a novel framework ReDEE. This framework features a new RAAT structure which can incorporate the relation knowledge. The extensive experimental results can demonstrate the effectiveness of our proposed method which makes the state-of-the-art performance on two benchmark datasets. In the future, we will make more efforts to accelerate training and inference process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>In the appendix, we incorporate the following details that are omitted in the main body due to the space limit.</p><p>A.1 Distribution of Event Type DuEE-fin <ref type="table" target="#tab_11">Table 8</ref> shows the complete event type and corresponding distribution of DuEE-fin dataset. Overall, there are 13 event types in total with uneven distribution. Only train and development sets are shown since test set is not publicly available.</p><p>A.2 Complete Relation Triples <ref type="table" target="#tab_12">Table 9</ref> demonstrates the complete of relation triples of the document event extraction example shown in <ref type="figure">Figure 1</ref>. Entities in blue are involved in both two event records, while those in green and orange are exclusive to record 1 and 2 respectively. Heavy coupling of arguments among events increases the difficulty of multi-event issue. <ref type="table" target="#tab_0">Table 10</ref> shows the relation statistics of ChiFinAnn dataset. There are 85 relation types in total, and train, development, and test sets have similar pattern in distribution. <ref type="figure" target="#fig_2">Figure 4</ref> shows the prediction results of our model and the best baseline model GIT on the example in <ref type="figure">Figure 1</ref>. Compared with the ground truth, our model correctly predicts all event arguments except one, while GIT only captures one event, with an argument missed. This example explicitly shows the superiority of our model in dealing with multievents issue. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Relation Statistics for ChiFinAnn</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 More Training Settings</head><p>For all native transformers and RAATs, the dimensions of hidden layers and feed-forward layers are set to 768 and 1,024 respectively. During training, we set the learning rate lr = 5e ?5 , batch size b = 64. The four loss weights are set to ? 1 = ? 3 = 0.05, ?2 = 1.0, ? 4 = 0.95. We use 8 V100 GPUs and set gradient accumulation steps to 8. The train epoch are set to 100, and the best epoch are selected by the best validation score on development set for the evaluation of test set. And we use Adam to optimize the whole learning task.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overall of our proposed ReDEE framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>RAAT structure. Firstly each relation between entities and sentences are represented as matrices. Then the matrices are clustered by the head entities. At last the clustered matrices are integrated into the transformer structure for attention calculation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Case study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>shows a snippet of statistics and the full edition can be found in Appendix A.3.</figDesc><table><row><cell>Relation Type</cell><cell cols="3">#Train #Dev #Test</cell></row><row><cell>Pledger2PledgedShares</cell><cell>20002</cell><cell>2567</cell><cell>2299</cell></row><row><cell>Pledger2Pledgee</cell><cell>20002</cell><cell>2567</cell><cell>2299</cell></row><row><cell>PledgedShares2Pledgee</cell><cell>20002</cell><cell>2567</cell><cell>2299</cell></row><row><cell>Start2EndDate</cell><cell>19615</cell><cell>2239</cell><cell>1877</cell></row><row><cell cols="2">Pledger2TotalHoldingShares 18552</cell><cell>2412</cell><cell>2173</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The example relations with top 5 quantities in the ChiFinAnn dataset. The complete statistic can refer to the Appendix A.3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>All types of dependency among sentences and entities head and tail entities in relation triple are the same (i.e. StartDate and EndDate share same entities in some event records), then Co-relation and Coreference are both held between them.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>shows the comparison results of our model with baselines on the developing set of 37.8 46.7 84.5 86.0 80.0 60.8 39.0 47.5 46.9 46.5 46.7 64.2 49.8 56.1 67.7 54.4 60.3 DCFEE-M ? 44.6 40.9 42.7 75.2 71.5 73.3 51.4 41.4 45.8 42.8 46.7 44.6 55.3 52.4 53.8 58.1 55.2 56.6 Greedy-Dec ? 78.5 45.6 57.7 83.9 75.3 79.4 69.0 40.7 51.2 64.8 40.6 50.0 82.1 40.4 54.2 80.4 49.1 61.0 Doc2EDAG ? 78.7 64.7 71.0 90.0 86.8 88.4 80.4 61.6 69.8 77.2 70.1 73.5 76.7 73.0 74.8 80.3 75.0 77.5 GIT ? 78.9 68.5 73.4 92.3 89.2 90.8 83.9 66.6 74.3 80.7 72.3 76.3 78.6 76.9 77.7 82.3 78.</figDesc><table><row><cell>Model</cell><cell></cell><cell>EF</cell><cell></cell><cell></cell><cell>ER</cell><cell></cell><cell></cell><cell>EU</cell><cell></cell><cell></cell><cell>EO</cell><cell></cell><cell></cell><cell>EP</cell><cell></cell><cell></cell><cell>Avg</cell></row><row><cell></cell><cell>P.</cell><cell>R.</cell><cell>F1.</cell><cell>P.</cell><cell>R.</cell><cell>F1.</cell><cell>P.</cell><cell>R.</cell><cell>F1.</cell><cell>P.</cell><cell>R.</cell><cell>F1.</cell><cell>P.</cell><cell>R.</cell><cell>F1.</cell><cell>P.</cell><cell>R.</cell><cell>F1.</cell></row><row><cell>DCFEE-S ?</cell><cell cols="18">61.1 4 80.3</cell></row><row><cell>DE-PPN?</cell><cell cols="15">78.2 69.4 73.5 89.3 85.6 87.4 69.7 79.9 74.4 81.0 71.3 75.8 83.8 73.7 78.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PTPCG?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">88.2 69.1 79.4</cell></row><row><cell cols="19">ReDEE(ours) 78.0 70.6 74.1 91.1 90.3 90.7 82.5 69.2 75.3 83.7 73.1 78.1 81.7 78.6 80.1 84.0 79.9 81.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of event extraction between baselines and our ReDEE model on the DuEE-fin dataset. ?: results from<ref type="bibr" target="#b35">(Zhu et al., 2021)</ref>.</figDesc><table><row><cell>Model</cell><cell>I</cell><cell>II</cell><cell>III</cell><cell>IV</cell></row><row><cell>DCFEE-S ?</cell><cell cols="4">64.6 70.0 57.7 52.3</cell></row><row><cell>DCFEE-M ?</cell><cell cols="4">54.8 54.1 51.5 47.1</cell></row><row><cell cols="5">Greedy-DEC ? 67.4 68.0 60.8 50.2</cell></row><row><cell>Doc2EDAG ?</cell><cell cols="4">79.6 82.4 78.4 72.0</cell></row><row><cell>GIT ?</cell><cell cols="4">81.9 85.7 80.0 75.7</cell></row><row><cell>ReDEE(ours)</cell><cell cols="4">83.9 85.8 81.7 77.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Single v.s. Multi Events To illustrate how well our model performs in the multi-event aspect, we split the test set of ChiFinAnn into two parts: one for documents with single event record, and the other for documents including multiple events.</figDesc><table><row><cell>average number of involved sentences while the</cell></row><row><cell>IV set has the largest ones. According to table 5,</cell></row><row><cell>our model outperforms other baseline models in</cell></row><row><cell>all settings, and meets the largest growth of 2.2%</cell></row><row><cell>F1 score in IV, the most challenging set of all. It</cell></row><row><cell>indicates that our model is capable of capturing</cell></row><row><cell>longer dependency of records across sentences via</cell></row><row><cell>relation dependency modeling, thus alleviating the</cell></row><row><cell>argument scattering challenge.</cell></row><row><cell>Argument Scattering The across-sentence issue</cell></row><row><cell>widely exists in datasets. By our statistics, the train-</cell></row><row><cell>ing sets of ChiFinAnn and DuEE-fin have about</cell></row><row><cell>98.0% and 98.9% records that scatter across sen-</cell></row><row><cell>tences respectively. To evaluate the performance of</cell></row><row><cell>our model in different argument scattering degree,</cell></row><row><cell>we compute the average number of sentences in-</cell></row><row><cell>volved in records for each document and sort them</cell></row><row><cell>in the increasing average number order. Then, all</cell></row><row><cell>documents for testing are evenly divided into four</cell></row><row><cell>sets, namely, I, II, III and IV, which means the I</cell></row><row><cell>set is a cluster of documents that have the smallest</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison of event extraction between singular (S.) and multiple (M.) event documents on the ChiFi-nAnn. ?: results from<ref type="bibr" target="#b27">(Xu et al., 2021b)</ref>; ?: results from<ref type="bibr" target="#b30">(Yang et al., 2021)</ref>; ?: results from<ref type="bibr" target="#b35">(Zhu et al., 2021)</ref>.</figDesc><table><row><cell>Model</cell><cell></cell><cell>ChiFinAnn</cell><cell></cell><cell></cell><cell>DuEE-fin</cell></row><row><cell></cell><cell>P.</cell><cell>R.</cell><cell>F1.</cell><cell>P.</cell><cell>R.</cell><cell>F1.</cell></row><row><cell>ReDEE</cell><cell cols="6">84.0 79.9 81.9 69.2 57.4 62.8</cell></row><row><cell>-RAAT-1</cell><cell cols="6">+0.4 -1.1 -0.4 +1.5 -1.7 -0.5</cell></row><row><cell>-RAAT-2</cell><cell cols="6">+1.3 -2.4 -0.7 +0.8 -3.2 -1.7</cell></row><row><cell cols="4">-RAAT-1&amp;2 -3.1 -0.1 -1.5</cell><cell cols="3">-1.3 -5.1 -3.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Ablation studies on ReDEE variants for RAAT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Distribution of Duee-fin dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Complete relation triplets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Relation statistics of ChiFinAnn dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">L dre = ? y i,j ?Y logP (yi,j|D)(3)where y i,j denotes ground truth label between the i th and j th entity, D for document text and Y for set of all relation pairs among entities.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their careful reading of our paper and their many insightful comments and suggestions. This work was supported by Tencent Cloud and Tencent Youtu Lab.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event Type</head><p>#Train. #Dev <ref type="table">.  ShareRedemption  1309  243  FinanceDeficit  1062  163  Pledge  1027  160  EnterpriseAcquisition  934  142  BidWin  915  134  ExecutiveChange  901  134  ShareholderHoldingDecrease  876  147  PledgeRelease  728  118  CorporateFinace  535  72  CompanyListing  482  82  ShareholderHoldingIncrease  321  62  CompanyBankruptcy  236  44  Admonition  172  32  Total  9498  1533</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The stages of event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>David Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Annotating and Reasoning about Time and Events</title>
		<meeting>the Workshop on Annotating and Reasoning about Time and Events</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What question answering can learn from trivia nerds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>B?rschinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7422" to="7435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformer: Decomposing pre-trained transformers for faster question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4487" to="4497" />
		</imprint>
	</monogr>
	<note>Aruna Balasubramanian, and Niranjan Balasubramanian</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multipooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collective event detection via a hierarchical and bias tagging networks with gated multi-level attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1267" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Event extraction by answering (almost) natural questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="671" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collaborative social group influence for event recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Event extraction as multi-turn question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="829" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Duee-fin: a document-level event extraction dataset in the financial domain released by baidu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<ptr target="https://aistudio.baidu.com/aistudio/competition/detail/46" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using document level cross-event inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cpmf: A collective pairwise matrix factorization model for upcoming event recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploiting the ground-truth: An adversarial imitation based knowledge distillation approach for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6754" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Jointly multiple events extraction via attentionbased graph information aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhunchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1247" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1907.11692.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Text2event: Controllable sequence-tostructure generation for end-to-end event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2795" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint event extraction via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Thien Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="829" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Jointly extracting event triggers and arguments by dependency-bridge rnn and tensor-based argument interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving event detection via open-domain trigger knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meihan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5887" to="5897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic knowledge graph construction: A report on the 2019 icdm/icbk contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14149" to="14157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Document-level event extraction via heterogeneous graph-based interaction model with a tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3533" to="3546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Event detection with multi-order graph convolution and aggregated attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1267" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dcfee: A document-level chinese financial event extraction system based on automatically labeled training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="50" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Document-level event extraction via parallel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6298" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring pretrained language models for event extraction and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5284" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Revisiting pretrained models for chinese natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Yiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Wanxiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Shijin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Guoping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP: Findings</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="657" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Document embedding enhanced event detection with hierarchical and supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="414" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Doc2edag: An end-to-end document-level framework for chinese financial event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient document-level event extraction via pseudo-trigger-aware pruned complete graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhefeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxing</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2112.06013.pdf" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
