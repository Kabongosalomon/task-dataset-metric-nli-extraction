<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FogAdapt: Self-Supervised Domain Adaptation for Semantic Segmentation of Foggy Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javed</forename><surname>Iqbal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Information Technology University</orgName>
								<address>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rehan</forename><surname>Hafiz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Information Technology University</orgName>
								<address>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ali</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Information Technology University</orgName>
								<address>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FogAdapt: Self-Supervised Domain Adaptation for Semantic Segmentation of Foggy Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Foggy Scene Understanding</term>
					<term>Domain Adaptation</term>
					<term>Semantic Segmentation</term>
					<term>Self-supervised Learning</term>
					<term>Scale-invariance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents FogAdapt, a novel approach for domain adaptation of semantic segmentation for dense foggy scenes. Although significant research has been directed to reduce the domain shift in semantic segmentation, adaptation to scenes with adverse weather conditions remains an open question. Large variations in the visibility of the scene due to weather conditions, such as fog, smog, and haze, exacerbate the domain shift, thus making unsupervised adaptation in such scenarios challenging. We propose a self-entropy and multi-scale information augmented self-supervised domain adaptation method (FogAdapt) to minimize the domain shift in foggy scenes segmentation. Supported by the empirical evidence that an increase in fog density results in high self-entropy for segmentation probabilities, we introduce a self-entropy based loss function to guide the adaptation method. Furthermore, inferences obtained at different image scales are combined and weighted by the uncertainty to generate scale-invariant pseudo-labels for the target domain. These scale-invariant pseudo-labels are robust to visibility and scale variations. We evaluate the proposed model on real clear-weather scenes to real foggy scenes adaptation and synthetic non-foggy images to real foggy scenes adaptation scenarios. Our experiments demonstrate that FogAdapt significantly outperforms the current state-of-the-art in semantic segmentation of foggy images. Specifically, by considering the standard settings compared to state-of-the-art (SOTA) methods, FogAdapt gains 3.8% on Foggy Zurich, 6.0% on Foggy Driving-dense, and 3.6% on Foggy Driving in mIoU when adapted from Cityscapes to Foggy Zurich.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is one of the important components of autonomous systems, i.e., self-driving cars <ref type="bibr" target="#b0">[1]</ref>. Deep Learning approaches for semantic segmentation, relying on the large tagged datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, have resulted in substantially improved performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> in the last few years. However, similar to many other supervised learning problems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, semantic segmentation models exhibit large generalization errors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. This behavior is ascribed to the domain shift between the distribution of the test and training data domains <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. One challenging case of the domain adaptation can be attributed to weather conditions such as rain, fog, snowfall, lightning, and strong wind <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref>. Fog specifically degrades the visibility and contrast significantly <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>, deteriorating the performance of the computer vision applications, e.g., segmentation. Domain adaptation algorithms have been presented to overcome the domain shift (synthetic to real <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">19]</ref> or real to real <ref type="bibr" target="#b20">[20]</ref> datasets) specific to the case of semantic segmentation. However, very little attention has been devoted to address domain shift caused by foggy weather conditions <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>In this work, we present a novel self-supervised domain adaptation method, FogAdapt, for semantic segmentation of images captured in dense foggy weather. In foggy conditions, the image contrast and color quality drop significantly degrading the clarity and visibility of the scene. This occurs due to the presence of particles in the atmosphere which scatter and absorb light <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b16">16]</ref>. Since these particles might be nonuniformly present in different parts of the scene, fog could eventually be of different densities at different locations. Similarly, depending upon the distance between the camera and the objects, fog affects the visibility of objects differently. Specifically, the visibility is decreased with increasing distance, e.g., the farthest objects are more difficult to recognize. The source trained segmentation models and even adapted for non-foggy scenarios of a similar scene <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b19">19]</ref> fails to mitigate the dense fog, and ends up with deteriorated performance. An illustration is shown in <ref type="figure">Fig. 1</ref>, where keeping everything else constant, as the fog density is varied from clear to dense, the corresponding semantic segmentation deteriorates accordingly, with the farthest regions most affected. Combination of these variations results in a considerably large set of scenarios, making collection and labeling costly and laborious, especially for the semantic segmentation and robust supervised learning techniques. Hence, a robust unsupervised domain adaptation (UDA) method for such a challenging scenario of dense fog in Input Images Seg. Output</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Maps</head><p>Original Image 600m visibility 300m visibility 150m visibility <ref type="figure">Figure 1</ref>: Image contrast and color quality degradation due to fog and resultant deterioration in the segmentation performance. Row-1: images with decreasing visibility. Row-2: corresponding segmentation outputs. Row-3: error maps, where the grey and black regions show correct and erroneous segmentation respectively, while the white regions are untagged. The segmentation performance deteriorates with increasing fog density. The results are generated using <ref type="bibr" target="#b19">[19]</ref>, an adaptation approach for GTA to (non-foggy) Cityscapes.</p><p>the target images is needed.</p><p>To counter the challenges posed by domain shift in dense foggy scenes, we present a novel self-supervised domain adaptation method (FogAdapt) for UDA of foggy scenes segmentation. Our domain (fog) specific empirical analysis led us to discover relationships between the effect of fog and roadscene segmentation. We exploit the relationship between uncertainty, measured by self-entropy, and the density of fog <ref type="figure">(Fig. 2)</ref> by defining a self-entropy minimization loss for the target images, when a source (clear weather images) trained semantic segmentation model is utilized for the target (foggy image) dataset. Similarly, we explore the image scale and fog relationship ( <ref type="figure">Fig. 3</ref>) and generate pseudo-labels at pixel level by exploiting the consistency constraint over image scaling to counter the effect of fog. Below we discuss in detail the empirical analysis and the proposed solution specifically designed to counter the effects of fog with further details in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self Entropy loss &amp; Fog Density:</head><p>Images taken in fog exhibit degradation of color quality, low contrast, and other artifacts associated with low visibility. Overall this results in images with texture, edges, and color information deteriorated enough to make it difficult to differentiate between different objects and stuff classes. This loss of information results in a confused semantic segmentation network (trained in normal weather), making it unable to differentiate between different category pixels, and resulting in high self-entropy. This relationship between the self-entropy and the density of the fog has been presented in <ref type="figure">Fig. 2</ref>. To observe how the self-entropy changes with respect to the fog-density, we use Foggy-Cityscapes, a simulated fog added real imagery dataset <ref type="bibr" target="#b3">[4]</ref>, where the same images and their foggy versions at multiple visibility ranges are available. We manually choose and extract small patches (100 ? 150 pixels) from the same locations of the normal images and their modi-fied versions with simulated dense and moderate fog added to it. Self-entropy maps are computed for each patch after passing these images through semantic segmentation network, trained on GTA non-foggy images. Few of them are shown in <ref type="figure">Fig. 2</ref>. The three distributions obtained for all the extracted patches at respective fog levels ( <ref type="figure">Fig. 2 (a)</ref>), visualized in <ref type="figure">Fig. 2 (b)</ref>, indicate a strong relationship between fog density and self-entropy, i.e., the denser the fog, the higher the self-entropy. This lead us to our hypothesis that minimizing the self-entropy may force the network to learn to compensate for the information loss occurring due to the fog.</p><p>Scale Invariance &amp; Fog Density : Previously, LSE <ref type="bibr" target="#b10">[11]</ref> introduced scale-invariant examples in the target dataset to minimize the inconsistency between normal and larger scales. More specifically, they observed that in clear weather conditions, images at normal scale are segmented well instead of larger scale and hence they generated pseudo-labels at normal scale. However, in dense foggy scenes, this hypothesis is not completely true. In foggy scenes, resizing results in different segmentation accuracy at different locations of the input image depending upon the density of fog and how far or near the object is from the camera. This is especially true for the road scenes. Due to fog, the objects that are far from the camera (and hence smaller in scale) have lower visibility, (Eq.1) making it further challenging to segment it correctly. Since we are employing a self-supervised training approach, pseudo-labels for the small and far away objects disguised in fog will thus not be available as they will have low segmentation scores. Therefore, we propose a scale-invariant pseudo-labels generation process for foggy scenes adaptation by exploiting the relationship between scale, fog, and self-entropy ( <ref type="figure">Fig. 3</ref>). We make a reasonable assumption that pseudo-labels should be scale-invariant. Using the same source trained model, the input target image is seg-Medium Self-Entropy Low Self-Entropy Segmentation Network High Self-Entropy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Fog Without Fog</head><p>Moderate Fog (a) (b) <ref type="figure">Figure 2</ref>: Relationship between fog density and uncertainty measured by self-entropy in segmentation probabilities. (a) Self-entropy maps of semantic segmentation computed over same images as fog changes from none to dense. The denser the fog, the higher the self-entropy. (b) shows the self-entropy distributions for dense, moderate, and without fog image patches. The mean self-entropy increases with increasing fog density. (The visualizations are generated using a GTA dataset trained model, and for better visualization image-patches are shown instead of full images.).</p><p>mented at multiple image scales (higher and lower spatial resolution than original) independently and the output probability volume is aggregated. Segmenting at large scale extrapolates the local context and hence produces better segmentation and low entropy for faraway dense foggy regions compared to normal scale. Similarly, segmenting at small scale benefits large and near to camera objects disguised by fog as shown in <ref type="figure">Fig.  3</ref>. The combined effect of these three scales produces better pseudo-labels compared to single normal scale as shown later in <ref type="table">Table.</ref> 6.</p><p>To summarize, this work produces the following contributions. 1. A self-supervised domain adaptation strategy for foggy scenes segmentation with pixel-level pseudo-labels to adapt the output space. 2. Exploiting relationship between the image scale and fogdensity to design a strategy for generating scale invariant pixel-wise pseudo-labels. 3. Based on empirical evidence, we define a relation between fog density and self-entropy, i.e., self-entropy minimization loss to mitigate the effects of dense fog in segmentation model and produce confident segmentation output. 4. We show state-of-the-art (SOTA) performances on benchmark datasets by augmenting the scale invariance and self-entropy with spatial distribution priors of the source dataset. The reminder of the paper is arranged as follows: Section 2 describes related work. Section 3 details the proposed approach and Section 4 presents the experiments and results. In Section 5 we summarize our work for the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Domain adaptation approaches have been presented to overcome the domain shift specific to the case of semantic seg-mentation <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">19]</ref>. However, very little attention has been devoted to address domain shift caused by foggy weather conditions <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">22]</ref>. Below, we provide a succinct review of schemes related to generic domain adaptation for Semantic Segmentation followed by the schemes specific to Domain Adaptation for Foggy Scenes Segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Domain Adaptation for Semantic Segmentation</head><p>Adversarial learning based UDA of semantic segmentation is the most explored approach in literature <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref>. In UDA, adversarial loss-based training is leveraged for input space adaptation (re-weighting) <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>, feature matching <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34]</ref>, structured output matching <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">26]</ref> or combination of these strategies <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b25">25]</ref>. However, due to the global nature of adversarial learning even if the objective is to match the output probabilities or the high dimensional feature representation at latent space, the adversarial domain adaptation alone produces sub-optimal results <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b31">31]</ref>.</p><p>Besides adversarial learning, self-supervised domain adaptation is gaining attention for many computer vision applications <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b6">7]</ref>. The authors in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">36]</ref> presented a class balanced pseudo-labels generation and confidence regularized self-training with class spatial priors. The authors in MLSL <ref type="bibr" target="#b19">[19]</ref> leveraged spatial invariance to generate consistent pseudolabels at pixels and image level for UDA of semantic segmentation in clear-weather scenes. LSE <ref type="bibr" target="#b10">[11]</ref> tried to generate scaleinvariant examples and minimized the loss between pseudolabels generated at normal scale and its zoomed version. Compared to proposed FogAdapt, the LSE <ref type="bibr" target="#b10">[11]</ref> and MLSL <ref type="bibr" target="#b19">[19]</ref> do not exploit multi-scale information during pseudo-label generation, hence producing inferior performance when exposed to foggy scenes. Zhang et al. <ref type="bibr" target="#b37">[37]</ref> proposed a curriculum domain adaptation by defining land-mark super-pixels classification based loss at the output while addressing the easy examples Far away from Camera Objects <ref type="figure">Figure 3</ref>: If an object is far or near the camera, resizing the foggy input image has a different effect on the self-entropy (SE) map. Segmenting foggy scenes at a higher scale provides extra local context, i.e., minimizes the effect of fog by producing better segmentation with comparatively sharp edges. Contrary to that, segmenting images at a lower scale produce better outputs for large and near to camera objects disguised by fog.</p><p>These visualizations are generated using source domain (GTA) trained segmentation model.</p><p>first. PyCDA <ref type="bibr" target="#b12">[13]</ref> combined <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b37">[37]</ref> in a single framework to generate pseudo-labels at multiple sized windows. The authors in <ref type="bibr" target="#b9">[10]</ref> used a direct entropy minimization (only high entropy pixels) approach along with the adversarial learning applied on the self-entropy maps of the semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Domain Adaptation for Foggy Scenes Segmentation 2.2.1. Image Defogging/Dehazing</head><p>Color quality and contrast of the outdoor scenes are degraded due to fog/haze. There have been many classical <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref> and deep learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b45">45]</ref> based methods trying to improve color quality or contrast enhancement with an attempt to defog or dehaze. However, as the fog density increases, the defogging models' performance is degraded significantly. Therefore, the attempt to use them as pre-processing step before feeding the data to computer vision models created/trained in normal light settings does not provide desired performance enhancement <ref type="bibr" target="#b46">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Foggy Scenes Segmentation and Adaptation</head><p>Besides the great progress for generic semantic segmentation and domain adaptation, very little attention is being devoted to handle foggy scenes. This is mainly due to the unavailability of annotated datasets for foggy scene segmentation. The authors in <ref type="bibr" target="#b3">[4]</ref> leveraged the stereo property of Cityscapes images to estimate the depth and proposed a fog simulation method for real imagery. They tried to add synthetic fog to real Cityscapes images at multiple fog density levels defined by Eq. 2 to generate synthetic fog added to real images with multiple visibility ranges ( <ref type="figure">Fig. 1)</ref>. Alongside, they also developed a small annotated dataset having real foggy scenes; Foggy Driving. Further, they fine-tuned the normal Cityscapes trained model on Foggy-Cityscapes images to address foggy scenes in real imagery. Similarly, the authors in <ref type="bibr" target="#b47">[47]</ref> tried to address the real foggy scenes segmentation problem with the help of purely synthetic foggy data. They fine-tuned the normal cityscapes trained models on the synthetic foggy images. Sakaridis et al. <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref> proposed a curriculum adaptation learning approach for real foggy scenes understanding. They developed a large dataset, Foggy Zurich, by capturing road driving scenes under real foggy scenarios. The dataset is unlabeled except a small chunk of 40 images that have dense annotations available. They adapted to Foggy Zurich alongside the fully labeled Foggy-Cityscapes images. They generated pseudo-labels for target images using <ref type="bibr" target="#b3">[4]</ref> and defined a fog estimator for curriculum learning. However, they did not investigate the relationship between the fog density and induced uncertainty for pseudo-labels generation and model adaptation.</p><p>In summary, the existing solutions for foggy scenes adaptation have multiple shortcomings. The generic adaptation methods fail to perform in foggy conditions due to lack of domain knowledge. The fog-specific approaches proposed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref> do not specifically investigate the effect of fog density and respective induced uncertainty. Besides, we introduce a self-supervised domain adaptation approach for foggy scenes by exploiting the relationships between fog density and uncertainty and scale invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we present the details of the proposed approach for self-supervised domain adaptation of semantic segmentation model for dense foggy scenes. We start with an introduction to the optical model for fog, basic architecture for semantic segmentation <ref type="bibr" target="#b23">[23]</ref> and self-training method for domain adaptation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">19]</ref>. Next, we present the proposed FogAdapt algorithm including the loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Optical Model for Fog</head><p>In general, the image fogging/hazing process is often represented as the physical corruption model given by Eq. 1 <ref type="bibr" target="#b3">[4]</ref> </p><formula xml:id="formula_0">I d (r, c) = J(r, c) t(r, c) + A(1 ? t(r, c)),<label>(1)</label></formula><p>where I d is degraded image, t is the transmittance map, J is the fog-free radiance of the original image and A represents the global atmospheric lightning ((r, c) in Eq. 1 shows the pixel locations). The transmittance map t is dependent on the distance l(r, c) of the observer from the object having a homogeneous medium and is given by Eq. 2,</p><formula xml:id="formula_1">t(r, c) = exp(?? l(r, c)).<label>(2)</label></formula><p>The parameter ? is used to control the density of fog as leveraged by <ref type="bibr" target="#b3">[4]</ref>. Compared to daylight imagery with clear weather conditions, the foggy scenes are more challenging. As highlighted earlier, extensive research has been done on image defogging/dehazing, while less attention is being paid to foggy scenes segmentation and adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-Supervised Domain Adaptation: Preliminaries</head><formula xml:id="formula_2">Let X s ? R H s ?W s ?3 and X t ? R H t ?W t ?3</formula><p>be source domain (clear weather) and target domain (foggy) RGB images with spatial resolution H s ? W s and H t ? W t , respectively. The true segmentation labels for source domain images are denoted by y s ? R H s ?W s ?C (each pixel location is one-hot encoded) while the ground truth labels for target images are not available. C is total number of classes. Let F be the fully convolutional semantic segmentation model, with trainable parameters ?. For a given source image x s ? X s , let the output segmentation probability volume be denoted by P x s . For source domain images, the segmentation model is trained using the cross entropy loss defined in Eq. 3,</p><formula xml:id="formula_3">L(x s , y s ) = ? hs?Hs ws?Ws c?C y s (h s , w s , c) log(P xs (h s , w s , c)) (3)</formula><p>where y s ? Y s shows the corresponding ground-truth segmentation labels. Since the true labels for target domain images are not present, we use pseudo-labels generated by the source domain trained model for fine-tuning (adaptation). The corresponding cross entropy loss for the target images is defined in Eq. 4,</p><formula xml:id="formula_4">L(x t ,? t ) = ? ht?Ht wt?Wt c?C ?(h t , w t )? t (h t , w t , c) log(P xt (h t , w t , c)) (4)</formula><p>whereL(x t ,? t ) is self-supervised training loss for target domain images with pseudo-labels? t . The ? (H t ,W t ) is a binary map used to compute and backpropagate loss for only those pixels which are assigned pseudo-labels and ignore otherwise. More specifically, for any pixel location, ?(h t , w t ) = 1 if that pixel is assigned a pseudo-label and ?(h t , w t ) = 0 otherwise.</p><p>The pseudo-labels generation and training processes for Fo-gAdapt are shown in <ref type="figure" target="#fig_0">Fig. 4</ref> and detailed in Sec. 3.3. During target adaptation, the segmentation network is jointly trained using the generated pseudo-labels of the target images and the ground truth labels of source images. The corresponding joint loss function for self-supervised domain adaptation (SSDA) is given by</p><formula xml:id="formula_5">min ? L S S DA (x s , y s , x t ,? t ) = L(x s , y s ) +L(x t ,? t )<label>(5)</label></formula><p>The L S S DA in Eq. 5 is minimized following a sequential scheme, i.e., fix the segmentation model weight ? to generate pseudo-labels? t for target samples x t , and then use these pseudo-labels to minimize Eq. 5 with respect to ?. These steps are repeated for multiple iterations called rounds. The pseudo-labels generation exploiting scale invariance and other constraints are discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scale Invariant Pseudo-Labels</head><p>To adapt the target domain effectively, accurate pseudolabels are required. However, in dense foggy scenes, it is very difficult to generate accurate and consistent pseudo-labels. As described in <ref type="figure">Fig. 3</ref> and explained in Section 1, under dense foggy conditions, image regions behave differently at multiple scales. Hence, combining multi-scale output information intelligently ( <ref type="figure" target="#fig_0">Fig. 4(a)</ref>) is more effective compared to any single scale <ref type="table" target="#tab_7">(Table. 6</ref>). Therefore, we present scale-invariant pseudolabels created by weighted summation of the probability and uncertainty maps across different image scales.</p><p>As discussed in Section 1 and shown in <ref type="figure">Fig. 1</ref>, in foggy scenes the visibility of the object is correlated with density of fog present, and the distance between an object and the sensor. This combined effect deteriorates the performance of the semantic segmentation model. Pixels of the faraway and near to camera objects camouflaged in fog presents very limited information. More specifically, the model trained on clear weather images skip out small and washed-out content with respect to available large neighboring content-generating merged or inferior segmentation output. Similarly, due to the limited receptive field of the segmentation model compared to large and near to camera objects, i.e., bus, truck etc., the performance is effected negatively. The segmentation model might be relying on texture or structure information to make the decision. However, that information is corrupted by the presence of fog, resulting in the decreased confidence for these parts and hence fewer pseudolabels for it.</p><p>To minimize the effect of dense fog on far away objects and to overcome the problems associated with the fixed receptive field being unable to provide the complete coverage to larger and near to camera objects, we generate scale-invariant pseudolabels. Instead of generating pseudo-labels at the original (normal) scale that objects exist in an image, we process target images at multiple scales (spatial resolutions/zoom levels) to generate pseudo-labels, as shown <ref type="figure" target="#fig_0">Fig. 4(a)</ref>. To assure semantic consistency and scale-invariance, we assume that objects and stuff should be segmented the same irrespective of the scale they are presented. We evaluate the target image at three scales, e.g., scale parameters into three separate images and segmented independently. We combine these probability maps based on the confidence of the segmentation model. Specifically, corresponding to each scale, we generate normalized weight maps (w (1?s l ) , w and w (1+s u ) ) based on the self entropy H (H t ,W t ) (Eq. 8) of the segmentation probabilities corresponding to each scale. This process for w (1?s l ) is shown in Eq. 6.</p><formula xml:id="formula_6">w (1?s l ) = 1 ? H (Ht,Wt) (P x (1?s l ) ) j?S (1 ? H (Ht,Wt) (P x j ))<label>(6)</label></formula><p>where j ? S represents the image scale. Similar equations can be written for w and w (1+s u ) . Using these weight maps, we obtain a weighted summation after resizing to normal scale to form P x c ? R H t ?W t ?C .</p><formula xml:id="formula_7">P xc = j?S w j . P x j<label>(7)</label></formula><p>where w j with ( j ? S ) are the self-entropy based weight maps for respective scales and the (.) operator shows element-wise multiplication. The higher the self-entropy, the lower the contribution in P x c and vice versa. The P x c is used to select pseudolabel based on the confidence score. This process of scaleinvariant pseudo-label generation is shown in <ref type="figure" target="#fig_0">Fig. 4(a)</ref>. Hence, the generated pseudo-labels are scale-invariant and quantitatively better compared to single inference <ref type="table" target="#tab_7">(Table. 6</ref>).</p><p>To select pixels with high confidence as pseudo-labels and avoid class distribution imbalance problem, we adapt a class balancing and selecting criteria similar to one used in <ref type="bibr" target="#b36">[36]</ref>. Similarly, following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">19]</ref>, we use the spatial priors (SP) of the source domain labels characterized by the occurrences of an object in a specific location across the whole dataset. For each target image, we scale the output probabilities with the spatial-prior mask of each class, and then select the per-pixel high (maximum) probability values from the probability map P x c . These high-probability class values over the whole target set are sorted in descending order of confidence and the pixels with high probabilities are selected as pseudo-labels based on the pre-defined selection portion s p . Initially, s p = 15% of the total pixels belonging to any category and is incremented by 5% in each round. The resultant pseudo-labels are class-balanced, consistent, and scale-invariant representative of the whole target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Self-Entropy Minimization for Foggy Scenes Adaptation</head><p>The density of fog has a direct relation with the information contained in an image, e.g., the denser the fog, the lesser the information. This is evident from the semantic segmentation results, where dense foggy regions in an image have high selfentropy values <ref type="figure">(Fig. 2)</ref>. With increasing fog density, the segmentation model generates under-confident per-pixel predictions making the entropies high. We leverage this relationship between fog density and self-entropy and define a self-entropy minimization loss (9) alongside cross-entropy loss <ref type="bibr" target="#b4">(5)</ref>. The underlying idea for self-entropy minimization is to shift the mean self-entropy of dense foggy scenes towards clear images selfentropy mean, as shown in <ref type="figure">Fig. 2 (b)</ref>. The self-entropy H for a target image x t is given by Eq. 8,</p><formula xml:id="formula_8">H (Ht,Wt) (P xt ) = ? 1 log(C) c?C P (Ht,Wt,c) xt log(P (Ht,Wt,c) xt )<label>(8)</label></formula><p>where H (H t ,W t ) ? [0, 1] is the per-pixel standard entropy defined in <ref type="bibr" target="#b48">[48]</ref>. The loss function based on H for a target image x t is given by Eq. 9, </p><formula xml:id="formula_9">L se (P xt ) = 1 H t ? W t</formula><p>During adaptation, we jointly optimize the pseudo-labels based supervised lossL and the unsupervised self-entropy lossL se for an input target image x t . There is a strong resemblance in Eq. 4 and Eq. 9, where the former enforces the segmentation model to assign the correct class to an underlined pixel, while the later tries to maximize individuals confidence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Appearance Adaptation</head><p>As described in Section 2, many algorithms tried to exploit the self-training process by labeling the most confident predictions as pseudo-labels. However, it is very important for a pseudo-label to be accurate, consistent, and invariant. To generate such confident pseudo-labels the visual appearance of the target and source domain images also play a vital role. For example, a model trained on normal imagery fails to generate accurate pseudo-labels on dense foggy images. Hence, an appearance adaptation step is required to help self-supervised learning paradigms to generate consistent pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Image Translation Module</head><p>In this work, we leveraged the cycle-consistent adversarial learning algorithm (CycleGAN) <ref type="bibr" target="#b30">[30]</ref> to transform the source domain images to the visual appearance of the unlabeled target domain images. This process is named as Image Translation Module (ITM). The transformed images are nearly similar in visual appearance with target domain images and are used in the domain adaptation process. The loss function for the employed CycleGAN is given by Eq.10,</p><formula xml:id="formula_11">L c?Gan (x s , x t , G t , G s , D t , D s ) = L GAN (G t , D t , x s , x t ) +L GAN (G s , D s , x t , x s ) + L cyc (x s , G s (G t (x s ))) +L sc (x s , G t (x s )),<label>(10)</label></formula><p>where G s and G t represent the generator from target to source and source to target domain respectively. D t is the discriminator applied to classify between original target domain images and translated target domain images. D s expedites the same loss for the target to source transformation. Similarly, L cyc and L sc losses are applied to maintain the cycle and semantic consistency respectively. The optimization program can be defined as a min-max criterion given in Eq. 11,</p><formula xml:id="formula_12">min G t ,G s max D t ,D s L c?Gan (x s , x t , G t , G s , D t , D s ).<label>(11)</label></formula><p>The transformed source images with available ground truth, when used in domain adaptation helps to select better pseudolabels and eventually improves the adaptation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Combined Objective Function</head><p>The composite loss function for self-supervision based UDA of foggy scene segmentation is the composition of both Eq. 5 and Eq. 9, and is given by,</p><formula xml:id="formula_13">L cmp (x s , y s , x t ,? t , c) = L S S DA (x s , y s , x t ,? t ) +L se (P xt )<label>(12)</label></formula><p>Similarly, the combined loss function for ITM augmented with L cmp is the combination of Eq. 10 and Eq. 12 and is given by Eq. 13,</p><formula xml:id="formula_14">L IT M?cmp = L c?Gan (x s , x t , F t , F s , D t , D s ) + L cmp (x s , y s , x t ,? t , c). (13)</formula><p>To summarize the proposed approach, we train CycleGAN using Eq. 11 to translate source images to look like target images. Next, we generate scale-invariant consistent pseudo labels and adapt the baseline model in an iterative manner to minimize the loss function L cmp defined in Eq. 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>This section discusses experimental details and provides the results of our comparison with state-of-the-art techniques. We list down different configurations and their acronyms in <ref type="table">Table.</ref> 1 for better readability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We have performed multiple experiments with various datasets, weather conditions having varying fog densities and settings. The key points are discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Datasets</head><p>We adapt the standard real-to-real and synthetic-to-real setup for UDA of foggy scenes segmentation. We use Cityscapes <ref type="bibr" target="#b1">[2]</ref>, SYNTHIA <ref type="bibr" target="#b2">[3]</ref> and, GTA <ref type="bibr" target="#b49">[49]</ref> datasets as source domain and Foggy Driving <ref type="bibr" target="#b3">[4]</ref>, Foggy-Cityscapes <ref type="bibr" target="#b3">[4]</ref> and foggy zurich <ref type="bibr" target="#b22">[22]</ref> as real-world target domain datasets. Sample images from source and target domain datasets are shown in <ref type="figure">Figure.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>The SYNTHIA-RAND-CITYSCAPES, a sub-set from the SYNTHIA dataset consists of 9400 synthetic frames of spatial resolution 760 ? 1280. The baseline models and the adapted models are both evaluated with 16 and 13 categories in common between SYNTHIA and Foggy-Cityscapes as described in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b8">[9]</ref> for normal Cityscapes. Similarly, we use the GTA dataset having 24966 frames with a high spatial resolution of 1052 ? 1914. Pixel-level labels for classes compatible with Foggy-Cityscapes are available for all 24966 frames. The Cityscapes dataset consists of 3475 high resolution (1024 ? 2048) images with pixel-level annotations, where 2975 images are listed as the training set and the remaining 500 as validation set. Foggy-Cityscapes has the same images as cityscapes with fog being added synthetically by <ref type="bibr" target="#b21">[21]</ref>. The Foggy Driving (FD) dataset contains 101 images where 33 images have fine annotations and the remaining have coarse labels available. Foggy Driving Dense FDD is a subset of the FD dataset having 21 images with very dense fog. Similarly, the Foggy Zurich dataset contains 3808 high-resolution images of real foggy scenes. However, only a limited set of 40 images is labeled for semantic segmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Model Architecture</head><p>For semantic segmentation of foggy scenes, we use ResNet-38 <ref type="bibr" target="#b23">[23]</ref> as baseline. ResNet-38 is trained for segmentation of Cityscapes, SYNTHIA, and GTA using the ImageNet pretrained parameters <ref type="bibr" target="#b50">[50]</ref>. The architecture of ResNet-38 for segmentation in this work is the same as defined in <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b8">9]</ref>. The Image Translation Module (ITM) is adapted from <ref type="bibr" target="#b30">[30]</ref>. The ITM is employed to translate source images to the visual appearance of the target domain datasets, e.g., from GTA to Foggy-Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Implementation and Training Details</head><p>To perform the experiments, a core-i5 machine with a single GTX-1080Ti having 11GB of memory is used while MxNet <ref type="bibr" target="#b51">[51]</ref> is used as deep learning framework. SGD optimizer with an initial learning rate of 1 ? 10 ?4 for the segmentation model and 2 ? 10 ?4 for ITM respectively are used for training. The scale parameters s u and s l are both set to 0.25 (Sec. 4.3.5). Similarly, s p is initially set to 15% of the total pixels belonging to any category for pseudo-labels selection and is incremented by 5% in each round (Sec. 3.3). Due to GPU memory limitations, we process two images per mini-batch. The proposed (FogAdapt) iterative process of self-supervised domain adaptation is continued for 4 rounds where each round consists of 2 epochs of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>In this section, we show and discuss the experimental results of FogAdapt compared to ResNet-38 (baseline) and current state-of-the-art (SOTA) UDA approaches for foggy scenes. Our experiments are two fold: a) In the first setup, we use normal Cityscapes (clear-weather images) as source domain and Foggy Zurich and Foggy Driving (real-foggy imagery) as target domain datasets and (b) in the second setup, we use synthetic datasets, e.g., SYNTHIA and GTA as source domain and Foggy-Cityscapes (synthetic fog added to real images) dataset as target domain. The performance is reported using a standard evaluation metric for segmentation, i.e., Mean Intersection over Union (mIoU). The proposed FogAdapt performs superior compared to other domain adaptation approaches with SOTA performance on multiple benchmark datasets varying from synthetic to real for dense foggy conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Real Non-foggy to Real-Foggy Scenes Adaptation</head><p>Following <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>, we adapt the normal (clear-weather) Cityscapes dataset trained source model to the real foggy imagery dataset, Foggy Zurich. We also evaluate the source only and the Foggy Zurich (FZ) adapted models over Foggy Driving (FD) and Foggy Driving-dense (FDD) test sets (FD and FDD are small sets used for testing only.)</p><p>Cityscapes ? Foggy Zurich: <ref type="table">Table.</ref> 2 summarizes the quantitative results of the proposed approach compared to current SOTA methods. The proposed FogAdapt+ outperforms the ResNet-38 (baseline) and existing approaches with a high margin. Compared to the ResNet-38 <ref type="bibr" target="#b23">[23]</ref> baseline, we gain 16.0% in mIoU over all classes. We also evaluate FogAdapt over frequent classes in the FZ dataset, e.g., road, sidewalk, building, wall, fence, pole, traffic-light, traffic-sign, vegetation, sky, and car defined by <ref type="bibr" target="#b22">[22]</ref>. The FogAdapt+ attains a gain of 22.4% in mIoU over ResNet-38 baseline. Similarly, compared to Curriculum-Ada <ref type="bibr" target="#b22">[22]</ref> and Model-Ada <ref type="bibr" target="#b21">[21]</ref>, the proposed FogAdapt+ outperforms them with significant margins of about 3.0% and 7.0% over all classes respectively. The addition of spatial priors further improves the results as shown by SP-FogAdapt+ in <ref type="table">Table.</ref> 2. We exploit the concept of spatial priors (SP) as presented by CBST <ref type="bibr" target="#b8">[9]</ref> during pseudo-labels generation. The road scene imagery has a defined spatial structure and the spatial priors are used only if the geometry of the source and target images match. To have a fair qualitative comparison, we selected the same images presented in <ref type="bibr" target="#b22">[22]</ref> as shown in <ref type="figure">Fig. 6</ref>. The proposed FogAdapt shows a significant performance improvement over baselines and existing SOTA methods in most of the categories.</p><p>Evaluation on Foggy Driving: Since the FD dataset is small having 33 images with fine (every pixel is assigned a label) annotations and the remaining 68 images with coarse (polygon <ref type="table">Table 2</ref> Semantic segmentation performance of FogAdapt and its variants compared to SOTA methods on Foggy Zurich (FZ), Foggy Driving-dense (FDD) and Foggy Driving (FD) test sets when adapted from Cityscapes to FZ. We present mIoU for all classes compatible with Cityscapes <ref type="bibr" target="#b1">[2]</ref>, and frequent classes defined for FZ, FDD, and FD respectively. FogAdapt+: FogAdapt+CycleGAN, SP-FogAdapt+: FogAdapt+ combined with spatial priors defined in <ref type="bibr" target="#b8">[9]</ref>. The bold text shows highest whereas the underlined show the second-highest scores. annotations with no clear object boundaries) annotations, this dataset is used only for evaluation as suggested by <ref type="bibr" target="#b3">[4]</ref>. We evaluate our baseline and FZ adapted models over FD and its' subset FDD. The quantitative results for all 19 classes compatible with FZ dataset are shown in <ref type="table">Table.</ref> 2. The proposed FogAdapt+ performs superior compared to baselines and existing SOTA methods. Especially, in case of dense foggy scenes, FDD, our FogAdapt+ achieves a gain of 4.1% and 9.8% in mIoU compared to Curriculum-Ada <ref type="bibr" target="#b22">[22]</ref> and Model-Ada <ref type="bibr" target="#b21">[21]</ref>, respectively. Similar to the FZ dataset, we evaluate the FD dataset for frequent classes, e.g., road, sidewalk, building, pole, trafficlight, traffic-sign, vegetation, sky, person, and car as defined by <ref type="bibr" target="#b3">[4]</ref>. The proposed FogAdapt+ performs significantly better in mIoU, i.e., a minimum gain of 5.1% compared to strong MLSL <ref type="bibr" target="#b19">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Synthetic to Real-Foggy Scenes Adaptation</head><p>To comprehensively test the proposed approach, we also perform synthetic to real foggy domain adaptation experiments. Specifically, we use synthetic datasets, e.g., SYNTHIA and GTA as source domain datasets and Foggy-Cityscapes as target domain. The Foggy-Cityscapes dataset has the real Cityscapes images with synthetic fog added as proposed by <ref type="bibr" target="#b22">[22]</ref>. Foggy-Cityscapes has three levels of fog, e.g., low fog with 600-m visibility, medium fog with 300-m visibility, and dense fog with 150m visibility <ref type="figure">(Fig. 1</ref>). In this work, we have adapted our models to dense fog scenarios of the Foggy-Cityscapes dataset.</p><p>GTA ? Foggy-Cityscapes: As indicated in <ref type="table">Table.</ref> 3, the proposed FogAdapt+ for self-supervised domain adaptation shows SOTA performance. More specifically, the FogAdapt+ improves the segmentation performance on dense Foggy-Cityscapes by 11.6%, 7.3% and 5.9% compared to ResNet-38 baseline and previous SOTA methods, i.e., CBST <ref type="bibr" target="#b8">[9]</ref>, and MLSL <ref type="bibr" target="#b19">[19]</ref>, respectively. Similarly, compared to LSE <ref type="bibr" target="#b10">[11]</ref> and PyCDA <ref type="bibr" target="#b12">[13]</ref>, the proposed approach outperform with a minimum margin of 5.0%. <ref type="figure">Fig. 7</ref> shows semantic segmentation results before and after adaptation. The proposed approach significantly improves the segmentation performance of dense foggy scenes compared to the source-only model and previous SOTA methods.</p><p>SYNTHIA ? Foggy-Cityscapes: Compared to GTA and FoggyZurich, the SYNTHIA dataset has extra constraints like multiple viewpoint imagery making it a more difficult adaptation task. <ref type="table" target="#tab_8">Table 4</ref> presents the quantitative results of the proposed approach for 16 overlapping classes between SYNTHIA and Foggy-Cityscapes. Following <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b52">52]</ref>, we also show the 13 frequent classes results (mIoU*). Compared to baseline ResNet-38, the proposed FogAdapt+ shows a gain of 18.9% and 20.8% in mIoU and mIoU*, respectively. Similarly, compared to bidirectional learning <ref type="bibr" target="#b26">[26]</ref> (output space adversarial learning only), CBST <ref type="bibr" target="#b8">[9]</ref> and MLSL <ref type="bibr" target="#b19">[19]</ref>, FogAdapt+ shows a minimum gain of 3.9% and 4.4% in mIoU and mIoU*, respectively. A qualitative comparison of FogAdapt with existing SOTA methods is presented in <ref type="figure">Fig. 8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>This section investigates the effect of each part of the proposed approach and discusses it in relation to the results obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Self-Entropy Minimization</head><p>As discussed in Sec. 1, the higher the fog density, the more uncertain the segmentation model becomes about assigning a class to a specific pixel <ref type="figure">(Fig. 1</ref>). This decrease in information eventually increases the self-entropy of segmentation probabilities <ref type="figure">(Fig. 2)</ref>. We leverage this relation between entropy and fog density and add an entropy minimization constraint (Eq. 9) to the total loss function. Adding this constraint increases the mIoU performance compared to simple pseudo-labels based Target Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>ResNet-38 <ref type="bibr" target="#b23">[23]</ref> Model-Ada <ref type="bibr" target="#b21">[21]</ref> Curriculum <ref type="bibr" target="#b22">[22]</ref> Ours(FogAdapt) <ref type="figure">Figure 6</ref>: Segmentation results on Foggy Zurich test set when adapted from Cityscapes. For a fair comparison, we select the images shown by <ref type="bibr" target="#b22">[22]</ref>. The proposed FogAdapt performs better in most of the classes ranging from road to vegetation, train, sky, wall, and buildings. self-supervised domain adaptation (SSDA) by 3.3% for GTA to Foggy-Cityscapes as shown in <ref type="table">Table.</ref> 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Scale-Invariance</head><p>As the effect of fog increases with the distance between the object and the observer, the scale of an object has a major role ( <ref type="figure">Fig. 3)</ref> in properly segmenting the object. Increasing the object's size by resizing the image to a larger scale makes the object under fog clearer as it increases the local contextual information <ref type="figure">(Fig. 3</ref>). This helps in generating comparatively better pseudo labels <ref type="table" target="#tab_7">(Table. 6</ref>). On the other hand, resizing image to a smaller size allows the segmentation algorithm to properly segment near to camera objects disguised by fog which were erroneous previously due to limited receptive field, and capture a more global view. As described in Sec. 3.3 , combining these higher and lower scales results in the generation of robust and consistent pseudo-labels <ref type="table" target="#tab_7">(Table. 6</ref>), which eventually increases the performance over foggy scenes. Experimental results show a gain of 3.2% in mIoU compared to single scale for GTA to Foggy-Cityscapes adaptation <ref type="table" target="#tab_6">(Table. 5</ref>). Similarly, for normal Cityscapes to FZ dataset adaptation, the SI performs superior when combined with SE for FZ, FDD, and FD compared to all previous approaches as shown in <ref type="table">Table.</ref> 2 (FogAdapt+)). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Image</head><p>Gound Truth ResNet-38 <ref type="bibr" target="#b23">[23]</ref> CBST <ref type="bibr" target="#b8">[9]</ref> MLSL <ref type="bibr" target="#b19">[19]</ref> Ours(FogAdapt) <ref type="figure">Figure 7</ref>: Semantic segmentation qualitative results on the Foggy-Cityscapes validation set when adapted from the GTA dataset trained model. The FogAdapt performs better compared to existing methods. Specifically, the small, thin and far away objects disguised in fog and the stuff classes like road, sidewalk, buildings and sky are segmented better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Effect of Input Space Adaptation</head><p>With ResNet-38 <ref type="bibr" target="#b23">[23]</ref> as baseline model, we investigate the effect of image translation at input space. We train CycleGAN <ref type="bibr" target="#b30">[30]</ref> to translate source images (non-foggy) to the visual appearance (foggy) of the target domain images; GTA/SYNTHIA to Foggy-Cityscapes and Cityscapes to Foggy Zurich, respectively. This process generates foggy imagery for the source datasets helping FogAdapt to generate better pseudolabels. Adding this image translation module to FogAdapt: Fo-gAdapt+, significantly improves the segmentation performance for foggy scenes. For GTA to Foggy-Cityscapes, FogAdapt+ gains 1.8% in mIoU as shown in <ref type="table" target="#tab_5">Table 3</ref>. Similarly, for normal Cityscapes to real foggy datasets adaptation the FogAdapt+ gain 1.0%, 0.5% and 0.4% in mIoU for FZ, FDD and FD over the FogAdapt respectively <ref type="table">(Table.</ref> 2). Thus, the input space adaptation has an impact on the pseudo-label generation and adaptation process.</p><p>The proposed FogAdapt is a complementary method and any image translation approach can be used with it. A better image translation model shall lead to better adaptation performance. To validate this, we use CUT <ref type="bibr" target="#b53">[53]</ref> a more recent approach compared to CycleGAN <ref type="bibr" target="#b30">[30]</ref> to translate source images to visual appearance of the target domain. This image translation approach improves the mIoU of the SP-FogAdapt+ to 46.0% compared to 45.0% previously for GTA to Foggy Cityscapes adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Effect of Dehazing/Defogging</head><p>To investigate the effect of the defogging process, we conduct experiments with defogging as input space adaptation. We define an image defogging module (IDM) based on the method proposed by <ref type="bibr" target="#b13">[14]</ref>. FogAdapt is applied on these defogged images for domain adaptation. However, it is observed that defogging methods perform inferior in the presence of dense fog. The same observation was previously reported by <ref type="bibr" target="#b22">[22]</ref>. Compared to defogging as input space adaptation, the source images translation to the target domain appearance performs well (Table. 7). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5.">Effect of Scaling Parameters s u , s l</head><p>We investigate the effect of image scaling and its effect on the quality of pseudo-labels. <ref type="table">Table 8</ref> shows the mIoU of generated pseudo-labels for different upper and lower scaling factors s u , s l . It is observed that, that very large and very small image scales reduces the pseudo-labels quality significantly. The selected values (s u , s l = 0.25) of scaling factors produces better pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gound Truth</head><p>ResNet-38 <ref type="bibr" target="#b23">[23]</ref> CBST <ref type="bibr" target="#b8">[9]</ref> MLSL <ref type="bibr" target="#b19">[19]</ref> Ours(FogAdapt) <ref type="figure">Figure 8</ref>: Qualitative results of semantic segmentation on Foggy-Cityscapes validation set when adapted from SYNTHIA dataset trained model. The proposed FogAdapt performs better compared to <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b19">[19]</ref>. <ref type="table">Table 8</ref> Effect of lower and upper scale parameters (s l , s u ) on pseudo-labels.</p><p>GTA ? Foggy-Cityscapes s l , s u (0.5 0.5) (0.5, 0.25) (0.25, 0.25) (0.25, 0.5) mIoU 67.8 69.0 71.5 67.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a self-supervised domain adaptation strategy with self-entropy and scale-invariance constraints for UDA of foggy scene semantic segmentation. We empirically establish a relationship between the fog density and self-entropy of the source model's prediction over the foggy images. We exploit this relationship to define a self-entropy minimization objective function to adapt on images where color quality and contrast has been degraded due to fog. Having a fair assumption that under foggy conditions labels of stuff and objects should be the same regardless of their scale, we generate scale-invariant pixel level pseudo-labels. Scale-invariance helps us to counter the phenomena that in foggy weather, objects farther away are less visible and hence suffer from more information loss. The scale invariant pseudo-label generation and the self-entropy minimization for self-supervised domain adaptation allows the segmentation model to learn domain independent features to mitigate the effect of fog density. Rigorous experiments demonstrate that the proposed self-supervised domain adaptation method augmented with image translation module (ITM) outperforms the existing SOTA algorithms on benchmark datasets: mIoU improves from 46.8 to 50.6 on Foggy Zurich, 43.0 to 49.0 on Foggy Driving-dense and 49.8 to 53.4 Foggy Driving when adapted from Cityscapes to Foggy Zurich. Effectiveness of self-entropy minimization, scale invariant pseudo-labels, and ITM is highlighted by the considerable improvement of mIoU over the baseline model and SOTA methods. Being complementary in nature, when used with state of the art image translation model our results improve to 46.0. for GTA to Foggy-Cityscapes adaptation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>S = {1 + s u , 1, 1 ? s l }, where s u and s l are upper and lower scale parameters and are set to s u = s l = 0.25 empirically (Sec. 4.3.5). The target image is resized according to the three L The proposed FogAdapt framework. (a) Scale-invariant pseudo-labels generation process where, 1) a target image is resized at multiple scales. 2) the resized versions of the image are segmented independently, 3) uncertainty (self-entropy) based weight maps for each image scale are defined and the outputs are weighted respectively 4) the weighted outputs are then resized to the original scale and recombined, and 5) most confident pixels are assigned pseudo-labels. (b) shows the semantic segmentation adaptation using the generated pseudo-labels for target domain images in (a) and the true labels of source domain images simultaneously. x s , y s , x t , and? t are source image (after translation), source image labels, target image and target image pseudo-labels, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Sample images from source and target domains. There is a significant difference between the source and target domain images for both the real to real and synthetic to real domain adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Different configurations and their acronyms.</figDesc><table><row><cell>Acronyms</cell><cell>Configuration</cell></row><row><cell>FogAdapt</cell><cell>Self-entropy loss + Scale-invariance based pseudo-labels</cell></row><row><cell>FogAdapt+</cell><cell>ITM (Image Translation Module) + FogAdapt</cell></row><row><cell>SP-FogAdapt+</cell><cell>Spatial-Priors during pseudo-labels generation + FogAdapt+</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Segmentation results of adapting GTA to Foggy-Cityscapes. The abbreviations "S T ", "A I " and "A O " indicates the self-training (self-supervised domain adaptation), input space and output space adversarial learning respectively. PyCDA* is trained with batch size 2 instead of 8 ([13]) due to memory limitations. + A I 84.1 37.5 76.1 21.9 22.4 41.9 48.6 44.2 58.3 8.2 59.0 58.9 23.0 81.9 26.0 33.0 9.3 31.1 40.0 42.8 Ours (SP-FogAdapt+) S T + A I 89.6 41.0 77.7 22.6 24.7 42.0 49.9 47.7 80.3 16.4 68.7 50.2 21.9 81.6 25.3 34.5 10.7 30.4 40.5 45.0</figDesc><table><row><cell>GTA ? Foggy-Cityscapes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>Effect of self-entropy and scale invariance. SSDA: Self-supervised domain adaptation. Here SE is Self-entropy while SI is Scale Invariance.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">GTA ? Foggy-Cityscapes</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">ResNet-38 SSDA SSDA-SE</cell><cell cols="2">SSDA-SI FogAdapt</cell></row><row><cell>mIoU</cell><cell>33.4</cell><cell>37.0</cell><cell>40.3</cell><cell>40.2</cell><cell>41.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>Effect of incorporating uncertainty weighted scale invariance on the quality of pseudo-labels. Here wSI is WeightedScale-Invariance.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">GTA ? Foggy-Cityscapes</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Start of round 0</cell><cell></cell><cell>Start of round 1</cell></row><row><cell>Methods</cell><cell cols="2">Normal Scale wSI based multi-scale</cell><cell cols="2">Normal Scale wSI based multi-scale</cell></row><row><cell>mIoU</cell><cell>69.6</cell><cell>71.5</cell><cell>71.3</cell><cell>74.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>Segmentation results of adapting SYNTHIA to Foggy-Cityscapes. We present mIoU and mIoU* (13-categories as presented by<ref type="bibr" target="#b18">[18]</ref>) on the Foggy-Cityscapes validation set.<ref type="bibr" target="#b31">31</ref>.2 57.62 2.9 0.02 31.7 29.1 23.1 38.5 41.1 61.3 18.9 75.0 8.3 11.9 32.1 33.3 38.4 MLSL [19] S T 48.9 27.2 53.4 11.4 0.4 31.9 32.4 21.0 49.2 40.1 65.8 24.4 77.8 20.9 19.0 50.5 35.9 40.8 Ours (FogAdapt) S T 62.2 28.0 56.4 13.1 0.7 30.3 30.1 27.4 61.7 61.8 54.9 30.0 66.1 2.6 12.1 44.8 36.4 41.4 Ours (FogAdapt+) S T + A I 68.3 34.0 64.7 14.2 2.1 33.2 28.7 31.5 69.7 56.1 63.8 29.1 66.3 6.0 21.4 47.3 39.8 45.2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">SYNTHIA ? Foggy-Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Appr.</cell><cell>Road</cell><cell>Sidewalk</cell><cell>Building</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell>T. Light</cell><cell>T. Sign</cell><cell>Veg.</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Bus</cell><cell>M.cycle</cell><cell>Bicycle</cell><cell>mIoU</cell><cell>mIoU*</cell></row><row><cell>ResNet-38 [23]</cell><cell>-</cell><cell cols="3">29.3 21.3 34.5</cell><cell>0.8</cell><cell cols="7">0.0 17.5 15.8 8.2 17.1 33.5 57.1</cell><cell>4.7</cell><cell cols="3">71.2 12.2 2.9</cell><cell cols="3">8.9 20.9 24.4</cell></row><row><cell>BDL [26]</cell><cell>A O</cell><cell cols="7">83.2 43.2 63.6 2.38 0.1 19.1 6.8</cell><cell cols="11">5.3 35.4 19.9 55.4 31.8 65.2 21.0 27.6 37.1 32.3 38.1</cell></row><row><cell>CBST [9]</cell><cell>S T</cell><cell>70.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 A</head><label>7</label><figDesc>comparative analysis of image transformation methods in dense foggy scenes adaptation process.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">GTA ? Foggy-Cityscapes</cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">ResNet-38 FogAdapt IDM-FogAdapt ITM-FogAdapt</cell></row><row><cell>mIoU</cell><cell>33.4</cell><cell>41.0</cell><cell>40.0</cell><cell>42.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A robust learning approach to domain adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodabandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranjbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">People, penguins and petri dishes: adapting object counting models to new visual domains and object types without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning from scale-invariant examples for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Subhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Constructing self-motivated pyramid curriculums for cross-domain semantic segmentation: A non-adversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gated context aggregation network for image dehazing and deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semantic segmentation for road surface detection in snowy environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vachmanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ravankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Emaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kobayashi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1381" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Contrast restoration of weather degraded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="713" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Visibility in bad weather from a single image</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-level self-supervised learning for domain adaptation with spatially independent and semantically consistent labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>1, 3, 4, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Curriculum model adaptation with synthetic and real data for semantic foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Van Den Hengel, Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>1, 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12975" to="12984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6810" to="6818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bidirectional deep residual learning for haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>3, 4, 9</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain flow for adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dlow</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6810" to="6818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly-supervised domain adaptation for built-up region segmentation in aerial and satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="275" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Boosting domain adaptation by discovering latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dada: Depth-aware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Confidence regularized selftraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A curriculum domain adaptation approach to the semantic segmentation of urban scenes, IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Single image dehazing based on contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1273" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimized contrast enhancement for real-time image and video dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="410" to="425" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Effective contrast-based dehazing for robust image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<idno>1871-1875. 4</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote sensing letters</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature forwarding for efficient single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klinghoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised single image dehazing using dark channel prior loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recursive deep residual learning for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="730" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A physics based generative adversarial network for single image defogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<idno>103815. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Does haze removal help cnnbased image classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="682" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semantic understanding of foggy scenes with purely synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Zaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>B. Leibe, J. Matas, N. Sebe, M. Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
