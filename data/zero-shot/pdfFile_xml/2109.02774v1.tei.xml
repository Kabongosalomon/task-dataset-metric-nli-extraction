<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FastAudio: A Learnable Audio Front-End for Spoof Speech Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quchen</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Teng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jules</forename><surname>White</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Powell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">C</forename><surname>Schmidt</surname></persName>
						</author>
						<title level="a" type="main">FastAudio: A Learnable Audio Front-End for Spoof Speech Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Spoof Speech Detection</term>
					<term>Automatic Speaker Verification</term>
					<term>Learnable Audio Filterbanks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Voice assistants, such as smart speakers, have exploded in popularity. It is currently estimated that the smart speaker adoption rate has exceeded 35% in the US adult population. Manufacturers have integrated speaker identification technology, which attempts to determine the identity of the person speaking, to provide personalized services to different members of the same family. Speaker identification can also play an important role in controlling how the smart speaker is used. For example, it is not critical to correctly identify the user when playing music. However, when reading the user's email out loud, it is critical to correctly verify the speaker that making the request is the authorized user. Speaker verification systems, which authenticate the speaker identity, are therefore needed as a gatekeeper to protect against various spoofing attacks that aim to impersonate the enrolled user. This paper compares popular learnable frontends which learn the representations of audio by joint training with downstream tasks (End-to-End). We categorize the frontends by defining two generic architectures and then analyze the filtering stages of both types in terms of learning constraints. We propose replacing fixed filterbanks with a learnable layer that can better adapt to anti-spoofing tasks. The proposed FastAudio front-end is then tested with two popular back-ends to measure the performance on the LA track of the ASVspoof 2019 dataset. The FastAudio front-end achieves a relative improvement of 27% when compared with fixed front-ends, outperforming all other learnable front-ends on this task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The prevalence of smart appliances has streamlined our daily chores but also brought security concerns. It is currently estimated that over 35% of the US adult population has a smart speaker at home <ref type="bibr" target="#b0">[1]</ref>. These voice assistants are becoming ever more versatile and can automate tasks ranging from making a phone call to placing an order. However, many of these tasks require different levels of privileges that are tied to the identity of the person talking to the voice assistant. Therefore, identifying "who is speaking" has become the backbone of personalized voice services. Compared to Speaker Identification, which focuses on personalized services, Speaker Verification is a binary classification of the validity of user identity claims for biometric security.</p><p>Spoofing attacks against speaker verification approaches exist and can be broadly categorized into three groups: Text-To-Speech (TTS), Voice Conversion (VC), and Replay Attacks. Replay attacks, which attack the identification system by playing back the recorded sample of the victim's speech, are most prevalent as they require the least technological sophistication. However, the threat of replay attacks can be mitigated by adding random prompt words. With the rapid development of deep learning, TTS and VC have seen significant improvement in their ability to fool speaker verification systems. Popular models like the Tacotron2 <ref type="bibr" target="#b1">[2]</ref> can transform text into highquality synthetic speech that is almost indistinguishable from the speech of humans.</p><p>Audio files are usually stored as 1D vectors that are extremely long (1 second of audio recording with a sampling rate at 16kHz contains 16000 data points). Because of their length, they are traditionally prepossessed to create a compressed representation that is smaller in size but aims to preserve as many of the important features as possible before spoof detection is applied. The component that performs this preprocessing step is known as the front-end. Front-ends can be either handcrafted or learnable, and the process of choosing the proper handcrafted front-ends is also known as feature selection. Both types of front-ends contain filter layers, and constraints can be applied to the filters.</p><p>Though handcrafted front-ends have proven to be a strong baseline for a variety of tasks, the underlying idea guiding the design of these features is that they are modeled on the non-linearity of the human ear's sensitivity to frequency (Mel scale) and loudness (Log compression).</p><p>Therefore, they may not represent the most salient features for audio classifications under all domains. Empirically, learnable front-ends outperform handcrafted front-ends in 7 out of 8 audio classification tasks in recent studies <ref type="bibr" target="#b2">[3]</ref>.</p><p>This paper provides the following contributions to the study of defending against audio spoofing attacks:</p><p>1) It proposes a light-weight 1 learnable front-end called FastAudio that achieved the lowest min t-DCF in spoof speech detection compared to other front-ends, 2) It provides a comparison of feature selections for spoofing countermeasures, with a special focus on learnable audio front-ends, and shows how applying shape constraints can make the filterbank layer perform better while reducing the number of parameters, and 3) It describes the architecture that achieved top performance on the ASVspoof 2019 <ref type="bibr" target="#b3">[4]</ref> dataset. The remainder of this paper is organized as follows: Section II summarizes the classification of audio front-ends based on structure and the background for filter learning; Section III discusses different constraint types regarding filterbank learning; Section IV describes our experiment setups including dataset, metric, and model details; Section V analyzes the result and describes our experiment insights regarding filter learning for spoof speech detection, and Section VI presents concluding remarks and potential future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND ON AUDIO FRONT-END</head><p>Spoof speech detection is a single-task classification problem, for which many front-ends have been tested, including Instantaneous Frequency (IF), Group Delay (GD) and Mel-frequency cepstral coefficients (MFCC), etc. The front-ends used in the classification of speech have been dominated by MFCC and recently Log Mel FilterBanks (FBanks); both are hand-crafted features that are fixed and not learnable. Constant Q Transform (CQT) <ref type="bibr" target="#b4">[5]</ref> is another handcrafted front-end commonly used for music generation and music note recognition as it can better mimic musical scales; however, prior research reported CQT was also the best performing front-end for spoof detection <ref type="bibr" target="#b5">[6]</ref>.  <ref type="figure" target="#fig_0">Figure 1</ref>, front-ends can be categorized by the procedures they perform. There are two key categories: First-order Scattering Transform (FST) <ref type="bibr" target="#b6">[7]</ref> based front-ends and Short-Time Fourier Transform (STFT) based front-ends. Unlike STFT which multiplies a filterbank matrix with a spectrogram, FST uses a convolutional layer on the raw audio waveform to approximate the standard filtering process. While considerable progress has been made on FST based frontend approaches, literature has shown that they lose signal energy, which corresponds to information loss, since only the first-order coefficients of a scattering transform are used <ref type="bibr" target="#b6">[7]</ref>. The FST based approaches are also time-consuming <ref type="bibr" target="#b7">[8]</ref> since convolution layers with large kernels are computation intensive. STFT based front-ends remain popular, and FBanks are still the front-end for the state-of-the-art speaker identification <ref type="bibr" target="#b8">[9]</ref> and speech recognition <ref type="bibr" target="#b9">[10]</ref> systems. However, many STFT based front-ends are fixed and may not adapt well to certain downstream tasks.</p><p>Both types of front-ends employ some type of filter-like manipulations to model the non-linearity of the human ear's sensitivity to frequency. The distribution of filter center frequency is referred to as scale. Studies <ref type="bibr" target="#b10">[11]</ref> have shown that the Mel-scale, as shown in Equation 1, can capture human perception for pitch relatively well. m = 2595 log 10 1 + f 700</p><p>There also exists the Bark scale <ref type="bibr" target="#b11">[12]</ref> and Equivalent Rectangular Bandwidth (ERB) scale <ref type="bibr" target="#b12">[13]</ref>, which are less well-known. However, these scales are mostly based on past experience and are fixed equations. To make this manipulation in the front-end domain adaptable, filters can be made learnable. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, a filterbank can learn its center frequency c n , gain g n , bandwidth b n , and shape s n . The filter properties can be summarized in <ref type="bibr">Equation 2</ref>.</p><formula xml:id="formula_1">? n (f ) = g n s n (c n ; b n ; f )<label>(2)</label></formula><p>Recent research has made progress on learnable audio frontends. SincNet <ref type="bibr" target="#b13">[14]</ref> uses convolution to extract features with Sinc functions. TD-filterbanks <ref type="bibr" target="#b14">[15]</ref> uses Gabor convolution to replace front-end filtering. LEAF <ref type="bibr" target="#b2">[3]</ref>, proposed by Google, is the first fully learnable audio front-end with an added learnable compression layer. Wav2Vec <ref type="bibr" target="#b15">[16]</ref> is a CNN-based unsupervised audio training method for speech recognition that can use raw audio data directly for training. RawNet2 <ref type="bibr" target="#b16">[17]</ref> was used as the baseline system for the ASVspoof 2021 challenge and adopted Sinc Filters with CNN layers to extract audio features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESEARCH QUESTIONS REGARDING LEARNABLE</head><p>FILTERBANKS Previous research has explored the feasibility of learnable filterbanks. For example, nnAudio <ref type="bibr" target="#b17">[18]</ref> implemented a set of unconstrained learnable filterbanks; however, T. Sainath et al. <ref type="bibr" target="#b18">[19]</ref> reported limited improvement from unconstrained filterbank learning. DNN-FBCC <ref type="bibr" target="#b19">[20]</ref> explored some constraints over filters by adopting a mask matrix. Zhang and Wu <ref type="bibr" target="#b20">[21]</ref> described a detailed study on the shape and positiveness constraint's effect on the filterbanks. However, no systematic study has been done on constraining the filterbank shape in the STFT-based approach used for spoof speech detection. As shown in <ref type="table" target="#tab_0">Table I</ref>, all current FST based front-ends put shape constraints on the band-pass filters; however, STFT based front-ends, like DNN-FBCC, do not constrain the filter shape. Instead, a mask is put on the filters so that the bandwidth is clamped and the filters are sorted by center frequencies. Therefore, we designed a learnable front-end, called FastAudio, specifically focused on answering the following questions: 1) Is a shape constraint necessary for spoof detection, and which shape constraint has the lowest min t-DCF? 2) Does the center frequency need to be sorted for spoof detection?  3) What do trained filterbanks learn about spoof detection compared to handcrafted FBanks? These questions are discussed in subsection C and D of Section V.</p><formula xml:id="formula_2">Log (optional) - - Gammatonegram - STFT Modulus Gammatone Log (optional) - Mel IF Derivative - STFT Phase - - Derivative - IF - STFT Phase - - - - Fixed Mel-Filterbanks - STFT Modulus Triangular Log - Mel Hand-crafted MFCC Yes STFT Modulus Triangular Log DCT Mel Feature LFCC - STFT Modulus Triangular Log DCT Linear Extractor IMFCC - STFT Modulus Triangular Log DCT Inverse Mel RFCC - STFT Modulus Rectangular Log DCT Linear GFCC - STFT Modulus Gammatone Log DCT ERB CQT - STFT Modulus Constant-Q Log (optional) - CQT CQCC - STFT Modulus Constant-Q Log DCT CQT SPNCC/PNCC Yes STFT Modulus Gammatone Log DCT ERB RASTA-PLP - STFT Modulus RASTA Log IDFT RASTA* Partly PCEN - STFT Modulus Triangular PCEN - Mel Trainable Spline - STFT Modulus Spline PCEN - Mel nnAudio - Trainable STFT Modulus Conv1D Log - Mel/CQT DNN-FBCC - STFT Modulus Matrix Mask Log - Mel FastAudio Yes STFT Modulus Triangular Log - Mel(Trainable)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT AND DATASET</head><p>The ASVspoof 2019 corpus consists of two parts: Logical Access and Physical Access. Logical Access (LA) contains fake (spoof) speech generated from various text-to-speech and voice conversion techniques. Physical Access (PA) contains spoof speech that is simulated to mimic various room sizes, speaker orientations/distances, and hardware artifacts. The true speech audio files are referred to as Bona fide. Here we focus on the LA task. Since there are existing ASV (automatic speech verification) systems that provide some protection against spoofing attacks, the goal is to design a system that can best complement existing ASV systems (the result of the existing ASV system is provided by the dataset in labels). The system we are designing is called the countermeasures (CM). The evaluation metric is the tandem detection cost function (min t-DCF), which is designed to best reflect real-world protection effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>The performance of the FastAudio learnable front-end is evaluated on the ASVspoof 2019 LA dataset. As shown in <ref type="table" target="#tab_0">Table IV</ref>, the dataset was partitioned into three parts where the evaluation set is three times the size of the training set. The training and development sets contain data generated from the same algorithms; however, to ensure the spoof detection system can generalize well to audio of unseen types, the evaluation set also contains attacks that are generated from different algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Metrics</head><p>The primary metric for spoof speech detection is the minimum normalized tandem detection cost function (min t-DCF), as shown in Equation 3. The min t-DCF measures the overall protection rate for combined CM and ASV systems, where ? depends on application parameters (priors, costs) and ASV performance (miss, false alarm, and spoof miss rates), while P cm miss (s) and P cm fa (s) are the CM miss and false alarm rates at threshold s <ref type="bibr" target="#b3">[4]</ref>. </p><p>Equal error rate (EER) was used as a secondary metric to make comparison possible with earlier datasets like ASVSpoof 2017. EER is defined as the value of false acceptance rate and false rejection rates where they are equal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Back-end</head><p>We made a summary on the structures of common front-ends in <ref type="table" target="#tab_0">Table II and Table III</ref>. Our FastAudio front-end consists of an STFT transform followed by a learnable filterbank layer, and finally a log compression layer to mimic the non-linearity of human sensitivity to loudness. We integrated the front-ends with two of the most popular back-ends for audio classification: X-vector <ref type="bibr" target="#b21">[22]</ref> [23] and ECAPA-TDNN [24] <ref type="bibr" target="#b22">[23]</ref>. The backend turns the FBank-variant into a 256-dimensional embedding vector. The vectors are then fed into a linear classifier. A summary of other popular back-end architectures is shown in <ref type="table" target="#tab_4">Table V</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Setup</head><p>The model was trained on 2 Nvidia 2080 Ti GPUs for 100 epochs and the batch size was set to 12 (except for TDfilterbank whose batch size was 4 to stay within memory limits). We also compared the performance of our front-end with other STFT-based and FST-based front-ends, both under learnable and fixed settings. To make the comparison fair, we keep the hyperparameters across all experiments the same so that the front-end outputs have the same dimensions. The sampling rate was set to 16kHz, window length to 25ms, window stride to 10ms, and the number of filters to 40. All learnable front-ends were initialized to mimic Mel-FBanks, as previous research <ref type="bibr" target="#b14">[15]</ref> has shown that random initialization has worse performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND ANALYSIS A. How do learnable front-ends perform on min t-DCF compared with handcrafted front-ends for spoof speech detection?</head><p>Since the most recent systematic comparison of front-ends on spoof detection we can find was done in 2015 <ref type="bibr" target="#b24">[25]</ref>, we designed the experiments so that an updated baseline can be established that includes learnable front-ends. We choose a combination of FST and STFT front-ends with both fixed and learnable setting so that the experiment is comprehensive. As shown in <ref type="table" target="#tab_0">Table  VII</ref>, we found that the FST-based learnable front-ends need longer training time than hand-crafted features in the spoof speech detection task and cannot beat the performance of CQT. B. Can we design an STFT-based front-end for spoof speech detection that is learnable and can it beat the performance of CQT?</p><p>Since FST-based learnable front-ends failed to beat the performance of CQT, we designed a front-end following the traditional STFT-based approach and limited the number of trainable parameters. Since it trains faster than FST-based front-ends, we call it FastAudio. We hypothesize that instead of changing the front-end architecture completely like in the FST-based approach, we can boost the performance of the fixed STFT-based approach by making the filterbank layer learnable. We tested FastAudio under 3 different constraint settings and the best one achieved 27% decrease in min-tDCF compared to FBanks, outperforming CQT (See <ref type="table" target="#tab_0">Table VII</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Which set of constraints for filterbank learning performs best in spoof speech detection?</head><p>We found that the existence of shape constraint plays an important role in improving spoof detection accuracy. However, we did not find a significant difference in constraining the shape of the filters to be Gaussian or Triangular. We found that sorting the filterbanks by center frequency does not improve accuracy, which confirms the conclusion from previous study in LEAF <ref type="bibr" target="#b2">[3]</ref>. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the learned filterbank distribution closely follows the hand-crafted filterbanks in both center frequency and bandwidth. The similarity in c n and b n helps explain the strong performance of handcrafted features compared to the learnable front-end, especially compared to the FST-based front-ends. We hypothesize that during the training process, the filterbanks actually 'self-regulate' to remain mostly sorted in both center frequency and bandwidth. The visualization of the front-end output is shown in <ref type="figure">Figure  3</ref>. All of the outputs contain "horizontal lines" that correspond to certain frequencies, which is a sign of filter selectiveness. We found that front-end output like LEAF, TD-filterbanks and nnAudio changed greatly after training due to the number of trainable parameters. When the shape of the filters is not constrained, as shown in nnAudio, the trained front-end shows signs of over-fitting (many random distributed dots) and has the worst performance. Since the nnAudio has no constraint for filter shape, the learned filter shape is determined by 201 points, thus it may contain very sharp peaks and select frequencies of very narrow ranges, thus creating the irregular dots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. What does FastAudio learn about spoof speech detection and how can we interpret what it learns?</head><p>Formants are the spectral peaks resulted from acoustic resonance of the human vocal tract. Since in English vowels contain more energy than consonants, we expect our learned filters center frequencies to concentrate around the formants of average vowels of English <ref type="bibr" target="#b25">[26]</ref>. We plotted the cumulative frequency response of the FastAudio in <ref type="figure" target="#fig_3">Figure 4</ref>. We found 2 peaks in the lower frequency and 1 peak in the high frequency. The peaks in frequencies around 320Hz?440 Hz and 1120Hz may correspond to the 1st and 2nd formants averaged over all vowels in English <ref type="bibr" target="#b13">[14]</ref>. This adaptation to human speech suggests FastAudio was able to successfully learn what is important for spoof speech detection tasks. Similar adaptation was also reported in FST-based front-end for speech identification tasks <ref type="bibr" target="#b13">[14]</ref>. Interestingly, we also found peaks in the high pitch regions near the sampling boundary, which suggests spoof speech may differ from the real speech in frequencies that are "ignored" by scales used by handcrafted front-ends like the Mel-scale. High-frequency energy was thought to be less important and subsequently underrepresented in Mel-scales. However, in the spoof detection task, we suspect that because these high frequencies are "unimportant" to human hearing, the spoof Thus, the representation of high-frequency data may be a good indicator for spoof speech detection. Together, these findings indicated that: 1) Learned FastAudio filters are more selective than their initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) FastAudio emphasizes frequencies around 1st and 2nd</head><p>formants, which may be important for distinguishing between spoof and bona fide speech. 3) Learned FastAudio filters are more sensitive to highfrequency energy, which may be a salient feature of spoof detection. 4) Through end-to-end training, FastAudio can adapt to spoof detection tasks. The front-end successfully adapted to the downstream task and was able to learn the phonetics of human speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. How can people use FastAudio for spoof detection and suggestion for model fusion</head><p>People who focus on designing the back-end for spoof speech detection can use FastAudio as a drop-in replacement. Our experiment shows that FastAudio is a better front-end than CQT in spoof detection, despite CQT being reported as the best front-end in previous research <ref type="bibr" target="#b5">[6]</ref>. From the information theory's perspective, the fusion of the result from models whose front-end outputs are least similar tends to produce a better result. Thus, the visualizations of learnable front-ends output in this paper can provide guidance for feature selection, which can be used as a supplement to handcrafted front-ends for spoof detection <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Does the lesson learned generalize to ASVspoof 2021 dataset?</head><p>ASVspoof 2021 dataset kept the train and development part of 2019 data the same and added a much larger evaluation dataset that is 10 times the size as before. Thus, the ASVspoof 2021 challenge is more difficult because more spoofing techniques are added. Since the ASVspoof 2021 dataset was not officially released yet, we can not analyze the details as we did on the ASVspoof 2019 dataset. However, since we participated in the 2021 competition, we summarized a preliminary result of different front-end's performances in <ref type="table" target="#tab_0">Table VIII</ref>. We found that CQT performs poorly on the new dataset and TD-filterbanks is the best learnable frontend. FastAudio has a similar performance as its hand-crafted equivalent (FBanks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper investigates the performance of learnable frontends on spoof detection and proposes an STFT-based audio front-end called FastAudio. We tested the proposed front-end under different constraint settings and found FastAudio was able to successfully adapt to spoof detection. The proposed front-end achieves top performance on the ASVspoof 2019 dataset, beating the fixed equivalent by 27% and surpassing the performance of CQT, which was reported as the best handcrafted feature for spoof speech detection. From our work on learnable front-ends for spoof speech detection, we learned the following important lessons: 1) Learnable front-end can beat the best handcrafted features in spoof speech detection since they can adapt to the downstream tasks. 2) Shape constraint is important for filterbank learning to prevent over-fitting.</p><p>3) The center frequencies and bandwidth of filters do not need to be sorted, and 4) High-frequency information can be important for spoof speech detection. In future work, we plan to test FastAudio's potential outside spoof detection domain, on other datasets such as UrbanSound8K.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>FST based front-end and STFT based front-end As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>t</head><label></label><figDesc>? DCF min norm = min s {?P cm miss (s) + P cm fa (s)}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Heatmap of the magnitude of the frequency response for initialization filters (up) and learned filters (down).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Cumulative frequency response of the FastAudio filters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I FILTER</head><label>I</label><figDesc>COMPARISON OF LEARNABLE FRONT-END</figDesc><table><row><cell>Type</cell><cell>Name</cell><cell cols="4">Filter/BandWidth Center Frequency</cell><cell>Gain</cell></row><row><cell></cell><cell></cell><cell>Shape</cell><cell>Clamp</cell><cell>Sorted</cell><cell>Clamp</cell><cell></cell></row><row><cell>FST</cell><cell>TD-FBanks</cell><cell>Gabor</cell><cell>-</cell><cell>Yes</cell><cell>Yes</cell><cell>-</cell></row><row><cell>based</cell><cell>SincNet</cell><cell>Sinc</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>Fixed a</cell></row><row><cell></cell><cell>LEAF</cell><cell>Gabor</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>Fixed</cell></row><row><cell>STFT</cell><cell>nnAudio</cell><cell>-</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>-</cell></row><row><cell>based</cell><cell>DNN-FBCC</cell><cell>-</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>-</cell></row><row><cell></cell><cell>FastAudio</cell><cell>Triang</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>Fixed</cell></row></table><note>a The gain of each filter is not learned in the filter layer but in subsequent layers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II A</head><label>II</label><figDesc>STAGE-WISE COMPARISON OF STFT BASED FRONT-END</figDesc><table><row><cell>Type</cell><cell>Name</cell><cell>Pre-emph</cell><cell>FFT</cell><cell>Selection</cell><cell>Filter</cell><cell>Compression</cell><cell>Transform</cell><cell>Center freq</cell></row><row><cell></cell><cell>Spectrogram</cell><cell>-</cell><cell>STFT</cell><cell>Modulus</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III A</head><label>III</label><figDesc>STAGE-WISE COMPARISON OF DEEP SCATTERING SPECTRUM BASED FRONT-END</figDesc><table><row><cell>Type</cell><cell>Name</cell><cell>Pre-emph</cell><cell>Filter</cell><cell>Selection</cell><cell cols="3">Windowing/Pooling Compression Initial Center freq</cell></row><row><cell>Partly</cell><cell>TD-FBanks</cell><cell>-</cell><cell>Conv1D</cell><cell>Modulus</cell><cell>Lowpass</cell><cell>Log</cell><cell>Mel(Trainable)</cell></row><row><cell>Trainable</cell><cell>SincNet</cell><cell>-</cell><cell>Sinc</cell><cell>LeakyRelu</cell><cell>Maxpool</cell><cell>LayerNorm</cell><cell>Mel</cell></row><row><cell>Trainable</cell><cell>LEAF</cell><cell>-</cell><cell>Gabor</cell><cell>Modulus</cell><cell>Gaussian Lowpass</cell><cell>sPCEN</cell><cell>Mel(Trainable)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV DESCRIPTION</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">OF ASVSPOOF 2019 LA DATASET</cell></row><row><cell>Subset</cell><cell cols="2">#Speaker</cell><cell cols="2">#Utterances</cell></row><row><cell></cell><cell cols="2">Male Female</cell><cell>Bona fide</cell><cell>Spoofed</cell></row><row><cell>Training</cell><cell>8</cell><cell>12</cell><cell>2580</cell><cell>22800</cell></row><row><cell>Development</cell><cell>8</cell><cell>12</cell><cell>2548</cell><cell>22296</cell></row><row><cell>Evaluation</cell><cell>21</cell><cell>27</cell><cell>7355</cell><cell>63882</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V A</head><label>V</label><figDesc>STAGE-WISE COMPARISON OF DIFFERENT ARCHITECTURES</figDesc><table><row><cell>Name</cell><cell cols="3">Front-end First Layer Main Block</cell><cell>Pooling</cell><cell>Optional</cell><cell>Classifier</cell></row><row><cell>X-vector</cell><cell>MFCC</cell><cell>TDNN</cell><cell>CNN</cell><cell>Statistical Pooling</cell><cell>-</cell><cell>PLDA</cell></row><row><cell>ECAPA-TDNN</cell><cell>FBank</cell><cell>TDNN</cell><cell>CNN</cell><cell>Attention Statistical Pooling</cell><cell>PLDA</cell><cell>Cosine Similarity</cell></row><row><cell>Res2Net</cell><cell>CQT</cell><cell>TDNN</cell><cell>CNN</cell><cell>Pooling</cell><cell>PLDA</cell><cell>Cosine</cell></row><row><cell>RawNet2</cell><cell>SincNet</cell><cell>Conv</cell><cell>CNN</cell><cell>GRU</cell><cell>-</cell><cell>FC</cell></row><row><cell>EfficientNetB0</cell><cell>LEAF</cell><cell>Conv</cell><cell>CNN</cell><cell>Avg/Max pooling</cell><cell>-</cell><cell>FC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">X-VECTOR AND ECAPA-TDNN</cell><cell></cell></row><row><cell cols="2">Xvector</cell><cell>ECAPA-TDNN</cell><cell></cell></row><row><cell>Layer</cell><cell>Output</cell><cell>Layer</cell><cell>Output</cell></row><row><cell>Input</cell><cell>(N, T )</cell><cell>Input</cell><cell>(N, T )</cell></row><row><cell cols="2">TDNN X 5 (1500, T )</cell><cell cols="2">Conv1D + ReLU + BN (C, T )</cell></row><row><cell>Stats Pool</cell><cell>(3000, 1)</cell><cell>SE-Res2Block X 3</cell><cell>(3, C, T )</cell></row><row><cell>Linear</cell><cell>(256, 1)</cell><cell>Conv1D + ReLU</cell><cell>(1536, T )</cell></row><row><cell></cell><cell></cell><cell>Atten Stats Pool + BN</cell><cell>(3072, 1)</cell></row><row><cell></cell><cell></cell><cell>FC + BN</cell><cell>(256, 1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII A</head><label>VII</label><figDesc>STAGE-WISE COMPARISON OF THE DIFFERENT FRONT-ENDS' PERFORMANCE ON THE ASVSPOOF 2019 LA DATASET</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">ECAPA-TDNN</cell><cell></cell><cell>X-vector</cell><cell></cell><cell></cell></row><row><cell>Front-end</cell><cell>#Params</cell><cell>Constraint</cell><cell cols="4">EER min t-DCF EER min t-DCF</cell><cell>MACs</cell><cell>Train Time/Epoch</cell></row><row><cell>CQT</cell><cell>0</cell><cell>Fixed</cell><cell>1.73</cell><cell>0.05077</cell><cell>3.40</cell><cell>0.09510</cell><cell>0</cell><cell>10:58 min</cell></row><row><cell>Fbanks</cell><cell>0</cell><cell>Fixed</cell><cell>2.11</cell><cell>0.06425</cell><cell>2.39</cell><cell>0.06875</cell><cell>0</cell><cell>10:53 min</cell></row><row><cell>FastAudio-Tri</cell><cell>80</cell><cell>Shape+Clamp</cell><cell>1.54</cell><cell>0.04514</cell><cell>1.73</cell><cell>0.04909</cell><cell>0.00GMac</cell><cell>13:02 min</cell></row><row><cell>FastAudio-Gauss</cell><cell>80</cell><cell>Shape+Clamp</cell><cell>1.63</cell><cell>0.04710</cell><cell>1.67</cell><cell>0.05158</cell><cell>0</cell><cell>12:51 min</cell></row><row><cell>FastAudio-Sort</cell><cell>80</cell><cell>Shape+Clamp+Order</cell><cell>1.89</cell><cell>0.05204</cell><cell>1.69</cell><cell>0.05235</cell><cell>0</cell><cell>12:59 min</cell></row><row><cell>LEAF</cell><cell>282</cell><cell>Shape+Clamp</cell><cell>2.49</cell><cell>0.06445</cell><cell>3.28</cell><cell>0.07319</cell><cell>0.01GMac</cell><cell>34.45 min</cell></row><row><cell>nnAudio</cell><cell>8.04k</cell><cell>No</cell><cell>3.63</cell><cell>0.08929</cell><cell>5.56</cell><cell>0.14707</cell><cell>0</cell><cell>13:00 min</cell></row><row><cell>TD-filterbanks</cell><cell>31k</cell><cell>Shape+Clamp</cell><cell>1.83</cell><cell>0.05284</cell><cell>3.18</cell><cell>0.08427</cell><cell>1.32GMac</cell><cell>22.48 min</cell></row><row><cell>Front-end</cell><cell>Name</cell><cell>Constraint</cell><cell>EER</cell><cell>min-tDCF</cell><cell></cell><cell>Backend</cell><cell></cell><cell>Baseline</cell></row><row><cell>SincNet</cell><cell>RawNet2</cell><cell>Fixed</cell><cell>5.13</cell><cell>0.1175</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="5">Fig. 3. Visualization of Learnable Front-ends</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII A</head><label>VIII</label><figDesc>STAGE-WISE COMPARISON OF DIFFERENT FRONT-END'S PERFORMANCE ON ASVSPOOF 2021 LA DATASET</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ECAPA-TDNN</cell><cell>X-vector</cell><cell></cell><cell></cell></row><row><cell>Front-end</cell><cell>#Params</cell><cell>Constraint</cell><cell>min-tDCF</cell><cell>min-tDCF</cell><cell>MACs</cell><cell>Train Time/Epoch</cell></row><row><cell>CQT</cell><cell>0</cell><cell>Fixed</cell><cell>0.3676</cell><cell>0.3812</cell><cell>0</cell><cell>10:58 min</cell></row><row><cell>Fbanks</cell><cell>0</cell><cell>Fixed</cell><cell>0.26</cell><cell>0.2788</cell><cell>0</cell><cell>10:53 min</cell></row><row><cell>FastAudio-Tri</cell><cell>80</cell><cell>Shape+Clamp</cell><cell>0.2661</cell><cell>0.3047</cell><cell>0</cell><cell>13:02 min</cell></row><row><cell>FastAudio-Gauss</cell><cell>80</cell><cell>Shape+Clamp</cell><cell>0.2611</cell><cell>0.3122</cell><cell>0</cell><cell>12:51 min</cell></row><row><cell>FastAuido-Sort</cell><cell>80</cell><cell>Shape+Clamp+Order</cell><cell>0.388</cell><cell>0.293</cell><cell>0</cell><cell>12:59 min</cell></row><row><cell>LEAF</cell><cell>282</cell><cell>Shape+Clamp</cell><cell>0.2753</cell><cell>0.2794</cell><cell>0.01GMac</cell><cell>34.45 min</cell></row><row><cell>nnAudio</cell><cell>8.04k</cell><cell>No</cell><cell>0.2783</cell><cell>0.3376</cell><cell>0</cell><cell>13:00 min</cell></row><row><cell>TD-filterbanks</cell><cell>31k</cell><cell>Shape+Clamp</cell><cell>0.2522</cell><cell>0.2827</cell><cell>1.32GMac</cell><cell>22.48 min</cell></row><row><cell>Front-end</cell><cell>Name</cell><cell>Constraint</cell><cell>min-tDCF</cell><cell>EER</cell><cell>Backend</cell><cell>Baseline</cell></row><row><cell>SincNet</cell><cell>RawNet2</cell><cell>Fixed</cell><cell>0.4152</cell><cell>9.49</cell><cell>ResNet</cell><cell></cell></row><row><cell>LFCC</cell><cell>LFCC-LCNN</cell><cell>Fixed</cell><cell>0.3152</cell><cell>8.90</cell><cell>LCNN</cell><cell></cell></row><row><cell>LFCC</cell><cell>LFCC-GMM</cell><cell>Fixed</cell><cell>0.5836</cell><cell>21.13</cell><cell>GMM</cell><cell></cell></row><row><cell>CQCC</cell><cell>CQCC-GMM</cell><cell>Fixed</cell><cell>0.4948</cell><cell>15.80</cell><cell>GMM</cell><cell></cell></row><row><cell cols="3">speech generator does not create realistic imitation in high</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>frequencies.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Front-end trains faster and has the least computational complexity as estimated by multiply-accumulate operations (MACs) compared to other learnable front-ends. See Table VII, https://pypi.org/project/ptflops/.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">U.s. smart speaker growth flat lined in 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kinsella</surname></persName>
		</author>
		<ptr target="https://voicebot.ai/2021/04/14/u-s-smart-speaker-growth-flat-lined-in-2020/" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Leaf: A learnable frontend for audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Teboul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D C</forename><surname>Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<idno>abs/2101.08596</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Asvspoof 2019: Future horizons in spoofed and fake audio detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vestman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Calculation of a constant q spectral transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="425" to="434" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Replay and synthetic speech detection with res2net architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep scattering spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="4114" to="4128" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Samplecnn: End-to-end deep convolutional neural networks using very small filters for music classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">150</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Desplanques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thienpondt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Demuynck</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">State-of-the-art speaker recognition with neural network embeddings in nist sre18 and speakers in the wild evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Borgstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Garc?a-Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech recognition by machines and humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analytical expressions for critical-band rate and critical bandwidth as a function of frequency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Terhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="1523" to="1525" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Derivation of auditory filter shapes from notched-noise data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glasberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hearing Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="103" to="138" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speaker recognition from raw waveform with sincnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1021" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning filterbanks from raw speech for phone recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5509" to="5513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end anti-spoofing with rawnet2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Larcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">nnaudio: An on-the-fly gpu audio to spectrogram conversion toolbox using 1d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Agres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Herremans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="161" to="981" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning filter banks within a deep neural network framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="297" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dnn filter bank cepstral coefficients for spoofing detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4779" to="4787" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative frequency filter banks learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Audio, Speech, and Music Processing</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Xvectors: Robust dnn embeddings for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speechbrain: A general-purpose speech toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rouhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dawalatabad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rastorgueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="abs/2106.04624" />
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Desplanques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thienpondt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Demuynck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07143</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spoofing speech detection using high dimensional magnitude and phase features: the ntu approach for asvspoof 2015 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Siong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Explaining phonetic variation: A sketch of the h&amp;h theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lindblom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
