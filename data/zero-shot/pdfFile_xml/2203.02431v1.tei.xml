<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Efficient Lane Detection via Curve Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
							<email>zyfeng97@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Guo</surname></persName>
							<email>guoshaohua@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
							<email>wangmin@sensetime.com</email>
							<affiliation key="aff3">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">East China Normal University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Efficient Lane Detection via Curve Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel parametric curve-based method for lane detection in RGB images. Unlike stateof-the-art segmentation-based and point detection-based methods that typically require heuristics to either decode predictions or formulate a large sum of anchors, the curvebased methods can learn holistic lane representations naturally. To handle the optimization difficulties of existing polynomial curve methods, we propose to exploit the parametric B?zier curve due to its ease of computation, stability, and high freedom degrees of transformations. In addition, we propose the deformable convolution-based feature flip fusion, for exploiting the symmetry properties of lanes in driving scenes. The proposed method achieves a new state-ofthe-art performance on the popular LLAMAS benchmark. It also achieves favorable accuracy on the TuSimple and CULane datasets, while retaining both low latency (&gt;150 FPS) and small model size (&lt;10M). Our method can serve as a new baseline, to shed the light on the parametric curves modeling for lane detection. Codes of our model and PytorchAutoDrive: a unified framework for self-driving perception, are available at: https://github.com/ voldemortX/pytorch-auto-drive .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Lane detection is a fundamental task in autonomous driving systems, which supports the decision-making of lanekeeping, centering and changing, etc. Previous lane detection methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">12]</ref> typically rely on expensive sensors such as LIDAR. Advanced by the rapid development of deep learning techniques, many works <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b39">41]</ref> are proposed to detect lane lines from RGB inputs captured by commercial front-mounted cameras. Deep lane detection methods can be classified into three categories, i.e., segmentation-based, point detection-based, and curve-based methods <ref type="figure" target="#fig_0">(Figure 1</ref>). Among them, by relying on classic segmentation <ref type="bibr" target="#b4">[5]</ref> and object detection <ref type="bibr" target="#b26">[28]</ref> networks, the segmentation-based and point detectionbased methods typically achieve state-of-the-art lane detection performance. The segmentation-based methods <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b39">41]</ref> exploit the foreground texture cues to segment the lane pixels and decode these pixels into line instances via heuristics. The point detection-based methods <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b37">39]</ref> typically adopt the R-CNN framework <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b26">28]</ref>, and detect lane lines by detecting a dense series of points (e.g., every 10 pixels in the vertical axis). Both kinds of approaches represent lane lines via indirect proxies (i.e., segmentation maps and points). To handle the learning of holistic lane lines, under cases of occlusions or adverse weather/illumination conditions, they have to rely on low-efficiency designs, such as recurrent feature aggregation (too heavy for this realtime task) <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b39">41]</ref>, or a large number of heuristic anchors (&gt; 1000, which may be biased to dataset statistics) <ref type="bibr" target="#b31">[33]</ref>.</p><p>On the other hand, there are only a few methods <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b30">32]</ref> proposed to model the lane lines as holistic curves (typically the polynomial curves, e.g., x = ay 3 + by 2 + cy + d).</p><p>While we expect the holistic curve to be a concise and elegant way to model the geometric properties of lane line, the abstract polynomial coefficients are difficult to learn. Previous studies show that their performance lag behind the well-designed segmentation-based and point detectionbased methods by a large margin (up to 8% gap to state-ofthe-art methods on the CULane <ref type="bibr" target="#b20">[22]</ref> dataset). In this paper, we aim to answer the question of whether it is possible to build a state-of-the-art curve-based lane detector.</p><p>We observe that the classic cubic B?zier curves, with sufficient freedom degrees of parameterizing the deformations of lane lines in driving scenes, remain low computation complexity and high stability. This inspires us to propose to model the thin and long geometric shape properties of lane lines via B?zier curves. The ease of optimization from on-image B?zier control points enables the network to be end-to-end learnable with the bipartite matching loss <ref type="bibr" target="#b36">[38]</ref>, using a sparse set of lane proposals from simple column-wise Pooling (e.g., 50 proposals on the CU-Lane dataset <ref type="bibr" target="#b20">[22]</ref>), without any post-processing steps such as the Non-Maximum Suppression (NMS), or hand-crafted heuristics such as anchors, hence leads to high speed and small model size. In addition, we observe that lane lines appear symmetrically from a front-mounted camera (e.g., between ego lane lines, or immediate left and right lanes). To model this global structure of driving scenes, we further propose the feature flip fusion, to aggregate the feature map with its horizontally flipped version, to strengthen such coexistences. We base our design of feature flip fusion on the deformable convolution <ref type="bibr" target="#b40">[42]</ref>, for aligning the imperfect symmetries caused by, e.g., rotated camera, changing lane, non-paired lines. We conduct extensive experiments to analyze the properties of our method and show that it performs favorably against state-of-the-art lane detectors on three popular benchmark datasets. Our main contributions are summarized as follows:</p><p>? We propose a novel B?zier curve-based deep lane <ref type="bibr">de-</ref>tector, which can model the geometric shapes of lane lines effectively, and be naturally robust to adverse driving conditions.</p><p>? We propose a novel deformable convolution-based feature flip fusion module, to exploit the symmetry property of lanes observed from front-view cameras.</p><p>? We show that our method is fast, light-weight, and accurate through extensive experiments on three popular lane detection datasets. Specifically, our method outperforms all existing methods on the LLAMAS benchmark <ref type="bibr" target="#b2">[3]</ref>, with the light-weight ResNet-34 backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Segmentation-based Lane Detection. These methods represent lanes as per-pixel segmentation. SCNN <ref type="bibr" target="#b20">[22]</ref> formu-lates lane detection as multi-class semantic segmentation and is the basis of the 1st-place solution in TuSimple challenge [1]. It's core spatial CNN module recurrently aggregates spatial information to complete the discontinuous segmentation predictions, which then requires heuristic postprocessing to decode the segmentation map. Hence, it has a high latency, and only struggles to be real-time after an optimization of Zheng et al. <ref type="bibr" target="#b39">[41]</ref>. Others explore knowledge distillation <ref type="bibr" target="#b11">[13]</ref> or generative modeling <ref type="bibr" target="#b6">[8]</ref>, but their performance merely surpasses the seminal SCNN. Moreover, these methods typically assume a fixed number (e.g., 4) of lines. LaneNet <ref type="bibr" target="#b19">[21]</ref> leverages an instance segmentation pipeline to deal with a variable number of lines, but it requires post-inference clustering to generate line instances. Some methods leverage row-wise classification <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b38">40]</ref>, which is a customized down-sampling of per-pixel segmentation so that they still require post-processing. Qin et al. <ref type="bibr" target="#b24">[26]</ref> propose to trade performance for low latency, but their use of fully-connected layers results in large model size.</p><p>In short, segmentation-based methods all require heavy post-processing due to the misalignment of representations. They also suffer from the locality of segmentation task, so that they tend to perform worse under occlusions or extreme lighting conditions. Point Detection-based Lane Detection. The success of object detection methods drives researchers to formulate lane detection as to detect lanes as a series of points (e.g., every 10 pixels in the vertical axis). Line-CNN <ref type="bibr" target="#b13">[15]</ref> adapts classic Faster R-CNN <ref type="bibr" target="#b26">[28]</ref> as a one-stage lane line detector, but it has a low inference speed (&lt;30 FPS). Later, LaneATT <ref type="bibr" target="#b31">[33]</ref> adopts a more general one-stage detection approach that achieves superior performance.</p><p>However, these methods have to design heuristic lane anchors, which highly depend on dataset statistics, and require the Non-Maximum Suppression (NMS) as post-processing. On the contrary, we represent lane lines as curves with a fully end-to-end pipeline (anchor-free, NMS-free). Curve-based Lane Detection. The pioneering work <ref type="bibr" target="#b35">[37]</ref> proposes a differentiable least squares fitting module to fit a polynomial curve (e.g., x = ay 3 + by 2 + cy + d) to points predicted by a deep neural network. The PolyLaneNet <ref type="bibr" target="#b30">[32]</ref> then directly learns to predict the polynomial coefficients with simple fully-connected layers. Recently, LSTR <ref type="bibr" target="#b17">[19]</ref> uses transformer blocks to predict polynomials in an endto-end fasion based on the DETR <ref type="bibr" target="#b3">[4]</ref>.</p><p>Curve is a holistic representation of lane line, which naturally eliminates occlusions, requires no post-processing, and can predict a variable number of lines. However, their performance on large and challenging datasets (e.g., CU-Lane <ref type="bibr" target="#b20">[22]</ref> and LLAMAS <ref type="bibr" target="#b2">[3]</ref>) still lag behind methods of other categories. They also suffer from slow convergence (over 2000 training epochs on TuSimple), high latency architecture (e.g., LSTR <ref type="bibr" target="#b17">[19]</ref>  test set (lower is better). Since the official metrics are too lose to show any meaningful difference, we use the fine-grained LPD metric following <ref type="bibr" target="#b30">[32]</ref>. are difficult to optimize for low latency). We attribute their failure to the difficult-to-optimize and abstract polynomial coefficients. We propose to use the parametric B?zier curve, which is defined by actual control points on the image coordinate system 1 , to address these problems. B?zier curve in Deep Learning. To our knowledge, the only known successful application of B?zier curves in deep learning is the ABCNet <ref type="bibr" target="#b18">[20]</ref>, which uses cubic B?zier curve for text spotting. However, their method cannot be directly used for our tasks. First, this method still uses NMS so that it cannot be end-to-end. We show in our work that NMS is not necessary so that our method can be an endto-end solution. Second, it calculates L 1 loss directly on the sparse B?zier control points, which results in difficulties of optimization. We address this problem in our work by leveraging a fine-grained sampling loss. In addition, we propose the feature flip fusion module, which is specifically designed for the lane detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">B?zierLaneNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Preliminaries on B?zier Curve. The B?zier curve's formulation is shown in Equation (1), which is a parametric curve defined by n + 1 control points:</p><formula xml:id="formula_0">B(t) = n i=0 b i,n (t)P i , 0 ? t ? 1,<label>(1)</label></formula><p>where P i is the i ? th control point, b i,n are Bernstein basis polynomials of degree n:</p><formula xml:id="formula_1">b i,n = C i n t i (1 ? t) n?i , i = 0, ..., n.<label>(2)</label></formula><p>We use the classic cubic B?zier curve (n = 3), which is empirically found sufficient for modeling lane lines. It shows better ground truth fitting ability than 3rd order polynomial <ref type="table">(Table 1)</ref>, which is the base function for previous curve-based methods <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b30">32]</ref>. Higher-order curves do not bring substantial gains while the high degrees of freedom leads to instability. All coordinates for points discussed here are relative to the image size (i.e., mostly in range [0, 1]). + <ref type="figure">Figure 2</ref>. Pipeline. Feature from a typical encoder (e.g., ResNet) is strengthened by feature flip fusion, then pooled to 1D and two 1D convolution layers are applied. At last the network predicts B?zier curves through a classification branch and a regression branch.</p><p>The Proposed Architecture. The overall model architecture is shown in <ref type="figure">Figure 2</ref>. Specifically, we use layer-3 feature of ResNets <ref type="bibr" target="#b9">[11]</ref> as backbone following RESA <ref type="bibr" target="#b39">[41]</ref>, but we replace the dilation inside the backbone network by two dilated blocks outside with dilation rates <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">8]</ref> [6]. This strikes a better speed-accuracy trade-off for our method, which leaves a 16? down-sampled feature map with a larger receptive field. We then add the feature flip fusion module (Section 3.2) to aggregate opposite lane features. The enriched feature map (C ? H 16 ? W 16 ) is then pooled to (C ? W 16 ) by average pooling, resulting in W 16 proposals (50 for CULane <ref type="bibr" target="#b20">[22]</ref>). Two 1 ? 3 1D convolutions are used to transform the pooled features, while also conveniently modeling interactions between nearby lane proposals, guiding the network to learn a substitute for the non-maximal suppression (NMS) function. Lastly, the final prediction is obtained by the classification and regression branches (each is only one 1 ? 1 1D convolution). The outputs are W 16 ? 8 for regression of 4 control points, and W 16 ? 1 for existence of lane line object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Flip Fusion</head><p>By modeling lane lines as holistic curves, we focus on the geometric properties of individual lane lines (e.g., thin, long, and continuous). Now we consider the global structure of lanes from a front-mounted camera view in driving scenes. Roads have equally spaced lane lines, which appear symmetrical and this property is worth modeling. For instance, the existence of left ego lane line should very likely indicate its right counterpart, the structure of immediate left lane could help describe the immediate right lane, etc.</p><p>To exploit this property, we fuse the feature map with its horizontally flipped version ( <ref type="figure" target="#fig_1">Figure 3</ref>). Specifically, two separate convolution and normalization layers transform each feature map, they are then added together before a ReLU activation. With this module, we expect the model to base its predictions on both feature maps.</p><p>To account for the slight misalignment of camera captured image (e.g., rotated, turning, non-paired), we apply deformable convolution <ref type="bibr" target="#b40">[42]</ref> with kernel size 3 ? 3 for the flipped feature map while learning the offsets conditioned on the original feature map for feature alignment.</p><p>We add an auxiliary binary segmentation branch (to segment lane line or non-lane line areas, which would be removed after training) to the ResNet backbone, and we expect it to enforce the learning of spatial details. Interestingly, we find this auxiliary branch improves the performance only when it works with the feature fusion. This is because the localization of the segmentation task may provide a more spatially-accurate feature map, which in turn supports accurate fusion between the flipped features.</p><p>Visualizations are shown in <ref type="figure" target="#fig_2">Figure 4</ref>, from which we can see that the flipped feature does correct the error caused by the asymmetry introduced by the car <ref type="figure" target="#fig_2">(Figure 4</ref>(a)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">End-to-end Fit of a B?zier Curve</head><p>Distances Between B?zier Curves. The key to learning B?zier curves is to define a good distance metric measuring the distances between the ground truth curve and prediction. Naively, one can directly calculate the mean L 1 distance between B?zier curve control points, as in ABC-Net <ref type="bibr" target="#b18">[20]</ref>. However, as shown in <ref type="figure" target="#fig_3">Figure 5</ref>(a), a large L 1 error in curvature control points can demonstrate a very small visual distance between B?zier curves, especially on small or medium curvatures (which is often the case for lane lines). Since B?zier curves are parameterized by t ? [0, 1], we propose the more reasonable sampling loss for B?zier curves ( <ref type="figure" target="#fig_3">Figure 5</ref>(b)), by sampling curves at a uniformly spaced set of t values (T ), which means equal curve length between adjacent sample points. The t values can be further transformed by a re-parameterization function f (t). Specifically, given B?zier curves B(t),B(t), the sampling loss L reg is:</p><formula xml:id="formula_2">L reg = 1 n t?T ||B(f (t)) ?B(f (t))|| 1 ,<label>(3)</label></formula><p>where n is the total number of sampled points and is set to 100. We empirically find f (t) = t works well. This simple yet effective loss formulation makes our model easy to converge and less sensitive to hyper-parameters that typically involved in other curved-based or point detection- based methods, e.g., loss weighting for endpoints loss <ref type="bibr" target="#b17">[19]</ref> and line length loss <ref type="bibr" target="#b31">[33]</ref> (see <ref type="figure" target="#fig_3">Figure 5</ref>(b,c)). B?zier Ground Truth Generation. Now we introduce the generation of B?zier curve ground truth. Since lane datasets are currently annotated by on-line key points, we need the B?zier control points for the above sampling loss. Given the annotated points {(k xi , k yi )} m i=1 on one lane line, where (k xi , k yi ) denotes the 2D-coordinates of the i-th point. Our goal is to obtain control points</p><formula xml:id="formula_3">{P i (x i , y i )} n i=1 .</formula><p>Similarly to <ref type="bibr" target="#b18">[20]</ref>, we use standard least squares fitting:</p><formula xml:id="formula_4">? ? ? ? ? P 0 P 1 . . . P n ? ? ? ? ? = ? ? ? ? ? k x0 k y0 k x1 k y1 . . . . . . k xm k ym ? ? ? ? ? ? ? ? ? ? b 0,n (t 0 ) ? ? ? b n,n (t 0 ) b 0,n (t 1 ) ? ? ? b n,n (t 1 ) . . . . . . . . . b 0,n (t m ) ? ? ? b n,n (t m ) ? ? ? ? ? T (4) {t i } m i=0</formula><p>is uniformly sampled from 0 to 1. Different from <ref type="bibr" target="#b18">[20]</ref>, we do not restrict ground truth to have same endpoints as original annotations, which leads to better quality labels. Label and Prediction Matching. After obtaining the ground truth, in training, we perform a one-to-one assignment between G labels and N predictions (G &lt; N ) using optimal bipartite matching, to attain a fully end-toend pipeline. Following Wang et al. <ref type="bibr" target="#b36">[38]</ref>, we find a Gpermutation of N predictions ? ? ? N G that formulates the best bipartite matching: where Q i,?(i) ? [0, 1] represents matching quality of the ith label with the ?(i)-th prediction, based on L 1 distance between curves b i ,b ?(i) (sampling loss) and class scor? p ?(i) . ? is set to 0.8 by default. The above equations can be efficiently solved by the well-known Hungarian algorithm. Wang et al. <ref type="bibr" target="#b36">[38]</ref> also use a spatial prior that restricts the matched prediction to a spatial neighborhood of the label (object center distance, the centerness prior in FCOS <ref type="bibr" target="#b33">[35]</ref>). However, since lots of lanes are long lines with a large slope, this centerness prior is not useful. See Appendix E for more investigations on matching priors. Overall Loss. Other than B?zier curve sampling loss, there is also the classification loss L c for the lane object classification (existence) branch. Since the imbalance between positive and negative examples is not as severe in lane detection as in object detection, instead of the focal loss <ref type="bibr" target="#b14">[16]</ref>, we use the simple weighted binary cross-entropy loss:</p><formula xml:id="formula_5">? = arg max ??? N G G i Q i,?(i) ,<label>(5)</label></formula><formula xml:id="formula_6">Q i,?(i) = p ?(i) 1?? ? 1 ? L 1 b i ,b ?(i) ? ,<label>(6)</label></formula><formula xml:id="formula_7">L cls = ?(y log(p) + w(1 ? y) log(1 ? p)),<label>(7)</label></formula><p>where w is the weighting for negative samples, which is set to 0.4 in all experiments. The loss L seg for the binary segmentation branch (Section 3.2) takes the same format. The overall loss is a weighted sum of all three losses:</p><formula xml:id="formula_8">L = ? 1 L reg + ? 2 L cls + ? 3 L seg ,<label>(8)</label></formula><p>where ? 1 , ? 2 , ? 3 are set to 1, 0.1, 0.75, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>To evaluate the proposed method, we conduct experiments on three well-known datasets: TuSimple [1], CU-Lane <ref type="bibr" target="#b20">[22]</ref> and LLAMAS <ref type="bibr" target="#b2">[3]</ref>. TuSimple dataset was collected on highways with high-quality images, under fair weather conditions. CULane dataset contains more complex urban driving scenarios, including shades, extreme illuminations, and road congestion. LLAMAS is a newly formed large-scale dataset, it is the only lane detection benchmark without public test set labels. Details of these datasets can be found in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evalutaion Metics</head><p>For CULane <ref type="bibr" target="#b20">[22]</ref> and LLAMAS <ref type="bibr" target="#b2">[3]</ref>, the official metric is F1 score from <ref type="bibr" target="#b20">[22]</ref>:</p><formula xml:id="formula_9">F1 = 2 ? Precision ? Recall Precision + Recall ,<label>(9)</label></formula><p>where Precision = T P T P +F P and Recall = T P T P +F N . Lines are assumed to be 30 pixels wide, prediction and ground truth lines with pixel IoU over 0.5 are considered a match.</p><p>For TuSimple [1] dataset, the official metrics include Accuracy, false positive rate (FPR), and false negative rate (FNR). Accuracy is computed as N pred Ngt , where N pred is the number of correctly predicted on-line points and N gt is the number of ground truth on-line points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Fair Comparison. To fairly compare among different stateof-the-art methods, we re-implement representative methods <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b39">41]</ref> in a unified PyTorch framework. We Also provide a semantic segmentation baseline <ref type="bibr" target="#b4">[5]</ref> originally proposed in <ref type="bibr" target="#b20">[22]</ref>. All our implementations do not use val set in training, and tune hyper-parameters only on val set. Some methods with reliable open-source codes are reported from their own codes <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b31">33]</ref>. For platform sensitive metric Frames-Per-Second (FPS), we re-evaluate all reported methods on the same RTX 2080 Ti platform. More details for implementations and FPS tests are in Appendices A to C. Training. We train 400, 36, 20 epochs for TuSimple, CU-Lane, and LLAMAS, respectively (training of our model takes only 12 GPU hours on a single RTX 2080 Ti), and the input resolution is 288?800 for CULane <ref type="bibr" target="#b20">[22]</ref> and 360?640 for others, following common practice. Other than these, all hyper-parameters are tuned on CULane <ref type="bibr" target="#b20">[22]</ref> val set and remain the same for our method across datasets. We use Adam optimizer with learning rate 6 ? 10 ?4 , weight decay 1 ? 10 ?4 , batch size 20, Cosine Annealing learning rate schedule as in <ref type="bibr" target="#b31">[33]</ref>. Data augmentation includes random affine transforms, random horizontal flip, and color jitter. Curve-based  Testing. No post-processing is required for curve methods. Standard Gaussian blur and row selection post-processing is applied to segmentation methods. NMS is used for LaneATT <ref type="bibr" target="#b31">[33]</ref>, while we remove its post-inference B-Spline interpolation in CULane <ref type="bibr" target="#b20">[22]</ref>, to align with our framework.</p><formula xml:id="formula_10">PolyLaneNet (EfficientNet-B0) [32]** ? ? ? ? ? ? ? ? ? ? ? 2695 93.36 0.094 0.093 LSTR (ResNet-18, 1?) [19]* ? ? ? ? ? ? ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons</head><p>Overview. Experimental results are shown in <ref type="table" target="#tab_4">Tables 3  and 4</ref>. TuSimple [1] is a small dataset that features clearweather highway scenes and has a relatively easy metric, most methods thrive in this dataset. Thus, we mainly focus on the other two large-scale datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">22]</ref>, where there is still a rather clear difference between methods. For highperformance methods (&gt; 70% F1 on CULane <ref type="bibr" target="#b20">[22]</ref>), we also show efficiency metrics (FPS, Parameter count) in <ref type="table" target="#tab_9">Table 5</ref>.</p><p>Comparison with Curve-based Methods. As shown in <ref type="table" target="#tab_4">Tables 3 and 4</ref>, in all datasets, B?zierLaneNet outperforms previous curve-based methods <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b30">32]</ref> by a clear margin, advances the state-of-the-art of curve-based methods by 6.85% on CULane <ref type="bibr" target="#b20">[22]</ref> and 6.77% on LLAMAS <ref type="bibr" target="#b2">[3]</ref>. Thanks to our fully convolutional and fully end-toend pipeline, B?zierLaneNet runs over 2? faster than LSTR <ref type="bibr" target="#b17">[19]</ref>. LSTR has a speed bottleneck from transformer architecture, the 1? and 2? model have FPS 98 and 97, respectively <ref type="bibr" target="#b1">2</ref> . While curves are difficult to learn, our method converges 4-5? faster than LSTR. For the first time, an elegant curve-based method can challenge well-designed segmentation methods or point detection methods on these datasets <ref type="bibr" target="#b1">2</ref> The original 420 FPS report from LSTR paper <ref type="bibr" target="#b17">[19]</ref>, is throughput with batch size 16, detailed discussions in Appendix A.</p><p>while showing a favorable trade-off, with an acceptable convergence time.</p><p>Comparison with Segmentation-based Methods. These methods tend to have a low speed due to recurrent feature aggregation <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b39">41]</ref>, and the use of high-resolution feature map <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b39">41]</ref>. B?zierLaneNet outperforms them in both speed and accuracy. Our small models even compare favorably against RESA <ref type="bibr" target="#b39">[41]</ref> and SCNN <ref type="bibr" target="#b20">[22]</ref> with large ResNet-101 backbone, surpassing them in CULane <ref type="bibr" target="#b20">[22]</ref> with a clear margin (1 ? 2%). On LLAMAS <ref type="bibr" target="#b2">[3]</ref>, where the dataset restricts testing on 4 center lines, the segmentation approach shows strong performance <ref type="table" target="#tab_6">(Table 4</ref>). Nevertheless, our ResNet-34 model still outperforms SCNN by 0.92%.</p><p>UFLD <ref type="bibr" target="#b24">[26]</ref> reformulates segmentation to row-wise classification on a down-sampled feature map to achieve fast speed, at the cost of accuracy. Compared to us, UFLD (ResNet-34) is 0.9% lower on CULane Normal, while 7.4%, 3.0%, 3.2% worse on Shadow, Crowd, Night, respectively. Overall, our method with the same backbones outperforms UFLD by 3 ? 5%, while being faster on ResNet-34. Besides, UFLD uses large fully-connected layers to optimize latency, which causes a huge model size (the largest in <ref type="table" target="#tab_9">Table 5</ref>).</p><p>A drawback for all segmentation methods is the weaker performance on Dazzle Light. Per-pixel (or per-pixel grid for UFLD <ref type="bibr" target="#b24">[26]</ref>) segmentation methods may rely on information from local textures, which is destroyed by extreme exposure to light. While our method predicts lane lines as holistic curves, hence robust to changes in local textures.  CurveLanes-NAS also performs worse under occlusions, a similar drawback as the segmentation methods without recurrent feature fusion <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">26]</ref>. As shown in Recently, LaneATT <ref type="bibr" target="#b31">[33]</ref> achieves higher performance with a point detection network. However, their design is not fully end-to-end (requires Non-Maximal Suppression (NMS)), based on heuristic anchors (&gt;1000), which are calculated directly from the dataset's statistics, thus may systematically pose difficulties in generalization. Still, with ResNet-34, our method outperforms LaneATT on the LLA-MAS <ref type="bibr" target="#b2">[3]</ref> test server (1.43%), with a significantly higher recall (3.58%). We also achieve comparable performance to LaneATT on TuSimple [1] using only the train set, and only ? 1% worse on CULane. Our method performs significantly better in Dazzle Light (3.3% better), comparably in Night (0.4% lower). It also has a lower False Positive (FP) rate on Crossroad scenes (Cross), even though LaneATT shows an extremely low-FP characteristic (large Precision-Recall gap in <ref type="table" target="#tab_6">Table 4</ref>). Methods that rely on heuristic anchors <ref type="bibr" target="#b31">[33]</ref> or heuristic decoding process <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b39">41]</ref> tend to have more false predictions in this scene. Moreover, the NMS is a sequential process that could have unstable runtime in real-world applications. Even when NMS was not evaluated on real inputs, our models are 29%, 28% faster, have 2.9?, 2.3? fewer parameters, compared to LaneATT on ResNet-18 and ResNet-34 backbones, respectively.</p><p>To summarize, previous curve-based methods (Poly-LaneNet <ref type="bibr" target="#b30">[32]</ref>, LSTR <ref type="bibr" target="#b17">[19]</ref>) have significantly worse performance. Fast methods trades either accuracy (UFLD <ref type="bibr" target="#b24">[26]</ref>) or model size (UFLD <ref type="bibr" target="#b24">[26]</ref>, LaneATT <ref type="bibr" target="#b31">[33]</ref>) for speed. Accurate   methods either discards the end-to-end pipeline (LaneATT <ref type="bibr" target="#b31">[33]</ref>), or entirely fails the real-time requirement (SCNN <ref type="bibr" target="#b20">[22]</ref>, RESA <ref type="bibr" target="#b39">[41]</ref>). While our B?zierLaneNet is fully endto-end, fast (&gt;150 FPS), light-weight (&lt;10 million parameters) and maintains consistent high accuracy across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis</head><p>Although we develop our method by tuning on the val set, we re-run ablation studies with ResNet-34 backbone (including our full method) and report performance on the CULane test set for clear comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curve representation F1</head><p>Cubic B?zier curve baseline 68.89 3rd Polynomial baseline 1.49</p><p>B?zierLaneNet 75.41 3rd Polynomial from B?zierLaneNet 5.01 <ref type="table">Table 6</ref>. Curve representations. Baselines directly predict curve coefficients without feature flip fusion.</p><p>Importance of Parametric B?zier Curve. We first replace the B?zier curve prediction with a 3rd order polynomial, adding auxiliary losses for start and end points. As shown in <ref type="table">Table 6</ref>, polynomials catastrophically fail to converge in our fully convolutional network, even when trained with 150 epochs (details in Appendix B.8). Then we consider modifying the LSTR <ref type="bibr" target="#b17">[19]</ref> to predict cubic B?zier curves, the performance is similar to predicting polynomials. We conclude that heavy MLP may be necessary to learn polynomials <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b30">32]</ref>, while predicting B?zier control points from position-aware CNN is the best choice. The transformerbased LSTR decoder destroys the fine spatial information, suppresses the advancement of curve function. Feature Flip Fusion Design. As shown in <ref type="table" target="#tab_10">Table 7</ref>, feature flip fusion brings 4.07% improvement. We also find that the auxiliary segmentation loss can regularize and increase  <ref type="bibr" target="#b18">[20]</ref>. SP: The proposed sampling loss. Flip: The feature flip fusion module. Deform: Employ the deformable convolution in feature flip fusion. Seg: Auxiliary segmentation loss. the performance further, by 2.45%. It is worth noting that auxiliary loss only works with feature fusion, it can lead to degenerated results when directly applied on the baseline (?3.07%). A standard 3 ? 3 convolution performs worse than deformable convolution, by 2.68% and 1.44%, before and after adding the auxiliary segmentation loss, respectively. We attribute this to the effects of feature alignment. B?zier Curve Fitting Loss. As shown in <ref type="table" target="#tab_10">Table 7</ref>, replacing the sampling loss by direct loss on control points lead to inferior performance (?5.15% in the baseline setup). Inspired by the success of IoU loss in object detection. We also implemented a IoU loss (formulas in Appendix D) for the convex hull of B?zier control points. However, the convex hull of close-to-straight lane lines are too small, the IoU loss is numerically unstable, thus failing to facilitate the sampling loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Aug F1 Importance of Strong Data Augmentation. Strong data augmentation is defined by a series of affine transforms and color distortions, the exact policy may slightly vary for different methods. For instance, we use random affine transform, random horizontal flip, and color jitter. LSTR <ref type="bibr" target="#b17">[19]</ref> also uses random lighting. Default augmentation includes only a small rotation (3 degrees). As shown in <ref type="table" target="#tab_11">Table 8</ref>, strong augmentation is essential to avoid over-fitting for curve-based methods.</p><p>For segmentation-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b39">41]</ref>, we fast validated strong augmentation on the smaller TuSimple [1] dataset. All shows a 1 ? 2% degradation. This suggests that they may be robust due to per-pixel prediction and heuristic post-processing. But they highly rely on learning the distribution of local features such as texture, which could become confusing by strong augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Limitations and Discussions</head><p>Curves are indeed a natural representation of lane lines. However, their elegance in modeling inevitably brings a drawback. It is difficult for the curvature coefficients to generalize when the data distribution is highly biased (almost all lane lines are straight lines in CULane). Our B?zier curve approach has already alleviated this problem to some extent and has achieved an acceptable performance (62.45) in CULane Curve. On datasets such as TuSimple and LLA-MAS <ref type="bibr">[1,</ref><ref type="bibr" target="#b2">3]</ref>, where the curvature distribution is fair enough for learning, our method achieves even better performance. To handle broader corner cases, e.g., sharp turns, blockages and bad weather, datasets such as <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b37">39</ref>] may be useful.</p><p>The feature flip fusion is specifically designed for a front-mounted camera, which is the typical use case of deep lane detectors. Nevertheless, there is still a strong inductive bias by assuming scene symmetry. In future work, it would be interesting to find a replacement for this module, to achieve better generalization and to remove the deformable convolution operation, which poses difficulty for effective integration into edge devices such as Jetson.</p><p>More discussions in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have proposed B?zierLaneNet: a novel fully end-to-end lane detector based on parametric B?zier curves. The on-image B?zier curves are easy to optimize and naturally model the continuous property of lane lines, without heavy designs such as recurrent feature aggregation or heuristic anchors. Besides, a feature flip fusion module is proposed. It efficiently models the symmetry property of the driving scene, while also being robust to slight asymmetries by using deformable convolution. The proposed model has achieved favorable performance on three datasets, defeating all existing methods on the popular LLAMAS benchmark. It is also both fast (&gt;150 FPS) and light-weight (&lt;10 million parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>This work has been sponsored by National Key Research and Development Program of China (2019YFC1521104), National Natural Science Foundation of China (61972157, 72192821), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), Shanghai Science and Technology Commission (21511101200), Art major project of National Social Science Fund (I8ZD22), and SenseTime Collaborative Research Grant. We thank Jiaping Qin for guidance on road design and geometry, Yuchen Gong and Pan Chen for helping with CAM visualizations, Zhijun Gong, Jiachen Xu and Jingyu Gong for insightful discussions about math, Fengqi Liu for providing GPUs, Lucas Tabelini for cooperation in evaluating <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b31">33]</ref>, and the CVPR reviewers for constructive comments.</p><p>Appendix Overview. The Appendix is organized as follows: Appendix A describes the FPS test protocol and environments; Appendix B introduces implementation details for each compared method (including ours in Appendix B.8); Appendix C provides implementation details for B?zier curves, including sampling, ground truth generation and transforms; Appendix D formulates the IoU loss for B?zier curves and discusses why it failed; Appendix E explores matching priors other than the centerness prior; Appendix F shows extra ablation studies on datasets other than CULane <ref type="bibr" target="#b20">[22]</ref>, to verify the generalization of feature flip fusion; Appendix G discusses limitations and recognizes new progress in the field; Appendix H presents qualitative results from our method, visualized on three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. FPS Test Protocol</head><p>Let one Frames-Per-Second (FPS) test trial be the average runtime of 100 consecutive model inference with its Py-Torch <ref type="bibr" target="#b22">[24]</ref> implementation, without calculating gradients. The input is a 3x360x640 random Tensor (some use all 1 <ref type="bibr" target="#b31">[33]</ref>, which does not have impact on speed). Note that all methods do not use optimization from packages like Ten-sorRT. We wait for all CUDA kernels to finish before counting the whole runtime. We use Python time.perf counter() since it is more precise than time.time(). For all methods, the FPS is reported as the best result from 3 trials.</p><p>Before each test trial, at least 10 forward pass is conducted as warm-up of the device. For each new method to be tested, we keep running warm-up trials of a recorded method until the recorded FPS is reached again, so we can guarantee a similar peak machine condition as before. Evaluation Environment. The evaluation platform is a 2080 Ti GPU (standard frequency), on a Intel Xeon-E3 CPU server, with CUDA 10.2, CuDNN 7.6.5, PyTorch 1.6.0. FPS is a platform-sensitive metric, depending on GPU frequency, condition, bus bandwidth, software versions, etc. Also using 2080 Ti, Tabelini et al. <ref type="bibr" target="#b31">[33]</ref> can achieve a better peak performance for all methods. Thus we use the same platform for all FPS tests, to provide fair comparisons. Remark. Note that FPS (image/s) is different from throughput (image/s). Since FPS restricts batch size to 1, which better simulates the real-time application scenario. While throughput considers a batch size more than 1. LSTR <ref type="bibr" target="#b17">[19]</ref> reported a 420 FPS for its fastest model, which is actually throughput with batch size 16. Our re-tested FPS is 98.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Specifications for Compared Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Segmentation Baseline</head><p>The segmentation baseline is based on DeeplabV1 <ref type="bibr" target="#b4">[5]</ref>, originally proposed in SCNN <ref type="bibr" target="#b20">[22]</ref>. It is essentially the orig-inal DeeplabV1 without CRF, lanes are considered as different classes, and a separate lane existence branch (a series of convolution, pooling and MLP) is used to facilitate lane post-processing. We optimized its training and testing scheme based on recent advances <ref type="bibr" target="#b39">[41]</ref>. Re-implemented in our codebase, it attains higher performance than what recent papers usually report. Post-processing. First, the existence of a lane is determined by the lane existence branch. Then, the predicted per-pixel probability map is interpolated to the input image size. After that, a 9 ? 9 Gaussian blur is applied to smooth the predictions. Finally, for each existing lane class, the smoothed probability map is traversed by pre-defined Y coordinates (quantized), and corresponding X coordinates are recorded by the maximum probability position on the row (provided it passes a fixed threshold). Lanes with less than two qualified points are simply discarded. Data Augmentation. We use a simple random rotation with small angles (3 degrees), then resize to input resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. SCNN</head><p>Our SCNN <ref type="bibr" target="#b20">[22]</ref> is re-implemented from the Torch7 version of the official code. Advised by the authors, we added an initialization trick for the spatial CNN layers, and learning rate warm-up, to prevent gradient explosion caused by recurrent feature aggregation. Thus, we can safely adjust the learning rate. Our improved SCNN achieves significantly better performance than the original one.</p><p>Some may find reports of 96.53 accuracy of SCNN on TuSimple. However, that was a competition entry trained with external data. We report SCNN with ResNet backbones, trained with the same data as other re-implemented methods in our codebase. Post-processing. Same as Appendix B.1. Data Augmentation. Same as Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. RESA</head><p>Our RESA <ref type="bibr" target="#b39">[41]</ref> is implemented based on its published paper. A main difference to the official code release is we do not cutout no-lane areas (in each dataset, there is a certain height range for lane annotation). Because that trick is dataset specific and not generalizable, we do not use that for all compared methods. Other differences are all validated to have better performance than the official code, at least on the CULane val set. Post-processing. Same as Appendix B.1. Data Augmentation. Same as Appendix B.1. The original RESA paper <ref type="bibr" target="#b39">[41]</ref> also apply random horizontal flip, which was found ineffective in our re-implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. UFLD</head><p>Ultra Fast Lane Detection (UFLD) <ref type="bibr" target="#b24">[26]</ref> is reported from their paper and open-source code. Since TuSimple FP and FN information is not in the paper, and training from source code leads to very high FP rate (almost 20%), we did not report their performance on this dataset. We adjusted its profiling scripts to calculate number of parameters and FPS in our standard. Post-processing. Since this method uses gridding cells (each cell is equivalent to several pixels in a segmentation probability map), each point's X coordinate is calculated as the expectation of locations (cells from the same row), i.e. a weighted average by probability. Differently from segmentation post-processing, it is possible to be efficiently implemented. Data Augmentation. Augmentations include random rotation and some form of random translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. PolyLaneNet</head><p>PolyLaneNet <ref type="bibr" target="#b30">[32]</ref> is reported from their paper and opensource code. We added a profiling script to calculate number of parameters and FPS in our standard. Post-processing. This method requires no post-processing. Data Augmentation. Augmentations include large random rotation (10 degrees), random horizontal flip and random crop. They are applied with a probability of 10 11 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. LaneATT</head><p>LaneATT <ref type="bibr" target="#b31">[33]</ref> is reported from their paper and opensource code. We adjusted its profiling scripts to calculate parameters and FPS in our standard. Post-processing. Non-Maximal Suppression (NMS) is implemented by a customized CUDA kernel. An extra interpolation of lanes by B-Spline is removed both in testing and profiling, since it is slowly executed on CPU and provides little improvement (? 0.2% on CULane). Data Augmentation. LaneATT uses random affine transforms including scale, translation and rotation. While it also uses random horizontal flip. Followup. We did not have time to validate the reimplementation of LaneATT in our codebase, prior the submission deadline. Therefore, the LaneATT performance is still reported from the official code. Our re-implementation indicates that all LaneATT results are reproducible except for the ResNet-34 backbone on CULane, which is slightly outside the standard deviation range, but still reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7. LSTR</head><p>LSTR <ref type="bibr" target="#b17">[19]</ref> is re-implemented in our codebase. All ResNet backbone methods start from ImageNet <ref type="bibr" target="#b12">[14]</ref> pretraining. While LSTR <ref type="bibr" target="#b17">[19]</ref> use 256 channels ResNet-18 for CULane (2?), 128 channels for other datasets (1?), which makes it impossible to use off-the-shelf pre-trained ResNets. Although whether ImageNet pre-training helps lane detection is still an open question. Our reported performance of LSTR on CULane, is the first documented report of LSTR on this dataset. With tuning of hyper-parameters (learning rate, epochs, prediction threshold), bug fix (the original classification branch has 3 output channels, which should be 2), we achieve 4% better performance on CU-Lane than the authors' trial. Specifically, we use learning rate 2.5 ? 10 ?4 with batch size 20. 150 and 2000 epochs, 0.95 and 0.5 prediction thresholds, for CULane and TuSimple. The lower threshold in TuSimple is due to the official test metric, which significantly favors a high recall. However, for real-world applications, a high recall leads to high False Positive rate, which is undesired.</p><p>We divide the curve loss weighting by 10 with our LSTR-Beizer ablation, since there were 100 sample points with both X and Y coordinates to fit, that is a loss scale about 10 times the original loss (LSTR loss takes summation of point L1 distances instead of average). This modulation achieves a similar loss landscape to original LSTR. Post-processing. This method requires no post-processing. Data Augmentation. Data augmentation includes Poly-LaneNet's (Appendix B.5), then appends random color distortions (brightness, contrast, saturation, hue) and random lighting by a light source calculated from the COCO dataset <ref type="bibr" target="#b15">[17]</ref>. That is by far the most complex data augmentation pipeline in this research field, we have validated that all components of this pipeline helps LSTR training. Remark. The polynomial coefficients of LSTR are unbounded, which leads to numerical instability (while the bipartite matching requires precision), and high failure rate of training. The failure rate of fp32 training on CULane is ? 30%. This is circumvented in B?zierLaneNet, since our L1 loss can be bounded to [0, 1] without influence on learning (control points easily converges to on-image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.8. B?zierLaneNet</head><p>B?zierLaneNet is implemented in the same code framework where we re-implemented other methods. Same as LSTR, the default prediction threshold is set to 0.95, while 0.5 is used for TuSimple <ref type="bibr">[1]</ref>. Post-processing. This method requires no post-processing. Data Augmentation. We use augmentations similar to LSTR (Appendix B.7). Concretely, we remove the random lighting from LSTR (to strictly avoid using knowledge from external data), and replace the PolyLaneNet 10 11 chance augmentations with random affine transforms and random horizontal flip, like LaneATT (Appendix B.6). The random affine parameters are: rotation (10 degrees), translation (maximum 50 pixels on X, 20 on Y), scale (maximum 20%). Polynomial Ablations. For the polynomial ablations <ref type="table" target="#tab_10">(Table  7)</ref>, we modified the network to predict 6 coefficients for 3rd order Polynomial (4 curve coefficients and start/end Y coordinates). Extra L1 losses are added for the start/end Y coordinates similar to LSTR <ref type="bibr" target="#b17">[19]</ref>. With extensive tryouts (ad-justing learning rate, loss weightings, number of epochs), even at the full B?zierLaneNet setup, with 150 epochs on CULane, the models still can not converge to a good enough solution. In other word, not precise enough to pass the CULane metric. The sampling loss on polynomial curves can only get to 0.02, which means 0.02 ? 1640pixels = 32.8pixels average X coordinate error on training set. CU-Lane requires a 0.5 IoU between curves, which are enlarged to 30 pixels wide, thus at least around 10 pixels average error is needed to get meaningful results. By loosen up the IoU requirement to 0.3, we can get F1 score 15.82 for "3rd Polynomial from B?zierLaneNet". Although the reviewing committee suggested adding simple regularization for this ablation to converge, regretfully we failed to do this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. B?zier Curve Implementation Details</head><p>Fast Sampling. The sampling of B?zier curves may seem tiresome due to the complex Bernstein basis polynomials. To fast sample a B?zier curve by a series of fixed t values, simply pre-compute the results from Bernstein basis polynomials, thus only one simple matrix multiplication is left. Remarks on GT Generation. The ground truth of B?zier curves are generated with least squares fitting, a common technique for polynomials. We use it for its simplicity and the fact that it already shows near-perfect lane line fitting ability (99.996 and 99.72 F1 score on CULane test and LLAMAS val, respectively). However, it is not an ideal algorithm for parametric curves. There is a whole research field for fitting B?zier curves better than least squares <ref type="bibr" target="#b21">[23]</ref>. B?zier Curve Transform. Another implementation difficulty on B?zier curves is how to apply affine transform (for transforming ground truth curves in data augmentation). Mathematically, affine transform on the control points is equivalent to affine transform on the entire curve. However, translation or rotation can move control points out of the image. In this case, a cutting of B?zier curves is required. The classical De Casteljau's algorithm is used for cutting an on-image B?zier curve segment. Assume a continuous onimage segment, valid sample points with minimum boundary t = t 0 , maximum boundary t = t 1 . The formula to cut a cubic B?zier curve defined by control points P 0 , P 1 , P 2 , P 3 to its on-image segment P 0 , P 1 , P 2 , P 3 , is derived as:</p><formula xml:id="formula_11">P 0 = u 0 u 0 u 0 P 0 + (t 0 u 0 u 0 + u 0 t 0 u 0 + u 0 u 0 t 0 )P 1 + (t 0 t 0 u 0 + u 0 t 0 t 0 + t 0 u 0 t 0 )P 2 + t 0 t 0 t 0 P 3 , P 1 = u 0 u 0 u 1 P 0 + (t 0 u 0 u 1 + u 0 t 0 u 1 + u 0 u 0 t 1 )P 1 + (t 0 t 0 u 1 + u 0 t 0 t 1 + t 0 u 0 t 1 )P 2 + t 0 t 0 t 1 P 3 , P 2 = u 0 u 1 u 1 P 0 + (t 0 u 1 u 1 + u 0 t 1 u 1 + u 0 u 1 t 1 )P 1 + (t 0 t 1 u 1 + u 0 t 1 t 1 + t 0 u 1 t 1 )P 2 + t 0 t 1 t 1 P 3 , P 3 = u 1 u 1 u 1 P 0 + (t 1 u 1 u 1 + u 1 t 1 u 1 + u 1 u 1 t 1 )P 1 + (t 1 t 1 u 1 + u 1 t 1 t 1 + t 1 u 1 t 1 )P 2 + t 1 t 1 t 1 P 3 ,<label>(10)</label></formula><p>where u 0 = 1 ? t 0 , u 1 = 1 ? t 1 . This formula can be efficiently implemented by matrix multiplication. The possibility of noncontinuous cubic B?zier segment on lane detection datasets is extremely low and thus ignored for simplicity. If it does happen, Equation (10) will not change the curve, while our network can also predict out-of-image control points, which still fit the on-image lane segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. IoU Loss for B?zier Curves</head><p>Here we briefly introduce how we formulated the IoU loss between B?zier curves. Before diving into the algorithm, there are two preliminaries.</p><p>? Polar sort: By anchoring on an arbitrary point inside the N-sided polygon with vertices</p><formula xml:id="formula_12">c i (x i , y i ) N i=1</formula><p>(normally the mean coordinate between vertices c = ( 1</p><formula xml:id="formula_13">N N i=1 x i , 1 N N i=1 y i ))</formula><p>, vertices are sorted by its atan2 angles. This will return a clockwise or counterclockwise polygon.</p><p>? Convex polygon area: A sorted convex polygon can be efficiently cut into consecutive triangles by simple indexing operations. The convex polygon area is the sum of these triangles. The area S of triangle ((x 1 , y 1 ), (x 2 , y 2 ), (x 3 , y 3 )) is:</p><formula xml:id="formula_14">S = 1 2 |x 1 (y 2 ? y 3 ) + x 2 (y 3 ? y 1 ) + x 3 (y 1 ? y 2 )|.</formula><p>Assume we have two convex hulls from B?zier curves (there are a lot of convex hull algorithms). Now the IoU between B?zier curves are converted to IoU between convex polygons. Based on the simple fact that the intersection of convex polygons is still a convex polygon, after polar sorting all the convex hulls and determining the intersected polygon, we can easily formulate IoU calculations as a series of convex polygon area calculations. The difficulty lies in how to efficiently determine the intersection between convex polygon pairs.</p><p>Consider two intersected convex polygons, their intersection includes two types of vertices:</p><p>? Intersections: intersection points between edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Insiders: vertices inside/on both polygons.</head><p>For Intersections, we first represent every polygon edge as the general line equation: ax + by = c. Then, for line a 1 x + b 1 y = c 1 and line a 2 x + b 2 y = c 2 , the intersection (x , y ) is calculated by:</p><formula xml:id="formula_15">x = (b 2 c 1 ? b 1 c 2 )/det y = (a 1 c 2 ? a 2 c 1 )/det,<label>(11)</label></formula><p>where det = a 1 b 2 ? a 2 b 1 . All (x , y ) that is on the respective line segments are Intersections. For Insiders, there is a certain definition:</p><p>Def. 1 For a convex polygon, point P (x, y) on the same side of each edge is inside the polygon.</p><p>A sorted convex polygon is a series of edges (line segments defined by P 0 (x 0 , y 0 ), P 1 (x 1 , y 1 )), the equation to decide which side a point is to a line segment is as follows:</p><formula xml:id="formula_16">sign = (y ? y 0 )(x 1 ? x 0 ) ? (x ? x 0 )(y 1 ? y 0 ). (12)</formula><p>sign &gt; 0 means P is on the right side, sign &lt; 0 is the left side, and sign = 0 means P is on the line segment. Note that equality is not a stable operation for float computations. But there are simple ways to circumvent that in coding, which we will not elaborate here.</p><p>There are other ways to determine Intersections and Insiders, but the above formulas can be efficiently implemented with matrix operations and indexing, making it possible to quickly train networks with batched inputs.</p><p>Finally, after being able to compute convex polygon intersections and areas, the Generalized IoU loss (GIoU) is simply (as in <ref type="bibr" target="#b27">[29]</ref>): The enclosing convex object C can be computed as the convex hull of two convex polygons, or upper-bounded by a enclosing rectangle. We implement the IoU computation purely in PyTorch <ref type="bibr" target="#b22">[24]</ref>, the runtime for our implementation is only about 5? the runtime of rectangle IoU loss computation.</p><p>However, lane lines are mostly straight based on road design regulations <ref type="bibr">[7,</ref><ref type="bibr" target="#b34">36]</ref>. This leads to extremely small convex hull area for B?zier curves, thus introduces numerical instabilities in optimization. Although succeeded in a toy polygon fitting experiment, we currently failed to observe the loss's convergence to help learning on lane datasets. Instead of the centerness prior, we explore a local maximum prior, i.e., restricts matched prediction to have a local maximum classification logit. This prior can facilitate the model to understand the spatially sparse structure of lane lines. As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, the learned feature activation for classification logits exhibits a similar structure as an actual driving scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. GT and Prediction Matching Prior</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Extra Results</head><p>TuSimple [1] LLAMAS <ref type="bibr" target="#b2">[3]</ref> B?zier Baseline 93.36 95.27 + Feature Flip Fusion 95.26 (+1.90) 96.00 (+0.73) <ref type="table">Table 9</ref>. Ablation study on TuSimple (test set Accuracy) and LLA-MAS (val set F1), before and after adding the Feature Flip Fusion module. Reported 3-times average with the ResNet-34 backbone, since ablations often are not stable enough on these datasets to exhibit a clear difference between methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Discussions</head><p>There exists a primitive application of lane detectors from lateral views to estimate the distance to the border of the drivable area <ref type="bibr" target="#b8">[10]</ref>, which contradicts the use of feature flip fusion. In this case, possibly a lower order B?zier curve baseline (with row-wise instead of column-wise pooling) would suffice. This is out of the focus of this paper. Recent Progress. Recently, others have explored alternative lane representation or formulation methods that do not fully fit in the three categories (segmentation, point detection, curve). Instead of the popular top-down regime, <ref type="bibr" target="#b25">[27]</ref> propose a bottom-up approach that focus on local details. <ref type="bibr" target="#b16">[18]</ref> achieve state-of-the-art performance, but the complex conditional decoding of lane lines results in unstable runtime depending on the input image, which is not desirable for a real-time system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Qualitative Results</head><p>Qualitative results are shown in <ref type="figure" target="#fig_9">Figure 7</ref>, from our ResNet-34 backbone models. For each dataset, 4 results are shown in two rows: first row shows qualitative successful predictions; second row shows typical failure cases. TuSimple. As shown in <ref type="figure" target="#fig_9">Figure 7</ref>(a), our model fits highway curves well, only slight errors are seen on the far side where image details are destroyed by projection. Our typical failure case is a high FP rate, mostly attributed to the use of low threshold (Appendix B.8). However, in the bottomright wide road scene, our FP prediction is actually a meaningful lane line that is ignored in center line annotations. CULane. As shown in <ref type="figure" target="#fig_9">Figure 7(b)</ref>, most lanes in this dataset are straight. Our model can make accurate predictions under heavy congestion (top-left) and shadows (topright, shadow cast by trees). A typical failure case is inaccurate prediction under occlusion (second row), in these cases   one often cannot visually tell which one is better (ground truth or our FP prediction). LLAMAS. As shown in <ref type="figure" target="#fig_9">Figure 7</ref>(c), our method performs accurate for clear straight-lines (top-left), and also good for large curvatures in a challenging scene almost entirely cov-ered by shadow. In bottom-left image, our model fails in a low-illumination, tainted road. While in the other lowillumination scene (bottom-right), the unsupervised annotation from LIDAR and HD-map is misled by the white arrow (see the zigzag shape of the right-most blue line).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Lane detection strategies. Segmentation-based and point detection-based representations are local and indirect. The abstract coefficients (a, b, c, d) used in polynomial curve are hard to optimize. The cubic B?zier curve is defined by 4 actually existing control points, which roughly fit line shape and wrap the lane line in its convex hull (dashed red lines). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>+Figure 3 .</head><label>3</label><figDesc>Feature flip fusion. Alignment is achieved by calculating deformable convolution offsets, conditioned on both the flipped and original feature map. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Grad-CAM [31] visualization on the last layer of ResNet backbone. (a) Our model can infer existence of an ill-marked lane line, from clear markings and cars around the opposite line. Note that the car is deviated to the left, this scene was not captured with perfect symmetry. (b) When entire road lacks clear marking, both sides are used for a better prediction. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Lane loss functions. (a) The L1 distance of control points is not highly correlated with the actual distance between curves. (b) The proposed sampling loss is one unified distance metric by t-sampling. (c) Typical loss for polynomial regression<ref type="bibr" target="#b17">[19]</ref>, at least 3 separate losses are required: y-sampling loss, y start point loss, y end point loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Method FPS ?</head><label></label><figDesc>Params (M) ? Segmentation-based (ignored post-processing time)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>input:B| 3 .</head><label>3</label><figDesc>Two arbitrary convex shapes: A, B ? S ? R n output: GIoU 1. For A and B, find the smallest enclosing convex object C, where C ? S ? R n 2. IoU = |A ? B| |A ? GIoU = IoU ? |C\(A ? B)| |C| Union is computed as A ? B = A + B ? A ? B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Logits activation statistics (1 ? W 16 ) on CULane<ref type="bibr" target="#b20">[22]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results from B?zierLaneNet (ResNet-34) on val sets. False Positives (FP) are marked by red, True Positives (TP) are marked by green, ground truth are drawn in blue. Blue lines that are barely visible are precisely covered by green lines. B?zier curve control points are marked with solid circles. Images are slightly resized for alignment. Best viewed in color, in 2? scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>uses transformer blocks which</figDesc><table><row><cell>n</cell><cell cols="2">B?zier Polynomial</cell></row><row><cell cols="2">2nd 0.653</cell><cell>0.945</cell></row><row><cell cols="2">3rd 0.471</cell><cell>0.558</cell></row><row><cell cols="2">4th 0.315</cell><cell>0.330</cell></row><row><cell cols="3">Table 1. Comparison of n-order B?zier curves and polynomials</cell></row><row><cell>(x = n i=0 aiy</cell><cell></cell></row></table><note>i ) on TuSimple [1]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>CULane [22] 88880 9675 34680 590 ? 1640 ? 4 LLAMAS [3] 58269 20844 20929 717 ? 1276 ? 4</figDesc><table><row><cell>Dataset</cell><cell>Train</cell><cell>Val</cell><cell>Test</cell><cell cols="2">Resolution #Lines</cell></row><row><cell>TuSimple [1]</cell><cell>3268</cell><cell>358</cell><cell cols="2">2782 720 ? 1280</cell><cell>? 5</cell></row></table><note>* Table 2. Details of datasets. *Number of lines in LLAMAS dataset is more than 4, but official metric only evaluates 4 lines.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results on test set of CULane<ref type="bibr" target="#b20">[22]</ref> and TuSimple [1]. *reproduced results in our code framework, best performance from three random runs. **reported from reliable open-source codes from the authors.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Comparison with Point Detection-based Methods. Xu et al. [39] finds a series of point detection-based models with</figDesc><table><row><cell></cell><cell></cell><cell cols="2">LLAMAS [3]</cell><cell></cell></row><row><cell>Method</cell><cell>Ep.</cell><cell>F1</cell><cell cols="2">Precision Recall</cell></row><row><cell>Segmentation-based</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline (ResNet-34)*</cell><cell>18</cell><cell>93.43</cell><cell>92.61</cell><cell>94.27</cell></row><row><cell>SCNN (ResNet-34) [22]*</cell><cell>18</cell><cell>94.25</cell><cell>94.11</cell><cell>94.39</cell></row><row><cell>Point detection-based</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LaneATT (ResNet-18) [33]**</cell><cell>15</cell><cell>93.46</cell><cell>96.92</cell><cell>90.24</cell></row><row><cell>LaneATT (ResNet-34) [33]**</cell><cell>15</cell><cell>93.74</cell><cell>96.79</cell><cell>90.88</cell></row><row><cell>LaneATT (ResNet-122) [33]**</cell><cell>15</cell><cell>93.54</cell><cell>96.82</cell><cell>90.47</cell></row><row><cell>Curve-based</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PolyLaneNet (EfficientNet-B0) [32]** 75</cell><cell>88.40</cell><cell>88.87</cell><cell>87.93</cell></row><row><cell>B?zierLaneNet (ResNet-18)</cell><cell>20</cell><cell>94.91</cell><cell>95.71</cell><cell>94.13</cell></row><row><cell>B?zierLaneNet (ResNet-34)</cell><cell cols="2">20 95.17</cell><cell>95.89</cell><cell>94.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Results from LLAMAS [3] test server.</figDesc><table /><note>neural architecture search techniques called CurveLanes- NAS. Despite its complex pipeline and extensive architec- ture search for the best accuracy-FLOPs trade-off, our sim- ple ResNet-34 backbone model (29.9 GFLOPs) still sur- passes its large model (86.5 GFLOPs) by 0.8% on CULane.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc></figDesc><table /><note>, with similar model capacity compared to our ResNet-34 model, CurveLanes-NAS-M (35.7 GFLOPs) is 1.4% worse on Normal scenes, but the gap on Shadow and Crowd are 7.4% and 2.7%.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>FPS (image/s) and model size. All FPS results are tested with 360 ? 640 random inputs on the same platform. Here only shows models with &gt; 70% CULane<ref type="bibr" target="#b20">[22]</ref> F1 score.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Ablations. CP: Control point loss</figDesc><table><row><cell>CP SP Flip Deform Seg</cell><cell>F1</cell></row><row><cell></cell><cell>63.74</cell></row><row><cell></cell><cell>68.89</cell></row><row><cell></cell><cell>65.82</cell></row><row><cell></cell><cell>70.28</cell></row><row><cell></cell><cell>72.96</cell></row><row><cell></cell><cell>73.97</cell></row><row><cell></cell><cell>75.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>LSTR (ResNet-18, 2?) [19]</cell><cell>68.72</cell></row><row><cell>LSTR (ResNet-18, 2?) [19]</cell><cell>39.77(?28.95)</cell></row><row><cell>B?zierLaneNet (ResNet-34)</cell><cell>75.41</cell></row><row><cell>B?zierLaneNet (ResNet-34)</cell><cell>55.11(?20.30)</cell></row></table><note>. Augmentation ablations. Aug: Strong data augmentation.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Actually control points of B?zier curves can be outside the image, but statistically that rarely happens in autonomous driving scenes.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tusimple/Tusimple-Benchmark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-driving cars: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?nik</forename><surname>Guidolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Vivacqua Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vinicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avelino</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Forechi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Jesus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised labeled lane markers using maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Soussan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">You only look one-level feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">El-gan: Embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N?ra</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplanes: End-to-end lane position estimation using deep neural networksa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Gurghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejaswi</forename><surname>Koduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Smita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vidya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recent progress in road and lane detection: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename><surname>Bar Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MVA</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Line-cnn: End-to-end traffic line detection with line proposal unit. ITS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Condlanenet: a top-to-down lane detection framework based on conditional convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Endto-end lane shape prediction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Abcnet: Real-time scene text spotting with adaptive bezier-curve network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bezier curve fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pastva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAVAL POSTGRADUATE SCHOOL MONTEREY CA</title>
		<imprint>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fastdraw: Addressing the long tail of lane detection by adapting a sequential prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ultra fast structureaware deep lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Focus on local: Detecting lane marker from bottom up via key point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semantic foggy scene understanding with synthetic data. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Polylanenet: Lane estimation via deep polynomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto F De</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Keep your eyes on the lane: Real-time attention-guided lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto F De</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Night-time scene parsing with a large real dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Federal Highway Administration under United States Department of Transportation. Standard Specifications for Construction of Roads and Bridges on Federal Highway Projects</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end lane detection through differentiable least-squares fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end object detection with fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Curvelane-nas: Unifying lanesensitive architecture search and adaptive point blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end lane marker detection via row-wise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">Seok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesoo</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrack</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghoon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duck Hoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Resa: Recurrent feature-shift aggregator for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
