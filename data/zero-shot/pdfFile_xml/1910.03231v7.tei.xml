<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Guo</surname></persName>
						</author>
						<title level="a" type="main">Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning with noisy labels is a common challenge in supervised learning.</p><p>Existing approaches often require practitioners to specify noise rates, i.e., a set of parameters controlling the severity of label noises in the problem, and the specifications are either assumed to be given or estimated using additional steps. In this work, we introduce a new family of loss functions that we name as peer loss functions, which enables learning from noisy labels and does not require a priori specification of the noise rates. Peer loss functions work within the standard empirical risk minimization (ERM) framework. We show that, under mild conditions, performing ERM with peer loss functions on the noisy data leads to the optimal or a near-optimal classifier as if performing ERM over the clean training data, which we do not have access to. We pair our results with an extensive set of experiments. Peer loss provides a way to simplify model development when facing potentially noisy training labels, and can be promoted as a robust candidate loss function in such situations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The quality of supervised learning models depends on the quality of the training dataset {(x n , y n )} N n=1 . In practice, label noise can arise due to a host of reasons. For instance, the observed labels? n s may represent human observations of a ground truth label. In this case, human annotators may observe the label imperfectly due to differing degrees of expertise or measurement error, see e.g., medical examples such as labeling MRI images from patients. There exist ex-1 Computer Science and Engineering, UC Santa Cruz, Santa Cruz, CA, USA 2 Computer Science and Engineering, Shanghai Jiao Tong University, China. Correspondence to: Yang Liu &lt;yan-gliu@ucsc.edu&gt;, Hongyi Guo &lt;guohongyi@sjtu.edu.cn&gt;.</p><p>Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s). tensive prior works in the literature that aim to develop algorithms to learn models that are robust to label noise <ref type="bibr">(Bylander, 1994;</ref><ref type="bibr">Cesa-Bianchi et al., 1999;</ref><ref type="bibr">2011;</ref><ref type="bibr">Ben-David et al.;</ref><ref type="bibr" target="#b15">Scott et al., 2013;</ref><ref type="bibr" target="#b9">Natarajan et al., 2013;</ref><ref type="bibr" target="#b14">Scott, 2015)</ref>. Typical solutions that have theoretical guarantees often require a priori knowledge of noise rates, i.e., a set of parameters that control the severity of label noise. Working with unknown noise rates is difficult in practice: Usually, one must estimate the noise rates from data, which may require additional data collection or requirement <ref type="bibr" target="#b9">(Natarajan et al., 2013;</ref><ref type="bibr" target="#b14">Scott, 2015;</ref><ref type="bibr" target="#b20">Van Rooyen et al., 2015a</ref>) (e.g., a set of ground truth labels for tuning these parameters) and may introduce estimation error that can affect the final model in less predictable ways. Our main goal is to provide an alternative that does not require the specification of the noise rates, nor an additional estimation step for the noise. This target solution benefits the practitioner when he or she does not have access to reliable estimates of the noise rates (e.g., when the training data has a limited size for the estimation tasks, or when the training data is already collected in a form that makes the estimation hard to perform).</p><p>In this paper, we introduce a new family of loss functions, peer loss functions, to empirical risk minimization (ERM), for a broad class of learning with noisy labels problems. Peer loss functions operate under different noise rates without requiring either a priori knowledge of the embedded noise rates, or an estimation procedure. This family of loss functions builds on approaches developed in the peer prediction literature <ref type="bibr" target="#b8">(Miller et al., 2005;</ref><ref type="bibr">Dasgupta &amp; Ghosh, 2013;</ref><ref type="bibr" target="#b16">Shnayder et al., 2016)</ref>, which studies how to elicit information from self-interested agents without verification. Results in the peer prediction literature focused on designing scoring functions to score each reported data using another noisy reference answer, without accessing ground truth information. We borrow this idea and the associated scoring functions via making a connection through treating each classifier's predictions as an agent's private information to be elicited and evaluated, and the noisy labels as imperfect reference answers reported from a "noisy label agent". The specific form of peer loss evaluates classifiers' prediction using noisy labels on both the samples to-beevaluated and carefully constructed "peer" samples. The evaluation on the constructed peer sample encodes implic-arXiv:1910.03231v7 <ref type="bibr">[cs.</ref>LG] 14 Aug 2020 itly the information about the noise as well as the underlying true labels, which helps us offset the effects of label noise. The peer sample evaluation returns us a favorable property that the expected risk of peer loss computed on the noisy distribution turns to be an affine transformation of the true risk of the classifier defined on the clean distribution. In other words, peer loss is invariant to label noise when optimizing with it. This effect helps us get rid of the estimation of noise rates.</p><p>The main contributions of this work are:</p><p>1. We propose a new family of loss functions that can easily adapt to existing ERM framework that i) is robust to asymmetric label noise with formal theoretical guarantees and ii) requires no prior knowledge or estimation of the noise rates (no need for specifying noise rates).</p><p>We believe having the second feature above is non-trivial progress, and it features a promising solution to deploy in an unknown noisy training environment.</p><p>2. We present formal results showing that performing ERM with a peer loss function can recover an optimal, or a near-optimal classifier f * as if performing ERM on the clean data <ref type="bibr">(Theorem 2,</ref><ref type="bibr">3,</ref><ref type="bibr">4)</ref>. We also provide peer loss functions' risk guarantees (Theorem 5, 7).</p><p>3. We present extensive experimental results to validate the usefulness of peer loss functions (Section 5 and Appendix). This result is encouraging as it demonstrates the practical effectiveness in removing the requirement of error rates of noise before many of the existing training methods can be applied. We also provide preliminary results on how peer loss generalizes to multi-class classification problems.</p><p>4. Our implementation of peer loss functions is available at https://github.com/gohsyi/PeerLoss.</p><p>Due to space limit, the full version of this paper with all proof and experiment details can be found in <ref type="bibr" target="#b4">(Liu &amp; Guo, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>We go through the most relevant works. 1</p><p>Learning from Noisy Labels Our work fits within a stream of research on learning with noisy labels. A large portion of research on this topic works with the random classification noise (RCN) model, where observed labels are flipped independently with probability ? [0, 1 2 ] <ref type="bibr">(Bylander, 1994;</ref><ref type="bibr">Cesa-Bianchi et al., 1999;</ref><ref type="bibr">2011;</ref><ref type="bibr">Ben-David et al.)</ref>. Recently, learning with asymmetric noisy data (or also referred as class-conditional random classification noise (CCN)) for binary classification problems has been <ref type="bibr">1</ref> We provide more detailed discussions in the Appendix. rigorously studied in <ref type="bibr" target="#b18">(Stempfel &amp; Ralaivola, 2009;</ref><ref type="bibr" target="#b15">Scott et al., 2013;</ref><ref type="bibr" target="#b9">Natarajan et al., 2013;</ref><ref type="bibr" target="#b14">Scott, 2015;</ref><ref type="bibr" target="#b20">Van Rooyen et al., 2015a;</ref><ref type="bibr" target="#b7">Menon et al., 2015)</ref>.</p><p>For RCN, where the noise parameters are symmetric, there exist works that show symmetric loss functions <ref type="bibr" target="#b6">(Manwani &amp; Sastry, 2013;</ref><ref type="bibr">Ghosh et al., 2015;</ref><ref type="bibr" target="#b20">Van Rooyen et al., 2015a)</ref> are robust to the underlying noise, without specifying the noise rates. Our focus departs from this line of works and we exclusively focus on asymmetric noise setting, and study the possibility of an approach that can ignore the knowledge of noise rates. Follow-up works include <ref type="bibr">(Du Plessis et al., 2013;</ref><ref type="bibr" target="#b21">Van Rooyen et al., 2015b;</ref><ref type="bibr" target="#b7">Menon et al., 2015;</ref><ref type="bibr">Charoenphakdee et al., 2019)</ref>.</p><p>More Recent Works More recent developments include an importance re-weighting algorithm <ref type="bibr" target="#b2">(Liu &amp; Tao, 2016)</ref>, a noisy deep neural network learning setting <ref type="bibr" target="#b19">(Sukhbaatar &amp; Fergus, 2014;</ref><ref type="bibr">Han et al., 2018;</ref><ref type="bibr" target="#b17">Song et al., 2019)</ref>, and learning from massive noisy data for image classification <ref type="bibr" target="#b25">(Xiao et al., 2015;</ref><ref type="bibr">Goldberger &amp; Ben-Reuven, 2016;</ref><ref type="bibr">Zhang et al., 2017;</ref><ref type="bibr">Jiang et al., 2017;</ref><ref type="bibr">Jenni &amp; Favaro, 2018;</ref><ref type="bibr">Yi &amp; Wu, 2019)</ref>, robust cross entropy loss for neural network <ref type="bibr">(Zhang &amp; Sabuncu, 2018)</ref>, loss correction <ref type="bibr" target="#b10">(Patrini et al., 2017)</ref>, among many others. Loss or sample correction has also been studied in the context of learning with unlabeled data with weak supervisions <ref type="bibr" target="#b5">(Lu et al., 2018)</ref>. Most of the above works either lacks theoretical guarantees of the proposed method against asymmetric noise rates <ref type="bibr" target="#b19">(Sukhbaatar &amp; Fergus, 2014;</ref><ref type="bibr">Zhang &amp; Sabuncu, 2018)</ref>, or require estimating the noise rate (or transition matrix between the noisy and true labels) <ref type="bibr" target="#b2">(Liu &amp; Tao, 2016;</ref><ref type="bibr" target="#b25">Xiao et al., 2015;</ref><ref type="bibr" target="#b10">Patrini et al., 2017;</ref><ref type="bibr" target="#b5">Lu et al., 2018)</ref>.</p><p>A recent work <ref type="bibr" target="#b26">(Xu et al., 2019)</ref> proposes an information theoretical loss, an idea adapted from an earlier theoretical contribution <ref type="bibr">(Kong &amp; Schoenebeck, 2018)</ref>, which is also robust to asymmetric noise rates. We aimed for a simpleto-optimize loss function that can easily adapt to existing ERM solutions.</p><p>Peer Prediction Our work builds on the literature of peer prediction <ref type="bibr" target="#b11">(Prelec, 2004;</ref><ref type="bibr" target="#b8">Miller et al., 2005;</ref><ref type="bibr" target="#b22">Witkowski &amp; Parkes, 2012;</ref><ref type="bibr" target="#b12">Radanovic &amp; Faltings, 2013;</ref><ref type="bibr" target="#b23">Witkowski et al., 2013;</ref><ref type="bibr">Dasgupta &amp; Ghosh, 2013;</ref><ref type="bibr" target="#b16">Shnayder et al., 2016;</ref><ref type="bibr" target="#b3">Liu &amp; Chen, 2017)</ref>. Most relevant to us is <ref type="bibr">(Dasgupta &amp; Ghosh, 2013;</ref><ref type="bibr" target="#b16">Shnayder et al., 2016)</ref> where a correlated agreement (CA) type of mechanism was proposed. CA evaluates a report's correlations with another reference agent -its specific form inspired our peer loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Suppose (X, Y ) ? X ? Y are drawn from a joint distribution D, with their marginal distributions denoted as P X , P Y . We assume X ? R d , and Y = {?1, +1}, that is we consider a binary classification problem. Denote by p := P(Y = +1) ? (0, 1). There are N training samples (x 1 , y 1 ), ..., (x N , y N ) drawn i.i.d. from D. For positive integer n, denote by [n] := {1, 2, ..., n}.</p><p>Instead of observing y n s, the learner can only collect a noisy set of training labels? n s, generated according to y n s and a certain error rate model; that is we observe a dataset {(x n ,? n )} N n=1 . We assume uniform error for all the training samples we collect, in that errors in? n s follow the same error rate model: denoting the random variable for noisy labels as? and we define e +1 := P(? = ?1|Y = +1), e ?1 := P(? = +1|Y = ?1)</p><p>Label noise is conditionally independent from the features, that is the error rate is uniform across x n s: P(? = y |Y = y) = P(? = y |X, Y = y), ?y, y ? {?1, +1}.</p><p>We assume 0 ? e +1 +e ?1 &lt; 1 -this condition is not unlike the ones imposed in the existing learning literature <ref type="bibr" target="#b9">(Natarajan et al., 2013)</ref>, and it simply implies that the noisy labels are positively correlating with the true labels (informative about the true labels). Denote the distribution of the noisy data (X,? ) asD.</p><p>f : X ? R is a real-valued decision function, and its risk w.r.t. the 0-1 loss is defined as</p><formula xml:id="formula_0">E (X,Y )?D [1(f (X), Y )].</formula><p>The Bayes optimal classifier f * is the one that minimizes the 0-1 risk:</p><formula xml:id="formula_1">f * = argmin f E (X,Y )?D [1(f (X), Y )].</formula><p>Denote this optimal risk as R * . Instead of minimizing the above 0-1 risk, the learner often seeks a surrogate loss function : R ? {?1, +1} ? R + , and finds a f ? F that minimizes the following error:</p><formula xml:id="formula_2">E (X,Y )?D [ (f (X), Y )]. F is the hypothesis space for f . Denote the following mea- sures: R D (f ) = E (X,Y )?D [1(f (X), Y )] and R ,D (f ) = E (X,Y )?D [ (f (X), Y )].</formula><p>When there is no confusion, we will also short-hand</p><formula xml:id="formula_3">E (X,Y )?D [ (f (X), Y )] as E D [ (f (X), Y )]. Denoting D a dataset collected from distribution D (correspondingl? D := {(x n ,? n )} n?[N ] fromD), the empirical risk measure for f is defined asR ,D (f ) = 1 |D| (x,y)?D (f (x), y) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Learning with Noisy Labels</head><p>Typical methods for learning with noisy labels include developing noise correction surrogates loss function to learn with noisy data <ref type="bibr" target="#b9">(Natarajan et al., 2013)</ref>. For instance, <ref type="bibr" target="#b9">(Natarajan et al., 2013)</ref> tackles this problem by defining the following un-biased surrogate loss functions over to help "remove" noise in expectation:? (t, y) :=</p><p>(1?e?y)? (t,y)?ey? (t,?y) 1?e?1?e+1 , ?t, y.? is identified such that when a prediction is evaluated against a noisy label using this surrogate loss function, the prediction is as if evalu-ated against the ground-truth label using in expectation. Hence the loss of the prediction is "unbiased", that is ? prediction t, E? |y [? (t,? )] = (t, y) [Lemma 1, <ref type="bibr" target="#b9">(Natarajan et al., 2013)</ref>].</p><p>One important note to make is most, if not all, existing solutions require the knowledge of the error rates e ?1 , e +1 . Previous works either assumed the knowledge of it, or needed additional assumptions, clean labels or redundant noisy labels to estimate them. This becomes the bottleneck of applying these great techniques in practice. Our work is also motivated by the desire to remove this limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Peer Prediction</head><p>Peer prediction is a technique developed to truthfully elicit information when there is no ground truth verification. Suppose we are interested in eliciting private observations about a binary event Y ? {?1, +1} generated according to a random variable Y . There are K agents indexed by <ref type="bibr">[K]</ref>. Each of them holds a noisy observation of the truth Y = y, denoted as y A ? {?1, +1}, A ? [K]. We would like to elicit the y A s, but they are completely private and we will not observe y to evaluate agents' reports. Denote by r A the reported data from each agent A. r A = y A if agents are not compensated properly for their information.</p><p>Results in peer prediction have proposed scoring or reward functions that evaluate an agent's report using the reports of other peer agents. For example, a peer prediction mechanism may reward agent A for her report r A using S(r A , r B ) where r B is the report of a randomly selected reference agent B ? [K]\{A}. The scoring function S is designed so that truth-telling is a strict Bayesian Nash Equilibrium (implying other agents truthfully report their y B ), that is, <ref type="bibr" target="#b16">(Shnayder et al., 2016;</ref><ref type="bibr">Dasgupta &amp; Ghosh, 2013</ref>) (CA) is an established peer prediction mechanism for a multi-task setting 2 . CA is also the core and the focus of our subsequent sections on developing peer loss functions. This mechanism builds on a ? matrix that captures the stochastic correlation between the two sources of predictions y A and y B . Denote the following relabeling function: g(1) = ?1, g(2) = +1, ? ? R 2?2 is a squared matrix with its entries defined as follows:</p><formula xml:id="formula_4">E y B [S(y A , y B )|y A ] &gt; E y B [S(r A , y B )|y A ], ?r A = y A . Correlated Agreement</formula><formula xml:id="formula_5">? k, l = 1, 2 ? k,l = P y A = g(k), y B = g(l) ? P y A = g(k) P y B = g(l) ,</formula><p>The intuition of above ? matrix is that each (k, l) entry of ? captures the marginal correlation between the two predictions y A and y B . When there is no confusion in the text, we will always follow this relabeling function to map a ?1 label to 1 and +1 to 2 when defining or calling an entry in the ? matrix without explicitly spelling out g(?), that is we will write ? k,l = P y A = k, y B = l ? P y A = k ? P y B = l , as well as ? y,y = P y A = y, y B = y ? P y A = y ? P y B = y .</p><p>We further define M : {?1, +1} ? {?1, +1} ? {0, 1} as the sign matrix of ?:</p><formula xml:id="formula_6">M (y, y ) =: Sgn (? y,y ) ,<label>(1)</label></formula><p>where Sgn(x) = 1, x &gt; 0; Sgn(x) = 0, otherwise.</p><p>CA requires each agent A to perform multiple tasks: denote agent A's predictions for the N tasks as y A 1 , . . . , y A N . Ultimately the scoring function S(?) for each task n that is shared between A, B is defined as follows: randomly draw two tasks n 1 , n 2 , n 1 = n 2 ,</p><formula xml:id="formula_7">S y A n , y B n :=M y A n , y B n ? M y A n1 , y B n2 .</formula><p>A key difference between the first and second M (?) terms is that the second term is defined for two independent peer tasks n 1 , n 2 (as the reference answers). It was established in <ref type="bibr" target="#b16">(Shnayder et al., 2016)</ref> that CA is truthful at a Bayesian Nash Equilibrium (Theorem 5.2, <ref type="bibr" target="#b16">(Shnayder et al., 2016)</ref>.) 3 ; in particular, if y B is categorical w.r.t. y A : P(y B = y |y A = y) &lt; P(y B = y ), ?A, B ? [K], y = y then S(?) is strictly truthful (Theorem 4.4, <ref type="bibr" target="#b16">(Shnayder et al., 2016)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning with Noisy Labels: a Peer Prediction Approach</head><p>In this section, we show that peer prediction scoring functions, when specified properly, will adopt Bayes optimal classifier as their maximizers (or minimizers for the corresponding loss form).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning with Noisy Labels as an Elicitation Problem</head><p>We first state our problem of learning with noisy labels as a peer prediction problem. The connection is made by firstly rephrasing the two data sources, the classifiers' predictions and the noisy labels, from agents' perspective. For a task Y ? {?1, +1}, say +1 for example, denote the noisy la-bels? as Z(X), X ? P X|Y =1 . In general, Z(X) can be interpreted as the agent that "observes"? 1 , ...,? N for a set of randomly drawn feature vectors x 1 , ..., x N :? n ? Z(X).</p><p>Denote the following error rates for the agent's observations (similar to the definition of e +1 , e ? ): P(Z(X) = ?1|Y = +1) = e +1 , P(Z(X) = +1|Y = ?1) = e ?1 .</p><p>There is another agent whose observations "mimic" the Bayes optimal classifier f * . Again denote this optimal classifier agent as Z * (X) := f * (X): P(Z * (X) = ?1|Y = +1) = e * +1 , P(Z * (X) = +1|Y = ?1) = e * ?1 .</p><p>Elicited report as the classifier prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max reward = Min loss</head><p>Reference report as the noisy label <ref type="figure">Figure 1</ref>. S is the peer prediction function; peer is to "evaluate" a classifier's prediction using a noisy label.</p><p>Suppose we would like to elicit predictions from the optimal classifier agent Z * , while the reports from the noisy label agent Z will serve as the reference reports. Both Z and Z * are randomly assigned a task X = x, and each of them observes a signal Z(x) and Z * (x) respectively. Denote the report from agent Z * as r * . A scoring function S : R ? R ? R is called to induce truthfulness if the following fact holds: ?r * (X) = Z * (X),</p><formula xml:id="formula_8">E X S Z * (X), Z(X) ? E X S r * (X), Z(X) . (2)</formula><p>Taking the negative of S(?) (changing a reward score one aims to maximize to a loss to minimize) we also have</p><formula xml:id="formula_9">E X ?S Z * (X), Z(X) ? E X ?S r * (X), Z(X) ,</formula><p>implying when taking ?S(?) as the loss function, minimizing ?S(?) w.r.t. Z will the Bayes optimal classifier f * .</p><p>Our idea is summarized in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Peer Prediction Mechanisms Induce Bayes Optimal Classifier</head><p>When there is no ambiguity, we will shorthand Z(X), Z * (X) as Z, Z * , with keeping in mind that Z, Z * encode the randomness in X. In the elicitation setting, a potentially misreported classifier f (X) only disagrees with f * (X) according to its local ob-</p><formula xml:id="formula_10">servation f * (X) but not Y (unobservable), that is P(f (X) = f * (X)|f * (X) = l, Y = +1) = P(f (X) = f * (X)|f * (X) = l, Y = ?1), l ? {?1, +1}.</formula><p>Denote this reporting space of f as F report . Suppose Z * has the correct prior p of Y . Then we have: Theorem 1. Suppose S(?) induces truthful f * (Eqn.</p><p>(2)), that is S(?) is able to elicit the Bayes optimal classifier f * (agent Z * ) using Z. Then f * = argmin f ?Freport E (X,? )?D ?S(f (X),? ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This proof can be done via showing that any non-optimal</head><p>Bayes classifier corresponds to a non-truthful misreporting strategy. We emphasize that it is not super restrictive to have a truthful peer prediction scoring function S. We provide discussions in Appendix. Theorem 1 provides a conceptual connection and can serve as an anchor point when connecting a peer prediction score function to the problem of learning with noisy labels. So far we have not discussed a specific form of how we construct a loss function using ideas from peer prediction, and have not mentioned the requirement of knowing the noise rates. We will provide the detail about a particular peer loss in the next section, and explain its independence of noise rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Peer Loss Function</head><p>We now present peer loss, a family of loss functions inspired by a particular peer prediction mechanism, the correlated agreement (CA), as presented in Section 2.2. We are going to show that peer loss is able to induce the minimizer of a hypothesis space F, under a broad set of non-restrictive conditions. In this Section, we do not restrict to Bayes optimal classifiers, nor do we impose any restrictions on the loss functions' elicitation power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Preparing CA for Noisy Learning Problem</head><p>To give a gentle start, we repeat the setting of CA for our classification problem.</p><p>? and scoring matrix First recall that ? ? R 2?2 is a squared matrix with entries defined between Z * (the f * ) and Z (i.e., the noisy labels? ): ?k, l = 1, 2 ? k,l = P f * (X) = k,? = l ? P f * (X) = k P ? = l , ? characterizes the "marginal" correlations between the optimal classifier' prediction and the noisy label? . Then the following scoring matrix M is computed using Sgn(?), the sign matrix of ?.</p><p>Example 1. Consider a binary class label case: P(Y = ?1) = 0.4, P(Y = +1) = 0.6, the noise in the labels are e ?1 = 0.3, e +1 = 0.4 and e * ?1 = 0.2, e * +1 = 0.3. Then we have ? 1,1 = 0.036, ? 1,2 = ?0.036, ? 2,1 = ?0.036, ? 2,2 = 0.036. The details of the calculation can be found in the Appendix. And:</p><formula xml:id="formula_11">? = 0.036 ?0.036 ?0.036 0.036 ? Sgn(?) = 1 0 0 1 .</formula><p>Peer samples For each sample (x n ,? n ), randomly draw another two samples (x n1 ,? n1 ), (x n2 ,? n2 ) such that n 1 = n 2 . We will name (x n1 ,? n1 ), (x n2 ,? n2 ) as n's peer samples. After pairing x n1 with? n2 (two independent instances), the scoring function S(?) for each sample point x n is defined as follows:</p><formula xml:id="formula_12">S(f (x n ),? n )) = M f (x n ),? n ? M f (x n1 ),? n2 .</formula><p>Define loss function? (?) as the negative of S(?), which we will name as the (Generic Peer Loss)</p><formula xml:id="formula_13">f (xn),?n := 1 ? M f (xn),?n ? 1 ? M f (xn 1 ),?n 2 .<label>(3)</label></formula><p>The first term above evaluates the classifier's prediction on x n using noisy label? n , and the second "peer" term defined on two independent tasks n 1 , n 2 "punishes" the classifier from overly agreeing with the noisy labels. We will see this effect more clearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Peer Loss</head><p>We need to know Sgn(?) in order to specify M and? , which requires certain information about f * and? . We show that Example 1 is not a special case, and for the scenarios that the literature is broadly interested in, Sgn(?) is simply the identify matrix: Lemma 1. When e ?1 +e +1 &lt; 1, we have Sgn(?) = I 2?2 , the identity matrix.</p><p>The above implies that for ? k,k , k = 1, 2, f * and? are positively correlated, so the marginal correlation is positive; while for off-diagonal entries, they are negatively correlated.</p><p>Peer Loss When Sgn(?) = I 2?2 , M (y, y ) = 1 if y = y , and 0 otherwise.? (?) defined in Eqn.</p><p>(3) reduces to the following form:</p><formula xml:id="formula_14">1 peer (f (x n ),? n ) = 1(f (x n ),? n ) ? 1(f (x n1 ),? n2 ) (4) To see this, for instance 1 ? M f (x n ) = +1,? n = +1 = 1 ? M (2, 2) = 1 ? 1 = 0 = 1(f (x n ) = +1,? n = +1).</formula><p>Replacing 1(?) with any generic loss (?) we define:</p><formula xml:id="formula_15">peer (f (x n ),? n ) = (f (x n ),? n ) ? (f (x n1 ),? n2 ) (5)</formula><p>We name the above loss as peer loss. This strikingly simple form of peer (f (x n ),? n ) implies that knowing e ?1 +e +1 &lt; 1 holds is all we need to specify peer .</p><p>Later we will show this particular form of loss is invariant under label noise, which gives peer loss the ability to drop the requirement noise rates. We will instantiate this argument formally with Lemma 2 and establish a link between the above measure and the true risk of a classifier on the clean distribution. The rest of presentation focuses on peer (Eqn. (5)), but peer recovers 1 peer via replacing with 1.</p><p>ERM with Peer Loss Performing ERM with peer loss returns usf * peer :</p><formula xml:id="formula_16">f * peer = arg min f ?F 1 N N n=1 peer (f (x n ),? n )<label>(6)</label></formula><p>Note again that the definition of peer does not require the knowledge of either e +1 , e ?1 or e * +1 , e * ?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Property of Peer Loss</head><p>We now present a key property of peer loss, which shows that its risk over the noisy labels is simply an affine transformation of its true risk on clean data. We denote by E D [ peer (f (X), Y )] the expected peer loss of f when (X, Y ), as well as its peer samples, are drawn i.i.d. from distribution D.</p><p>Lemma 2. Peer loss is invariant to label noise:</p><formula xml:id="formula_17">ED[ peer (f (X),? )] = (1?e ?1 ?e +1 )?E D [ peer (f (X), Y )].</formula><p>The above Lemma states that peer loss is invariant to label noise in expectation. We have also empirically observed this effect in our experiment. Therefore minimizing it over noisy labels is equivalent to minimizing over the true clean distribution. The theorems below establish the connection between E D [ peer (f (X), Y )], the expected peer loss over clean data, with the true risk:</p><formula xml:id="formula_18">Denotef * 1peer = arg min f ?F R 1peer,D (f ).</formula><p>With Lemma 2, we can easily prove the following:</p><formula xml:id="formula_19">Theorem 2. [Optimality guarantee with equal prior] When p = 0.5,f * 1peer ? arg min f ?F R D (f ).</formula><p>The above theorem states that for a class-balanced dataset with p = 0.5, peer loss induces the same minimizer as the one that minimizes the 0-1 loss on the clean data. Removing the constraint of F, i.e.,f * 1peer = arg min f R 1peer,D (f ) ?f * 1peer = f * . In practice we can balance the dataset s.t. p ? 0.5.</p><p>When p = 0.5, denote ? p = P(Y = +1) ? P(Y = ?1), we prove:</p><formula xml:id="formula_20">Theorem 3.</formula><p>[Approximate optimality guarantee with unequal prior] When p = 0.5,</p><formula xml:id="formula_21">|R D (f * 1peer ) ? min f ?F R D (f )| ? |? p |.</formula><p>When |? p | is small, i.e., p is closer to 0.5, this bound becomes tighter.</p><p>Multi-class extension Our results in this section are largely generalizable to the multi-class classification setting. Suppose we have K classes of labels, denoting as {1, 2, ..., K}. One can show that for many classes of noise matrices, the M (?) matrix is again an identify matrix. This above fact will help us reach the conclusion that minimizing peer loss leads to the same minimizer on the clean data. We provide experiment results for multi-class tasks in Section 5.</p><p>Why do we not need the knowledge of noise rates explicitly? Both of the terms 1(f (x n ),? n ) and 1(f (x n1 ),? n2 ) encoded the knowledge of noise rates implicitly. The carefully constructed form as presented in Eqn. (4) allows peer loss to be invariant against noise (Lemma 2, a property we will explain later). For a preview, for example if we take expectation of 1 peer (f (x n ) = +1,? n = +1) we will have</p><formula xml:id="formula_22">E [1 peer (f (x n ) = +1,? n = +1)] = P(f (X) = +1,? = +1) ? P(f (X) = +1)P(? = +1),</formula><p>the marginal correlation between f and? , which is exactly capturing the entries of ? defined between f and? ! The second term above is a product of marginals because of the independence of peer samples n 1 , n 2 . Using the constructed peer term is all we need to recover this information measure in expectation. In other words, both the joint and marginal product distribution terms encode the noise rate information in an implicit way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">?-weighted Peer Loss</head><p>We take a further look at the case with p = 0.5. De-</p><formula xml:id="formula_23">note by R +1 (f ) = P(f (X) = ?1|Y = +1), R ?1 (f ) = P(f (X) = +1|Y = ?1). It is easy to prove: Lemma 3. Minimizing E[1 peer (f (X),? )] is equivalent to minimizing R ?1 (f ) + R +1 (f ). However, minimizing the true risk R D (f ) is equivalent to minimizing p ? R +1 (f ) + (1 ? p) ? R ?1 (f ), a weighted sum of R +1 (f ) and R ?1 (f ).</formula><p>The above observation and the failure to reproduce the strong theoretical guarantee when p = 0.5 motivated us to study a ?-weighted version of peer loss, to make peer loss robust to the case p = 0.5. We propose the following ?-weighted peer loss via adding a weight ? ? 0 to the second term, the peer term:</p><formula xml:id="formula_24">?-peer f (x n ),? n = (f (x n ),? n ) ? ? ? (f (x n1 ),? n2 )</formula><p>Denote 1 ?-peer as ?-peer when = 1,f * 1?-peer = arg min f ?F R 1?-peer,D (f ) as the optimal classifier under 1 ?-peer , and ?p = P(? = +1) ? P(? = ?1). Then when ?p = 0 (when this condition does not hold, we can perturb the training data by downsampling one of the two classes according to the noisy labels.), we prove:</p><formula xml:id="formula_25">Theorem 4. Let ? = 1 ? (1 ? e ?1 ? e +1 ) ? ?p ?p . We hav? f * 1?-peer ? arg min f ?F R D (f ). Denote ? * := 1 ? (1 ? e ?1 ? e +1 ) ? ?p ?p</formula><p>. Several remarks follow: (1) When p = 0.5, ? p = 0, we have ? * = 1, i.e. we recover the earlier definition of peer . (2) When e ?1 = e +1 , ? * = 0 (see Appendix for details), we recover the for the clean learning setting, which has been shown to be robust under symmetric noise rates <ref type="bibr" target="#b6">(Manwani &amp; Sastry, 2013;</ref><ref type="bibr" target="#b20">Van Rooyen et al., 2015a)</ref>. <ref type="formula" target="#formula_13">(3)</ref> When the signs of P(Y = 1) ? P(Y = ?1) and P(? = 1) ? P(? = ?1) are the same, ? * &lt; 1. Otherwise, ? * &gt; 1. In other words, when the label noise changes the relative quantitative relationship of P(Y = 1) and P(Y = ?1), ? * &gt; 1 and vice versa. (4) Knowing ? * requires a certain knowledge of e +1 , e ?1 when p = 0.5. Though we do not claim this knowledge, this result implies tuning ? * (using validation data) may improve the performance.</p><p>Theorem 2 and 4 and sample complexity theories imply that performing ERM with 1 ? * -peer :f * 1 ? * -peer = arg min fR1 ? * -peer ,D (f ) converges to f * :</p><formula xml:id="formula_26">Theorem 5. With probability at least 1 ? ?, R D (f * 1 ? * -peer ) ? R * ? 1 + ? * 1 ? e ?1 ? e +1 2 log 2/? N .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Calibration and Generalization</head><p>So far our results focused on minimizing 0-1 losses, which is hard in practice. We provide evidence of peer 's, and ?-peer 's in general, calibration and convexity for a generic and differentiable calibrated loss. We consider a that is classification calibrated, convex and L-Liptchitz.</p><p>Classification calibration describes the property that the excess risk when optimizing using a loss function would also guarantee a bound on the excessive 0-1 loss:</p><formula xml:id="formula_27">Definition 1. is classification calibrated if there ? a convex, invertible, nondecreasing transformation ? with ? (0) = 0 s.t. ? (R D (f ) ? R * ) ? R ,D (f ) ? min f R ,D (f ), ?f .</formula><p>Denote f * ? arg min f R ,D (f ). Below we provide sufficient conditions for ?-peer to be calibrated. Theorem 6. ?-peer is classification calibrated when either of the following two conditions holds: (1) ? = 1 (i.e., ?-peer = peer ), p = 0.5, and f * satisfies the fol-</p><formula xml:id="formula_28">lowing: E[ (f * (X), ?Y )] ? E[ (f (X), ?Y )], ?f. (2) ? &lt; 1, max{e +1 , e ?1 } &lt; 0.5, (t, y) = (t, ?y), ?t, and ?(1 ? 2p)(1 ? e +1 ? e ?1 ) = (1 ? ?)(e +1 ? e ?1 ).</formula><p>(1) states that f * not only achieves the smallest risk over the clean distribution (X, Y ) but also performs the worst on the "opposite" distribution with flipped labels ?Y . (2) (t, y) = (t, ?y) is satisfied by some common loss function, such as square and logistic losses, as noted in <ref type="bibr" target="#b9">(Natarajan et al., 2013)</ref>, Under the calibration condition, and denote the corresponding calibration transformation function for ?-peer as ? ?-peer . Denote b?</p><formula xml:id="formula_29">f * ?-peer = arg min f ?FR ?-peer ,D (f ) := 1 N N n=1 ?-peer(f (xn),?n).</formula><p>Consider a bounded with? , denoting its max and min value. We have the following generalization bound:</p><p>Theorem 7. With probability at least 1 ? ?:</p><formula xml:id="formula_30">RD(f * ? * -peer ) ? R * ? 1 1 ? e?1 ? e+1 ? ? ?1 ? * -peer min f ?F R ? * -peer ,D (f ) ? min f R ? * -peer ,D (f ) + 4(1 + ? * )L ? (F) + 2 log 4/? 2N 1 + (1 + ? * )(? ? )</formula><p>where (F) is Rademacher complexity of F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Convexity</head><p>In experiments, we use neural networks which are more robust to non-convex loss functions. Nonetheless, despite the fact that ?-peer (?) is not convex in general, Lemma 5 in <ref type="bibr" target="#b9">(Natarajan et al., 2013)</ref> informs us that as long a? R ?-peer ,D (f ) is close to some convex function, mirror gradient type of algorithms will converge to a small neighborhood of the optimal point when performing ERM with ?-peer . A natural candidate for this convex function is the expectation ofR</p><formula xml:id="formula_31">?-peer ,D (f ) asR ?-peer ,D (f ) ? R ?-peer ,D (f ) when N ? ?. Lemma 4. When ? &lt; 1, max{e +1 , e ?1 } &lt; 0.5, (t, y) = (t, ?y), ?t, and ?(1 ? 2p)(1 ? e +1 ? e ?1 ) = (1 ? ?)(e +1 ? e ?1 ), R ?-peer ,D (f )</formula><p>is convex. This is the same condition as specified in <ref type="formula">(2)</ref> of Theorem 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We implemented a two-layer ReLU Multi-Layer Perceptron (MLP) for classification tasks on 10 UCI Benchmarks and applied our peer loss to update their parameters. We show the robustness of peer loss with increasing rates of label noise on 10 real-world datasets. We compare the performance of our peer loss based method with surrogate loss method <ref type="bibr" target="#b9">(Natarajan et al., 2013)</ref> (unbiased loss correction with known error rates), symmetric loss method <ref type="bibr">(Ghosh et al., 2015)</ref>, DMI <ref type="bibr" target="#b26">(Xu et al., 2019)</ref>, C-SVM <ref type="bibr" target="#b1">(Liu et al., 2003)</ref> and PAM <ref type="bibr">(Khardon &amp; Wachman, 2007)</ref>, which are state-of-the-art methods for dealing with random binaryclassification noise, as well as a neural network baseline solution with binary cross entropy loss (NN). We use a cross-validation set to tune the parameters specific to the algorithms. For surrogate loss, we use the true e ?1 and e +1 instead of learning them separately. Thus, surrogate loss could be considered a favored and advantaged baseline method. Accuracy of a classification algorithm is defined as the fraction of examples in the test set classified correctly with respect to the clean and true label. For given noise rates e +1 and e ?1 , labels of the training data are flipped accordingly.</p><p>A subset of the experiment results is shown in <ref type="table">Table 1</ref>. A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Peer Loss Functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>With Prior Equalization p = 0.5 Without Prior Equalization p = 0.  full table with all details can be found in the Appendix.</p><p>Equalized Prior means that we balance the dataset to guarantee p = 0.5. For this case we used peer (i.e., ? = 1 as in ?-peer ). For p = 0.5, we use validation dataset (still with noisy labels) to tune ?. Our method is competitive across all datasets and is even able to outperform the surrogate loss method with access to the true noise rates in a number of datasets, as well as the symmetric loss functions (which does not require the knowledge of noise rates when error rates are symmetric) and the recently proposed information theoretical loss <ref type="bibr" target="#b26">(Xu et al., 2019)</ref>. <ref type="figure" target="#fig_0">Figure 2</ref> shows that peer loss can prevent over-fitting when facing noisy labels.</p><p>A closer look at our decision boundary To have a better understanding of peer loss, we visualize the decision boundary returned by peer loss with a 2D synthetic experiment: the outer circle of randomly places points correspond to one class and the inner one is the other class. <ref type="figure" target="#fig_1">From Figure 3</ref> we observe that when using cross entropy for training, the decision boundary is sharp on clean data but becomes much less so on noisy data (we have more examples with higher noise rate in the Appendix). Peer loss returns sharp boundaries even under a high noise rate <ref type="figure" target="#fig_2">(Figure 4)</ref>. Preliminary results on multi-class classification We provide preliminary results on CIFAR-10 <ref type="bibr" target="#b0">(Krizhevsky et al., 2009)</ref> in <ref type="table">Table 2</ref>. We followed the setup in <ref type="bibr" target="#b26">(Xu et al., 2019)</ref> and used <ref type="bibr">ResNet (He et al., 2016)</ref> as the underlying optimization solution. However, different from settings in   <ref type="bibr" target="#b26">Xu et al., 2019)</ref> where label noise only exists between specific class pairs, our noise is universal across classes. For each class, we flip the label to any other label with a probability of /9, where is the error rate and 9 is the number of other classes. We do show peer loss is competitive against cross entropy and DMI <ref type="bibr" target="#b26">(Xu et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Error Rate = 0.2 Error Rate = 0.4 cross entropy 86.67 82.09 DMI <ref type="bibr" target="#b26">(Xu et al., 2019)</ref> 85.11 81.67 Peer Loss 87.72 83.81 <ref type="table">Table 2</ref>. Accuracy on CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Discussion</head><p>This paper introduces peer loss, a family of loss functions that enables training a classifier over noisy labels, but without using explicit knowledge of the noise rates of labels.</p><p>Peer loss had made the assumption that label noise is homogeneous across training data instances. Future extensions of this work includes extension to instance based <ref type="bibr">(Cheng et al., 2020;</ref><ref type="bibr" target="#b24">Xia et al., 2020)</ref> and margin based (Amid et al., 2019) label noise. We are also interested in exploring the application of peer loss in differentially private ERM (Chaudhuri et al., 2011), as well as in semisupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Lemma 2</head><p>Proof. We sketch the main steps. We denote by X n1 ,? n2 the random variable corresponding to the peer samples x n1 ,? n2 .</p><p>First we have</p><formula xml:id="formula_32">E[ peer (f (X),? )] = E[ (f (X),? )] ? E[ (f (X n1 ),? n2 )]<label>(7)</label></formula><p>Consider the two terms on the RHS separately.</p><formula xml:id="formula_33">E[ (f (X),? )] =E X,Y =?1 P(? = ?1|Y = ?1) ? (f (X), ?1) + P(? = +1|Y = ?1) ? (f (X), +1) + E X,Y =+1 P(? = +1|Y = +1) ? (f (X), +1) + P(? = ?1|Y = +1) ? (f (X), ?1) (Independence between? and X given Y ) =E X,Y =?1 (1 ? e ?1 ) (f (X), ?1) + e ?1 (f (X), +1) +E X,Y =+1 (1 ? e +1 ) (f (X), +1) + e +1 (f (X), ?1)<label>(8)</label></formula><p>The above is done mostly via law of total probability and using the assumption that? is conditionally (on Y ) independent of X. Subtracting and adding e +1 ? (f (X), ?1) and e ?1 ? (f (X), +1) to the two expectation terms separately we have</p><formula xml:id="formula_34">Eqn. (16) =E X,Y =?1 (1 ? e ?1 ? e +1 ) ? (f (X), ?1) + e +1 ? (f (X), ?1) + e ?1 ? (f (X), +1) + E X,Y =+1 (1 ? e ?1 ? e +1 ) ? (f (X), +1) + e ?1 ? (f (X), +1) + e +1 ? (f (X), ?1) =(1 ? e ?1 ? e +1 ) ? E X,Y (f (X), Y ) + E X e +1 ? (f (X), ?1) + e ?1 ? (f (X), +1)</formula><p>And consider the second term:</p><formula xml:id="formula_35">E[ (f (X n1 ),? n2 )] =E X [ (f (X), ?1)] ? P(? = ?1) + E X [ (f (X), +1)] ? P(? = +1)</formula><p>(Independence between n 1 and n 2 )</p><formula xml:id="formula_36">=E X (e +1 ? p + (1 ? e ?1 )(1 ? p)) ? (f (X), ?1) + ((1 ? e +1 )p + e ?1 (1 ? p)</formula><p>) ? (f (X), +1) (Expressing P(? ) using p and e +1 , e ?1 )</p><formula xml:id="formula_37">=E X (1 ? e ?1 ? e +1 )(1 ? p) ? (f (X), ?1) + (1 ? e ?1 ? e +1 )p ? (f (X), +1) + E X (e +1 ? p + e +1 (1 ? p)) ? (f (X), ?1) + (e ?1 (1 ? p) + e ?1 p) ? (f (X), +1) =(1 ? e ?1 ? e +1 ) ? E[ (f (X n1 ), Y n2 )] + E X e +1 ? (f (X), ?1) + e ?1 ? (f (X), +1)</formula><p>Subtracting the first and second term on RHS of Eqn. <ref type="formula" target="#formula_6">(15)</ref>:  <ref type="bibr">, 1999;</ref><ref type="bibr">2011;</ref><ref type="bibr">Ben-David et al.)</ref>. Recently, learning with asymmetric noisy data (or also referred as class-conditional random classification noise (CCN)) for binary classification problems has been rigorously studied in <ref type="bibr" target="#b18">(Stempfel &amp; Ralaivola, 2009;</ref><ref type="bibr" target="#b15">Scott et al., 2013;</ref><ref type="bibr" target="#b9">Natarajan et al., 2013;</ref><ref type="bibr" target="#b14">Scott, 2015;</ref><ref type="bibr" target="#b20">Van Rooyen et al., 2015a;</ref><ref type="bibr" target="#b7">Menon et al., 2015)</ref>.</p><formula xml:id="formula_38">E[ peer (f (X),? )] = E[ (f (X),? )] ? E[ (f (X n1 ),? n2 )] =(1 ? e ?1 ? e +1 ) ? E[ peer (f (X), Y )]<label>(9</label></formula><p>Symmetric loss For RCN, where the noise parameters are symmetric, there exist works that show symmetric loss functions <ref type="bibr" target="#b6">(Manwani &amp; Sastry, 2013;</ref><ref type="bibr">Ghosh et al., 2015;</ref><ref type="bibr" target="#b20">Van Rooyen et al., 2015a)</ref> are robust to the underlying noise, without specifying the noise rates. It was also shown that under certain conditions, the proposed loss functions are able to handle asymmetric noise. Our focus departs from this line of works and we exclusively focus on asymmetric noise setting, and study the possibility of an approach that can ignore the knowledge of noise rates. More recent works More recent developments include an importance re-weighting algorithm <ref type="bibr" target="#b2">(Liu &amp; Tao, 2016)</ref>, a noisy deep neural network learning setting <ref type="bibr" target="#b19">(Sukhbaatar &amp; Fergus, 2014;</ref><ref type="bibr">Han et al., 2018;</ref><ref type="bibr" target="#b17">Song et al., 2019)</ref>, and learning from massive noisy data for image classification <ref type="bibr" target="#b25">(Xiao et al., 2015;</ref><ref type="bibr">Goldberger &amp; Ben-Reuven, 2016;</ref><ref type="bibr">Zhang et al., 2017;</ref><ref type="bibr">Jiang et al., 2017;</ref><ref type="bibr">Jenni &amp; Favaro, 2018;</ref><ref type="bibr">Yi &amp; Wu, 2019)</ref>, robust cross entropy loss for neural network (Zhang &amp; Sabuncu, 2018), loss correction <ref type="bibr" target="#b10">(Patrini et al., 2017)</ref>, among many others. Loss or sample correction has also been studied in the context of learning with unlabeled data with weak supervisions <ref type="bibr" target="#b5">(Lu et al., 2018)</ref>. Most of the above works either lacks theoretical guarantee of the proposed method against asymmetric noise rates <ref type="bibr" target="#b19">(Sukhbaatar &amp; Fergus, 2014;</ref><ref type="bibr">Zhang &amp; Sabuncu, 2018)</ref>, or require estimating the noise rate or transition matrix between noisy and true labels <ref type="bibr" target="#b2">(Liu &amp; Tao, 2016;</ref><ref type="bibr" target="#b25">Xiao et al., 2015;</ref><ref type="bibr" target="#b10">Patrini et al., 2017;</ref><ref type="bibr" target="#b5">Lu et al., 2018)</ref>. A good number of the recent works can be viewed as derivatives or extension of the unbiased surrogate loss function idea introduced in <ref type="bibr" target="#b9">(Natarajan et al., 2013)</ref>, therefore they would naturally require the knowledge of the noise rates or transition matrix. We do provide thorough comparisons between peer loss and the unbiased surrogate loss methods.</p><p>A recent work <ref type="bibr" target="#b26">(Xu et al., 2019)</ref> proposes an information theoretical loss (an idea adapted from an earlier theoretical contribution <ref type="bibr">(Kong &amp; Schoenebeck, 2018)</ref>) that is also robust to asymmetric noise rate. We aimed for a simple-to-optimize loss function that can easily adapt to existing ERM solutions. <ref type="bibr" target="#b26">(Xu et al., 2019)</ref> involves estimating a joint distribution matrix between classifiers and noisy labels, and then invokes computing a certain information theoretical measure based on this matrix. Therefore, its sample complexity requirement and the sensitivity to noise in this estimation are not entirely clear to us (not provided in the paper either). We do provide calibration guarantees, generalization bounds, and conditions under which the loss functions are convex. In general, we do think computationally peer loss functions are easy to optimize with, in comparison to information theoretical measures. Experiments comparing with <ref type="bibr" target="#b26">(Xu et al., 2019)</ref> are also given in Section 5.</p><p>Peer Prediction Our work builds on the literature for peer prediction <ref type="bibr" target="#b11">(Prelec, 2004;</ref><ref type="bibr" target="#b8">Miller et al., 2005;</ref><ref type="bibr" target="#b22">Witkowski &amp; Parkes, 2012;</ref><ref type="bibr" target="#b12">Radanovic &amp; Faltings, 2013;</ref><ref type="bibr" target="#b23">Witkowski et al., 2013;</ref><ref type="bibr">Dasgupta &amp; Ghosh, 2013;</ref><ref type="bibr" target="#b16">Shnayder et al., 2016;</ref><ref type="bibr" target="#b3">Liu &amp; Chen, 2017)</ref>. <ref type="bibr" target="#b8">(Miller et al., 2005)</ref> established that strictly proper scoring rule <ref type="bibr">(Gneiting &amp; Raftery, 2007)</ref> could be adopted to elicit truthful reports from self-interested agents. Follow-up works that have been done to relax the assumptions imposed <ref type="bibr" target="#b22">(Witkowski &amp; Parkes, 2012;</ref><ref type="bibr" target="#b12">Radanovic &amp; Faltings, 2013;</ref><ref type="bibr" target="#b23">Witkowski et al., 2013;</ref><ref type="bibr" target="#b13">Radanovic et al., 2016;</ref><ref type="bibr" target="#b3">Liu &amp; Chen, 2017)</ref>. Most relevant to us is <ref type="bibr">(Dasgupta &amp; Ghosh, 2013;</ref><ref type="bibr" target="#b16">Shnayder et al., 2016)</ref> where a correlated agreement (CA) type of mechanism was proposed. CA evaluates a report's correlations with another reference agent -its specific form inspired our peer loss.  <ref type="figure">Figure A1</ref>. Illustration of our peer loss implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Illustration of our implementation of peer loss</head><p>We illustrate our peer loss method in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other peer prediction functions</head><p>Other notable examples include quadratic and logarithmic scoring function, defined as follows: Example 2. Quadratic scoring function: We know the following is true: Lemma 1 ( <ref type="bibr" target="#b8">(Miller et al., 2005)</ref>). Similarly we conclude that when y A and y B are stochastic relevant, the correlated agreement scoring rule, quadratic scoring rule and logarithmic scoring rule are strictly truthful.</p><formula xml:id="formula_39">S r A , r B := 2P y B = r B |y A = r A ? s?{?1,+1} P y B = s|y A = r A 2 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Theorem 1</head><p>Proof. Note that proving f * = argmin f E (X,? )?D ?S(f (X),? ) is equivalent with proving f * = argmax f E (X,? )?D S(f (X),? ) .</p><p>First note that the expected score of a classifier over the data distribution further writes as follows:</p><formula xml:id="formula_40">ED S(f (X),? ) = p ? ED |Y =+1 S(f (X),? ) + (1 ? p) ? ED |Y =?1 S(f (X),? ] .</formula><p>S(?) is able to elicit the Bayes optimal classifier f * using? implies that</p><formula xml:id="formula_41">p ? ED |Y =+1 S(f * (X),? ) + (1 ? p) ? ED |Y =?1 S(f * (X),? ) &gt; p ? ED |Y =+1 S Z (X),? + (1 ? p) ? ED |Y =?1 S(Z (X),? ) , ?Z = f * .</formula><p>Denote by f a sub-optimal classifier that disagrees with f * on set X + dis|Y =+1 = {X|Y = +1 : f (X) = f * (X), f * (X) = +1}. Denote + := P(X ? X + dis|Y =+1 ). Construct the following reporting strategy for f * (X) = +1:</p><formula xml:id="formula_42">Z (X) = f * (X), w.p. 1 ? + ?f * (X), w.p. +</formula><p>We can similarly construct the above reporting strategy for</p><formula xml:id="formula_43">X ? dis|Y =+1 = {X|Y = +1 : f (X) = f * (X), f * (X) = ?1} with parameter ? := P(X ? X ? dis|Y =+1 ): Z (X) = f * (X), w.p. 1 ? ? ?f * (X), w.p. ?</formula><p>By definition of sub-optimality of f in reporting we know that max{ + , ? } &gt; 0, as a zero measure mis-reporting strategy does not affect its optimality. Not hard to check that</p><formula xml:id="formula_44">ED |Y =+1 S(f (X),? ) = P(f * (X) = +1|Y = +1) ? ED |f * (X)=+1,Y =+1 S(f (X),? ) + P(f * (X) = ?1|Y = +1) ? ED |f * (X)=?1,Y =+1 S(f (X),? ) .</formula><p>Each of the two conditional expectation terms further derives:</p><formula xml:id="formula_45">ED |f * (X)=+1,Y =+1 S(f (X),? ) = 1 ? P(X ? X + dis|Y =+1 ) ? ED |f * (X)=+1,Y =+1,f (X)=f * (X) [S(f (X) = +1,? )] + P(X ? X + dis|Y =+1 ) ? ED |f * (X)=+1,Y =+1,f (X) =f * (X) [S(f (X) = ?1,? )] = (1 ? + ) ? ED |f * (X)=+1,Y =+1 [S(f (X) = +1,? )] + + ? ED |f * (X)=+1,Y =+1 [S(f (X) = ?1,? )] = ED |f * (X)=+1,Y =+1 [S(Z (X),? )] .</formula><p>In the second equality above, the dropping of conditions f (X) = f (X) in E is due to the fact that? is conditionally independent of X given Y . The above argument repeats for ED |f * (X)=?1,Y =+1 S(f (X),? ) . Therefore we conclude</p><formula xml:id="formula_46">ED |Y =+1 S(f (X),? ) = ED |Y =+1 S Z (X),? .</formula><p>Yet we have the following fact that</p><formula xml:id="formula_47">ED |Y =+1 S Z (X),? = P(f * (X) = +1|Y = +1) ? (1 ? + ) ? ED |Y =+1,f * (X)=+1 S(f * (X),? ) + + ? ED |Y =+1,f * (X)=+1 S(?f * (X),? ) + P(f * (X) = ?1|Y = +1) ? (1 ? ? ) ? ED |Y =+1,f * (X)=?1 S(f * (X),? ) + ? ? ED |Y =+1,f * (X)=?1 S(?f * (X),? )<label>(10)</label></formula><p>Subtracting and adding ? ?ED |Y =+1,f * (X)=+1 S(f * (X),? ) and + ?ED |Y =+1,f * (X)=?1 S(f * (X),? ) in above partial sums in (?), and combining the first (1 ? + ? ? ) terms, we have</p><formula xml:id="formula_48">Eqn. (12) = (1 ? + ? ? ) ? ED |Y =+1 S(f * (X),? ) + + ? P(f * (X) = +1|Y = +1) ? ED |Y =+1,f * (X)=+1 S(?f * (X),? ) + P(f * (X) = ?1|Y = +1) ? ED |Y =+1,f * (X)=?1 S(f * (X),? ) + ? ? P(f * (X) = +1|Y = +1) ? ED |Y =+1,f * (X)=+1 S(f * (X),? ) + P(f * (X) = ?1|Y = +1) ? ED |Y =+1,f * (X)=?1 S(?f * (X),? ) = (1 ? + ? ? ) ? ED |Y =+1 S(f * (X),? ) + + ? ED |Y =+1 S(Z ? ? ?1,? ) + ? ? ED |Y =+1 S(Z ? ? +1,? ) .<label>(11)</label></formula><p>Note that ED |Y =+1 S(Z ? ? ?1,? ) corresponds to a reporting strategy that always reports ?1, and ED |Y =+1 S(Z ? ? +1,? ) is the one to always report +1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarly for ED</head><formula xml:id="formula_49">|Y =?1 S(f (X),? ) : Suppose f disagrees with f * on set X + dis|Y =?1 = {X|Y = ?1 : f (X) = f * (X), f * (X) = +1}. Note because of the assumption P(f (X) = f * (X)|f * (X) = +1, Y = +1) = P(f (X) = f * (X)|f * (X) = +1, Y = ?1)</formula><p>, in above we have the same + as in the Y = +1. Construct the following reporting strategy for f * (X) = +1</p><formula xml:id="formula_50">Z (X) = f * (X), w.p. 1 ? + ?f * (X), w.p. +</formula><p>We can similarly construct the above reporting strategy for</p><formula xml:id="formula_51">X ? dis|Y =?1 = {X|Y = ?1 : f (X) = f * (X), f * (X) = ?1} with parameter ? := P(X ? X ? dis|Y =?1 ): Z (X) = f * (X), w.p. 1 ? ? ?f * (X), w.p. ?</formula><p>Similarly we claim that (details not repeated)</p><formula xml:id="formula_52">ED |Y =?1 S(f (X),? ) = ED |Y =?1 S Z (X),? .</formula><p>Again we note that</p><formula xml:id="formula_53">ED |Y =?1 S Z (X),? = P(f * (X) = +1|Y = ?1) ? (1 ? + ) ? ED |Y =?1,f * (X)=+1 S(f * (X),? ) + + ? ED |Y =?1,f * (X)=+1 S(?f * (X),? ) + P(f * (X) = ?1|Y = ?1) ? (1 ? ? ) ? ED |Y =?1,f * (X)=?1 S(f * (X),? ) + ? ? ED |Y =?1,f * (X)=?1 S(?f * (X),? ) .<label>(12)</label></formula><p>And by further rearranging terms as done above we have (with only difference being replacing Y = +1 with Y = ?1; we omit the details)</p><formula xml:id="formula_54">ED |Y =?1 S Z (X),? = (1 ? + ? ? ) ? ED |Y =?1 S(f * (X),? ) + + ? ED |Y =?1 S(Z ? ? ?1,? ) + ? ? ED |Y =?1 S(Z ? ? +1,? ) . P(? = +1, f * (X) = +1) = P(Y = +1)(1 ? e +1 )(1 ? e * +1 ) + P(Y = ?1) ? e ?1 ? e * ?1</formula><p>We also have</p><formula xml:id="formula_55">P(? = +1) = P(Y = +1)(1 ? e +1 ) + P(Y = ?1) ? e ?1 P(f * (X) = +1) = P(Y = +1)(1 ? e * +1 ) + P(Y = ?1) ? e * ?1</formula><p>Then we have</p><formula xml:id="formula_56">P(? = +1, f * (X) = +1) ? P(? = +1)P(f * (X) = +1) =P(Y = +1)P(Y = ?1)(1 ? e +1 ? e ?1 )(1 ? e * +1 ? e * ?1 ) &gt;0, when 1 &gt; e * +1 + e * ?1 .</formula><p>e * ?1 + e * +1 &lt; 1 means that the Bayes' optimal classifier is at least informative ( <ref type="bibr" target="#b3">(Liu &amp; Chen, 2017)</ref>) -if otherwise, we can flip the classifier's output to obtain one, which contradicts the optimality of Bayes optimal classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details for Example 1</head><p>First of all, we compute the marginals of f * and? :</p><formula xml:id="formula_57">P (f * (X) = ?1) = P (f * (X) = ?1|Y = ?1) P(Y = ?1) + P (f * (X) = ?1|Y = +1) P(Y = +1) = 1 ? e * ?1 ? 0.4 + e * +1 ? 0.6 = 0.5</formula><p>And easily For the joint distribution,</p><formula xml:id="formula_58">P (f * (X) = ?1, Y = ?1) =P (f * (X) = ?1, Y = ?1|Y = ?1) P(Y = ?1) + P (f * (X) = ?1, Y = ?1|Y = +1) = 1 ? e * ?1 (1 ? e ?1 ) ? 0.4 + e * +1 ? e +1 ? 0.6 = 0.296 P f * (X) = ?1,? = +1 = P (f * (X) = ?1) ? P (f * (X) = ?1, Y = ?1) = 0.264</formula><p>Further, P f * (X) = +1,? = ?1 = P(? = ?1) ? P (f * (X) = ?1, Y = ?1) = 0.224 P (f * (X) = +1, Y = +1) = P (f * (X) = +1) ? P (f * (X) = +1, Y = ?1) = 0.216</p><p>With above, the entries in Delta can be computed easily, for instance ? 1,1 = P (f * (X) = ?1, Y = ?1) ? P (f * (X) = ?1) ? P(Y = ?1) = 0.296 ? 0.5 ? 0.52 = 0.036</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Lemma 1</head><p>Proof. Again recall that</p><formula xml:id="formula_59">P(Z * = +1, Z = +1) = P(Y = +1)(1 ? e +1 )(1 ? e * +1 ) + P(Y = ?1)e ?1 ? e * ?1 P(Z = +1) = P(Y = +1)(1 ? e +1 ) + P(Y = ?1) ? e ?1 P(Z * = +1) = P(Y = +1)(1 ? e * +1 ) + P(Y = ?1) ? e * ?1</formula><p>Then we have</p><formula xml:id="formula_60">P(Z * = +1, Z = +1) ? P(Z * = +1)P(Z = +1) =P(Y = +1)P(Y = ?1)(1 ? e +1 ? e ?1 )(1 ? e * +1 ? e * ?1 ) &gt; 0 when 1 ? e +1 ? e ?1 &gt; 0, 1 ? e * +1 ? e * ?1 &gt; 0.</formula><p>Interestingly this coincides with the condition imposed in <ref type="bibr" target="#b9">(Natarajan et al., 2013)</ref>. Similarly we can prove that</p><formula xml:id="formula_61">P(Z * = +1, Z = ?1) ? P(Z * = +1)P(Z = ?1) = ? P(Y = +1)P(Y = ?1)(1 ? e +1 ? e ?1 )(1 ? e * +1 ? e * ?1 ) &lt; 0<label>(14)</label></formula><p>The other entries for P(Z * = ?1, Z = ?1) ? P(Z * = ?1)P(Z = ?1) and P(Z * = ?1, Z = +1) ? P(Z * = ?1)P(Z = +1) are symmetric. Therefore the sign matrix of above score matrix is exactly the diagonal matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Lemma 2</head><p>Proof. We denote by X n1 ,? n2 the random variable corresponding to the peer samples x n1 ,? n2 .</p><p>First we have</p><formula xml:id="formula_62">E[ peer (f (X),? )] = E[ (f (X),? )] ? E[ (f (X n1 ),? n2 )]<label>(15)</label></formula><p>Consider the two terms on the RHS separately.</p><formula xml:id="formula_63">E[ (f (X),? )] =E X,Y =?1 P(? = ?1|X, Y = ?1) ? (f (X), ?1) + P(? = +1|X, Y = ?1) ? (f (X), +1) + E X,Y =+1 P(? = +1|X, Y = +1) ? (f (X), +1) + P(? = ?1|X, Y = +1) ? (f (X), ?1) =E X,Y =?1 P(? = ?1|Y = ?1) ? (f (X), ?1) + P(? = +1|Y = ?1) ? (f (X), +1) + E X,Y =+1 P(? = +1|Y = +1) ? (f (X), +1) + P(? = ?1|Y = +1) ? (f (X), ?1)</formula><p>(Independence between? and X given Y )</p><formula xml:id="formula_64">=E X,Y =?1 (1 ? e ?1 ) (f (X), ?1) + e ?1 (f (X), +1) + E X,Y =+1 (1 ? e +1 ) (f (X), +1) + e +1 (f (X), ?1)<label>(16)</label></formula><p>The above is done mostly via law of total probability and using the assumption that? is conditionally (on Y ) independent of X. Subtracting and adding e +1 ? (f (X), ?1) and e ?1 ? (f (X), +1) to the two expectation terms separately we have Eqn. (16) =E X,Y =?1 (1 ? e ?1 ? e +1 ) ? (f (X), ?1) + e +1 ? (f (X), ?1) + e ?1 ? (f (X), +1)</p><formula xml:id="formula_65">+ E X,Y =+1 (1 ? e ?1 ? e +1</formula><p>) ? (f (X), +1) + e ?1 ? (f (X), +1) + e +1 ? (f (X), ?1)</p><formula xml:id="formula_66">=(1 ? e ?1 ? e +1 ) ? E X,Y (f (X), Y ) + E X e +1 ? (f (X), ?1) + e ?1 ? (f (X), +1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Peer Loss Functions</head><p>And consider the second term:</p><formula xml:id="formula_67">E[ (f (X n1 ),? n2 )] =E X [ (f (X), ?1)] ? P(? = ?1) + E X [ (f (X), +1)] ? P(? = +1)</formula><p>(Independence between n 1 and n 2 ) <ref type="figure">1 (1 ? p)</ref>) ? (f (X), +1) (Expressing P(? ) using p and e +1 , e ?1 ) <ref type="figure">1 ? p)</ref>) ? (f (X), ?1)</p><formula xml:id="formula_68">=E X (e +1 ? p + (1 ? e ?1 )(1 ? p)) ? (f (X), ?1) + ((1 ? e +1 )p + e ?</formula><formula xml:id="formula_69">=E X (1 ? e ?1 ? e +1 )(1 ? p) ? (f (X), ?1) + (1 ? e ?1 ? e +1 )p ? (f (X), +1) + E X (e +1 ? p + e +1 (</formula><formula xml:id="formula_70">+ (e ?1 (1 ? p) + e ?1 p) ? (f (X), +1) =(1 ? e ?1 ? e +1 ) ? E[ (f (X n1 ), Y n2 )] + E X e +1 ? (f (X), ?1) + e ?1 ? (f (X), +1)</formula><p>Subtracting the first and second term on RHS of Eqn. <ref type="formula" target="#formula_6">(15)</ref>:</p><formula xml:id="formula_71">E[ peer (f (X),? )] =E[ (f (X),? )] ? E[ (f (X n1 ),? n2 )] =(1 ? e ?1 ? e +1 ) ? E[ peer (f (X), Y )]<label>(17)</label></formula><p>Multi-class extension for 0-1 loss Proof. We denote by Q a transition matrix that characterizes the relationships between noisy label? and the true label Y . The (i, j) entry of Q is defined as Q ij = P(? = j|Y = i). We write Q ij = q ij .</p><p>Consider the following case: suppose the noisy labels have the same probability of flipping to a specific wrong class, that is, we pose the following conditions: q ij = q kj , for all j = k = i. This condition allows us to define K new quantities:</p><formula xml:id="formula_72">e j = q ij for all i = j, q ii = 1 ? j =i e j<label>(18)</label></formula><p>Note that this condition is easily satisfied for the binary case since there is only one other class to be flipped to wrongly.</p><p>We show that M (?) is a diagonal matrix when K j=1 e j &lt; 1, a similar condition as e ?1 + e +1 &lt; 1. Notice the following facts:</p><formula xml:id="formula_73">E[1(f (X),? )] ? E[1(f (X n1 ),? n2 )] = P(f (X) =? ) ? P(f (X n1 ) =? n2 ) = ?P(f (X) =? ) + P(f (X n1 ) =? n2 )</formula><p>and using Eqn. 18 we have</p><formula xml:id="formula_74">K k=1 P(Y = k) ? q kj = P(Y = j) ? ? 1 ? k =j e k ? ? + (1 ? P(Y = j))e j = 1 ? k e k P(Y = j) + e j Then P(f (X) =? ) = K k=1 P(Y = k) K j=1 P(f (X) = j,? = j|Y = k) = K k=1 P(Y = k) K j=1 P(f (X) = j|Y = k) ? P(? = j|Y = k) (Conditional independence) = K k=1 P(Y = k) K j=1 P(f (X) = j|Y = k) ? q kj = K j=1 K k=1 P(f (X) = j|Y = k)P(Y = k) ? q kj = K j=1 P(f (X) = j|Y = j)P(Y = j) ? ? 1 ? k =j e k ? ? + K j=1 k =j P(f (X) = j|Y = k)P(Y = k)e j (Eqn. 18) = K j=1 P(f (X) = j|Y = j)P(Y = j) ? ? 1 ? k =j e k ? ? + K j=1 e j ? (P(f (X) = j) ? P(f (X) = j|Y = j)P(Y = j)) = 1 ? k e k K j=1 P(f (X) = j|Y = j)P(Y = j) + K j=1 e j ? P(f (X) = j)</formula><p>Now consider the following</p><formula xml:id="formula_75">P(f (X n1 ) =? n2 ) = K j=1 P(f (X) = j)P(? = j) = K j=1 P(f (X) = j) K k=1 P(Y = k) ? q kj = K j=1 P(f (X) = j) (1 ? k e k )P(Y = j) + e j (Eqn. (19)) Therefore E[1(f (X),? )] ? E[1(f (X n1 ),? n2 )] = ? P(f (X) =? ) + P(f (X n1 ) =? n2 ) = ? 1 ? k e k K j=1 (P(f (X) = j|Y = j)P(Y = j) ? P(f (X) = j)P(Y = j)) = 1 ? k e k K j=1 P(Y = j) (P(f (X) = j) ? P(f (X) = j|Y = j))</formula><p>For clean distribution we have</p><formula xml:id="formula_76">E[1(f (X), Y )] = P(f (X) = Y ) = 1 ? K j=1 P(f (X) = j|Y = j)P(Y = j)</formula><p>For the second term above we have</p><formula xml:id="formula_77">E[1(f (X n1 ), Y n2 )] = P(X n1 ) = Y n2 ) = 1 ? K j=1 P(f (X) = j)P(Y = j) Therefore E[1(f (X), Y )] ? E[1(f (X n1 ), Y n2 )] = K j=1 P(Y = j) (P(f (X) = j) ? P(f (X) = j|Y = j))</formula><p>and the above concludes</p><formula xml:id="formula_78">E[1(f (X),? )] ? E[1(f (X n1 ),? n2 )] = 1 ? k e k (E[1(f (X), Y )] ? E[1(f (X n1 ), Y n2 )]),</formula><p>where the RHS above is the peer loss computed on the clean distribution.</p><p>Again when we have balanced label distribution that P(Y = j) = 1/K,</p><formula xml:id="formula_79">K j=1 P(Y = j) (P(f (X) = j) ? P(f (X) = j|Y = j)) = 1 K ? K j=1 P(Y = j)P(f (X) = j|Y = j) Accuracy = 1 -0-1 risk<label>(19)</label></formula><p>Therefore minimizing peer loss on the clean distribution returns the same minimizer as for the true and clean 0-1 risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Theorem 2</head><p>Proof. From Lemma 2 we know</p><formula xml:id="formula_80">E[ peer (f (X),? )] =(1 ? e ?1 ? e +1 ) ? E[ peer (f (X), Y )] (Lemma 2) =(1 ? e ?1 ? e +1 ) ? E[ (f (X), Y )] ? E[ (f (X n1 ), Y n2 )] =(1 ? e ?1 ? e +1 ) ? E (f (X), Y )] ? 0.5 ? E X [ (f (X), ?1)] ? 0.5 ? E X [ (f (X), +1)]</formula><p>(Independence between n 1 and n 2 , and equal prior)</p><p>When is the 0-1 loss we have (f (X), ?1) + (f (X), +1) = 1, ?x, and therefore</p><formula xml:id="formula_81">E[ peer (f (X),? )] = (1 ? e ?1 ? e +1 ) ? E[ (f (X), Y )] ? 1 With above we provedf * 1peer ? arg min f ?F R D (f ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Theorem 3</head><p>Proof. Apply Lemma 2 we know</p><formula xml:id="formula_82">E[ peer (f (X),? )] = (1 ? e +1 ? e ?1 )E[ peer (f (X), Y )],</formula><p>Denote by f * F ? arg min f ?F R D (f ). From the optimality off * 1peer we have</p><formula xml:id="formula_83">E[ peer (f * 1peer (X),? )] ? E[ peer (f * F (X),? )] ? E[ peer (f * 1peer (X), Y )] ? E[ peer (f * F (X), Y )]</formula><p>i.e.,</p><formula xml:id="formula_84">R D (f * 1peer ) ? p ? E X [ (f * 1peer (X), +1)] ? (1 ? p) ? E X [ (f * 1peer (X), ?1)] ? R D (f * F ) ? p ? E X [ (f * F (X), +1)] ? (1 ? p) ? E X [ (f * F (X), ?1)],<label>(20)</label></formula><p>Note ?f :</p><formula xml:id="formula_85">p ? E X [ (f (X), +1)] + (1 ? p) ? E X [ (f (X), ?1)] ? 0.5 ? E X [ (f (X), +1)] ? 0.5 ? E X [ (f (X), ?1)] =|p ? 0.5| ? E X [ (f (X), +1)] ? E X [ (f (X), ?1)] ?|p ? 0.5|.<label>(21)</label></formula><p>Then we have</p><formula xml:id="formula_86">R D (f * 1peer ) ? 0.5 = R D (f * 1peer ) ? 0.5 ? E X [ (f * 1peer (X), +1)] ? 0.5 ? E X [ (f * 1peer (X), ?1)] ( (f * 1peer (X), +1) + (f * 1peer (X), ?1) = 1) ? R D (f * 1peer ) ? p ? E X [ (f * 1peer (X), +1)] ? (1 ? p) ? E X [ (f * 1peer (X), ?1)] + |p ? 0.5| (Eqn. (21)) ? R D (f * F ) ? p ? E X [ (f * F (X), +1)] ? (1 ? p) ? E X [ (f * F (X), ?1)] + |p ? 0.5| (Eqn. (20)) ? R D (f * F ) ? 0.5 ? E X [ (f * F (X), +1)] ? 0.5 ? E X [ (f * F (X), ?1)] + 2|p ? 0.5| (Eqn. (21)) = R D (f * F ) ? 0.5 + 2|p ? 0.5| ( (f * F (X), +1) + (f * F (X), ?1) = 1) Therefore R D (f * 1peer ) ? R D (f * F ) ? 2|p ? 0.5| = |? p |.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Lemma 3</head><p>Proof.</p><formula xml:id="formula_87">E[1 peer (f (X),? )] =(1 ? e ?1 ? e +1 ) ? E[ peer (f (X), Y )] (Lemma 2) =(1 ? e ?1 ? e +1 ) ? (P(f (X) = Y ) ? P(f (X n1 ) = Y n2 )) =(1 ? e ?1 ? e +1 ) ? (P(f (X) = ?1, Y = +1) + P(f (X) = +1, Y = ?1)</formula><p>? P(f (X) = ?1)P(Y = +1) ? P(f (X) = +1)P(Y = ?1)) (Independence between n 1 and n 2 )</p><formula xml:id="formula_88">=(1 ? e ?1 ? e +1 ) ? (p ? R +1 + (1 ? p) ? R ?1 ? p ? P(f (X) = ?1) ? (1 ? p) ? P(f (X) = +1)) (Law of total probability) =(1 ? e ?1 ? e +1 ) ? (p ? R +1 + (1 ? p) ? R ?1 ? p ? p ? R +1 + (1 ? p) ? (1 ? R ?1 ) ? (1 ? p) ? p ? (1 ? R +1 ) + (1 ? p) ? R ?1 ) =2(1 ? e ?1 ? e +1 ) ? p(1 ? p) ? (R ?1 + R +1 ? 1)<label>(22)</label></formula><p>Again since E[1 peer (f (X),? )] is an affine transform of R ?1 + R +1 we conclude the proof.</p><p>Proof.</p><formula xml:id="formula_89">E[1 ?-peer (f (X),? )] =E[1(f (X),? )] ? ? ? E[1(f (X n1 ),? n2 )] =E[1 peer (f (X),? )] + (1 ? ?) ? E[1(f (X n1 ),? n2 )]</formula><p>(Subtracting and adding E[1(f (X n1 ),? n2 )] to the first and second term)</p><formula xml:id="formula_90">=E[1 peer (f (X),? )] + (1 ? ?) ? P(f (X n1 ) =? n2 ) =E[1 peer (f (X),? )] + (1 ? ?) ? P(f (X) = +1) ? P(? = ?1) + P(f (X) = ?1) ? P(? = +1)</formula><p>Again the last equality is due to the independence between n 1 and n 2 . Replace P(f (X) = +1) and P(f (X) = ?1) as functions of p, R +1 , R ?1 :</p><formula xml:id="formula_91">P(f (X) = +1) = p ? (1 ? R +1 ) + (1 ? p) ? R ?1 P(f (X) = ?1) = p ? R +1 + (1 ? p)(1 ? R ?1 ), we further have E[1 ?-peer (f (X),? )] =E[1 peer (f (X),? )] + (1 ? ?) ? p ? (1 ? R +1 ) + (1 ? p) ? R ?1 ? P(? = ?1) + p ? R +1 + (1 ? p)(1 ? R ?1 ) ? P(? = +1) =E[1 peer (f (X),? )] + (1 ? ?) ? (P(? = +1) ? P(? = ?1)) ? (p ? R +1 ? (1 ? p) ? R ?1 ) + C (C is a constant: C = (1 ? ?) ? (1 ? p) ? P(? = +1) + p ? P(? = ?1) ) =2(1 ? e ?1 ? e +1 ) ? p(1 ? p) ? (R ?1 + R +1 ? 1) (Eqn. (22), Proof of Lemma 3) + (1 ? ?) ? (P(? = +1) ? P(? = ?1)) ? p ? R +1 ? (1 ? p) ? R ?1 + C =R +1 ? 2(1 ? e ?1 ? e +1 ) ? p(1 ? p) + (1 ? ?)p ? (P(? = +1) ? P(? = ?1)) + R ?1 ? 2(1 ? e ?1 ? e +1 ) ? p(1 ? p) ? (1 ? ?)(1 ? p) ? (P(? = +1) ? P(? = ?1)) + C ,</formula><p>where C is a constant:</p><formula xml:id="formula_92">C = C ? 2(1 ? e ?1 ? e +1 ) ? p(1 ? p) Let p 1 ? p = 2(1 ? e ?1 ? e +1 ) ? p(1 ? p) + (1 ? ?) ? p ? (P(? = +1) ? P(? = ?1)) 2(1 ? e ?1 ? e +1 ) ? p(1 ? p) ? (1 ? ?) ? (1 ? p) ? (P(? = +1) ? P(? = ?1)) . that ? = 1 ? (1 ? e ?1 ? e +1 ) ? ? p ?p .</formula><p>we obtain that</p><formula xml:id="formula_93">E[1 ?-peer (f (X),? )] ? (1 ? e ?1 ? e +1 )E[1(f (X), Y )] + const.,<label>(23)</label></formula><p>concluding our proof. The last equation Eqn.(23) also implies the following proposition:</p><p>Proposition 1. For any f, f , we have</p><formula xml:id="formula_94">ED[1 ?-peer (f (X),? )] ? ED[1 ?-peer (f (X),? )] ? (1 ? e ?1 ? e +1 ) E[1(f (X), Y )] ? E[1(f (X), Y )] .</formula><p>e +1 = e ?1 = e: When e +1 = e ?1 = e, we have</p><formula xml:id="formula_95">P(? = +1) ? P(? = ?1) =p ? P(? = +1|Y = +1) + (1 ? p) ? P(? = +1|Y = ?1) ? p ? P(? = ?1|Y = +1) ? (1 ? p) ? P(? = ?1|Y = ?1) =p ? (1 ? e) + (1 ? p) ? e ? p ? e ? (1 ? p) ? (1 ? e) =p(1 ? 2e) ? (1 ? p)(1 ? 2e) =(p ? (1 ? p)) ? (1 ? 2e).</formula><p>That is</p><formula xml:id="formula_96">(1 ? e +1 ? e ?1 ) ? p ?p = (1 ? 2e) ? p ? (1 ? p) P(? = +1) ? P(? = ?1) = 1</formula><p>Therefore ? = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Theorem 5</head><p>Proof. ?f , using Hoeffding's inequality with probability at least 1 ? ?</p><formula xml:id="formula_97">R 1?-peer,D (f ) ? R 1?-peer,D (f ) ? log 2/? 2N 1 ??peer ? 1 ??peer (1 ??peer = 1, 1 ??peer = ??) =(1 + ?) log 2/? 2N ,<label>(24)</label></formula><p>where 1 ??peer = 1, 1 ??peer = ?? denote the upper and lower bound on 1 ??peer .</p><p>Note we also have the following:</p><formula xml:id="formula_98">R 1?-peer,D (f * 1?-peer ) ? R 1?-peer,D (f * 1?-peer ) ?R 1?-peer,D (f * 1?-peer ) ?R 1?-peer,D (f * 1?-peer ) + R 1?-peer,D (f * 1?-peer ) ?R 1?-peer,D (f * 1?-peer ) + R 1?-peer,D (f * 1?-peer ) ? R 1?-peer,D (f * 1?-peer ) ?0 + 2 max f |R 1?-peer,D (f ) ? R 1?-peer,D (f )| Now we show R D (f * 1 ? * -peer ) ? R * =R D (f * 1 ? * -peer ) ? R D (f * 1 ? * -peer ) (Theorem 4) = 1 1 ? e ?1 ? e +1 R 1 ? * -peer ,D (f * 1 ? * -peer ) ? R 1 ? * -peer ,D (f * 1 ? * -peer ) (Proposition 1) ? 2 1 ? e ?1 ? e +1 max f R 1 ? * -peer ,D (f ) ? R 1 ? * -peer ,D (f ) ? 2(1 + ? * ) 1 ? e ?1 ? e +1 log 2/? 2N .</formula><p>(Eqn. <ref type="formula" target="#formula_97">(24))</ref> We conclude the proof.</p><p>Proof. We start with condition (1). From Lemma 2,</p><formula xml:id="formula_99">E[ peer (f (X),? )] =(1 ? e ?1 ? e +1 ) ? E[ peer (f (X), Y )] =(1 ? e ?1 ? e +1 ) ? E[ (f (X), Y )] ? 0.5 ? E[ (f (X), ?1)] ? 0.5 ? E[ (f (X), +1)]</formula><p>The above further derives as</p><formula xml:id="formula_100">E[ peer (f (X),? )] =(1 ? e ?1 ? e +1 ) ? E[ (f (X), Y )] ? 0.5 ? E[ (f (X), Y )] ? 0.5 ? E[ (f (X), ?Y )] = 1 ? e ?1 ? e +1 2 ? E[ (f (X), Y )] ? E[ (f (X), ?Y )]</formula><p>Denote by c := 2 1?e?1?e+1 we have</p><formula xml:id="formula_101">E[ (f (X), Y )] = c ? E[ peer (f (X),? )] + E[ (f (X), ?Y )] Then E[ (f (X), Y )] ? E[ (f * (X), Y )] ? (E[ (f (X), ?Y )] ? E[ (f * (Y ), ?Y ))] =c ? (E[ peer (f (X),? )] ? E[ peer (f * (X),? )]) ?c ? (E[ peer (f (X),? )] ? E[ peer (f * peer (X),? )]) Further by our conditions we know E[ (f (X), Y )]?E[ (f * (X), Y )] ? (E[ (f (X), ?Y )] ? E[ (f * (Y ), ?Y ))] ? E[ (f (X), Y )] ? E[ (f * (X), Y )].</formula><p>Therefore we have proved</p><formula xml:id="formula_102">E[ peer (f (X),? )] ? E[ peer (f * peer (X),? )] ? 1 c E[ (f (X), Y )] ? E[ (f * (X), Y )] .</formula><p>Since (?) is calibrated, and according to Proposition 1 and Theorem 2:</p><formula xml:id="formula_103">ED[1 ?-peer (f (X),? )] ? ED[1 ?-peer (f * (X),? )] =(1 ? e ?1 ? e +1 ) E[1(f (X), Y )] ? E[1(f * (X), Y )] ?(1 ? e ?1 ? e +1 ) ? ? ?1 (E[ (f (X), Y )] ? E[ (f * (X), Y )]) ?(1 ? e ?1 ? e +1 ) ? ? ?1 c ? (E[ peer (f (X),? )] ? E[ peer (f * peer (X),? )]) . Therefore ? peer (x) = 1 c ? ( x 1?e?1?e+1 ),</formula><p>where ? is the calibration transformation function for . It's straight-forward to verify that ? peer (x) satisfies the conditions in Definition 1, when ? satisfied it, c &gt; 0 and 1 ? e ?1 ? e +1 &gt; 0. We conclude the proof. Now we check condition (2). Denotep y = p y (1 ? e y ) + (1 ? p y )e ?y (marginal distribution of the noisy label), where p +1 = p, p ?1 = 1 ? p, then we have :</p><formula xml:id="formula_104">E[ ?-peer (f (X),? )] =E[ (f (X),? ) ? ? ? (f (X n1 ),? n2 )] =E (1 ? e Y ) (f (X), Y ) + e Y ? (f (X), ?Y ) ? ? ?p Y ? (f (X), Y ) ? ? ? (1 ?p Y ) (f (X), ?Y ) =E (1 ? e Y ? ? ?p Y ) (f (X), Y ) + (e Y ? ? ? (1 ?p Y )) (f (X), ?Y ) ? ?p Y ? (f (X), Y ) + ? ? (1 ?p Y ) (f (X), ?Y )</formula><p>encodes the expectation of the peer term: due to the random sampling of n 1 , each X the same chance P(X = x) being paired with other samples. Regardless of the realization of Y , the two terms are exactly onep +1 (f (X), +1) and onep ?1 (f (X), ?1) -this is due to the independence between n 1 and n 2 .</p><formula xml:id="formula_105">Let ?(f (X) ? Y ) := (f (X), Y ), we have E[ ?-peer (f (X),? )] = E (1 ? e Y ? ? ?p Y )?(f (X) ? Y ) + (e Y ? ? ? (1 ?p Y ))?(?f (X) ? Y ) . When e +1 ? ? ? (1 ?p +1 ) = e ?1 ? ? ? (1 ?p ?1 )</formula><p>we also know that</p><formula xml:id="formula_106">1 ? e +1 ? ? ?p +1 = 1 ? e ?1 ? ? ?p ?1</formula><p>This is because</p><formula xml:id="formula_107">1 ? e +1 ? ? ?p +1 + e +1 ? ? ? (1 ?p +1 ) = 1 ? ? and 1 ? e ?1 ? ? ?p ?1 + e ?1 ? ? ? (1 ?p ?1 ) = 1 ? ? From e +1 ? ? ? (1 ?p +1 ) = e ?1 ? ? ? (1 ?p ?1 ) we obtain e +1 ? e ?1 = ?(p ?1 ?p +1 ) = ?(2p ?1 ? 1) ?e +1 ? e ?1 = ?(2(1 ? p)(1 ? e ?1 ) + 2p ? e +1 ? 1) (p ?1 = (1 ? p)(1 ? e ?1 ) + p ? e +1 ) ?e +1 ? e ?1 = ?((1 ? 2p)(1 ? e +1 ? e ?1 ) + e +1 ? e ?1 ) ?(1 ? ?)(e +1 ? e ?1 ) = ?(1 ? 2p)(1 ? e +1 ? e ?1 )</formula><p>But when 1 ? e Y ? ? ?p Y and e Y ? ? ? (1 ?p Y ) are constants that are independent of Y , we can further denote</p><formula xml:id="formula_108">?(f (X) ? Y ) := (1 ? e Y ? ? ?p Y )?(f (X) ? Y ) + (e Y ? ? ? (1 ?p Y ))?(?f (X) ? Y ) := c 1 ? ?(f (X) ? Y ) + c 2 ? ?(?f (X) ? Y ) That is E[?(f (X) ? Y )] = E[ ?-peer (f (X),? )]</formula><p>. Therefore proving calibration for E[ ?-peer (f (X),? )] is equivalent with proving the calibration property for E[?(f (X) ? Y )].</p><p>We now introduce a theorem:</p><p>Theorem 2 (Theorem 6, <ref type="bibr" target="#b26">(Bartlett et al., 2006)</ref>). Let ? be convex. Then ? is classification-calibrated if and only if it is differentiable at 0 and ? &lt; 0.</p><p>We now show that ? is convex:</p><formula xml:id="formula_109">? (?) =c 1 ? ? (?) + c 2 ? ? (??) =(1 ? e Y ? ? ?p Y ) ? ? (?) + (e Y ? ? ? (1 ?p Y ))? (?) =(1 ? e Y ? ? ?p Y + e Y ? ? ? (1 ?p Y ))? (?) =(1 ? ?)? (?) &gt; 0</formula><p>when ? &lt; 1. The last inequality is due to the fact that is convex.</p><p>Secondly we show the first derivative of ? is negative at 0: ? (0) &lt; 0:</p><formula xml:id="formula_110">? (0) =c 1 ? ? (0) ? c 2 ? ? (0) =(1 ? e Y ? ? ?p Y ) ? ? (0) ? (e Y ? ? ? (1 ?p Y ))? (0) =(1 ? 2e Y + ?(1 ? 2p Y ))? (0)<label>(25)</label></formula><p>Due to the random sampling of n 1 , n 2 for the peer term, we have</p><formula xml:id="formula_111">E n1,n2 1 N N n=1 ?-peer (f (x n ),? n ) = 1 N N n=1? (f (x n ),? n ).</formula><p>In above, ? ?p? n (f (x n ),? n ) + ? ? (1 ?p? n ) (f (x n ), ?? n ) encodes the expectation of the peer term (similar to the arguments in Theorem 6) each x n has 1 N chance being paired with each of the training samples -so the expected number of count is 1. Regardless of? n , the two terms are exactly onep +1 (f (x n ), +1) and onep ?1 (f (x n ), ?1) -this is due to the independence between n 1 and n 2 .</p><p>Then via Hoeffding inequality, with probability at least 1 ? ? (over randomness of n 1 , n 2 ),</p><formula xml:id="formula_112">1 N N n=1 ?-peer (f (x n ),? n ) ? 1 N N n=1? (f (x n ),? n ) ? log 2/? 2N ? ( ??peer ? ??peer )<label>(27)</label></formula><p>??peer , ??peer denote the upper and lower bound of ??peer respectively. Further we know that</p><formula xml:id="formula_113">E[ ?-peer (f (X),? )] = E E n1,n2 [ ?-peer (f (X),? )] = E[? (f (X),? )].<label>(28)</label></formula><p>Via Rademacher bound on the maximal deviation we have with probability at least 1 ? ?</p><formula xml:id="formula_114">max f ?F R? ,D (f ) ? R? ,D (f ) ? 2 (? ? F) + log 1/? 2N<label>(29)</label></formula><p>Since is L-Lipschitz, due to the fact that? is linear in ,? is (1 + ?)L-Lipschitz. Based on the Lipschitz composition of Rademacher averages, we have</p><formula xml:id="formula_115">(? ? F) ? (1 + ?)L ? (F)</formula><p>Therefore, via union bound (events in Eqn. <ref type="formula" target="#formula_32">(27)</ref> and Eqn. <ref type="formula" target="#formula_114">(29)</ref>), we know with probability at least 1 ? 2?:</p><formula xml:id="formula_116">1 N N n=1 ?-peer (f (x n ),? n ) ? R ?-peer ,D (f ) = 1 N N n=1 ?-peer (f (x n ),? n ) ?R? ,D (f ) +R? ,D (f ) ? R ?-peer,D (f ) ? 1 N N n=1 ?-peer (f (x n ),? n ) ?R? ,D (f ) + R? ,D (f ) ? R ?-peer ,D (f ) ? log 2/? 2N ? ( ??peer ? ??peer ) + |R? ,D (f ) ? R? ,D (f ) (Eqn. (27) and R ?-peer ,D (f ) = R? ,D (f ): Eqn. (28)) ? log 2/? 2N ? ( ??peer ? ??peer ) + 2(1 + ?)L ? (F) + log 1/? 2N (Eqn. (29)) ?2(1 + ?)L ? (F) + log 2/? 2N ? 1 + ??peer ? ??peer</formula><p>In above R ?-peer ,D (f ) = R? ,D (f ) because ?-peer and? share the same expected risk overD by construction. Plug in the fact that ?-peer is linear in : ??peer ?? ? ? ? , ??peer ? ? ? ?? and an easy consequence that ??peer ? ??peer ? (1 + ?)(? ? ).</p><p>Let ? := ?/2, we conclude the proof. Proof. This was proved in the proof for Theorem 6, when proving the classification calibration property of ?-peer under condition (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Implementation Details</head><p>On each benchmark, we use the same hyper-parameters for all neural network based methods. For C-SVM, we fix one of the weights to 1, and tune the other. For PAM, we tune the margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The full experiment results are shown in <ref type="table">Table.</ref>A1. Equalized Prior indicates that in the corresponding experiments, we resample to make sure P(Y = +1) = P(Y = ?1) and we fix ? = 1 in these experiments. Our method is competitive in all the datasets and even able to outperform the surrogate loss method with access to the true noise rates in most of them. C-SVM is also robust when noise rates are symmetric, and is competitive in 8 datasets.</p><p>From <ref type="figure" target="#fig_0">Figure A2</ref>, we can see our peer loss can prevent over-fitting, which is also part of the reason of its achieved high robustness across different datasets and noise rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D visualization of decision boundary</head><p>Peer Loss Functions  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Peer Loss Functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>With Prior Equalization p = 0.5 Without Prior Equalization p = 0.   <ref type="table">Table A1</ref>. Experiment Results on 10 UCI Benchmarks. Entries within 2% from the best in each row are in bold. Surr: surrogate loss method <ref type="bibr" target="#b9">(Natarajan et al., 2013)</ref>; DMI: <ref type="bibr" target="#b26">(Xu et al., 2019)</ref>; Symm: symmetric loss method <ref type="bibr">(Ghosh et al., 2015)</ref>. All method-specific parameters are estimated through cross-validation. The proposed method (Peer) are competitive across all the datasets. Neural-networkbased methods (Peer, Surrogate, NN, Symmetric, DMI) use the same hyper-parameters. All the results are averaged across 8 random seeds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Accuracy on test set during training. Splice (e?1 = 0.4, e+1 = 0.4). More examples can be found in Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Decision boundary for cross entropy. Left: trained on clean data. Right: trained on noisy labels, e+1 = e?1 = 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Decision boundary for peer loss. Left: e+1 = e?1 = 0.2. Right: e+1 = 0.2, e?1 = 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Follow-up works(Du Plessis et al., 2013;<ref type="bibr" target="#b21">Van Rooyen et al., 2015b;</ref><ref type="bibr" target="#b7">Menon et al., 2015;</ref> Charoenphakdee et al., 2019)  have looked into leveraging symmetric conditions and 0-1 loss with asymmetric noise, and with more evaluation metrics, such as balanced error rate and AUROC. In particular, experimental evidence is reported in (Charoenphakdee et al., 2019) on the importance of symmetricity when learning with noisy labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Example 3 .</head><label>3</label><figDesc>Logarithmic scoring function: S r A , r B := log P y B = r B |y A = r A .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>S defined in Example 1 &amp; 2 induce strict truthfulness when y A and y B are stochastically relevant. with defining stochastic relevance as follows: Definition A1. y A and y B are stochastically relevant if ? s ? {?1, +1} s.t. P y B = s|y A = +1 = P y B = s|y A = ?1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>P</head><label></label><figDesc>(f * (X) = +1) = 1 ? P (f * (X) = ?1) = 0.5 For noisy labels: P(? = ?1) = P(? = ?1|Y = ?1)P(Y = ?1) + P(? = ?1|Y = +1)P(Y = +1) = (1 ? e ?1 ) ? 0.4 + e +1 ? 0.6 = 0.52, and P (f * (X) = +1) = 1 ? P (f * (X) = ?1) = 0.48</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Breast (e?1 = 0.4, e+1 = 0.4) Figure A2. Accuracy on test set during training Proof for Lemma 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure A3 .</head><label>A3</label><figDesc>Decision boundary for cross entropy trained on clean and noisy data (Left: trained on noisy labels, e+1 = e?1 = 0.2. Middle: trained on noisy labels, e+1 = e?1 = 0.4. Right: asymmetric noise e+1 = 0.2, e?1 = 0.3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure A4 .</head><label>A4</label><figDesc>Decision boundary for peer loss. Left: e+1 = e?1 = 0.2. Middle: e+1 = e?1 = 0.4. Right: asymmetric noise e+1 = 0.2, e?1 = 0.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, 0.3 0.977 0.968 0.969 0.974 0.964 0.977 0.968 0.969 0.974 0.964 Twonorm 0.2, 0.4 0.976 0.919 0.959 0.966 0.911 0.976 0.919 0.959 0.966 0.911 (20,3700,3700) 0.4, 0.4 0.973 0.934 0.958 0.936 0.883 0.973 0.934 0.958 0.936 0.883 0.1, 0.3 0.919 0.878 0.851 0.875 0.811 0.925 0.885 0.868 0.889 0.809 Splice 0.2, 0.4 0.901 0.832 0.757 0.801 0.714 0.912 0.84 Table</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell></row><row><cell>(d, N+, N?)</cell><cell cols="2">e?1, e+1 Peer</cell><cell cols="3">Surr Symm DMI</cell><cell>NN</cell><cell>Peer</cell><cell>Surr Symm DMI</cell><cell>NN</cell></row><row><cell></cell><cell cols="7">0.10.782</cell><cell>0.81 0.725</cell></row><row><cell cols="5">(60,1527,1648) 0.4, 0.4 0.819 0.754 0.657</cell><cell cols="3">0.66 0.626 0.822 0.755 0.674 0.647 0.601</cell></row><row><cell></cell><cell cols="3">0.1, 0.3 0.833 0.78</cell><cell cols="4">0.777 0.797 0.756 0.856 0.802 0.803</cell><cell>0.83</cell><cell>0.75</cell></row><row><cell>Diabetes</cell><cell cols="7">0.2, 0.4 0.755 0.681 0.634 0.682 0.596 0.739 0.705 0.695 0.707 0.672</cell></row><row><cell>(8,268,500)</cell><cell cols="7">0.4, 0.4 0.719 0.645 0.619 0.637 0.551 0.651 0.685</cell><cell>0.68</cell><cell>0.633 0.583</cell></row><row><cell></cell><cell cols="7">0.1, 0.3 0.639 0.563 0.507 0.529 0.519 0.727 0.645 0.709 0.666 0.648</cell></row><row><cell>German</cell><cell cols="3">0.2, 0.4 0.664 0.59</cell><cell>0.6</cell><cell cols="3">0.618 0.572 0.676 0.681 0.537 0.573 0.535</cell></row><row><cell>(23,300,700)</cell><cell cols="3">0.4, 0.4 0.606 0.55</cell><cell cols="4">0.573 0.573 0.556 0.654 0.632 0.549 0.611 0.553</cell></row><row><cell></cell><cell>0.1, 0.3</cell><cell cols="6">0.89 0.895 0.892 0.856 0.868 0.893 0.898 0.883 0.785 0.863</cell></row><row><cell>Waveform</cell><cell cols="3">0.2, 0.4 0.881 0.89</cell><cell cols="4">0.828 0.835 0.81 0.884 0.884 0.745 0.761 0.837</cell></row><row><cell cols="2">(21,1647,3353) 0.4, 0.4</cell><cell cols="6">0.87 0.866 0.867 0.773 0.835 0.853 0.852 0.852 0.672 0.828</cell></row><row><cell></cell><cell cols="2">0.1, 0.3 0.906</cell><cell>0.9</cell><cell>0.89</cell><cell cols="3">0.87 0.909 0.943 0.909 0.897 0.811 0.93</cell></row><row><cell>Image</cell><cell cols="7">0.2, 0.4 0.836 0.862 0.719 0.845 0.832 0.672 0.755 0.722</cell><cell>0.86 0.599</cell></row><row><cell>(18,1320,990)</cell><cell cols="3">0.4, 0.4 0.741 0.72</cell><cell cols="4">0.788 0.763 0.732 0.806 0.803 0.823 0.762</cell><cell>0.8</cell></row></table><note>1. Experiment results on 6 UCI Benchmarks (The full table of all details on 10 UCI Benchmarks are deferred to Appendix; N+, N? are the numbers of positive and negative samples). Surr: surrogate loss method (Natarajan et al., 2013); DMI: (Xu et al., 2019); Symm: symmetric loss method (Ghosh et al., 2015). Entries within 2% from the best in each row are highlighted in bold. All results are averaged across 8 random seeds. Neural-network-based methods (Peer, Surrogate, NN, Symmetric, DMI) use the same hyper-parameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>)</head><label></label><figDesc>Acknowledgement Yang Liu would like to thank Yiling Chen for inspiring early discussions on this problem. The authors thank Tongliang Liu, Ehsan Amid and Manfred Warmuth for constructive comments and conversations, and Nontawat Charoenphakdee for his comments on related works. The authors also would like to thank Xingyu Li, Zhaowei Zhu and Jiaheng Wei for detailed discussions, suggestions and help with generating Figures 3 and 4. Cheng, J., Liu, T., Ramamohanarao, K., and Tao, D. Learning with bounded instance-and label-dependent label noise. ICML, arXiv:1709.03768, 2020. Yi, K. and Wu, J. Probabilistic end-to-end noise correction for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7017-7025, 2019. Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.Learning from Noisy Labels Our work fits within a stream of research on learning with noisy labels. A large portion of research on this topic works with the random classification noise (RCN) model, where observed labels are flipped independently with probability e ? [0, 1</figDesc><table><row><cell cols="2">Appendix</cell></row><row><cell>Related work with more details</cell><cell></cell></row><row><cell></cell><cell>Dasgupta, A. and Ghosh, A. Crowdsourced judgement</cell></row><row><cell></cell><cell>elicitation with endogenous proficiency. In Proceed-</cell></row><row><cell></cell><cell>ings of the 22nd international conference on World Wide</cell></row><row><cell></cell><cell>Web, pp. 319-330. International World Wide Web Con-</cell></row><row><cell>Zhang, Z. and Sabuncu, M. R. Generalized cross entropy</cell><cell>ferences Steering Committee, 2013.</cell></row><row><cell>loss for training deep neural networks with noisy labels, This work is partially funded by the Defense Advanced contained herein are those of the authors and should not tract No. N66001-19-C-4014. The views and conclusions Warfare Systems Center Pacific (SSC Pacific) under Con-Research Projects Agency (DARPA) and Space and Naval 2018.</cell><cell>Du Plessis, M. C., Niu, G., and Sugiyama, M. Clustering telligence, pp. 1-6. IEEE, 2013. ence on Technologies and Applications of Artificial In-datasets having different class balances. In 2013 Confer-unclustered data: Unsupervised binary labeling of two</cell></row><row><cell>be interpreted as necessarily representing the official poli-</cell><cell>Ghosh, A., Manwani, N., and Sastry, P. Making risk mini-</cell></row><row><cell>cies, either expressed or implied, of DARPA, SSC Pacific</cell><cell>mization tolerant to label noise. Neurocomputing, 2015.</cell></row><row><cell>or the U.Cesa-Bianchi, N., Dichterman, E., Fischer, P., Shamir, E., and Simon, H. U. Sample-efficient strategies for learning</cell><cell>Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Ad-vances in neural information processing systems, 2018. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016. Jenni, S. and Favaro, P. Deep bilevel learning. In Pro-ceedings of the European Conference on Computer Vi-sion (ECCV), pp. 618-633, 2018.</cell></row><row><cell>in the presence of noise. Journal of the ACM, 1999.</cell><cell>Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L.</cell></row><row><cell></cell><cell>Mentornet: Learning data-driven curriculum for very</cell></row><row><cell>Cesa-Bianchi, N., Shalev-Shwartz, S., and Shamir, O. On-</cell><cell>deep neural networks on corrupted labels. arXiv preprint</cell></row><row><cell>line learning of noisy data. IEEE Transactions on Infor-</cell><cell>arXiv:1712.05055, 2017.</cell></row><row><cell>mation Theory, 57(12):7907-7931, 2011.</cell><cell></cell></row><row><cell></cell><cell>Khardon, R. and Wachman, G. Noise tolerant variants of</cell></row><row><cell>Charoenphakdee, N., Lee, J., and Sugiyama, M. On sym-</cell><cell>the perceptron algorithm. J. Mach. Learn. Res., 8:227-</cell></row><row><cell>metric losses for learning from corrupted labels. In In-</cell><cell>248, May 2007.</cell></row><row><cell>ternational Conference on Machine Learning, 2019.</cell><cell></cell></row></table><note>S. Government. The U.S. Government is autho- rized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.References Amid, E., Warmuth, M. K., Anil, R., and Koren, T. Robust bi-tempered logistic loss based on bregman divergences. In Advances in Neural Information Processing Systems, pp. 15013-15022, 2019. Bartlett, P. L., Jordan, M. I., and McAuliffe, J. D. Convex- ity, classification, and risk bounds. Journal of the Amer- ican Statistical Association, 101(473):138-156, 2006. Ben-David, S., P?l, D., and Shalev-Shwartz, S. Agnostic online learning. In COLT 2009. Bylander, T. Learning linear threshold functions in the presence of classification noise. In Proceedings of the seventh annual conference on Computational learning theory, pp. 340-347. ACM, 1994.Chaudhuri, K., Monteleoni, C., and Sarwate, A. D. Differ- entially private empirical risk minimization. Journal of Machine Learning Research, 12(3), 2011.Ghosh, A., Kumar, H., and Sastry, P. Robust loss functions under label noise for deep neural networks. In Thirty- First AAAI Conference on Artificial Intelligence, 2017. Gneiting, T. and Raftery, A. E. Strictly proper scoring rules, prediction, and estimation. Journal of the American Sta- tistical Association, 102(477):359-378, 2007. Goldberger, J. and Ben-Reuven, E. Training deep neural- networks using a noise adaptation layer. 2016.Kong, Y. and Schoenebeck, G. Water from two rocks: Maximizing the mutual information. In Proceedings of the 2018 ACM Conference on Economics and Computa- tion, pp. 177-194. ACM, 2018.2 ] (Bylander, 1994; Cesa-Bianchi et al.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We provide other examples of peer prediction functions in the Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">To be precise, it is an informed truthfulness. We refer interested readers to<ref type="bibr" target="#b16">(Shnayder et al., 2016)</ref> for details.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Theorem 7</head><p>Proof. We first prove the following Rademacher complexity bound:</p><p>Lemma 3. Let (F) denote the Rademacher complexity of F. L denote the Lipschitz constant of . Then with probability</p><p>Note we also have the following ??:</p><p>with probability at least 1 ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Lemma 3</head><p>Proof. Definep y = p y (1 ? e y ) + (1 ? p y )e ?y ? (0, 1) (marginal distribution of the noisy label), where p +1 = p, p ?1 = 1 ? p; and define the following loss function:</p><p>(x n ,? n ) := (f (x n ),? n ) ? ? ?p? n (f (x n ),? n ) ? ? ? (1 ?p? n ) (f (x n ), ?? n )</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building text classifiers using positive and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third IEEE International Conference on Data Mining, ICDM &apos;03</title>
		<meeting>the Third IEEE International Conference on Data Mining, ICDM &apos;03<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page">179</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="447" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Machine Learning aided Peer Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
			<publisher>ACM EC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Peer loss functions: Learning from noisy labels without knowing noise rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the minimal supervision for training any binary classifier from only unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10585</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Noise tolerance under risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning from corrupted binary labels via classprobability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Eliciting informative feedback: The peer-prediction method. Management Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeckhauser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1359" to="1373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A bayesian truth serum for subjective data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prelec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">306</biblScope>
			<biblScope unit="issue">5695</biblScope>
			<biblScope unit="page" from="462" to="466" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A robust bayesian truth serum for non-binary signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Radanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 27th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incentives for effort in crowdsourcing using the peer truth serum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Radanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Faltings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jurca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A rate of convergence for mixture proportion estimation, with application to learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classification with asymmetric label noise: Consistency and maximal denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Handy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flaska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Informed truthfulness in multi-task peer prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shnayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frongillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Parkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM Conference on Economics and Computation</title>
		<meeting>the 2016 ACM Conference on Economics and Computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="179" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Selfie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning svms from sloppily labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stempfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="884" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An average classification algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01520</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A robust bayesian truth serum for small populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Witkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th AAAI Conference on Artificial Intelligence, AAAI &apos;12</title>
		<meeting>the 26th AAAI Conference on Artificial Intelligence, AAAI &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dwelling on the Negative: Incentivizing Effort in Peer Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Witkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Key</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Parkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st AAAI Conference on Human Computation and Crowdsourcing (HCOMP&apos;13)</title>
		<meeting>the 1st AAAI Conference on Human Computation and Crowdsourcing (HCOMP&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugiyama</forename></persName>
		</author>
		<title level="m">M. Parts-dependent label noise: Towards instance-dependent label noise</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An information-theoretic noise-robust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bartlett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03388</idno>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>we proved that ? (0) &lt; 0. Then based on Theorem 6 of. we know ?-peer is classification calibrated</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
