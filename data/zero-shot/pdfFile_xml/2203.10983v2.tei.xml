<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BNS-GCN: EFFICIENT FULL-GRAPH TRAINING OF GRAPH CONVOLUTIONAL NETWORKS WITH PARTITION-PARALLELISM AND RANDOM BOUNDARY NODE SAMPLING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Sung</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
						</author>
						<title level="a" type="main">BNS-GCN: EFFICIENT FULL-GRAPH TRAINING OF GRAPH CONVOLUTIONAL NETWORKS WITH PARTITION-PARALLELISM AND RANDOM BOUNDARY NODE SAMPLING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art method for graph-based learning tasks. However, training GCNs at scale is still challenging, hindering both the exploration of more sophisticated GCN architectures and their applications to real-world large graphs. While it might be natural to consider graph partition and distributed training for tackling this challenge, this direction has only been slightly scratched the surface in the previous works due to the limitations of existing designs. In this work, we first analyze why distributed GCN training is ineffective and identify the underlying cause to be the excessive number of boundary nodes of each partitioned subgraph, which easily explodes the memory and communication costs for GCN training. Furthermore, we propose a simple yet effective method dubbed BNS-GCN that adopts random Boundary-Node-Sampling to enable efficient and scalable distributed GCN training. Experiments and ablation studies consistently validate the effectiveness of BNS-GCN, e.g., boosting the throughput by up to 16.2? and reducing the memory usage by up to 58%, while maintaining a full-graph accuracy. Furthermore, both theoretical and empirical analysis show that BNS-GCN enjoys a better convergence than existing sampling-based methods. We believe that our BNS-GCN has opened up a new paradigm for enabling GCN training at scale.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph convolutional networks (GCNs) <ref type="bibr" target="#b17">(Kipf &amp; Welling, 2016)</ref> have emerged as the state-of-the-art (SOTA) method for various graph-based learning tasks, including node classification <ref type="bibr" target="#b17">(Kipf &amp; Welling, 2016)</ref>, link prediction <ref type="bibr" target="#b38">(Zhang &amp; Chen, 2018)</ref>, graph classification <ref type="bibr" target="#b35">(Xu et al., 2018)</ref>, and recommendation systems <ref type="bibr" target="#b36">(Ying et al., 2018)</ref>. The outstanding performance of GCNs is attributed to their unrestricted and irregular neighborhood connectivity, which provides them a greater applicability to graph-based data than convolutional neural networks (CNNs) that adopt a fixed regular neighborhood structure. Specifically, given a node in a graph, a GCN first aggregates the features of its neighbors and then updates its own feature through a hierarchical feed-forward propagation. The two dominant operations, aggregate and update of node features, enables GCNs to take advantage of the graph structure and thus outperform their structureunaware alternatives.</p><p>Despite their promising performance, training GCNs at scale has been very challenging, thereby hindering the exploration of more sophisticated GCN architectures and restricting their real-world applications to large graphs. This is because as the graph size grows, the sheer number of node features and the giant adjacency matrix can easily explode the required memory and communications. To tackle this challenge, several sampling-based methods have been developed at a cost of approximation errors. For example, GraphSAGE <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref> and VR-GCN <ref type="bibr">(Chen et al., 2018b</ref>) reduce a full graph into a mini-batch via neighbor sampling; alternative methods <ref type="bibr" target="#b5">(Chiang et al., 2019;</ref><ref type="bibr" target="#b37">Zeng et al., 2020)</ref> extract sub-graphs as training samples.</p><p>In parallel with sampling-based methods, a more recent direction for handling large-graph training is distributed GCN training, which aims at training a large full-graph over multiple GPUs without degrading the accuracy. The key idea is to partition a giant graph into small subgraphs such that each can fit a single GPU, and train them in parallel with necessary communication. Following this "partition-parallelism" paradigm, pioneering efforts (NeuGraph , ROC <ref type="bibr" target="#b15">(Jia et al., 2020)</ref>, CAGNET <ref type="bibr" target="#b31">(Tripathy et al., 2020)</ref>, Dorylus <ref type="bibr" target="#b30">(Thorpe et al., 2021)</ref>, and PipeGCN <ref type="bibr" target="#b33">(Wan et al., 2022)</ref>) have demonstrated a promising training performance. Nonetheless, these works still suffer from heavy communication traffics, limiting their achievable training efficiency, let arXiv:2203.10983v2 <ref type="bibr">[cs.</ref>LG] 26 Mar 2022 alone the potentially harmful staleness due to asynchronous training <ref type="bibr" target="#b30">(Thorpe et al., 2021)</ref>.</p><p>To enable scalable and efficient large-graph GCN training without compromising the full-graph accuracy, this work sets out to understand the underlying cause of the communication and memory explosion in distributed GCNs training and finds that distributed GCN training can be ineffective if it is not designed properly, which motivates us to make the following contributions:</p><p>? We first analyze and identify three main challenges in partition-parallel training of GCNs: (1) overwhelming communication volume, (2) prohibitive memory requirement, and (3) imbalanced memory consumption. We further localize their cause to be an excessive number of boundary nodes (rather than boundary edges) associated with each partitioned subgraph, which is unique to GCNs due to their neighbor aggregation (see Section 3.1). This finding enhances the understanding in distributed GCN training and can potentially inspire further ideas in this direction.</p><p>? To tackle all above challenges in one shot, we propose a simple yet effective method dubbed BNS-GCN which randomly samples features of boundary nodes at each training iteration and achieves a triple win -aggressively shrinking the communication and memory requirements while leading to a better generalization accuracy (see Section 3.2). To the best of our knowledge, this is the first work directly targeting at reducing the communication volume in distributed GCN training, without incurring extra computing resource overhead (e.g., CPU) or hurting the achieved accuracy.</p><p>? We further provide theoretical analysis to validate the improved convergence offered by BNS-GCN (see <ref type="bibr">Section 3.3)</ref>. Extensive experiments and ablation studies consistently validate the benefit of BNS-GCN in both training efficiency and accuracy, e.g., boosting the throughput by up to 16.2? and reducing the memory usage by up to 58% while achieving the same or an even better accuracy, over the SOTA methods, when being applied to Reddit, ogbn-products, Yelp, and ogbn-papers100M datasets (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORKS</head><p>Graph Convolutional Networks. GCNs take graphstructured data as inputs and learn feature vectors (embedding) for each node of a graph. Specifically, GCN performs two major steps in each layer, i.e., neighbor aggregation and node update, which can be represented as:</p><formula xml:id="formula_0">z ( ) v = ? ( ) h ( ?1) u | u ? N (v) (1) h ( ) v = ? ( ) z ( ) v , h ( ?1) v (2)</formula><p>where N (v) denotes the neighbor set of node v in the graph, h ( ) u denotes the learned feature vector of node u at the -th layer, ? ( ) denotes the aggregation function that takes neighbor features to generate aggregation result z ( ) v for node v, and finally ? ( ) gets the feature of node v updated. A famous instance of GCNs is GraphSAGE with a mean aggregator <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref></p><formula xml:id="formula_1">, in which ? ( ) is the mean function and ? ( ) is ? W ( ) ? CONCAT z ( ) v , h ( ?1) v ,</formula><p>where W ( ) is the weight matrix and ? is a non-linear activation. While we mainly use this instance for evaluating our BNS-GCN, our approach can be easily extended to other popular aggregators and update functions.</p><p>Sampling-Based GCN Training. Real-world graphs consist of millions of nodes and edges <ref type="bibr" target="#b13">(Hu et al., 2020)</ref>, far beyond the capability of vanilla GCNs. As such, samplingbased methods were proposed, e.g., neighbor sampling <ref type="bibr" target="#b11">(Hamilton et al., 2017;</ref><ref type="bibr">Chen et al., 2018b)</ref>, layer sampling <ref type="bibr" target="#b1">(Chen et al., 2018a;</ref><ref type="bibr" target="#b14">Huang et al., 2018;</ref><ref type="bibr" target="#b43">Zou et al., 2019)</ref>, and subgraph sampling <ref type="bibr" target="#b5">(Chiang et al., 2019;</ref><ref type="bibr" target="#b37">Zeng et al., 2020)</ref>, which yet suffer from:</p><p>? Inaccurate feature estimation: although most sampling methods provide unbiased estimation of node features, the variance of these estimation hurts the model accuracy. As <ref type="bibr" target="#b6">(Cong et al., 2020)</ref> shows, a smaller variance is beneficial to improving the accuracy of a samplingbased method;</p><p>? Neighbor explosion: <ref type="bibr" target="#b11">Hamilton et al. (2017)</ref> first uses node sampling to randomly select several neighbors in the previous layer, but as GCNs get deeper the size of selected nodes exponentially increases. <ref type="bibr">Chen et al. (2018b)</ref> further proposes samplers for restricting the size of neighbor expansion, which yet suffers from heavy memory requirements;</p><p>? Sampling overhead: All sampling-based methods incur extra time for generating mini-batches, which can occupy 25%+ of the training time <ref type="bibr" target="#b37">(Zeng et al., 2020)</ref>.</p><p>Distributed Training for GCNs. To train GCNs for realworld large graphs, distributed training leveraging multiple GPUs to enable full-graph training has been shown to be promising. Nevertheless, GCNs training is different from the challenge of classical distributed DNN training where (1) data samples are small yet the model is large (model parallelism <ref type="bibr" target="#b18">(Krizhevsky, 2014;</ref><ref type="bibr" target="#b12">Harlap et al., 2018)</ref>) and <ref type="formula">(2)</ref> data samples do not have dependency (data parallelism <ref type="bibr" target="#b22">2018b;</ref><ref type="bibr">a)</ref>), both violating the nature of GCNs. As such, GCN-oriented methods should partition the full graph into small subgraphs such that each could be fitted into a single GPU memory, and train them in parallel, where communication across subgraphs is necessary to exchange boundary node features to perform GCNs' neighbor aggregation, which is called vanilla partition parallelism as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a). Following this paradigm, several works have been proposed. ROC <ref type="bibr" target="#b15">(Jia et al., 2020)</ref>, NeuGraph , and AliGraph <ref type="bibr" target="#b41">(Zhu et al., 2019)</ref> partition large graphs and store all partitions in CPUs and swaps a fraction of each partition to compute in GPUs (see <ref type="figure" target="#fig_0">Figure 1</ref>(b)). Their training efficiency are thus compromised due to expensive CPU-GPU swaps. CAGNET <ref type="bibr" target="#b31">(Tripathy et al., 2020)</ref> and P 3 <ref type="bibr" target="#b8">(Gandhi &amp; Iyer, 2021)</ref> further split node features and layers to enable intra-layer model parallelism (see <ref type="bibr">Figure 1(c)</ref>), which however incurs a heavy communication overhead especially when the feature dimension is large. Dorylus <ref type="bibr" target="#b30">(Thorpe et al., 2021)</ref> improves the vanilla partition parallelism by pipelining each fine-grain computation operation in GCN training over numerous CPU threads, which still suffers from the communication bottleneck.</p><p>Distributed Graph Systems. Distributed graph systems were proposed to solve general graph problems <ref type="bibr" target="#b9">(Gonzalez et al., 2012;</ref><ref type="bibr" target="#b28">Shun &amp; Blelloch, 2013;</ref><ref type="bibr" target="#b25">Nguyen et al., 2013;</ref><ref type="bibr" target="#b42">Zhu et al., 2016;</ref><ref type="bibr" target="#b4">Chen et al., 2019)</ref>.  also proposes a distributed learning system for graph embedding. However, none of these considers node features and hence cannot be used for GCN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED BNS-GCN FRAMEWORK</head><p>Overview. To address all aforementioned limitations (see <ref type="figure" target="#fig_0">Figure 1</ref>(a-c)), we propose partition-parallel training of GCNs with Boundary Node Sampling, dubbed BNS-GCN, as shown in <ref type="figure" target="#fig_0">Figure 1(d)</ref>. BNS-GCN partitions a full-graph with minimized boundary nodes and then further randomly samples the boundary nodes to shrink both communication and memory costs, enabling efficient large-graph training while maintaining the full-graph accuracy. We develop BNS-GCN by first analyzing the three major challenges in partition-parallel training of GCNs and then pinpoint their underlying cause (see Section 3.1). To tackle the cause directly, we design a simple yet effective sampling strategy that can simultaneously alleviate all three challenges (see Section 3.2), while achieving much reduced variances (i.e., closer to that of the full-graph one) of feature approximation as compared to existing sampling methods (see Section 3.3). We further discuss the difference between BNS-GCN and other sampling-based methods (see Section 3.4) for better understanding our new contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Challenges in Partition-Parallel Training</head><p>To enable full-graph training, the original graph can be partitioned into smaller subgraphs (i.e., partitions) to be trained locally on each accelerator/node while communicating dependent node features across subgraphs, which is termed as partition parallelism. As shown in <ref type="figure">Figure 2</ref>, each subgraph contains a subset of nodes from the original graph, termed as an inner node set (see "Inner"). Additionally, each subgraph holds a boundary node set (see "Boundary") containing dependent nodes from other subgraphs. Such a boundary node set is dictated by GCNs' neighbor aggregation from neighbor subgraphs, e.g., node-5 in <ref type="figure">Figure 2</ref>   <ref type="figure">Figure 2</ref>: Illustrating vanilla partition parallel training. A large graph is partitioned into smaller subgraphs (see the inner nodes in black) with each being able to fit into one GPU memory. The key challenge is that excessive boundary nodes (in orange) associated with each subgraph (due to GCNs' neighbor aggregation) can lead to a heavy communication overhead, extra memory cost, and memory imbalance among subgraphs, thus limiting the achievable scalability and efficiency of distributed GCN training.  <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref>. The standard METIS <ref type="bibr" target="#b16">(Karypis &amp; Kumar, 1998)</ref> is used for graph partition. However, vanilla partition parallelism is neither inefficient nor scalable due to the following three major challenges:</p><p>i Heavy Communication Overhead is resulting from exchanging boundary nodes across partitions, limiting the scalability to larger graphs or using more partitions.</p><p>ii Prohibitive Memory Requirement is incurred in each partition to hold both the inner and boundary sets, the latter of which can overflow a GPU's memory capacity.</p><p>iii Imbalanced Memory Requirements exists across all partitions, where the memory straggler (i.e., the partition requiring a significantly larger memory than others) not only determines the memory requirement but also causes under-utilization of other partitions' GPUs.</p><p>We identify that all three challenges above share the same underlying cause -the overhead of extra boundary nodes associated with each partition due to distributed partitions.</p><p>Communication Cost Analysis of Vanilla Partition Parallelism. For a partition G i , its communication volume can be defined as Vol</p><formula xml:id="formula_2">(G i ) = v?Gi D(v) where D(v)</formula><p>is the number of different partitions in which v has at least one neighbor node, excluding G i <ref type="bibr" target="#b0">(Bulu? et al., 2016)</ref>. This value quantifies the total amount of features G i needs to send during each propagation (Equation 1). As the total number of received messages equals to the total number of sent messages, the total communication volume equals to the total number of boundary nodes (instead of boundary edges):</p><formula xml:id="formula_3">Vol total = i Vol(G i ) = i n (i) bd (3) where n (i)</formula><p>bd is the number of boundary nodes in partition G i . Memory Cost Analysis of Vanilla Partition Parallelism. For a -th layer, suppose the input feature is of dimension d ( ) , and the numbers of inner nodes and boundary nodes in partition G i are n (i) in and n (i) bd , respectively. Considering a general case where all node features and inner nodes' aggregated features are saved for the back propagation in both Equation 1 and Equation 2. When using a GraphSAGE layer with a mean aggregator, the memory cost is:</p><formula xml:id="formula_4">Mem ( ) (G i ) = (3n (i) in + n (i) bd )d ( )<label>(4)</label></formula><p>As a result, the memory requirement increases linearly with the number of boundary nodes (instead of boundary edges).</p><p>The challenge is that the number of boundary nodes can be excessive. <ref type="table" target="#tab_1">Table 1</ref> shows a typical example, where the number of boundary nodes in each partition can be as high as 5.5? of that of inner nodes, leading to both prohibitive communication and memory overhead.</p><p>Memory Imbalance Analysis in Vanilla Partition Parallelism. The memory cost can be highly imbalanced across partitions due to the irregular amounts of boundary nodes despite the balanced amount of inner nodes (see  <ref type="table" target="#tab_1">Table 1</ref>). Furthermore, when scaling up to more partitions, the memory imbalance becomes more severe. <ref type="figure" target="#fig_1">Figure 3</ref> shows such an example where we split a giant graph (ogbn-papers100M <ref type="bibr" target="#b13">(Hu et al., 2020)</ref>) into 192 parts. The memory straggler (the one with boundary-inner ratio of 8) costs significantly more memory than other partitions, which not only raises the memory requirement but also incurs memory under-utilization for all other partitions' GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Proposed BNS-GCN Technique</head><p>Graph Partition. As boundary nodes are the cause for the efficiency bottleneck of partition parallelism, the graph partition has to minimize all boundary node sets to minimize subsequent communication and memory overheads, dubbed Goal-1. Besides, the graph partition must also achieve balanced computation time across all partitions, dubbed Goal-2, since partition parallelism is a synchronous training paradigm that requires frequent synchronization at each layer (again due to GCNs' neighbor aggregation), under which unbalanced partition results in stragglers that block other partitions to proceed.</p><p>Prior works <ref type="bibr" target="#b31">(Tripathy et al., 2020;</ref><ref type="bibr" target="#b34">Zheng et al., 2020)</ref> aim at achieving only Goal-2 yet ignore Goal-1, while this work achieves both. For Goal-2, we approximate the computational complexity of each node, aiming to balance computations across all partitions (e.g., when GraphSAGE computation is dominated by Equation 2, the complexity is proportional to the number of nodes, so we set partitions with an equal size in this case). Then we optimize the graph partition algorithm for Goal-1. In this work, the popular METIS <ref type="bibr" target="#b16">(Karypis &amp; Kumar, 1998</ref>) is adopted as the default graph partition algorithm and its objective is set to minimize the communication volume, i.e., minimize the number of boundary nodes <ref type="formula">(Equation 3</ref>). Besides METIS, other partitioning algorithms are also compatible with BNS-GCN Algorithm 1: Boundary node sampling for partitionparallel training (per-partition view) Input: partition number m, partition id i, graph partition</p><formula xml:id="formula_5">G i , boundary node set B i , node feature X i , label Y i , sampling rate p, initial model w[0], learning rate ? Output: trained model w[T ] after T iterations 1 V i ? {node v ? G i : v / ? B i }; create inner node set 2 H (0) ? X i ; initialize input features 3 for t ? 1 : T do 4 U i ? randomly pick elements in B i with probability p; 5 H i ? node induced subgraph of G i from V i ? U i ; 6 Broadcast U i and Receive [U 1 , ? ? ? , U m ]; 7 [S i,1 , ? ? ? , S i,m ] ? [U 1 ? V i , ? ? ? , U m ? V i ]; 8 for ? 1 : L do 9 Send [H ( ?1) Si,1 , ? ? ? , H ( ?1) Si,m ] to partition [1, ? ? ? , m] and Receive H ( ?1) Ui ; 10 H ( ) ? GCN ( ) H i , H ( ?1) H ( ?1) Ui , w[t ? 1] ; 11 end 12 f i ? v?Vi loss(h (L) v , y v ); calculate loss 13 g i [t] ? ?fi ?w[t?1] ; backward pass 14 g[t] ? AllReduce(g i [t]); share gradients 15 w[t] ? w[t ? 1] ? ? ? g[t]; update model 16 end 17 return w[T ]</formula><p>(see <ref type="table" target="#tab_9">Tables 7</ref> <ref type="bibr">-8)</ref>. Note that the time complexity of METIS is O(|E|) where E is the set of edges, and only needs to be performed once during the preprocessing stage, the cost of which can thus be amortized over numerous training iterations and leads to a negligible overhead. In addition, METIS is widely adopted in scalable GCN training <ref type="bibr" target="#b41">(Zhu et al., 2019;</ref><ref type="bibr" target="#b34">Zheng et al., 2020;</ref><ref type="bibr" target="#b7">Fey et al., 2021;</ref><ref type="bibr" target="#b33">Wan et al., 2022)</ref> where the objective function is mostly set as the minimum cut (i.e., minimize the number of edges).</p><p>Boundary Node Sampling (BNS). Even with an optimal graph partition, the boundary node issue still remains (see <ref type="table" target="#tab_1">Table 1</ref>), calling for innovative methods to reduce the boundary node volume. An ideal method should achieve three goals: (1) substantially shrinking the size of boundary node sets, (2) incurring a minimal overhead, and (3) maintaining the full-graph accuracy. As such, we adopt a random sampling method called boundary node sampling. The key idea is to independently select a subset of boundary nodes from each partition, then to store and communicate merely those selected ones instead of the full boundary sets, with a random selection varying from one epoch to another.</p><p>Algorithm 1 outlines our proposed BNS-GCN. In the i-th partition, we randomly keep the boundary node set U i with a </p><formula xml:id="formula_6">|B i | |N i | |V|. Method Variance Notation GraphSAGE O(D? 2 /s n ) s n : sampled neighbor size D: average degree VR-GCN O(D?? 2 /s n ) FastGCN O(|V|? 2 /s ) s : sampled node set size V, N i , B i : global node set, neighbor set, boundary set LADIES O(|N i |? 2 /s ) BNS-GCN O(|B i |? 2 /s )</formula><p>probability p and drop the rest at the beginning of each epoch (Lines 4-5). These selected nodes' indices are then broadcasted among partitions such that each partition "knows" others' selections (Line 6) and can also record its local node S i,j that is selected by the other j-th partition (Line 7). During the forward pass of the -th layer, each partition sends those features H</p><formula xml:id="formula_7">( ?1) Si,j</formula><p>of the previously recorded nodes to the corresponding j-th partition and meanwhile receives features H ( ?1) Ui of its own selected boundary nodes to perform GCN operations (Lines 9-10). For a mean aggregator, we replace the sent/received feature matrix H with H/p towards an unbiased feature estimation. During the backward pass of every layer, each partition sends and receives feature gradients of the selected boundary nodes while generating GCNs' weight gradients (Line 13). Lastly, weight gradients are shared across partitions via AllReduce <ref type="bibr" target="#b29">(Thakur et al., 2005)</ref> to perform weight updates (Lines 14-15).</p><p>The proposed BNS-GCN reduces the number of boundary nodes by a factor of 1 p , achieving a proportional reduction in both memory and communication costs (Equation 3-4). Meanwhile, BNS-GCN pays negligible overhead due to its simplicity, which costs 0%?7% of the training time in practice 1 . Note that our BNS-GCN can not only boost the efficiency and scalability of vanilla partition parallelism, but also be easily plugged into any partition-parallel training methods (e.g., ROC and CAGNET) for further improving their training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variance Analysis</head><p>Theoretically, we study the effect of BNS-GCN on GCNs' performance by analyzing its feature approximation variance and comparing it with the SOTA methods. As the feature approximation variance controls the upper bound of gradient noise <ref type="bibr" target="#b6">(Cong et al., 2020)</ref>, a sampling method 1 Details can be found in Appendix D.</p><p>with a lower approximation variance usually enjoys a better convergence speed <ref type="bibr" target="#b10">(Gower et al., 2019)</ref> and higher accuracy. <ref type="table" target="#tab_3">Table 2</ref> summarizes our results, where V, N i , and B i denote the global node set, neighbor set, and boundary neighbor set, respectively. The detailed variance analysis of BNS-GCN can be found in Appendix A and variances for the other methods are based on <ref type="bibr" target="#b43">(Zou et al., 2019)</ref>. We find that BNS-GCN enjoys the smallest variance compared with FastGCN and LADIES, when fixing the number of sampled nodes, as we strictly have B i ? N i ? V. To be able to compare with GraphSAGE, we fix the sampling size (s = s n ), then BNS-GCN is strictly better than GraphSAGE, as BNS-GCN neither samples neighbors within the inner node sets nor samples the same nodes for multiple times, leading to |B i | ? D|V i |. For VR-GCN, it is not comparable with BNS-GCN because the variance of VR-GCN is based on the difference between embedding feature and its history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">BNS-GCN vs. Existing Sampling Methods</head><p>We further discuss the difference between BNS-GCN and existing sampling methods:</p><p>? Node Sampling: GraphSAGE <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref> and VR-GCN <ref type="bibr">(Chen et al., 2018b)</ref> adopt node sampling which is likely to sample the same nodes multiple times from the previous layers, limiting GCNs' depth and training efficiency. Additionally, BNS-GCN does not sample neighbors within each subgragh, reducing both the estimation variance and sampling overhead.</p><p>? Layer Sampling: BNS-GCN is similar to layer sampling in that nodes within the same partition share the same sampled boundary nodes in the previous layer. Unlike FastGCN <ref type="bibr" target="#b1">(Chen et al., 2018a)</ref>, AS-GCN <ref type="bibr" target="#b14">(Huang et al., 2018)</ref> or LADIES <ref type="bibr" target="#b43">(Zou et al., 2019)</ref>, BNS-GCN has much denser sampled layers, potentially leading to a higher accuracy.</p><p>? Subgraph Sampling: BNS-GCN could be viewed as one kind of subgraph sampling that drops boundary nodes from other partitions. ClusterGCN <ref type="bibr" target="#b5">(Chiang et al., 2019)</ref> and GraphSAINT <ref type="bibr" target="#b37">(Zeng et al., 2020)</ref> propose subgraph sampling, yet their number of selected nodes are small, i.e., only 1.3% and 5.3% of the total nodes, respectively, causing a higher variance of gradient estimation.</p><p>? Edge Sampling: Applying edge sampling (e.g., DropEdge <ref type="bibr" target="#b27">(Rong et al., 2019)</ref>) to distributed GCN training is not efficient, as it does not directly reduce the number of boundary nodes (see Section 4.3).  <ref type="formula">(c=1)</ref> CAGNET <ref type="formula">(c=2)</ref> BNS-GCN (p = 1.0) BNS-GCN (p = 0.1) BNS-GCN (p = 0.01) 8 10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of GPUs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first introduce our experiment setups, then compare with the SOTA baselines, and further provide ablation studies for a thorough evaluation on BNS-GCN.</p><p>Datasets. We evaluate BNS-GCN on four large-scale datasets: 1) Reddit <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref> for community prediction based on the posts' contents and users' comments, 2) ogbn-products <ref type="bibr" target="#b13">(Hu et al., 2020)</ref> for classifing Amazon products based on customers' review, 3) Yelp <ref type="bibr" target="#b37">(Zeng et al., 2020)</ref> for predicting the types of business based on reviews and users' relationship, and 4) ogbn-papers100M <ref type="bibr" target="#b13">(Hu et al., 2020)</ref> for predicting the category of an arXiv publication based on its title and abstract. Details of these four datasets are provided in <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models.</head><p>We adopt a GraphSAGE model with an Adam optimizer for all datasets. The details are listed below:</p><p>? Reddit: We use a 4-layer model with 256 hidden units and set the learning rate as 0.01 with 3000 epochs and 0.5 dropout rate.</p><p>? ogbn-products: We use a 3-layer model with 128 hidden units and set the learning rate as 0.003 with 500 epochs and 0.3 dropout rate.</p><p>? Yelp: We use a 4-layer model with 512 hidden units and set the learning rate as 0.001 with 3000 epochs and 0.1 dropout rate.</p><p>? ogbn-papers100M: We use a 3-layer model with 128 hidden units and set the learning rate as 0.01 with 100 epochs and 0.5 dropout rate.</p><p>Setups. We implement BNS-GCN in DGL  and PyTorch <ref type="bibr" target="#b26">(Paszke et al., 2019)</ref> with the default backend of Gloo. We conduct the experiments of Reddit, ogbn-products and Yelp on a machine with 10 RTX-2080Ti (11GB), Xeon 6230R@2.10GHz (187GB), and PCIe3x16 connecting CPU-GPU and GPU-GPU. The minimal number of partitions for full-graph training are 2, 5, 3 for Reddit, ogbn-products, and Yelp, respectively. For ogbn-papers100M, the experiment is conducted on 32 machines, each of which has 6 Tesla V100 (16GB) with IBM Power9 (605GB). To ensure the reproducibility and robustness of BNS-GCN, we do not tune but fix the hyper-parameters for BNS-GCN throughout all experiments, and we show evaluation results based on average of 10 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with the SOTA Baselines</head><p>Full-Graph Training Speedup. <ref type="figure" target="#fig_2">Figure 4</ref> compares the training throughput of BNS-GCN against the SOTA fullgraph training methods, ROC 2 <ref type="bibr" target="#b15">(Jia et al., 2020)</ref> and CAGNET 3 <ref type="bibr" target="#b31">(Tripathy et al., 2020)</ref>. We observe that BNS-GCN consistently outperforms both baselines across different number of GPUs and boundary node sampling rates p.</p><p>For the instance of training GCN on Reddit, BNS-GCN with p = 0.01 offers a promising throughput improvement of 8.9??16.2? over ROC and 9.2??13.8? over CAGNET (c = 2) across different number of GPUs. Even when p = 1, BNS-GCN still improves the throughput by 1.8??3.7? over ROC and 1.0??5.5? over CAGNET (c = 2). The advantage of BNS-GCN is attributed to not only the reduced communication overhead with boundary node sampling, but also no swap between CPU and GPU as ROC nor redundant broadcast and synchronization overhead as CAGNET. Furthermore, increasing the number of partitions boosts the performance of BNS-GCN (p &lt; 1) substantially, but not for other methods, validating BNS-GCN's advantageous scalability thanks to its effectiveness in reducing communication overhead by dropping boundary nodes. The advantage of BNS-GCN is similar for the other two datasets.</p><p>Full-Graph Accuracy. Now we show that BNS-GCN maintains or even improves the accuracy of full-graph training, while boosting the training efficiency. <ref type="table" target="#tab_5">Table 4</ref> summarizes our extensive evaluations of test scores when BNS-GCN adopts various sampling rates and different numbers of partitions, and compare with seven SOTA sampling-based methods <ref type="bibr" target="#b13">(Hu et al., 2020;</ref><ref type="bibr">with Code, 2020;</ref><ref type="bibr" target="#b5">Chiang et al., 2019;</ref><ref type="bibr">Chen et al., 2018b;</ref><ref type="bibr" target="#b37">Zeng et al., 2020;</ref><ref type="bibr" target="#b11">Hamilton et al., 2017;</ref><ref type="bibr" target="#b6">Cong et al., 2020;</ref><ref type="bibr" target="#b23">Liu et al., 2022;</ref><ref type="bibr" target="#b43">Zou et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref>. We observe that full-graph training (BNS-GCN with p = 1) always achieves a higher or comparable accuracy than existing sampling-based methods, regardless of datasets or number of partitions, which is consistent with the results of ROC <ref type="bibr" target="#b15">(Jia et al., 2020)</ref>. More importantly, BNS-GCN always maintains or even increases the full-graph accuracy, regardless of the sampling rates (e.g., p = 0.1/0.01), the number of partitions, or different datasets. For instance, on Reddit, p = 0.1 achieves a test accuracy of 97.15%?97.17% under 2?8 partitions, which are consistently better than the 97.11% accuracy of full-graph unsampled training, validating the effectiveness and robustness of BNS-GCN. Meanwhile, we also observe that the special case of BNS-GCN, p = 0, always suffers from the worst test score on the three datasets, compared with other cases (p &gt; 0). We understand that this accuracy/score drop is due to the full isolation of each partition after completely removing all boundary nodes, leading to no boundary node features during neighbor aggregation throughout the endto-end training. To the best of our knowledge, BNS-GCN achieves the best accuracy of training GraphSAGE-layer based GCNs on all three datasets compared with all existing works.</p><p>Improvement over Sampling-based Methods. Besides the full-graph training comparison, we also validate BNS-GCN's advantage over the SOTA sampling-based methods (implemented by the OGB team 4 <ref type="bibr" target="#b13">(Hu et al., 2020)</ref>) on ogbnproducts as shown in <ref type="table" target="#tab_6">Table 5</ref>. We observe that BNS-GCNs with p = 0.1 and p = 0.01 outperform all the samplingbased methods in terms of both efficiency and accuracy thanks to its lower approximation variance and substantially higher achieved throughput. More comparisons between BNS-GCN and sampling-based methods can be found in * We find these cases run out of memory, which are consistent with <ref type="bibr" target="#b37">(Zeng et al., 2020)</ref>. Epoch time (s) p = 1 p = 1 p = 1 p = 0.1 p = 0.1 p = 0.1 p = 0.01 p = 0.01 p = 0.01 Epoch time (s) p = 1 p = 1 p = 1 p = 0.1 p = 0.1 p = 0.1 p = 0.01 p = 0.01 p = 0.01 obgn-products  ogbn-products (10 partitions) p = 1 p=0.1 p=0.01 p=0 <ref type="figure">Figure 7</ref>: Test-accuracy convergence comparison among unsampled full-graph training (BNS-GCN with p = 1), boundarynode sampled training (p = 0.1/0.01), and isolated training (p = 0) on ogbn-products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reddit</head><p>Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Analysis</head><p>Training Time Improvement Breakdown. To further understand the improvement of BNS-GCN, we breakdown the training time into three major components (local computation, communication for boundary nodes, and allreduce on model gradient) as shown in <ref type="figure">Figure 5</ref>. We observe that communication dominates the training time (up to 67% and 64% in baselines (p = 1) on Reddit and obgn-products, respectively). As expected, with boundary node sampling (p &lt; 1), the communication overhead is substantially reduced, thus the total training time is improved. Specifically, p = 0.01 sharply cuts 74%?93% and 83%?91% of the communication time from that of the baselines on Reddit and obgn-product, respectively, where this benefit consistently holds as the number of partitions scales. Furthermore, in addition to single machine training, we also study BNS-GCN's benefits for multi-machine training and evaluate the performance on ogbn-papers100M. Specifically, we separate ogbn-papers100M into 192 parts and deploy the training on 32 machines (6 GPUs per machine), and provide results in <ref type="table" target="#tab_8">Table 6</ref>. We can see that BNS-GCN with p = 0.01 considerably reduce the total training time by 99%, showing that distributed GCN training with multiple machines suffers from a more severe communication bottleneck and thus making BNS-GCN more desirable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Total Comp. Comm. Reduce BNS-GCN (p = 1.0) 554.1s 5.3s 550.3s 0.8s BNS-GCN (p = 0.1) 58.7s 1.0s 56.9s 0.8s BNS-GCN (p = 0.01) 6.0s 0.6s 4.8s 0.6s</p><p>Memory Saving. BNS-GCN's advantage in terms of memory usage reduction is shown in <ref type="figure">Figure 6</ref>. We observe that BNS-GCN consistently reduces the memory usage across different number of partitions on both graphs. Specifically, for the denser Reddit graph (with an average node degree of 984.0), p = 0.01 saves 58% memory usage for 8 GPUs.</p><p>Even for the sparser obgn-products graph (with an average node degree of 50.5), p = 0.01 still saves 27% memory usage for 10 GPUs. Note that the memory saving of BNS-GCN scales with the numbers of partition, because the number of boundary nodes increases with more partitions, indicating BNS-GCN's scalability to training larger graphs. Also, we find that BNS-GCN's memory reduction is not linear with reduced p, as besides the tensors analyzed in Equation 4 there are other objects (e.g., caches for non-linear activations and dropout) occupying the memory during training.</p><p>Generalization Improvement. To understand the effect of BNS-GCN's generalization capability, we also evaluate the test-accuracy convergence in <ref type="figure">Figure 7</ref>. Here ogbn-products   is adopted as the study case because the distribution of its test set largely differs from that of its training set <ref type="bibr" target="#b13">(Hu et al., 2020)</ref>. From <ref type="figure">Figure 7</ref>, we observe that full-graph training without boundary node sampling (p = 1) or completely isolated training (p = 0) can overfit rapidly, regardless of different number of partitions. With boundary node sampling (p = 0.1/0.01), this overfitting issue is mitigated, i.e., both the convergence and the optimality are improved substantially and consistently across different number of partitions. This is because BNS-GCN randomly modifies the graph throughout end-to-end training. More convergence curves on other datasets can be found in Appendix B.</p><p>Balanced Memory Usage. To validate the benefit of BNS-GCN in balancing memory usage across partitions, we measure per-partition memory usage of ogbn-papers100M with 192 partitions and show their box plots in <ref type="figure" target="#fig_6">Figure 8</ref>. We observe that the unsampled case (p = 1.0) suffers from a severe memory imbalance, where one straggler increases the memory requirement by around 20% and more than three-fourths partitions occupy less than 60% memory. By contrast, with boundary node sampling, p = 0.1/0.01 balances the memory usage and thus better utilizes memory resource, i.e., all partitions leverage more than 70% memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>BNS-GCN with Random Partition. To further understand the effectiveness of BNS-GCN and whether it relies on the adopted METIS partitioner, we conduct an ablation study by replacing METIS with random partition (i.e., randomly assign nodes to each partition without optimization) and provide the resulting accuracy in <ref type="table" target="#tab_9">Table 7</ref>. We observe that when p = 0.1, i.e., sampling normally, random partition plus BNS-GCN still offers a comparable performance (-0.20?+0.27) as the original METIS plus BNS-GCN, thus showing that the proposed BNS-GCN is orthogonal to the adopted graph partitioning technique and is not necessarily limited to METIS. We further study whether BNS-GCN consistently improves training efficiency on top of different partition algorithms, and demonstrate the result in <ref type="table" target="#tab_10">Table 8</ref>. We observe that random partition gains more benefits in throughput improvement and memory saving from BNS-GCN (p = 0.1) than the METIS, because the former partitioner creates more boundary nodes.</p><p>The Special Case p = 0. The special case of the proposed boundary node sampling, p = 0, is not recommended to use in practice. First, p = 0 always suffers from the worst test accuracy/score (compared with other cases (p &gt; 0)) on all datasets and with different partition methods (see <ref type="table" target="#tab_5">Table 4</ref> and <ref type="table" target="#tab_9">Table 7</ref>). Specifically, with random partition, p = 0 drops the test accuracy on Reddit from 97.11% (p = 1) to 93.37% (lower than FastGCN <ref type="bibr" target="#b1">(Chen et al., 2018a)</ref>). This drop is due to the absolute isolation of each partition af- ter completely removing all boundary nodes, leading to no boundary node features during neighbor aggregation (Equation 1) throughout the end-to-end training. Second, p = 0 also suffers from the slowest convergence regardless of different numbers of partitions or different datasets. Third, p = 0 overfits severely (see <ref type="figure">Figure 7</ref>). Therefore, we suggest a small but non-zero sampling rate (p = 0.1/0.01).</p><p>More discussion regarding the choice of sampling rate can be found in Appendix E.</p><p>BNS-GCN vs. Boundary Edge Sampling. As many works <ref type="bibr" target="#b41">(Zhu et al., 2019;</ref><ref type="bibr" target="#b34">Zheng et al., 2020;</ref><ref type="bibr" target="#b7">Fey et al., 2021)</ref> assume that communication overhead of distributed GCN training is caused by the inter-partition edges (rather than boundary nodes) and thus pursuing a minimal edge cut, one could reduce communication overhead by cutting edges using sampling techniques like DropEdge <ref type="bibr" target="#b27">(Rong et al., 2019)</ref> or even an enhanced version that samples only the boundary edges (rather than at a global scale). To understand this, we implemented the enhanced version, dubbed Boundary Edge Sampling (BES), and apply both BES and DropEdge to the partition-parallel training.  BNS-GCN Reddit ogbn-products Yelp p = 1 1.00? (0.84s) 1.00? (0.71s) 1.00? (0.33s) p = 0.1 1.53? 1.78? 1.83? p = 0.01 1.58? 1.91? 2.06? p = 0 1.68? 2.03? 2.20?</p><p>BNS-GCN Benefit on GAT. To validate the general applicability of BNS-GCN across different types of GCN models (i.e., not just GraphSAGE), we train GAT <ref type="bibr" target="#b32">(Veli?kovi? et al., 2017)</ref> with BNS-GCN and provide the improvement for a 2-layer GAT with 10 partitions in <ref type="table" target="#tab_1">Table 10</ref>. We observe that BNS-GCN is consistently effective and speedups the training by 58%?106%, despite GAT being more computationally intensive than GraphSAGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>While training GCNs at scale is challenging and increasingly important, existing methods for distributed GCN training are still limited in their achievable performance and scalability. This work takes the initial effort to analyze the three major challenges in distributed GCN training and then identify their underlying cause. On top of that, we propose an efficient and scalable method for full-graph GCN training, BNS-GCN, and then validate its effectiveness through both theoretical analysis and extensive empirical evaluations. We believe that these findings and the proposed method have provided a better understanding of distributed GCN training and will inspire further innovations in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The work is supported by the National Science Foundation (NSF) through the MLWiNS program (Award number: 2003137), the CC * Compute program (Award number: 2019007) and the NeTS program (Award number: 1801865).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A THE VARIANCE ANALYSIS</head><p>In this section, we derive the variance of embedding approximation when using our proposed BNS-GCN method, of which the result is listed in <ref type="table" target="#tab_3">Table 2</ref> of the main content.</p><p>For a given graph G = (V, E) with an adjacency matrix A, we define the propagation matrix P as P =D ?1/2?D?1/2 , where? = A + I,D u,u = v? u,v . One GCN layer performs one step of feature propagation <ref type="bibr" target="#b17">(Kipf &amp; Welling, 2016)</ref> as formulated below:</p><formula xml:id="formula_8">Z ( ) = P H ( ?1) W ( ?1)<label>(5)</label></formula><formula xml:id="formula_9">H ( ) = ? Z ( )</formula><p>where H ( ) , W ( ) , and Z ( ) denote the embedding matrix, the trainable weight matrix, and the intermediate embedding matrix in the -th layer, respectively, and ? denotes the non-linear function. Without loss of generality, we provide our analysis for one layer of GCNs and drop the superscripts of ( ) and ( ? 1) in the reminder of the discussion for simplicity.</p><p>For distributed GCN training using partition-parallelism, if denoting the inner node set and the boundary node set of the i-th partition as V i and B i , respectively, the operations of the i-th partition for calculating Equation 5 are as follows:</p><formula xml:id="formula_10">Z Vi, * = P Vi,Vi P Vi,Bi H Vi, * H Bi, * W</formula><p>In BNS-GCN, Z Vi, * is approximated asZ Vi, * due to its boundary node sampling, i.e.,</p><formula xml:id="formula_11">Z Vi, * = P Vi,Vi P Vi,Ui S H Vi, * H Ui, * W</formula><p>where U i denotes the sampled boundary node set, p denotes the sampling rate, and S is a diagnal matrix with its elements being defined as:</p><formula xml:id="formula_12">S u,u = 1, u ? |V i | 1/p, u &gt; |V i |</formula><p>Similar to the variance analysis in <ref type="bibr">(Chen et al., 2018b)</ref> and <ref type="bibr" target="#b43">(Zou et al., 2019)</ref>, our goal is to compute the average variance of the approximated embedding for one GCN layer, which can be defined as E U [ Z ? Z 2 F ]/|V|. In our analysis, we adopt the same assumption as that in <ref type="bibr" target="#b43">(Zou et al., 2019)</ref>, which bounds the matrix product HW as follows:</p><p>Assumption A.1. We assume that the L 2 -norm of each row in HW is upper bounded by a constant, i.e., there exists a constant ? such that H u, * W 2 ? ? for all u ? |V|.</p><p>Next, we calculate the total variance of the embedding approximation for the i-th partition: where the step of Equation 6 removes the common factor P Vi,Vi H Vi, * W and the step of Equation 7 uses the fact that the selection of nodes in B i are independent.</p><formula xml:id="formula_13">E Ui [ Z Vi, * ? Z Vi, * 2 F ] = E Ui P Vi,</formula><p>Based on Assumption A.1, we have H u, * W 2 ? ?. As a result, the above upper bound can be further written as:</p><formula xml:id="formula_14">E Ui [ Z Vi, * ? Z Vi, * 2 F ] ? 1 p v?Vi u?Bi P 2 v,u ? 2 = 1 p ? 2 P Vi,Bi 2 F</formula><p>Thus, the total variance of the embedding approximation for the i-th partition is O(|B i |? 2 /s ) as shown in <ref type="table" target="#tab_3">Table 2</ref> of the main content, where s denote the size of the sampled node set.</p><p>Finally, the global average variance can be calulated as: <ref type="figure">Figure 9</ref> shows convergence speedups of BNS-GCN on more datasets under the same setting of <ref type="figure">Figure 7</ref> of the main content.</p><formula xml:id="formula_15">E U [ Z ? Z 2 F ] |V| = i E Ui [ Z Vi, * ? Z Vi, * 2 F ] |V| ? ? 2 i P Vi,Bi 2 F p|V| ? ? 2 P 2 F p|V| B CONVERGENCE SPEEDUP (ADDITIONAL EXPERIMENTS)</formula><p>The observations from the main content still hold. Especially, boundary node sampling at a high rate (p = 0.1) achieves the best convergence regardless of different numbers of partitions or different datasets. A lower rate (p = 0.01) still remains a close convergence as p = 0.1. The special case p = 0 suffers from not only the worst convergence but also increased convergence gap between p = 0 and p = 0.1 as more partitions are involved, because of complete removal of boundary node information. Lastly, p = 1 and p = 0 can still overfit (see Yelp) but boundary node sampling (p = 0.1/0.01) mitigates the overfitting by random modification of the graph throughout training.  <ref type="figure">Figure 9</ref>: Convergence comparison between unsampled full-graph training (BNS-GCN with p = 1), boundary-node sampled training (0 &lt; p &lt; 1), and isolated training (p = 0) on Reddit and Yelp. <ref type="table" target="#tab_1">Table 11</ref> compares the training efficiency between the popular sampling-based methods and BNS-GCN under the same settings of the main content. As can be seen, BNS-GCN outperforms the sampling-based methods with a great margin, while achieving a higher accuracy (see <ref type="table" target="#tab_5">Table 4</ref> of the main content). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EFFICIENCY COMPARISON WITH SAMPLING-BASED METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D OVERHEAD OF BOUNDARY NODE SAMPLING</head><p>In this section, we evaluate the overhead introduced by the boundary node sampling of BNS-GCN under different sampling rates and number of partitions, and also compare it with the overhead of the state-of-the-art sampling methods. <ref type="table" target="#tab_1">Table 12</ref> summarizes the results. We observe that node, edge, and random walk sampling can introduce a non-trivial overhead, which is up to 24% of training time <ref type="bibr" target="#b37">(Zeng et al., 2020)</ref>. By contrast, boundary node sampling incurs only a negligible overhead, i.e., 0%?7.3%, because it only needs to perform sampling on the boundary region instead of the whole graph as used in the state-of-the-art methods. Also, the light weightiness of boundary nodes sampling lies in its parallizability across partitions, instead of requiring sequential processing. Besides, we also compare BNS-GCN with the graph-level sampling method such as ClusterGCN <ref type="bibr" target="#b5">(Chiang et al., 2019)</ref>. We find that the overhead of boundary node sampling is still much lower than ClusterGCN, because ClusterGCN needs to merge multiple subgraphs into one cluster with a sampling time roughly proportional to the number of edges in the whole graph. By contrast, boundary node sampling only needs to modify those boundary edges of selected boundary nodes, and its sampling time is proportional only to the number of boundary edges, which is only a fraction of ClusterGCN. <ref type="table" target="#tab_1">Table 12</ref>: Comparison of BNS-GCN's sampling overhead with the state-of-the-art methods in GraphSAINT <ref type="bibr" target="#b37">(Zeng et al., 2020)</ref> on Reddit, where the overhead percentage is calculated by the sampling time divided by the training time.</p><p>The state-of-the-art samplers Node 23% Edge 20% Random walk 24% BNS-GCN sampler # Partitions 2 4 8 p = 1.00 0% p = 0.10 1.7% 3.2% 6.6% p = 0.01 1.3% 3.0% 7.3% p = 0.00 0%</p><p>In this section, we discuss how to choose the boundary node sampling rate p in practice for maximizing the efficiency of GCN training. Empirically, p = 0.1 combines the best of all worlds: throughput boosting, communication reduction, memory saving, convergence speedup, and final accuracy, as well as sampling overhead, across different number of partitions and different datasets, according to our extensive experiments. To further validate this, we compare the test accuracy of p between 0.1 and 1 and summarize the results in <ref type="table" target="#tab_1">Table 13</ref>. We can see that the advantage of p = 0.1 still holds, i.e., offering similar accuracy but less communication/memory compared with higher p values. <ref type="table" target="#tab_1">Table 13</ref>: Test accuracy of BNS-GCN with a sampling rate p between 0.1 and 1.</p><p>Dataset p = 0.1 p = 0.3 p = 0.5 p = 0.8 p = 1.0 Reddit <ref type="formula">(2 partitions</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3.4 Data sets</head><p>All dataset (Reddit, ogbn-products, Yelp) are either included in our docker image or to be downloaded by our scripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Installation</head><p>Detailed instructions are provided in README.md in our GitHub repository. For example, just run docker pull cheng1016/bns-gcn followed by docker run --gpus all -it cheng1016/bns-gcn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Experiment workflow</head><p>The workflow involves invoking top-level main.py which then drives other modules for distributed GCN training. All "one-click-to-run" scripts to reproduce main experiments in the paper are provided in the scripts/ * .sh in our GitHub repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.6 Evaluation and expected result</head><p>All steps are in scripts/ * .sh in our GitHub repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.7 Experiment customization</head><p>We provide a detailed guide for customization in README.md in our GitHub repository. Hyper-parameters and configurations can be customized by the options fed to main.py, e.g., allowing users to choose the number of training epochs, the number of graph partitions (or GPUs), different partitioning methods, and even extending training to multiple machines with multiple GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.8 Notes</head><p>For the hyper-scale dataset ogbn-papers100M, the experiment was conducted on 32 machines, each of which has 6 Tesla V100 (16GB) with IBM Power9 (605GB).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustrative comparison between existing distributed GCN training methods and our BNS-GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The distributions of the boundary-inner ratios for the ogbn-papers100M dataset under 192 partitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Throughput comparison on Reddit, ogbn-products, and Yelp. Each partition uses one GPU (except CAGNET (c = 2) uses two). The boundary node sampling rate is denoted by p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>4 https://github.com/snap-stanford/ogb</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Training time breakdown of BNS-GCN with different boundary node sampling rates p. Memory usage reduction achieved by BNS-GCN, where the reduction is against p = 1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Normalized per-partition memory usage on ogbn-papers100M (192 partitions). The normalization is against the highest memory partition and is separated for different sampling rates p of BNS-GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison between the number of boundary nodes and inner nodes in partitioned Reddit graph</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparing feature approximation variance between SOTA sampling methods and BNS-GCN, where we fix the target node set V i across all methods. Here ? denotes the upper bound of the L 2 -norm of intermediate features, and ?? is the upper bound of the difference between the embedding feature and its history. We report the variance by ignoring the same factors. Note that</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Details of the graph datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Dataset</cell><cell></cell><cell cols="3"># Nodes # Edges # Feat. # Classes</cell><cell>Type</cell><cell>Train / Val / Test</cell></row><row><cell></cell><cell></cell><cell cols="2">Reddit (Hamilton et al., 2017)</cell><cell></cell><cell>233K</cell><cell>114M</cell><cell>602</cell><cell>41</cell><cell>inductive</cell><cell>0.66 / 0.10 / 0.24</cell></row><row><cell></cell><cell></cell><cell cols="2">ogbn-products (Hu et al., 2020)</cell><cell></cell><cell>2.4M</cell><cell>62M</cell><cell>100</cell><cell>47</cell><cell>transductive 0.08 / 0.02 / 0.90</cell></row><row><cell></cell><cell></cell><cell cols="2">Yelp (Zeng et al., 2020)</cell><cell></cell><cell>716K</cell><cell>7.0M</cell><cell>300</cell><cell>100</cell><cell>inductive</cell><cell>0.75 / 0.10 / 0.15</cell></row><row><cell></cell><cell cols="4">ogbn-papers100M (Hu et al., 2020)</cell><cell>111M</cell><cell>1.6B</cell><cell>128</cell><cell>172</cell><cell>transductive 0.78 / 0.08 / 0.14</cell></row><row><cell>Throughput (epochs/s)</cell><cell>0 1 2 3 4 5 6 7 8</cell><cell>2 ROC CAGNET</cell><cell>4 Number of GPUs Reddit</cell><cell>8</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of test accuracy (%) on Reddit and ogbn-products and of test F1-micro score (%) on Yelp.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Reddit</cell><cell></cell><cell cols="2">ogbn-products</cell><cell></cell><cell></cell><cell>Yelp</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Sampling-based methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FastGCN (Chen et al., 2018a)</cell><cell></cell><cell>93.7</cell><cell></cell><cell></cell><cell>60.42</cell><cell></cell><cell></cell><cell>26.5</cell><cell></cell></row><row><cell>GraphSAGE (Hamilton et al., 2017)</cell><cell></cell><cell>95.4</cell><cell></cell><cell></cell><cell>78.70</cell><cell></cell><cell></cell><cell>63.4</cell><cell></cell></row><row><cell>AS-GCN (Huang et al., 2018)</cell><cell></cell><cell>96.3</cell><cell></cell><cell></cell><cell>OOM  *</cell><cell></cell><cell></cell><cell>OOM  *</cell><cell></cell></row><row><cell>LADIES (Zou et al., 2019)</cell><cell></cell><cell>94.3</cell><cell></cell><cell></cell><cell>77.46</cell><cell></cell><cell></cell><cell>60.2</cell><cell></cell></row><row><cell>VR-GCN (Chen et al., 2018b)</cell><cell></cell><cell>96.3</cell><cell></cell><cell></cell><cell>OOM  *</cell><cell></cell><cell></cell><cell>64.0</cell><cell></cell></row><row><cell>ClusterGCN (Chiang et al., 2019)</cell><cell></cell><cell>96.6</cell><cell></cell><cell></cell><cell>78.97</cell><cell></cell><cell></cell><cell>60.9</cell><cell></cell></row><row><cell>GraphSAINT (Zeng et al., 2020)</cell><cell></cell><cell>96.6</cell><cell></cell><cell></cell><cell>79.08</cell><cell></cell><cell></cell><cell>65.3</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">BNS-GCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># Partitions</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>5</cell><cell>8</cell><cell>10</cell><cell>3</cell><cell>6</cell><cell>10</cell></row><row><cell>BNS-GCN (p = 1.0)</cell><cell cols="9">97.11 97.11 97.11 79.14 79.14 79.14 65.26 65.26 65.26</cell></row><row><cell>BNS-GCN (p = 0.1)</cell><cell cols="9">97.17 97.16 97.15 79.36 79.48 79.30 65.32 65.26 65.34</cell></row><row><cell>BNS-GCN (p = 0.01)</cell><cell cols="9">97.09 96.99 96.94 79.43 79.28 79.21 65.27 65.31 65.29</cell></row><row><cell>BNS-GCN (p = 0.0)</cell><cell cols="9">97.03 96.88 96.84 78.65 78.83 78.79 65.28 65.27 65.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">: Comparison between BNS-GCN (10 partitions)</cell></row><row><cell cols="3">and sampling-based methods on ogbn-products.</cell></row><row><cell>Method</cell><cell cols="2">Total Train Time Test Acc (%)</cell></row><row><cell>ClusterGCN</cell><cell>294.2s</cell><cell>78.97?0.33</cell></row><row><cell>NeighborSampling</cell><cell>281.8s</cell><cell>78.70?0.36</cell></row><row><cell>GraphSAINT</cell><cell>157.4s</cell><cell>79.08?0.24</cell></row><row><cell>BNS-GCN (p = 1.0)</cell><cell>269.1s</cell><cell>79.14?0.35</cell></row><row><cell>BNS-GCN (p = 0.1)</cell><cell>155.3s</cell><cell>79.30?0.36</cell></row><row><cell>BNS-GCN (p = 0.01)</cell><cell>142.9s</cell><cell>79.21?0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Epoch time breakdown of ogbn-papers100M.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Test score (%) of BNS-GCN on top of random partition, where +/-shows the accuracy difference from BNS-GCN on top of METIS inTable 4.</figDesc><table><row><cell>Method</cell><cell cols="2">Reddit (8 partitions)</cell><cell cols="2">ogbn-products (10 partitions)</cell><cell cols="2">Yelp (10 partitions)</cell></row><row><cell>Random+BNS (p = 1.0)</cell><cell>97.11</cell><cell>+0.00</cell><cell>79.14</cell><cell>+0.00</cell><cell>65.26</cell><cell>+0.00</cell></row><row><cell>Random+BNS (p = 0.1)</cell><cell>96.95</cell><cell>-0.20</cell><cell>79.57</cell><cell>+0.27</cell><cell>65.18</cell><cell>-0.16</cell></row><row><cell>Random+BNS (p = 0.0)</cell><cell>93.37</cell><cell>-3.47</cell><cell>75.39</cell><cell>-3.40</cell><cell>64.92</cell><cell>-0.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Training efficiency improvement of BNS-GCN (p = 0.1) on top of different partition methods.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Dataset</cell><cell cols="2">Throughput METIS Random</cell><cell cols="2">Memory METIS Random</cell><cell># Boundary Nodes METIS Random</cell></row><row><cell></cell><cell></cell><cell cols="2">Reddit (8 partitions)</cell><cell>3.1?</cell><cell>5.0?</cell><cell>0.47?</cell><cell>0.36?</cell><cell>460k</cell><cell>1,016k</cell></row><row><cell></cell><cell></cell><cell cols="2">ogbn-products (10 partitions)</cell><cell>3.4?</cell><cell>7.3?</cell><cell>0.75?</cell><cell>0.31?</cell><cell>1,848k</cell><cell>16,797k</cell></row><row><cell></cell><cell></cell><cell cols="2">Yelp (10 partitions)</cell><cell>3.1?</cell><cell>5.1?</cell><cell>0.83?</cell><cell>0.49?</cell><cell>649k</cell><cell>2,026k</cell></row><row><cell>Normalized Memory Usage</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>p = 1.0</cell><cell>p = 0.1 Sampling Rate</cell><cell>p = 0.01</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Comparison between BNS-GCN and edge sampling methods, DropEdge and Boundary Edge Sampling (BES).</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Epoch Comm (MB)</cell><cell>Epoch Time (sec)</cell><cell>Test Score (%)</cell></row><row><cell>Reddit (2 partitions)</cell><cell>DropEdge BES BNS-GCN</cell><cell>301.3 207.9 30.4</cell><cell>0.613 0.484 0.319</cell><cell>97.12 97.16 97.17</cell></row><row><cell>ogbn-products (5 partitions)</cell><cell>DropEdge BES BNS-GCN</cell><cell>1364.0 521.1 138.7</cell><cell>0.938 0.551 0.388</cell><cell>79.38 79.31 79.36</cell></row><row><cell>Yelp (3 partitions)</cell><cell>DropEdge BES BNS-GCN</cell><cell>718.7 195.3 75.7</cell><cell>0.606 0.328 0.270</cell><cell>65.30 65.30 65.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>compares their perfor-</cell></row><row><cell>mance with BNS-GCN. For a fair comparison, all methods</cell></row><row><cell>drop the same number of edges with BNS-GCN (p = 0.1)</cell></row><row><cell>over the full graph. We observe that edge-based sampling</cell></row><row><cell>methods are ineffective. For Reddit, DropEdge and BES</cell></row><row><cell>cause 10? and 7? communication overhead of BNS-GCN,</cell></row><row><cell>and thus 2.0? and 1.4? the overall training time. This</cell></row><row><cell>is because, in real-world graphs, multiple boundary edges</cell></row><row><cell>can connect to the same boundary nodes. Even if we drop</cell></row><row><cell>some of those edges, the remaining undropped edges still</cell></row><row><cell>demand communicating the connected boundary nodes to</cell></row><row><cell>satisfy neighbor aggregation of GCNs. Obviously, to erad-</cell></row><row><cell>icate such communication costs, boundary nodes should</cell></row><row><cell>be directly targeted and dropped, instead of using bound-</cell></row><row><cell>ary edges. For ogbn-products and Yelp, the advantage of</cell></row><row><cell>BNS-GCN still holds, where BNS-GCN reduces up to 90%</cell></row><row><cell>communication volume and speeds up training time by up</cell></row><row><cell>to 2.4?. Analytically, we've shown that the communication</cell></row><row><cell>cost of distributed GCN training is only proportional to the</cell></row><row><cell>number of boundary nodes (see Equation 3).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Epoch training time speedup on GAT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Comparison of training efficiency on Reddit, where BNS-GCN with various boundary sampling rates under 8 partitions are shown.</figDesc><table><row><cell>Method</cell><cell cols="7">GraphSAGE FastGCN VR-GCN ClusterGCN BNS-GCN(1) BNS-GCN(0.1) BNS-GCN(0.01)</cell></row><row><cell>Train time per epoch</cell><cell>6.20s</cell><cell>5.08s</cell><cell>3.85s</cell><cell>1.35s</cell><cell>0.777s</cell><cell>0.198s</cell><cell>0.150s</cell></row><row><cell>Speedup</cell><cell>1?</cell><cell>1.22?</cell><cell>1.61?</cell><cell>4.59?</cell><cell>8.0?</cell><cell>31.3?</cell><cell>41.3?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>) 97.17% 97.18% 97.15% 97.13% 97.11% ogbn-product (5 partitions) 79.36% 79.30% 79.34% 79.24% 79.14% ? PyTorch 1.8.0 ? customized DGL 0.7.0 ? OGB 1.3.0</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/jiazhihao/ROC 3 https://github.com/PASSIONLab/CAGNET</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recent advances in graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meyerhenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Safro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithm Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="117" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="942" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable graph neural networks via bidirectional propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Differentiated graph computation and partitioning on skewed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powerlyra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Parallel Computing (TOPC)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Minimal variance sampling with provable guarantees for fast training of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Forsati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1393" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gnnautoscale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05609</idno>
		<title level="m">Scalable and expressive graph neural networks via historical embeddings</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">P3: Distributed deep graph learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="551" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Powergraph: Distributed graph-parallel computation on natural graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI 12)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Gower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sailanbayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shulgin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richt?rik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgd</surname></persName>
		</author>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5200" to="5209" />
		</imprint>
	</monogr>
	<note>General analysis and improved rates</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pipedream</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03377</idno>
		<title level="m">Fast and efficient pipeline parallel dnn training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving the accuracy, scalability, and performance of graph neural networks with roc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
		<meeting>Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="187" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<title level="m">One weird trick for parallelizing convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pytorch-biggraph: A large-scale graph embedding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peysakhovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12287</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Salpekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Damania</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15704</idno>
		<title level="m">Experiences on accelerating data parallel training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A network-centric hardware/algorithm co-design to accelerate distributed training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;18)</title>
		<meeting>the 51st IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;18)<address><addrLine>Fukuoka City; Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pipe-sgd: A decentralized pipelined sgd framework for distributed deep net training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avestimehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Conference on Neural Information Processing Systems (NIPS&apos;18)</title>
		<meeting>the 32nd Conference on Neural Information Processing Systems (NIPS&apos;18)<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">EXACT: Scalable graph neural networks training via extreme activation compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=vkaMaq95_rX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">NeuGraph: Parallel deep neural network computation on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 USENIX Annual Technical Conference (USENIX ATC 19)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="443" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A lightweight infrastructure for graph analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lenharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pingali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="456" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dropedge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ligra: a lightweight graph processing framework for shared memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGPLAN symposium on Principles and practice of parallel programming</title>
		<meeting>the 18th ACM SIGPLAN symposium on Principles and practice of parallel programming</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimization of collective communication operations in mpich</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rabenseifner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perform. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="66" />
			<date type="published" when="2005-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dorylus: affordable, scalable, and accurate gnn training with distributed cpu servers and serverless threads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eyolfson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="495" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reducing communication in graph neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buluc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03300</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PipeGCN: Efficient full-graph training of graph convolutional networks with pipelined feature communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrillidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=kSwqMH0zn1F" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep graph library: A graphcentric, highly-performant package for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<ptr target="https://paperswithcode.com/sota/node-classification-on-reddit" />
	</analytic>
	<monogr>
		<title level="m">with Code, P. Node Classification on Reddit</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphsaint</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<title level="m">Graph sampling based inductive learning method</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distdgl: distributed graph neural network training for billion-scale graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/ACM 10th Workshop on Irregular Applications: Architectures and Algorithms (IA3)</title>
		<imprint>
			<biblScope unit="page" from="36" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aligraph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08730</idno>
		<title level="m">A comprehensive graph neural network platform</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gemini: A computation-centric distributed graph processing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">of BNS-GCN. It includes both the baseline (vanilla partition-parallel training) and the proposed boundary-node-sampled training for GCNs on various datasets. Running the code requires a machine (at least 120 GB host memory) with multiple (at least five) Nvidia GPUs (at least 11GB each). Software is provided with our docker image. With the aforementioned hardware and software, running our provided scripts will validate the main experiments in the paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11249" to="11259" />
		</imprint>
	</monogr>
	<note>Layer-dependent importance sampling for training deep and large graph convolutional networks. such as per-epoch training time, training time breakdown, memory usage, and accuracy</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Artifact check-list (meta-information)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distributed Training, Random Sampling ? Data set: Reddit, ogbn-products, Yelp (all included in our docker or software setup)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Algorithm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Convolutional Network (GCN)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Run</surname></persName>
		</author>
		<idno>18.04</idno>
		<imprint/>
	</monogr>
	<note>Python 3.8, CUDA 11.1, PyTorch 1.8.0, DGL 0.7.0, OGB 1.3.0</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A X64-CPU machine with at least 120 GB host memory, at least five Nvidia GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Hardware</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>at least 11GB each</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Execution: Bash scripts, Running each experiment takes less than 1 hour</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Training time, training time breakdown, memory usage, accuracies ? Output: Console, and log file ? How much disk space required (approximately</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Metrics</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">? How much time is needed to prepare workflow (approximately)?: 30 minutes ? How much time is needed to complete experiments (approximately</title>
		<idno type="DOI">10.5281/zenodo.6079700</idno>
	</analytic>
	<monogr>
		<title level="m">10 hours ? Publicly available?: yes ? Code licenses</title>
		<imprint/>
	</monogr>
	<note>MIT License ? Archived. provide DOI</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<idno type="DOI">10.5281/zenodo.6079700</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6079700" />
		<title level="m">? Source code in the archival repository for ACM badges</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title/>
		<ptr target="https://github.com/RICE-EIC/BNS-GCN" />
	</analytic>
	<monogr>
		<title level="j">? Latest source code in GitHub</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Docker Image</surname></persName>
		</author>
		<ptr target="https://hub.docker.com/r/cheng1016/bns-gcn" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">? Approximate disk space: 50GB, used for large datasets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">CPU machine with at least 120 GB host memory ? At least five Nvidia GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? A X86</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>at least 11 GB each</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Cuda</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
