<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Context Enhanced Region-based Convolutional Neural Network for End-to-End Lesion Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
							<email>ke.yan@nih.gov</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Imaging Biomarkers and Computer-Aided Diagnosis Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
							<email>mohammad.bagheri@nih.gov</email>
							<affiliation key="aff1">
								<orgName type="department">Clinical Image Processing Service Department of Radiology and Imaging Sciences</orgName>
								<orgName type="institution">National Institutes of Health Clinical Center</orgName>
								<address>
									<postCode>20892-1182</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Imaging Biomarkers and Computer-Aided Diagnosis Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3D Context Enhanced Region-based Convolutional Neural Network for End-to-End Lesion Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting lesions from computed tomography (CT) scans is an important but difficult problem because non-lesions and true lesions can appear similar. 3D context is known to be helpful in this differentiation task. However, existing end-to-end detection frameworks of convolutional neural networks (CNNs) are mostly designed for 2D images. In this paper, we propose 3D context enhanced region-based CNN (3DCE) to incorporate 3D context information efficiently by aggregating feature maps of 2D images. 3DCE is easy to train and end-to-end in training and inference. A universal lesion detector is developed to detect all kinds of lesions in one algorithm using the DeepLesion dataset. Experimental results on this challenging task prove the effectiveness of 3DCE. We have released the code of 3DCE in 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated lesion detection in computed tomography (CT) scans plays an important role in computer-aided disease screening and tracking. To differentiate lesions from non-lesions, 3D context is crucial <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b8">9]</ref>. However, existing detection frameworks using convolutional neural networks (CNNs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> are typically designed for 2D images. Therefore, algorithms that can take 3D context information into consideration are in need.</p><p>As a direct solution, Liao et al. <ref type="bibr" target="#b5">[6]</ref> extended the region proposal network (RPN) <ref type="bibr" target="#b7">[8]</ref> to 3D RPN to process volumetric CT data. However, 3D CNNs are very memory-consuming so that sometimes it is hard to fit a single sample into the memory of a mainstream GPU <ref type="bibr" target="#b5">[6]</ref>. To solve this problem, <ref type="bibr" target="#b5">[6]</ref> used small 3D patches as the input of the network. Besides, 3D bounding-boxes are generally more difficult to annotate than 2D ones, which leads to sparse training data for 3D RPNs. Hence, data augmentation was used in <ref type="bibr" target="#b5">[6]</ref> to combat over-fitting. In <ref type="bibr" target="#b3">[4]</ref>, 2D networks were first applied to generate lesion candidates, then 3D CNN classifiers were trained for false positive reduction (FPR). Some researchers trained classifiers on the aggregation of multiple 2D slices (e.g. three orthogonal views (2.5D) or random views of the candidate lesion) for FPR <ref type="bibr" target="#b8">[9]</ref>. FPR-based approaches have two stages and are not end-to-end.</p><p>In this paper, we propose 3D context enhanced region-based CNNs (3DCE) to incorporate 3D context into 2D regional CNNs. Multiple neighboring slices are sent into a 2D detection network to generate feature maps separately, which are then aggregated for final prediction. We improve the region-based fully convolutional network (R-FCN) <ref type="bibr" target="#b1">[2]</ref> for this task. 3DCE has many advantages: 1) Compared with the 2-stage candidate generation + FPR approaches, 3DCE is more efficient and end-to-end in both training and inference. 2) It can leverage popular 2D CNN backbones and pretrained weights such as VGG-16 <ref type="bibr" target="#b10">[11]</ref>. The weights are learned from millions of images <ref type="bibr" target="#b2">[3]</ref> and are known to be beneficial for transfer learning <ref type="bibr" target="#b9">[10]</ref>. On the contrary, 3D CNNs lack such pretrained models and have to be trained from scratch. 3) 3DCE only requires 2D bounding-box annotations to train. Compared with 3D methods, it can obtain large-scale training data more easily, e.g., from radiologists' routine lesion measurements <ref type="bibr" target="#b11">[12]</ref>.</p><p>Previous studies on lesion detection generally focused on specific types of lesions, such as lung nodules and liver lesions. While some common types receive much attention, many infrequent but still clinically significant types have been ignored. In this paper, we apply the proposed algorithm on DeepLesion <ref type="bibr" target="#b11">[12]</ref>, a large-scale and diverse lesion dataset. It contains over 32K 2D annotations of lesions with a variety of types. Using this dataset, we develop a universal lesion detection algorithm that finds all types of lesions with one unified framework. After incorporating 3D contexts with 3DCE, the sensitivity of lesion detection with 4 false positives per image is improved from 80.32% to 84.37% on the test set of the challenging dataset, proving the effectiveness of 3DCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our goal is to consider 3D context in lesion detection, meanwhile leveraging pretrained 2D CNN weights for transfer learning <ref type="bibr" target="#b9">[10]</ref>. A simple solution <ref type="bibr" target="#b3">[4]</ref> is to input multiple neighboring slices into object detection frameworks with pretrained backbones. Since current detectors are mostly designed for natural images with three channels (RGB), it is necessary to extend the filters in the first layer and pad it with zeros for the extra input channels. Thus, the new network can start from using the three channels with non-zero weights, and gradually learn the new weights in the first layer to fit the extra input channels. The drawback of this data-level fusion strategy is that the pretrained weights may need to change greatly to adapt to 3D textures. Our idea is to fuse information in the feature map level. We first group slices to 3-channel images and extract good feature maps for each image, then aggregate the feature maps of neighboring images to collect 3D information, and finally build lesion classifiers on top of the fused features. Therefore, the backbone network structure for 2D images can be kept, whereas its representation capability is enhanced for 3D by feature aggregation. The framework of the proposed 3D context enhanced region-based CNN (3DCE) is presented in <ref type="figure" target="#fig_0">Fig. 1</ref>. We adopt the R-FCN <ref type="bibr" target="#b1">[2]</ref> for this task, which we find is faster, and more accurate and memory-efficient than the widely-used faster region-based CNN (faster RCNN) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. Different from faster RCNN, R-FCN constructs a set of position-sensitive score maps, each encoding the object class or position information in a relative spatial position of the object. After that, a position-sensitive region of interest (PSROI) pooling layer summarizes these score maps on each lesion proposal. The object classification and boundingbox regression results are finally obtained by another pooling operation <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3DCE</head><p>We first improve R-FCN by adding 3 new layers after PSROI pooling: a 2048D fully-connected (FC) layer, a ReLU layer, and two FC layers for classification and bounding-box regression, respectively. A performance boost (0.7%) was observed with these additional layers, whereas the speed is comparable to the original R-FCN because we have made the feature maps before FC7 thinner. The last two pooling layers (pool4 and pool5) in VGG-16 are removed to enhance the resolution of the feature map, since lesions are often small and sparse. In 3DCE, 3M slices are grouped to M 3-channel images, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. During training, the central slice contains the ground-truth bounding-box and the other slices provide the 3D context. We combine the M images to be a sample to input into the convolutional blocks (Conv1-5 of VGG-16) to produce M feature maps. Only the feature map derived from the central image is sent to the region proposal network (RPN) to generate lesion proposals. All feature maps undergo another convolutional layer (Conv6). They are then concatenated to aggregate the 3D information, forming an S 2 DM -channel feature map, where S = 7 is the size of the pooled feature map for each proposal. D controls the number of 2D feature maps of 3DCE. We empirically set D = 10 in this paper. M determines the amount of 3D context to be incorporated. Larger M brings more information and more memory cost and risk of over-fitting. We found M = 3 ? 9 (18?54mm) a reasonable range in our lesion detection task.</p><p>In 3DCE, the weights of Conv1-6 are shared for different images in a sample. This strategy reduces the number of parameters to be learned compared with the 3D filters in 3D CNNs. Thus, 3DCE is less prone to overfitting. The 2D feature extractor (Conv1-6) and 3D feature classifier (FC7-end) are trained simultaneously, so Conv1-6 can learn useful features for both lesions and their contexts. This strategy can also be viewed as factorizing 3D filters to 2D+1D <ref type="bibr" target="#b6">[7]</ref>. There are four loss terms in 3DCE: lesion classification loss and bounding-box regression loss in RPN and improved R-FCN. They are optimized jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Implementation Details</head><p>The algorithms were implemented using MXNet <ref type="bibr" target="#b0">[1]</ref> and run on an NVIDIA Titan X Pascal GPU. We initialized the weights in Conv1-Conv5 with an ImageNet <ref type="bibr" target="#b2">[3]</ref> pretrained VGG-16 model. All other layers were randomly initialized. Five anchor scales (16, 24, 32, 48, 96) and three anchor ratios (1:2, 1:1, 2:1) were used in RPN. The loss weight of bounding-box regression in improved R-FCN was set to be 10. In training, each mini-batch had 2 samples (each sample had M 3-channel images) when M &lt; 7 and 1 sample when M ? 7. We adopted the stochastic gradient descent (SGD) optimizer and set the base learning rate to 0.001, then reduced it by a factor of 10 after the 4th and 5th epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DeepLesion Dataset</head><p>The DeepLesion dataset <ref type="bibr" target="#b11">[12]</ref> available at 2 was mined from a hospital's picture archiving and communication system (PACS) based on bookmarks, which are markers annotated by radiologists during their daily work to highlight significant image findings. It is a large-scale dataset with 32,735 lesions on 32,120 axial slices from 10,594 CT studies of 4,427 unique patients. Different from existing datasets that typically focus on one type of lesion, DeepLesion contains a variety of lesions including those in lungs, livers, kidneys, etc., and enlarged lymph nodes in the chest, abdomen, and pelvis (see examples in <ref type="figure">Fig. 3</ref>). Their diameters range from 0.21 to 342.5mm. The great diversity in type and size makes lesion detection in this dataset a challenging task. We rescaled the 12-bit CT intensity range to floating-point numbers in [0,255] using a single windowing (-1024-3071 HU) that covers the intensity ranges of the lung, soft tissue, and bone. Every image slice was resized so that each pixel corresponds to 0.8mm. The slice intervals of most CT scans in the dataset are either 1mm or 5mm. We interpolated in the z-axis to make the intervals of all volumes 2mm. The black borders in images were clipped for computation efficiency. We divided DeepLesion into training (70%), validation (15%), and test (15%) sets by randomly splitting the dataset at the patient level. No data augmentation was performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Discussion</head><p>A predicted box was regarded as correct if its intersection over union (IoU) with a ground-truth box is larger than 0.5. The free-response receiver operating characteristic (FROC) curves of several methods are shown in <ref type="figure">Fig. 2</ref>.</p><p>? Improved R-FCN, 1 slice: Only the key slice with the lesion annotation was used for training and inference, thus no 3D context information was exploited. Its inferior accuracy indicates the importance of 3D context. ? Faster RCNN, 3 slice: The baseline method. Images composed of 3 neighboring slices were input to faster RCNN <ref type="bibr" target="#b7">[8]</ref> with VGG-16 backbone. Pool4 and pool5 were removed similar to the improved R-FCN. ? Improved R-FCN, 3 slices: See Sec. 2.1. It outperformed faster RCNN using the same 3-slice 3D context. ? Data-level fusion, 11 slices: In this method, multiple slices are input into improved R-FCN. We found that 11 slices achieved the best performance, but it is still lower than 3DCE with 9 input slices, proving that the feature fusion strategy of 3DCE can better leverage the 3D context information. ? 3DCE: We tested applying 9, 15, 21, 27 slices (M = 3, 5, 7, 9). They achieved the best accuracy among all methods compared on the dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. FROC curves of various methods on the test set of DeepLesion (4802 images).</head><p>To analyze the detection accuracy on different lesions and images, we split the test set according to three criteria and display the results in <ref type="table">Table 1</ref>. Eight lesion types are provided for the test set of DeepLesion. It is found that lung, mediastinum, and liver lesions have high sensitivity, probably because their intensity and appearance is relatively distinctive from the background. The sensitivity of all types were improved by 3DCE. Bone lesions were improved the most. As for lesion size, smaller lesions (&lt; 10mm) are harder to detect and benefited more by 3D context. 3DCE works better on CT scans with finer slice intervals because more precise information can be provided by the intermediate slices, compared with the interpolated slices in scans with bigger intervals. <ref type="table">Table 1</ref>. Sensitivity (%) at 4 false positives (FPs) per image on the test set of DeepLesion. The baseline is the faster RCNN algorithm. Lesions were sorted according to their types, sizes, and the slice intervals of the CT scans. The abbreviations of lesion types stand for lung, mediastinum, liver, soft tissue, pelvis, abdomen, kidney, and bone, respectively <ref type="bibr" target="#b11">[12]</ref>. The mediastinum type mainly consists of lymph nodes in the chest. Abdomen lesions are miscellaneous ones that are not in liver or kidney. The soft tissue type contains lesions in the muscle, skin, and fat. During experiments, we noticed that sometimes the detector identified smaller parts of a large or scattered lesion with a big ground-truth bounding-box. Although the IoU is less than 0.5 in such cases, the detection may still be viewed as a true positive (TP) and it can also help the radiologists. To overcome this evaluation bias, we utilized the intersection over the detected bounding-box area ratio (IoBB) as another criterion. Besides, there are missing lesion annotations in the test set, because DeepLesion is based on radiologists' bookmarks, who typically mark only representative lesions in their daily work. Thus, an FP prediction may actually be a TP. We invited two experienced radiologists to re-annotate all lesions on 300 random slices in the test set of DeepLesion. The number of ground-truth lesions in the 300 slices grew from 305 to 768. We also assessed the algorithms on this all-lesion test set. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, 3DCE still has the best accuracy despite the test sets and evaluation criteria. The performance computed using IoBB is better than IoU. The sensitivity at 4 FPs on the all-lesion test set actually became lower than the original test set of DeepLesion. DeepLesion consists of only representative lesions that radiologists think are measurable in their daily work, which are often subjective choices. Meanwhile, the all-lesion set was intensively labeled to include every abnormality. Some lesions in the all-lesion set are not measurable or too small, thus do not exist in the training set, which affects the algorithms' performance. In other words, both the number of FPs and the sensitivity are decreased by the new annotations. The inference time is also shown in <ref type="table" target="#tab_2">Table 2</ref>. The improved R-FCN spent significantly less time than the baseline faster RCNN (since faster RCNN needs to run 4 FC layers on a thick feature map) while still got better 2D candidate generation + 3D/2.5D false positive reduction (FPR) methods were also tested. The accuracy of the FPR classifiers on this dataset is not promising, which is possibly due to the small inter-class variance (lesions and non-lesions look very similar) and large intra-class variance (many lesion types) of the candidates. We also designed a 3D CNN that receives 27-slice inputs (same as 3DCE), extracts features using 3D filters, and predicts 2D boxes on the key slice. It was adapted from improved R-FCN and trained from scratch. Its sensitivity at 4 FPs per image is 79.7% compared with 3DCE's 84.4%, proving that 3DCE with pretrained weights and factorized filters is superior.</p><p>Sample detection results are shown in <ref type="figure">Fig. 3</ref>. There are a large variety of lesions in DeepLesion. In (c), 14 lung nodules/masses were annotated and 10 were detected (the two FPs are actually TPs with bigger boxes). In (d), an axillary enlarged lymph node was detected but three small mediastinum ones were incorrectly spotted. The detector sometimes cannot distinguish enlarged and normal lymph nodes due to its scale robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on the official data split of DeepLesion</head><p>After the release of the DeepLesion dataset and its official random patient-level data split 3 , we ran lesion detection experiments again and append the results in this section. We removed the 35 noisy lesion annotations mentioned in the dataset. After that, there are 4,912 lesions from 4,817 images in the test set. Experimental configurations were kept the same as the last section. The original R-FCN was also compared. Results are displayed in <ref type="figure">Fig. 4</ref>, <ref type="table" target="#tab_4">Table 3, and Table  4</ref>. It can be found that the general trends of the results remain similar, except that the slice interval is no longer a critical factor for 3DCE. The changes in the  </p><formula xml:id="formula_0">(a) (b) (c) (d) (e)<label>(f)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we presented 3D context enhanced region-based CNN (3DCE) to leverage the 3D context when detecting lesions in volumetric data. 3DCE is memory-friendly, end-to-end, and simple to implement and train. It consistently improved the detection accuracy on the DeepLesion dataset. We expect it to be applicable in various detection problems where 3D context is helpful. We also developed a detector that can help radiologists find all types of lesions with one unified framework. It may serve as an initial screening tool and send its detection results to other specialist systems trained on certain types of lesions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The framework of 3DCE for lesion detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Sensitivity (%) at 4 FPs per image of different methods. Both the original test set and the all-lesion test set are investigated using two overlap computation criteria.</figDesc><table><row><cell></cell><cell cols="2">Original test set</cell><cell cols="3">All-lesion test set Inference</cell></row><row><cell></cell><cell>IoU</cell><cell>IoBB</cell><cell>IoU</cell><cell>IoBB</cell><cell>time (ms)</cell></row><row><cell>No 3D context</cell><cell>76.51</cell><cell>80.16</cell><cell>66.90</cell><cell>72.47</cell><cell>19</cell></row><row><cell>Faster RCNN, 3 slices</cell><cell>80.32</cell><cell>85.34</cell><cell>71.60</cell><cell>80.31</cell><cell>32</cell></row><row><cell>Improved R-FCN, 3 slices</cell><cell>81.53</cell><cell>85.89</cell><cell>74.39</cell><cell>81.88</cell><cell>19</cell></row><row><cell>Data-level fusion, 11 slices</cell><cell>82.94</cell><cell>86.52</cell><cell>74.56</cell><cell>80.49</cell><cell>28</cell></row><row><cell>3DCE, 9 slices</cell><cell>83.57</cell><cell>87.81</cell><cell>76.31</cell><cell>82.75</cell><cell>56</cell></row><row><cell>3DCE, 27 slices</cell><cell cols="2">84.37 87.85</cell><cell>75.09</cell><cell>82.75</cell><cell>114</cell></row><row><cell cols="6">accuracy. 3DCE's time complexity is roughly linearly proportional to the num-</cell></row><row><cell cols="6">ber of input slices, since it generates feature maps for multiple images. However,</cell></row><row><cell cols="6">if tested on volumetric data, this extra time cost can be largely reduced because</cell></row><row><cell cols="5">the feature maps of neighboring slices can be cached and reused.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Detection results of 3DCE with scores &gt; 0.9 on the test set of DeepLesion. Yellow, orange, green, and red boxes correspond to ground-truths in the test set, additional ground-truths in the all-lesion test set, predicted true positives, and false positives.results may be due to the difference in data split, the removal of noisy samples, and random factors.Fig. 4. FROC curves of various methods on the official test set of DeepLesion (4817 images).</figDesc><table><row><cell></cell><cell>0.92</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.88</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.86</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sensitivity</cell><cell>0.82 0.84</cell><cell>3DCE, 27 slices</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>3DCE, 21 slices</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell>3DCE, 15 slices 3DCE, 9 slices</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Data-level fusion, 11 slices</cell><cell></cell></row><row><cell></cell><cell>0.78</cell><cell cols="2">Improved R-FCN, 3 slices</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Original R-FCN, 3 slices</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Faster RCNN, 3 slices</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.76</cell><cell cols="3">Improved R-FCN, 1 slice (no 3D context)</cell></row><row><cell cols="2">(g) Fig. 3. 0</cell><cell>(h) Average false positives per image 5 10</cell><cell>15</cell><cell>(i)</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Sensitivity (%) at various FPs per image on the test set of the official data split of DeepLesion. IoU was used as the overlap computation criterion. 60.57 71.19 79.15 84.77 88.42 Faster RCNN, 3 slices 56.90 67.26 75.57 81.62 85.83 88.74 Original R-FCN, 3 slices 55.70 67.26 75.37 82.21 86.26 89.19 Improved R-FCN, 3 slices 56.49 67.65 76.89 82.76 87.03 89.82 Data-level fusion, 11 slices 58.49 70.03 77.89 83.02 86.71 89.19 3DCE, 9 slices 59.32 70.68 79.09 84.34 87.81 89.62 3DCE, 27 slices 62.48 73.37 80.70 85.65 89.09 91.06</figDesc><table><row><cell>FPs per image</cell><cell>0.5</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell></row><row><cell>No 3D context</cell><cell>48.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Sensitivity (%) at 4 false positives per image on the official test set of DeepLesion. The baseline is the faster RCNN algorithm.</figDesc><table><row><cell></cell><cell>Lesion type</cell><cell cols="3">Lesion diameter (mm)</cell><cell cols="2">Slice interval (mm)</cell></row><row><cell></cell><cell>LU ME LV ST PV AB KD BN</cell><cell cols="3">&lt; 10 10 ? 30 &gt; 30</cell><cell>&lt; 2.5</cell><cell>&gt; 2.5</cell></row><row><cell cols="2">Baseline 86 83 88 70 80 79 79 65</cell><cell>75</cell><cell>84</cell><cell>81</cell><cell>81</cell><cell>82</cell></row><row><cell>3DCE</cell><cell>89 88 90 74 84 84 82 75</cell><cell>80</cell><cell>87</cell><cell>84</cell><cell>86</cell><cell>86</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://nihcc.box.com/v/DeepLesion</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://nihcc.box.com/v/DeepLesion</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>This research was supported by the Intramural Research Program of the NIH Clinical Center. We thank NVIDIA for the GPU card donation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<title level="m">MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">R-FCN: Object Detection via Region-based Fully Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="379" to="387" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Accurate Pulmonary Nodule Detection in Computed Tomography Images Using Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="559" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multilevel Contextual 3-D CNNs for False Positive Reduction in Pulmonary Nodule Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1558" to="1567" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08324</idno>
		<title level="m">Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy-or Network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1170" to="1181" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mollura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition. ICLR pp</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Lesion Graphs in the Wild: Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-scale Lesion Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
