<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING 1 APP-Net: Auxiliary-point-based Push and Pull Operations for Efficient Point Cloud Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youxin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE, Limin</roleName><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>IEEE</roleName><forename type="first">Wang</forename><surname>Member</surname></persName>
						</author>
						<title level="a" type="main">SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING 1 APP-Net: Auxiliary-point-based Push and Pull Operations for Efficient Point Cloud Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D Shape Classification</term>
					<term>Local Aggregator</term>
					<term>Ef- ficient</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aggregating neighbor features is essential for point cloud classification. In the existing work, each point in the cloud may inevitably be selected as the neighbors of multiple aggregation centers, as all centers will gather neighbor features from the whole point cloud independently. Thus each point has to participate in the calculation repeatedly and generates redundant duplicates in the memory, leading to intensive computation costs and memory consumption. Meanwhile, to pursue higher accuracy, previous methods often rely on a complex local aggregator to extract fine geometric representation, which further slows down the classification pipeline. To address these issues, we propose a new local aggregator of linear complexity for point cloud classification, coined as APP. Specifically, we introduce an auxiliary container as an anchor to exchange features between the source point and the aggregating center. Each source point pushes its feature to only one auxiliary container, and each center point pulls features from only one auxiliary container. This avoids the re-computation issue of each source point. To facilitate the learning of the local structure of cloud point, we use an online normal estimation module to provide the explainable geometric information to enhance our APP modeling capability. Our built network is more efficient than all the previous baselines with a clear margin while still consuming a lower memory. Experiments on both synthetic and real datasets demonstrate that APP-Net reaches comparable accuracies to other networks. It can process more than 10,000 samples per second with less than 10GB of memory on a single GPU. We will release the code in https://github.com/MCG-NJU/APP-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the growing demand for 3D applications, how to classify 3D objects with neural networks has become an important topic in recent years. Extensive work has been devoted to obtaining a higher accuracy for this task. Based on the data type and the employed networks, existing methods can be grouped into two categories. The first one is the multi-viewbased 2D solution <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> which projects the 3D object into 2D image planes from multiple views and then applies the welldesigned 2D convolutional neural network <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> to learn cross-view consistent representations. These methods focus on the selection of informative views and cooperation across different views. The second solution directly learns from the 3D data with point-based networks <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. They focus on how to integrate the spatial relation into feature aggregation process. Several kinds of delicate local aggregators, like the T. Lu, C. Liu, G. Wu, L. Wang are with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China.</p><p>Y. Chen is with the Samsung Electronics (China) R&amp;D Centre, Nanjing, 210012, China. point-wise MLP style and the position adaptive weighting style, are proposed to extract the fine geometric structure.</p><p>Thanks to the previous efforts, more and more works hit an accuracy of over 93% in the most popular classification dataset of ModelNet40 <ref type="bibr" target="#b8">[9]</ref>, in the last three years. In fact, the benchmark performance has shown saturation for a long time. The detailed analysis in CloserLook3D <ref type="bibr" target="#b9">[10]</ref> points out that, under fair comparison, the performance gap among different local aggregators can be bridged by unified network architecture and fair training process. This conclusion reminds us to think about whether it is necessary to simply pursue a higher accuracy when designing point cloud network. Instead, we argue that, in practice, running speed and memory consumption are also important factors that should be taken into account.</p><p>The essential factors responsible for the total overhead are the amount of computation and the degree of parallelism. Specifically, in the multi-view based methods, the computation and memory consumption are both linearly growing with the number of the views due to each view is processed independently. Such solutions may take several times intensive overhead to obtain slight improvement by introducing more views. For the point-based methods, due to the lack of neighbor index in the irregular point cloud, each point takes extra efforts to query and gather neighbors during the learning process. As analyzed in PVCNN <ref type="bibr" target="#b10">[11]</ref>, the pure point-based methods are slowed down heavily by the random memory access since it corrupts the parallelism heavily by the bank conflicts. So they propose to gather neighbor features efficiently with a voxel branch by benefiting from the memory locality.</p><p>In this paper, we propose a computation and memoryefficient point-based solution to 3D classification based on the following three observations: (1) If a point is queried as multiple points' neighbors, it has to participate in computations repeatedly and occupies serveral times footprints in memory, which leads to redundant computations and memory consumption. (2) Previous architectures, except for the PointNet <ref type="bibr" target="#b6">[7]</ref>, are all designed to accomplish feature aggregation and receptive field expansion simultaneously through the overlapped neighboring area for different center points. The points in the overlapped area are inevitably queried more than one time. (3) Due to the natural sparsity in the point cloud, the extra effort on the neighbor query is unavoidable. Even for the voxel-aided neighbor gathering, the dense voxelization manner still wastes extra resources on a large amount of blank voxels. The sparse manner also suffers from memory bank conflicts in the scatter and gather process. According to <ref type="figure">Figure</ref>  Ours (a) Accuracy-Speed 2 2 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 <ref type="bibr" target="#b9">10</ref> Test Batch Size  methods. Based on the first two observations, we conclude that one key towards an efficient point-based classification network is to decouple the process of feature aggregation and receptive field expansion. In addition, the third observation suggests that for the classification task (often uses 1024?4096 points), the 1-NN algorithm is efficient enough to query neighbors. Based on the above analysis, we propose a new network whose overall computation is reduced to linear complexity, and it only costs linear memory occupations. Specifically, we aggregate features and expand the receptive field in separate steps. During aggregation, to avoid repeated operations to each point, we introduce an auxiliary container as a proxy to exchange features among points. The container consists of a series of pre-specified anchors in 3D space. Then each point is processed by two operations: first, push its feature to only one nearest auxiliary anchor; second, pull the feature from only one nearest auxiliary anchor. According to the corresponding anchor, the point cloud is split into several non-overlapped blocks. Points closest to the same auxiliary anchor in Euclidean space will fall into the same block and accomplish the features exchange automatically. Each anchor only costs a tiny maintenance overhead. To avoid the artifact introduced by the auxiliary anchor, we propose a novel push and pull pair through which the influence from the anchor is reducible. To enable receptive field expansion, we introduce a second auxiliary container to produce a different partition of the whole point cloud. Combining the two-stage block partitions, we obtain an equivalent receptive field expansion. Finally, to facilitate learning the local structure in the early layer, we use an online normal estimation module to provide explainable geometric information to enhance our APP block's modeling capability.</p><p>The auxiliary-anchor-based push and pull operations pair, the so-called APP operator, achieve a huge reduction in the memory consumption and computation complexity while maintaining comparable accuracy to the previous methods. The comparisons among different styles of network structure, including PointNet <ref type="bibr" target="#b6">[7]</ref>, PointNet++ <ref type="bibr" target="#b7">[8]</ref>, point cloud transformer <ref type="bibr" target="#b11">[12]</ref>, and the proposed APP-based network, are depicted in <ref type="figure" target="#fig_4">Fig 3.</ref> It is easy to see that the overhead of the APP is linear to the number of input points. We conducted a detailed  quantitative analysis of the running speed and memory consumption. As depicted in <ref type="figure" target="#fig_6">Fig 1,</ref> we achieve an inference speed of more than 2000 samples/second with batch size=16. Among those networks outperforming 92%, we are clearly faster than the second efficient network, ShellNet <ref type="bibr" target="#b12">[13]</ref>. Moreover, our network consumes a remarkably low GPU memory. According to <ref type="figure" target="#fig_6">Fig 1,</ref> APP-Net only costs memories less than 10GB with a batch size=1024. Correspondingly, our machine's maximum supported batch size for Point Transformer <ref type="bibr" target="#b13">[14]</ref> is 64, which costs more than 30GB of memory. And the lightweight version of PointMLP <ref type="bibr" target="#b14">[15]</ref> consumes more than 25GB with a maximum batch size of 256. Furthermore, according to <ref type="figure" target="#fig_6">Fig 1,</ref> we can even achieve a speed more than 10,000 samples/s with a batch size of 256, which is 5? faster than the peak of other baselines. More details and analysis are presented in the following sections. In summary, the main contributions of this paper are: 1) We propose to decouple the feature aggregation and receptive field expansion process to facilitate redundancy reduction. 2) We propose an auxiliary-anchor-based operator to exchange features among neighbor points with linear computation complexity and linear memory consumption.  3) We propose to use the online normal estimation to improve the classification task. 4) Experiments show that the proposed network achieves remarkable efficiency and low memory consumption while keeping competitive accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we review some remarkable works in 3D object classification. According to the data type, we divide those methods into multi-view-based and point-cloud-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-view Based Methods</head><p>Multi-view-based methods consider learning point features with the mature CNN by projecting the 3D object into a series of 2D images from multiple views. Some works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b19">[20]</ref> devoted to investigating how to fuse the features with pooling policy. Yang et al. <ref type="bibr" target="#b20">[21]</ref> propose to combine with the guidance of the inter-relationship among views. MVTN <ref type="bibr" target="#b21">[22]</ref> learns to search a series of more informative viewpoints. Wei et al. <ref type="bibr" target="#b2">[3]</ref> treats the different views as nodes in a graph, which facilitates the use of GCN to exploit the relation among views. Carlos et al. <ref type="bibr" target="#b22">[23]</ref> proposes to use rotation equivariant convolution on multi-views.Liu et al. <ref type="bibr" target="#b23">[24]</ref> introduces a more complex setting of fine-grained classification and proposes to explicitly detect the regions of interest.</p><p>In general, the projection process is view-dependent. It requires processing many views to alleviate the geometrical information loss caused by the projection, making it intensive to analyze each 3D object. And it's challenging to fuse the view-wise features from different views to obtain a consistent and discriminative representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Point Based Methods</head><p>This is a widely used category in 3D classification. Motivated by the 2D CNN <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, many works are designed to aggregate local point features. The local area is determined by the distance in Euclidean space or the topology. DGCNN <ref type="bibr" target="#b26">[27]</ref> constructs a graph to learn with the connections between nodes. PointNet++ <ref type="bibr" target="#b7">[8]</ref> provides a standard paradigm for fully point-based method. It points out how to locate the neighbor area and aggregate local features point-wisely. Subsequent works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b27">[28]</ref> improve the designs of local area structure, downsampling methods, and local aggregator. Different from PointNet++ <ref type="bibr" target="#b7">[8]</ref>, some works design convolution-like operators for point cloud. SpiderNet <ref type="bibr" target="#b28">[29]</ref> uses Taylor expansion to approximate the spatial distribution of filters. KPConv <ref type="bibr" target="#b29">[30]</ref> specifies a series of kernel points to implement convolution. PointConv <ref type="bibr" target="#b30">[31]</ref> directly learns the values of the filter through coordinates. And PAConv <ref type="bibr" target="#b31">[32]</ref> proposes to assemble the prespecified weight banks according to the spatial relation. The recent hot topic, transformer, has started to show its power in the point cloud. PCT <ref type="bibr" target="#b11">[12]</ref> is the first totally transformerbased network which conducts a global self-attention in each layer. It cuts down the process to query neighbors because each point serves as all the other points' neighbors. Point Transformer <ref type="bibr" target="#b13">[14]</ref> enhances the local aggregator with a Transformer-like operator. Point2SpatialCapsule <ref type="bibr" target="#b32">[33]</ref> proposes to not only model the geometric information but also model the spatial relation with the Capsule Network <ref type="bibr" target="#b33">[34]</ref>. L3DOC <ref type="bibr" target="#b34">[35]</ref> introduces lifelong learning to extending the 3D classification task into open environments. DSDAN <ref type="bibr" target="#b35">[36]</ref> investigates the problem of cross-domain 3D classification. These methods have achieved remarkable accuracy. However, efficiency and low memory consumption are not their main targets. Some of the few attempts for efficiency explore by eliminating the existing architectures. ShellNet <ref type="bibr" target="#b12">[13]</ref> proposes to shrink some heavy operations the PointNet++ to construct a lightweight architecture, PointMLP-Elite leverages the bottleneck architecture to reduce the feature transforming burden. Although they have shown effectiveness in accelerating, they do not solve the problem of redundant resource calls. So they leave us with a huge room for eliminating the overhead. We achieve the most efficient network for point cloud classification with the proposed APP operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>A. Background and Insights 1) Preliminaries: The source point cloud with N points is denoted as P = {p 1 , p 2 , ..., p N }. The target is to aggregate current source features into a set of center points Q = {q 1 , q 2 , ..., q M }. The general process of a local point feature aggregation is as follows:</p><formula xml:id="formula_0">g i = R({G(r(q i , p j , [f i , f j ]), f j )|p j ? N (q i )})<label>(1)</label></formula><p>where q i is the center point, g i is the output feature for the center point. N (q i ) queries neighbors for q i . f * ? R Cin denotes the input feature of p * or q * . r(q i , p j , [f i , f j ]) measures the relation between the neighbor point and the center point. Most methods mainly consider the position relation, and others combine the feature relation. G and R refer to the features transformation and reduction, respectively. The reduction function is MAX Pooling in most cases.</p><p>2) Analysis and Insights: For the common point-based architecture <ref type="bibr" target="#b7">[8]</ref>, each center point selects neighbors from the source points independently. Each source point would be selected as multiple centers' neighbors to accomplish the receptive field expansion. Thus, each source point has to repeatedly participate in the aggregation process and occupies several duplicates of memories to facilitate parallelization. The advantage of this manner is obvious: it enables the neighbor point to adaptively contribute to different center points according to the current spatial and feature relation, and it combines the feature extraction and receptive field expansion in one step. However, it inevitably induces extra computations and memory consumption for redundant operations. In the first layer of PointNet++ <ref type="bibr" target="#b7">[8]</ref>, N = 1024, M = 512, each center point queries 32 neighbors, which indicates that each source point would be replicated and re-computed for 512?32 1024 =16 times.</p><p>To avoid using redundant resources, we notice that all the previous methods conduct the aggregation procedure from the view of the center point Q, and each center point is agnostic of how many times other center points have accessed the neighbor. To limit the access times of each source point, we turn to guide the aggregation from the view of the source point. In this paper, we proposed a so-called APP (auxiliary-based push and pull) operator which introduces the auxiliary point set A as a bridge between the source points and the center points. Each source point p only pushes its information to one auxiliary point, and each center point q only pulls information from one auxiliary point, i.e., during the whole feature aggregation process, each source point only participates in emitting feature for one time. And the center point only participates in gathering features for one time. Such paired operations linearize the complexity of computation. Although reducing some overhead, auxiliary points may also introduce incorrect dependencies on the artifacts. To eliminate the influence of auxiliary points, we implement the operation pair in a novel manner, with which the influence of auxiliary points is reducible. The core idea is to find a group of functions {?, ?, ?, } which satisfy the following relation:</p><formula xml:id="formula_1">?(x ? y) = [?(x ? a), ?(a ? y)]<label>(2)</label></formula><p>where ?(x ? a) and ?(a ? y) denote the process of pushing features from x to the auxiliary point a and pulling features from a to destination point y respectively. combines ?( * ) and ?( * ) in a reducible way such that the resulted ?(x ? y) only depends on the x and y, not affected by the auxiliary point. In the following sections, we will introduce some instantiations of the proposed {?, ?, ?, }.</p><p>B. Auxiliary-based Push and Pull Operations 1) Auxiliary Point Generation: As introduced above, the auxiliary points serve as bridges to pass information among local points. For simplicity, we directly downsample the original point cloud to obtain a subset A = {a 0 , a 1 , ..., a N ra }, where the r a is a downsample ratio. Following RandLA-Net <ref type="bibr" target="#b36">[37]</ref>, we adopt the Random Sample instead of the widely-used FPS (Farthest Point Sample) <ref type="bibr" target="#b7">[8]</ref> for further acceleration. Although the uniformity is slightly disturbed, the experimental results show little sensitivity to the sampling strategy. As mentioned in Section III-A that each source point only emits its information to one auxiliary point, we conduct a 1-NN query for every source point from the auxiliary point set A. For a point p i , we denote its auxiliary point as A(p i ). Points close to each other in Euclidean space naturally choose the same auxiliary point. As a result, the whole point cloud is partitioned into several non-overlapped blocks {B(a 0 ), B(a 1 ), ..., B(a N ra )}, where B(a i ) denotes the source point set whose 1-NN auxiliary point is a i .</p><p>2) The Design of Operation Groups: According to the previous analysis, a reducible operation group conforming to Equation 2 eliminates the auxiliary point's influence to prevent artifacts. Given two points x and y belonging to the same block, we first instantiate the ( * ) with two optional basic operators: element-wise multiplication and element-wise addition. The Equation 2 is rewritten as follows,</p><formula xml:id="formula_2">?(x ? y) = ?(x ? a) ? ?(a ? y),<label>(3)</label></formula><p>or</p><formula xml:id="formula_3">?(x ? y) = ?(x ? a) + ?(a ? y).<label>(4)</label></formula><p>For simplicity, we will use ? to denote ?(x ? a) and ? to denote ?(a ? y) in the following. Then we construct satisfying ? and ? for the two operators. For the addition operator, it's obviously that if ? and ? are linear mappings, i.e.</p><formula xml:id="formula_4">?(x ? y) = W ? (x ? y), = ? + ?, where ? = W ? (x ? a), ? = W ? (a ? y),<label>(5)</label></formula><p>where W is a weight matrix shared by ? and ?, the influence from the auxiliary point is easily eliminated, and the resulted ? is also a linear mapping function. Following a similar idea, we construct the ? and ? with exponential mapping for the multiplication operator. Specifically,</p><formula xml:id="formula_5">?(x ? y) = e W ?(x?y) , = ? ? ?, where ? = e W ?(x?a) , ? = e W ?(a?y) .<label>(6)</label></formula><p>The above designs are based on a single operator (multiplication or addition). When we jointly employ multiple basic operators for ?, more sophisticated operation groups can be derived. According to the "sum-difference-product" formula for trigonometric functions, we obtain sin and cos based operator groups as follows,</p><formula xml:id="formula_6">?(x ? y) = cos(W ? (x ? y)) = ?[0]?[0] ? ?[1]?[1], where ? ? ? ? ? ? ? ? ? ?[0] = cos(W ? (x ? a)) ?[1] = sin(W ? (x ? a)) ?[0] = cos(W ? (a ? y)) ?[1] = sin(W ? (a ? y)) ,<label>(7)</label></formula><p>?(x ? y) = sin(W ? (x ? y)),</p><formula xml:id="formula_7">= ?[0]?[1] + ?[1]?[0], where ? ? ? ? ? ? ? ? ? ?[0] = sin(W ? (x ? a)), ?[1] = cos(W ? (x ? a)), ?[0] = sin(W ? (a ? y)), ?[1] = cos(W ? (a ? y)).<label>(8)</label></formula><p>We believe that there are infinite operation groups satisfying the reducible philosophy. And we have no intention of exhausting those potential superior combinations. We adopt the above operation groups to form the basis of this work. They decompose the inter-communication process within the local area. For a local patch with n points, to pass information between every point-pair, previous methods like <ref type="bibr" target="#b37">[38]</ref> induce O(n 2 ) overhead. While with the proposed operation groups, all points share the same ?(x ? a) to obtain the information of x. The number of all possible ?( * ) is n. Thus we obtain a O(n) complexity overhead. Furthermore, due to some mathematical properties (parity, reciprocal relation, opposite relation, and so on) of the basic operators, some parts of the operations can be reused by other parts, which also contributes a lot to lowering the consumption of computation and memory resources. We summarize the reusable parts as follows,</p><formula xml:id="formula_8">? Exponential-based: ? = 1 ? ; ? Cosine-based: ?[0] = ?[0], ?[1] = ??[1]; ? Sine-based: ?[0] = ??[0], ?[1] = ?[1].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Push and Pull based Feature Aggregation</head><p>One key advantage of the point cloud is the preservation of geometry. Previous local aggregators mine the 3D structure information through modeling the spatial relation among points. According to how to mine the spatial relation, they are classified into different categories, like the point-wise MLPbased or adaptive-weight-based methods. For the point-wise MLP type, the Equation 1 is usually instantiated as follows,</p><formula xml:id="formula_9">g i = R({M LP (q i ? p j , f j )|p j ? N (q i )}).<label>(9)</label></formula><p>The adaptive-weight-based methods generate position adaptive kernel to weight neighbors. The Equation 1 is instantiated as follows,</p><formula xml:id="formula_10">g i = R({W(q i ? p j ) ? f j |p j ? N (q i )})<label>(10)</label></formula><p>where W( * ) generates the kernel weight according to the spatial relation. In this part, we show examples of how to build the two types local aggregators based on the above operation groups. Only parts of the proposed operation groups will be exhibited, and the others can be implemented by following similar steps or referring to our released code. The process consists of Position Encoding, Push-step, Pull-step, Channel Mixing, and Block-Based Down Sample. </p><formula xml:id="formula_11">g pi?A(pi) = W ? [f i , ?(p i ) ? ?(A(p i ))],<label>(11)</label></formula><p>where the W ? R 2Cin?Cin is a weight matrix corresponding to the one in Equation 5. This step pushes the source point's feature to the auxiliary point according to the spatial relation. Mind that all the source points belonging to the same auxiliary block will push their feature to the same auxiliary point. Thus the final feature in the auxiliary point is computed by</p><formula xml:id="formula_12">g A(p i ) = 1 |B(A(pi))| p j ? B(A(p i )) W ? [fj, ?(pj) ? ?(A(pi))].<label>(12)</label></formula><p>In the pull step, following Equation 5, we apply an inverse operation for each source point to pull features from the corresponding auxiliary point. The inverse operation is formulated as follows:</p><formula xml:id="formula_13">gi = g A(p i )?p i , = g A(p i ) + W ? [?fi, ?(A(pi)) ? ?(pi)] = 1 |B(A(pi))| p j ? B(A(p i )) W ? [fj ? fi, ?(pj) ? ?(pi)].<label>(13)</label></formula><p>According to the output, the resulted gi is only computed by the point feature and spatial relation. The auxiliary point A(pi) only serves to provide a neighbor query. Different from the common practice, we adopt AVG Pooling as the reduction function. One more difference is, for leveraging the reusable computation, Equation 13 finally concatenates the feature difference and spatial relation. To realise the concatenation of original feature and spatial relation, one can modify the ? operation by replacing the feature with zeros vector, i.e. ? = W ? [0, ?(A(pi)) ? ?(pi)]. And the ? operation cannot reuse the results of ?. We will discuss its influence in the experiment section. 2) Adaptive Weight Following similar steps, we can easily construct an adaptive weight aggregator. Here we use the Exponential-based operation groups. The push step is defined by</p><formula xml:id="formula_14">g p i ?A(p i ) = fi ? e W ?[?(p i )??(A(p i ))] ,<label>(14)</label></formula><p>where W ? R C in ?C in generates weight kernel according to the spatial relation. The e ( * ) provides channel-wise weights to the input feature. The features in A(pi) is return out_F, centroids</p><formula xml:id="formula_15">g A(p i ) = 1 |B(A(pi))| p j ? B(A(p i )) fj ?e W ?[?(p j )??(A(p i ))] (15)</formula><p>Then the pull step is formulated as</p><formula xml:id="formula_16">gi = g A(p i )?p i = g A(p i ) ? e W ?[?(A(p i ))??(p i )] = 1 |B(A(pi))| p j ? B(A(p i )) fj ? e W ?[?(p j )??(p i )] .<label>(16)</label></formula><p>Essentially speaking, the final output is an instantiation of Formula 10, whose reduction function is AVG Pooling. It weights each channel according to the spatial relation with neighbors.</p><p>-Channel Mixing The push and pull steps are efficient operations for mixing the features among points in the local area. However, there exist two obstacles towards a better representation: first, the use of AVG Pooling tends to obscure some high-frequency patterns in each local area. Then, some push and pull operations, like the Exponential-based groups, only conduct channel-wise weighting without inter-channel interaction, which damages the model capacity. Thus we employ a skip connection from the input features to make up for the high-frequency information. And we introduce a Fully Connection layer to enhance channel mixing. The feature is updated by</p><formula xml:id="formula_17">g i = ?([g i , f i ]).<label>(17)</label></formula><p>where ?( * ) is a non-linear function constituted by {FC+BatchNorm+LeakyReLU}. The resulted g i is of C out channels. -Block Based Downsampling After processing by Push-step and Pull-step, the output point cloud still has the same number of points as the input point cloud, which is similar to the PointNet layer <ref type="bibr" target="#b6">[7]</ref>. We design a block-based down sample strategy to reduce the intermediate overhead further. Like the operation in Section III-B1, we downsample the point cloud with a rate of r d and re-split the whole cloud into serval nonoverlap blocks {D 0 , D 1 , ..., D N r d } based on 1-NN algorithm. Then, for all the points belonging to the same block D i , their features are aggregated by a MAX pooling function as follows:</p><formula xml:id="formula_18">g di = M AX{f j |p j ? D i }.<label>(18)</label></formula><p>Then the aggregated features are registered to the corresponding block centroids. A python-style pseudo-code for an Exponential-based adaptive weight aggregator is shown in Algorithm 1.</p><p>-The Full Structure of APP-Net This part introduces how to build an end-to-end network with the APP operator. As shown in <ref type="figure">Figure 5</ref>, the input is the information of the point, including the position xyz, the normal and local curvature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Local Geometry from Online Normal and Curvature Estimation</head><p>The normal and curvature reflect the local surface property. Previous methods often use the coordinates as input features and design a delicate and heavy local extractor to model the local geometric structure. In this work, we turn to directly feed the network with the explicit geometric descriptor, i.e., normal and curvature, to simplify the process of modeling geometric structure. Then the network can be more concentrated on learning the semantic relation. In point cloud, normal estimation is approximated by the problem of estimating the normal of a plane tangent to the surface <ref type="bibr" target="#b38">[39]</ref>. In this paper, we adopt the simplest PCA-based method. A brief revision is present here to make the text more self-contained. For the centroid point p, computing its local covariance matrix by  </p><formula xml:id="formula_19">C = 1 |N (p)| pi?N (p) (p i ? p) ? (p i ? p) T ,<label>(19)</label></formula><formula xml:id="formula_20">C ? v j = ? j ? v j , j ? {0, 1, 2}<label>(20)</label></formula><p>v * and ? * represent the eigenvectors and eigenvalues of the covariance matrix, respectively. The eigenvector v corresponding to the minimum eigenvalue is the estimated normal. Supposed that ? 0 is the minimum eigenvalue, then the curvature ? of the local surface is determined by</p><formula xml:id="formula_21">? = ? 0 ? 0 + ? 1 + ? 2<label>(21)</label></formula><p>A direction consistency check will flip those normal who do not orient towards a pre-specified viewpoint (0, 0, 0) to alleviate the ambiguity in the normal direction. The comparison among different normals is depicted in <ref type="figure" target="#fig_10">Fig 6.</ref> E. Discussion and Analysis on the APP-Net 1) Receptive Field Analysis: The auxiliary point-based feature aggregation process includes two non-overlapped partitions, one for point mixing in the push step and pull step and the other for downsampling points. It's a common practice that large and expandable receptive fields are critical to learning good representation. As illustrated in <ref type="figure">Fig 7,</ref> the receptive field expands rapidly by combining the two non-overlapped partitions. A difference from the previous methods is that the expanded receptive field is irregular and random. Although introducing some uncertainties to the local descriptor, the random receptive field does not damage the global descriptor. And the global descriptor is more crucial to the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First Partition</head><p>Second Partition Equivalent receptive field <ref type="figure">Fig. 7</ref>. Two-stage Block Partition. The first stage is in the push and pull step. The second stage is used for the block down sample. The whole point cloud are randomly partitioned into several non-overlapped blocks with different partition ratios. Thus the effect of the two-stage block partition is equivalent to the overlapped blocks.</p><p>2) Relation to Prior Work: Relation to PointNet Both PointNet <ref type="bibr" target="#b6">[7]</ref> and the APP-Net's complexity are O(n) in each layer. However, the PointNet layer lacks point mixing, while the APP-Net introduces point mixing with the push and pull step, enabling information passing among points. Moreover, PointNet lacks a downsampling operation. Thus its high layers suffer intensive computations. In the APP-Net, the block-based down sample makes the high layers lightweight. Relation to PointNet++ and its follow-up PointNet++ and its follow-ups gather features from a sphere area around the center point, while the APP-Net gathers features from a random and irregular area. Another crucial difference is that PointNet++ imitates the convolution operation to accomplish receptive field expansion in one step, while the APP-Net does it in two stages. And the two-step style facilitates the linearization of feature extraction. One more difference is that we use the normal and curvature as the input to the network, while most of the previous work use the coordinates as the input. Relation to Transformer-like work Both the local Transformer block and the APP block model the pair-wise relations in the local area. The self-attention in Transformer constructs a N ? N relation matrix. However, in the APP block, we decompose the N ? N relation matrix into a N ? 1 matrix and a 1 ? N matrix, corresponding to the push step and pull step, respectively. This implies that the APP block is a linear version of the Transformer block and models a low-rank relation matrix. Relation to Conv-Deconv architecture The APP block is similar to the Conv-Deconv operation pair, where the push step convolves the input from N points into M points, and the pull step deconvolves the M points back into N points. However, they are completely different designs. The convolution and deconvolution are independent steps and do not share the learnable parameters. And the intermediate M points are nonnegligible. However, in the APP block, the push and pull steps are highly coupled. They reuse the same learnable parameters, and the influence from the intermediate M points is reducible.</p><p>3) Complexity Analysis: We compare the overhead of the proposed auxiliary-based method with the point-wise-MLP layer. Given the input {P ? R N ?3 , F ? R N ?Cin }, we want to obtain a output of {P ? R M ?3 , F ? R M ?Cout }. <ref type="table" target="#tab_3">Table I and Table II</ref> show the computation complexity and memory consumption for the APP block and a single scale single MLP PointNet++ block, respectively. K denotes the number of nearest neighbors. The dominating computation is Step Step</p><formula xml:id="formula_22">2 * N ? C in ? C out for APP block, M ? K ? C in ? C out</formula><formula xml:id="formula_23">Computation Memory Position Encoding N ? 3 ? C in N ? C in Push N ? C in N ra ? C in N ? C in N ra ? C in Pull N ? C in N ? C in Channel Mixing N ? C in ? Cout N ? Cout Block Pool - M ? Cout</formula><formula xml:id="formula_24">Computation Memory Group - M ? K ? (C in + 3) MLP M ? K ? (C in + 3) ? Cout M ? K ? Cout Pooling - M ? Cout</formula><p>for the PointNet++ block. Due to M ? K 2 ? N in practice, the APP block owns lower computations. In the stable implementation of PointNet++ <ref type="bibr" target="#b39">[40]</ref>, M ? K = 16 ? N , which induces 8? computations. As to the memory, the dominating part is</p><formula xml:id="formula_25">3 ? N ? C in + N ? C out for APP block, M ? K ? C in + M ? K ? C out for PointNet++.</formula><p>Similarly, the APP block owns several times lower memory consumption. Meanwhile, the above analyses are based on the single scale and single MLP settings. If employing the commonly used multiple scales or multiple MLP for the PointNet++, the overhead advantages for APP block would be more obvious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To explore the characteristics of our method, we conduct extensive experiments on a synthetic dataset and a real scene dataset. Our method has achieved competent accuracy while maintaining very low overhead and high efficiency compared with existing methods. We further conduct some ablation studies on Section IV-D to test how every module works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings</head><p>Our experiments are evaluated on a server with one Tesla V100 GPU. Most of the projects are implemented with Py-Torch <ref type="bibr" target="#b40">[41]</ref>. For all the tasks, we use the default Adam <ref type="bibr" target="#b41">[42]</ref> to update the network parameters. And we use the cosine learning rate scheduler <ref type="bibr" target="#b42">[43]</ref> to update the learning rate, with an initial learning rate of 2e-3. The minimum learning rate threshold is set to 2e-4. The cycle for the cosine scheduler is T max = 200. For all the experiments, we train the network for 300 epochs with a training batch size of 32 and a test batch size of 16; we use the first epoch to warm up the training process. All the results of the comparison methods are obtained in three ways: 1. careful reproduction in our environment to prevent the unfairness caused by machine development (For fairness, if we fail to reproduce the public results, we will adopt the following two ways); 2. the reported results in the original papers; 3. the updated results on the public websites or other published papers. For simplicity, we denote each variant of APP-Net with the combination of "basic operator+aggregator type" in the following part. The basic operators contain {Exp, Sin, Cos} and the aggregator type contains {AW, PW}. 'AW' means to use the adaptive weight style, and 'PW' denotes the point-wise MLP style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Shape Classification on ModelNet40</head><p>ModelNet40 <ref type="bibr" target="#b8">[9]</ref> is the most influential benchmark for 3D classification task, consisting of 40 common categories. The point-cloud-type data is synthesized by randomly sampling the surface of 3D CAD models, with a training set of 9,843 samples and a testing set of 2,468 samples. We report the most widely used metric, Overall Accuracy, on the testing set. For fairness, we do not adopt the voting strategy for all the methods(which often improves the accuracy by about 0.4% for some methods). Besides, we also report the computation overhead. As shown in <ref type="table" target="#tab_3">Table III</ref>, we achieve comparable accuracy to these SOTAs and maintain a very efficient running speed. The speed is measured by T otal Samples T otal Inf erence T ime . Most of the baseline methods obtain a faster speed than the one reported in the previously published papers. We believe it is mainly attributed to the machine difference and the adoption of the optimized CUDA operations. Using 1024 points is a standard setting for this task. There are also some methods choosing to input more (4096 or more) points to boost the result at the price of a heavier burden. For ModelNet40, the proposed APP-Net is fed with 4096 points while still running faster than all the other baselines of 1024 points. And it's 3? faster than the PointNet++ <ref type="bibr" target="#b7">[8]</ref>, 19? faster than CurveNet <ref type="bibr" target="#b50">[51]</ref> during test, which is coherent with the analysis in section III-E3. APP-Net has a clear speed advantage over the other methods even with the online estimated normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Shape Classification on ScanObjectNN</head><p>Considering the saturation of ModelNet40 <ref type="bibr" target="#b8">[9]</ref> and challenging real-world cases, Uy et al. propose the ScanObjectNN <ref type="bibr" target="#b51">[52]</ref>, collecting by scanning the indoor objects. The real-world data often face some annoying issues, like the cluttered or occluded by fore/background. So ScanObjectNN reveals the great potential to promote the classification application in the real world. According to the common practice in other work, we use the hardest variant PB T50 RS to conduct our experiments. The whole dataset contains a training set with 11416 samples and a testing set with 2882 belonging to 15 categories. We choose the most representative point-based and multi-view methods as the baselines. The normal we put into the network is computed online, and the duration of normal estimation is considered in the speed test. The overall accuracy is shown in <ref type="table" target="#tab_6">Table V</ref>. Following SimpleView <ref type="bibr" target="#b54">[55]</ref>, we report the mean?std. We outperform all the baseline methods, including the PointMLP <ref type="bibr" target="#b14">[15]</ref>. The lightweight "Exp+AW" version is 14? faster than PointMLP <ref type="bibr" target="#b14">[15]</ref> and 3.8? faster than the lightweight PointMLP-elite. With more parameters, the APP-Net achieves the best accuracy among all the methods. And the large version is still faster than all the other methods. In <ref type="table" target="#tab_3">Table IV</ref>, we report the time cost of each module under test mode, with batch size=16 and N =1024. Each layer only costs 1.1 ? 1.4ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>There are some fine-designed structures in APP. To test their functionality and substitutability, we conducted some ablation studies and analyses based on the ScanObjectNN. The Advantages of the Reducible Operation One of the core designs of the APP is the reducible operation pair: Push-</p><p>Step and Pull-step. They make each point's representation independent of the auxiliary point. To verify the effectiveness, we design two types of non-reducible operations: 1. use two different mapping functions to compute the spatial relation for the Push-step and Pull-step, respectively; 2. add non-linear operation, i.e., BatchNorm and Leaky ReLU, to the spatial relation. The results in <ref type="table" target="#tab_3">Table VI</ref> show that the reducible operation is non-trivially superior to the others in accuracy.   The Input to the Network Unlike previous work, which takes the original coordinates as the input, we use the estimated normal and curvature as explicit descriptors to represent the local geometry. <ref type="table" target="#tab_3">Table VII</ref> shows the comparisons among different inputs to different configured networks. The results clearly indicate that the estimated normal and curvature significantly  <ref type="bibr" target="#b26">[27]</ref> performance with a clear overhead advantage. Model Scalability In this part, we scale up the network by enlarging the number of feature channels to test the scalability of the APP-Net. According to <ref type="figure" target="#fig_12">Figure 8</ref>, the performance grows along with the number of channels. The Sin-based and Cos-based networks outperform the Exponential Function; we think its due to the value range of the Exponential Function being unbounded and growing rapidly, thus making it hard to optimize. In the point-wise manner, the Cos-based method is marginally better than the Sin-based method, while in the adaptive weighting variant, the relation are reversed.</p><p>Co-operation with other operators According to <ref type="table" target="#tab_3">Table IV</ref>, the first layer in PointNet++ occupies a large ratio of time. We layer-wisely replace the APP layer in our network with the PointNet++ layer to explore the effect of combining different layers. According to <ref type="table" target="#tab_3">Table VIII</ref>, the overall accuracy of every combination is comparable, while replacing the PointNet++ layer with the APP layer greatly accelerates the network and reduces memory consumption. Meanwhile, the results of the 3 layers PointNet++ is more effective and efficient than the one in <ref type="table" target="#tab_6">Table V</ref>. We believe it is because we implement it with the single MLP layer and the input of the network is the normal and curvature rather than the coordinates. The identical memory consumption in "A+P+P" and "P+P+P" settings are caused by the internal memory allocation mechanism of PyTorch.   <ref type="table" target="#tab_3">Table XI</ref> show that the global position encoding obtains better results than the local one. We think it is due to the instability of the local position caused by the random block partition, which hinders the network convergence during training. Besides, the feature relation does not provide a positive effect. We guess it wrongly builds the dependencies between the position and feature. This also explains why the "Feat+Global Position Encoding" performs better than "Concat[Feat, Global Position] Encoding" since the former decouples the process of learning feature relation and spatial relation. Feature Updating Style At the end of the pull step, every point's feature is updated by concatenating the output with the original feature and sending it to a ?( * ) function. Among the comparisons, we try to remove some parts of it or leverage a residual structure. As shown in <ref type="table" target="#tab_3">Table XII</ref>, The concatenation manner is superior to the other configurations. Moreover, in the last two rows, we remove the ?( * ) function or the whole APP block; the results clearly state their indispensability. Network Depth We explore how the network depth affects the performance. In the 2-layer and 3-layer version, we adopt the same r a and r d . In the 4-layer version, due to the original number of the point being 1024, after two downsample operations, the remaining points are insufficient to support a large r a , so we adopt a smaller r a = 16 in the last two layers. According to <ref type="table" target="#tab_3">Table XIII</ref>, the 3-layer version achieves the best performance. Down Sample Rate The rate r a and r d serve as the network's kernel size. They control the receptive field for each center point. For simplicity, we adopt the same r d for all layers in each variant. Results in <ref type="table" target="#tab_3">Table XIV</ref> imply that a large r a for the auxiliary point generation is critical to a better representation, especially at the high level (according to the last two rows). And the result is relatively not sensitive to the rate r d for aggregation.</p><p>Pooling Policies in the Aggregation We have tested with the common pooling policies to explore a proper aggregation operation for the second block partition. The position-adaptive pooling aggregates local points weighted by the reciprocal distance in Euclidean space. The results in <ref type="table" target="#tab_6">Table XV</ref> show that combining the local mean context and the most representative feature can achieve better performance for the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Limitations</head><p>We note the following limitations of this work: 1) The proposed operator is designed for the classification task. However, this paper does not explore its generalization to other dense estimation tasks, like segmentation. Although it does not harm the global descriptor, the random receptive field may introduce noise to the local descriptor. So the generalization to other tasks is challenging.  <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8]</ref>,ra= <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b31">32]</ref> 82.6 r d = <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8]</ref>,ra= <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">64,</ref><ref type="bibr">64]</ref> 83.7 r d = <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8]</ref>,ra= <ref type="bibr">[64,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b31">32]</ref> 83.1 2) In the synthetic dataset, the ground truth normal is necessary for the APP-Net to classify some hard cases (confusing with similar categories). This is because the geometry information is too dependent on the estimated noisy normal and curvature. A better estimation method or a lightweight geometric learning layer may alleviate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORK</head><p>This paper proposes a novel APP operator aggregating local features through auxiliary points. It avoids redundant memory consumption and re-computation of the source point. Furthermore, the auxiliary points' influence is reducible, allowing the method to preserve more details. Experiments on the synthetic and real-scene datasets show a good balance between performance, speed, and memory consumption in the classification task. Especially in speed, it outperforms all the previous methods significantly. Our future goal is to extend this method to more tasks, like semantic segmentation and object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>I, although the kNN algorithm costs too much time, its 1-NN variant shows great efficiency surpassing all the other arXiv:2205.00847v2 [cs.CV] 18 Aug 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Comparisons among Point Cloud Networks, including the PointNet, PointNet++, Point Cloud Transformer, and our proposed APP-Net. The black boxes in each method refer to the learnable parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>The APP Block Structure. The inputs are coordinates with features, and the output is downsampled coordinates and updated features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 )</head><label>1</label><figDesc>-Position Encoding At the beginning of every setting, we implement a simple and fast position encoding to lift the original coordinate from a 3-channel vector to an embedding of C in channels. The global sharing encoding function ?( * ) is implemented by [FC Layer + BatchNorm + Leaky ReLU]. We use the position encoding, instead of the original coordinate, to cope with the features in all the following steps. -Push &amp; Pull In the Push-step, the information of the source point is delivered to the auxiliary point. And in the Pull-step, each point gathers features from the corresponding auxiliary point. We introduce how to build a point-wise MLP-based and adaptive-weight-based local aggregator as examples. Point-wise MLP According to Equation 9, a point-wise MLP style aggregator requires concatenating features and spatial relation. Thus the push step is defined by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1</head><label>1</label><figDesc>Exponential-based adaptive weight aggregator # points: [N, 3], F: [N, C] # r_a, r_d # Block Partition aux_points = rand_choice(points, N/r_a) # [N/r_a, 3] idx_PtoA = one_nn(points, aux_points) # [N, 1] # push and pull pos_enc = Linear_BN_LReLU(points) # [N, C] kernel = exp(Linear(pos_enc)) # [N, C] F_weighted = F * kernel # [N,C] F_PtoA = scatter_mean(F_weighted, idx_PtoA) # [N/r_a, C] F_AtoP = gather(F_PtoA, idx_PtoA) # [N, C] aggregated_F = F_AtoP / kernel # [N,C] #Channel Mixing new_F = Linear_BN_LReLU([F, new_F]) # [N,C_out] # Block Based Down Sample centroids = rand_choice(points, N/r_d) # [N/r_d, 3] idx_PtoC = one_nn(points, centroids) # [N, 1] out_F = scatter_max(new_F, idx_P2C) # [N/r_d,C_out]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4 Fig. 5 .</head><label>45</label><figDesc>The whole structure of the proposed APP-Net. The channel of normal estimation is 3 or 4, which corresponds to whether to use the curvature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Ground Truth Normal.(b) Estimated Normal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .</head><label>6</label><figDesc>Different types of normals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Performance gains with the growing model size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I APP</head><label>I</label><figDesc>BLOCK OVERHEAD ANALYSIS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II POINTNET++</head><label>II</label><figDesc>BLOCK OVERHEAD ANALYSIS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III CLASSIFICATION</head><label>III</label><figDesc>ON MODELNET40. WE REPORT THE OVERALL ACCURACY, TRAIN SPEED, TEST SPEED, AND THE NUMBER OF PARAMETERS OF SOME BASELINES. 5K DENOTES 4096 POINTS, AND THE 7K FOR KPCONV MEANS USING AROUND 7,000 POINTS. 'P' AND 'N' MEANS USING POINT AND GROUND TRUTH NORMAL, RESPECTIVELY. Bold NUMBER DENOTES THE BEST ONE.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell>Inputs</cell><cell cols="2">Train Speed (samples/s)</cell><cell cols="2">Test Speed (samples/s)</cell><cell>Param.</cell><cell>OA(%)</cell></row><row><cell>PointNet [7]</cell><cell></cell><cell></cell><cell>1k P</cell><cell cols="2">960.9</cell><cell></cell><cell>1422.4</cell><cell>3.5M</cell><cell>89.2</cell></row><row><cell cols="2">Pointnet++ [8]</cell><cell></cell><cell>1k</cell><cell cols="2">352.2</cell><cell></cell><cell>730.2</cell><cell>1.41M</cell><cell>90.7</cell></row><row><cell cols="2">PointNet++ [8]</cell><cell cols="2">5k P+N</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>1.41M</cell><cell>91.9</cell></row><row><cell cols="2">PointCNN [44]</cell><cell></cell><cell>1k P</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>92.5</cell></row><row><cell cols="2">PointConv [31]</cell><cell cols="2">1k P+N</cell><cell cols="2">104.5</cell><cell></cell><cell>76.4</cell><cell>18.6M</cell><cell>92.5</cell></row><row><cell>KPConv [30]</cell><cell></cell><cell></cell><cell>7k P</cell><cell cols="2">211.7</cell><cell></cell><cell>717.7</cell><cell>15.2M</cell><cell>92.9</cell></row><row><cell>DGCNN [27]</cell><cell></cell><cell></cell><cell>1k P</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>92.9</cell></row><row><cell>RS-CNN [45]</cell><cell></cell><cell></cell><cell>1k P</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>92.9</cell></row><row><cell cols="2">DensePoint [46]</cell><cell></cell><cell>1k P</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>92.8</cell></row><row><cell>ShellNet [13]</cell><cell></cell><cell></cell><cell>1k P</cell><cell cols="2">551.3</cell><cell></cell><cell>1077.5</cell><cell>0.48M</cell><cell>93.1</cell></row><row><cell cols="2">PointASNL [28]</cell><cell cols="2">1k P+N</cell><cell cols="2">285.9</cell><cell></cell><cell>316.4</cell><cell>3.2M</cell><cell>93.2</cell></row><row><cell>PosPool [10]</cell><cell></cell><cell></cell><cell>5k P</cell><cell>51.0</cell><cell></cell><cell></cell><cell>59.5</cell><cell>18.5M</cell><cell>93.2</cell></row><row><cell cols="2">Point Trans. [47]</cell><cell></cell><cell>1k P</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>92.8</cell></row><row><cell>GBNet [48]</cell><cell></cell><cell></cell><cell>1k P</cell><cell>17.7</cell><cell></cell><cell></cell><cell>175.9</cell><cell>8.39M</cell><cell>93.8</cell></row><row><cell>GDANet [49]</cell><cell></cell><cell></cell><cell>1k P</cell><cell>29.8</cell><cell></cell><cell></cell><cell>273.3</cell><cell>0.93M</cell><cell>93.4</cell></row><row><cell>PA-DGC [32]</cell><cell></cell><cell></cell><cell>1k P</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>93.6</cell></row><row><cell cols="2">MLMSPT [50]</cell><cell></cell><cell>1k P</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>92.9</cell></row><row><cell>PCT [12]</cell><cell></cell><cell></cell><cell>1k P</cell><cell cols="2">115.7</cell><cell></cell><cell>1044.9</cell><cell>2.88M</cell><cell>93.2</cell></row><row><cell cols="2">Point Trans. [14]</cell><cell></cell><cell>1k P</cell><cell>67.1</cell><cell></cell><cell></cell><cell>82.3</cell><cell>12.9M</cell><cell>93.7</cell></row><row><cell cols="2">CurveNet [51]</cell><cell></cell><cell>1k P</cell><cell>89.9</cell><cell></cell><cell></cell><cell>112.9</cell><cell>2.04M</cell><cell>93.8</cell></row><row><cell cols="2">PointMLP [15]</cell><cell></cell><cell>1k P</cell><cell>60.4</cell><cell></cell><cell></cell><cell>169.0</cell><cell>12.6M</cell><cell>94.1</cell></row><row><cell cols="2">PointMLP-elite [15]</cell><cell></cell><cell>1k P</cell><cell cols="2">240.1</cell><cell></cell><cell>632.8</cell><cell>0.68M</cell><cell>93.6</cell></row><row><cell cols="2">APP-Net(Exp+AW)</cell><cell></cell><cell>5k P</cell><cell cols="2">785.9</cell><cell></cell><cell>1451.8</cell><cell>0.77M</cell><cell>93.0</cell></row><row><cell cols="2">APP-Net(Exp+AW)</cell><cell cols="2">5k P+N</cell><cell cols="2">1440.6</cell><cell></cell><cell>2155.5</cell><cell>0.77M</cell><cell>94.0</cell></row><row><cell cols="2">APP-Net(Cos+AW)</cell><cell cols="2">5k P+N</cell><cell cols="2">1174.1</cell><cell></cell><cell>1971.2</cell><cell>0.79M</cell><cell>93.5</cell></row><row><cell cols="2">APP-Net(Cos+PW)</cell><cell cols="2">5k P+N</cell><cell cols="2">1274.2</cell><cell></cell><cell>2107.6</cell><cell>0.77M</cell><cell>93.4</cell></row><row><cell cols="2">APP-Net(Sin+AW)</cell><cell cols="2">5k P+N</cell><cell cols="2">1224.3</cell><cell></cell><cell>1995.1</cell><cell>0.79M</cell><cell>93.2</cell></row><row><cell cols="2">APP-Net(Sin+PW)</cell><cell cols="2">5k P+N</cell><cell cols="2">1264.3</cell><cell></cell><cell>2095.1</cell><cell>0.77M</cell><cell>93.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE IV</cell><cell></cell></row><row><cell cols="8">TIME COST ANALYSIS FOR EACH MODULE OF THE APP-NET.</cell></row><row><cell cols="3">Normal Estimation</cell><cell cols="2">Feature Embedding</cell><cell cols="2">Layer 1</cell><cell>Layer 2</cell><cell>Layer 3</cell><cell>Classifier</cell></row><row><cell>APP-Net</cell><cell>2ms</cell><cell></cell><cell cols="2">0.13ms</cell><cell cols="2">1.2ms</cell><cell>1.4ms</cell><cell>1.1ms</cell><cell>0.07ms</cell></row><row><cell>PointNet++ [8]</cell><cell>-</cell><cell></cell><cell>-</cell><cell cols="3">20.01ms</cell><cell>12.8ms</cell><cell>0.88ms</cell><cell>0.27ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V CLASSIFICATION</head><label>V</label><figDesc>ON SCANOBJECTNN. THE INPUT FOR APP-NET CONTAINS 1024 POINTS. WE RUN THE EXPERIMENT FIVE TIMES AND REPORT THE MEAN?STD. Bold NUMBER DENOTES THE BEST ONE. * ? USES MORE LEARNABLE PARAMETERS (STILL LESS THAN MOST OF THE BASELINES).</figDesc><table><row><cell>Methods</cell><cell>Inputs</cell><cell>OA(%)</cell><cell>Train Speed (samples/s)</cell><cell>Test Speed (samples/s)</cell></row><row><cell>PointNet [7]</cell><cell>Point</cell><cell>68.2</cell><cell>960.9</cell><cell>1422.4</cell></row><row><cell>SpiderCNN [53]</cell><cell>Point</cell><cell>73.7</cell><cell>-</cell><cell>-</cell></row><row><cell>PointNet++ [8]</cell><cell>Point</cell><cell>77.9</cell><cell>352.2</cell><cell>730.2</cell></row><row><cell>DGCNN [27]</cell><cell>Point</cell><cell>78.1</cell><cell>-</cell><cell>-</cell></row><row><cell>PointCNN [44]</cell><cell>Point</cell><cell>78.5</cell><cell>-</cell><cell>-</cell></row><row><cell>BGA-DGCNN [52]</cell><cell>Point</cell><cell>79.7</cell><cell>-</cell><cell>-</cell></row><row><cell>BGA-PN++ [52]</cell><cell>Point</cell><cell>80.2</cell><cell>-</cell><cell>-</cell></row><row><cell>DRNet [54]</cell><cell>Point</cell><cell>80.3</cell><cell>-</cell><cell>-</cell></row><row><cell>GBNet [48]</cell><cell>Point</cell><cell>80.5</cell><cell>-</cell><cell>-</cell></row><row><cell>SimpleView [55]</cell><cell>Multi-view</cell><cell>80.5 ? 0.3</cell><cell>-</cell><cell>-</cell></row><row><cell>PRANet [56]</cell><cell>Point</cell><cell>82.1</cell><cell>-</cell><cell>-</cell></row><row><cell>MVTN [22]</cell><cell>Multi-view</cell><cell>82.8</cell><cell>-</cell><cell>-</cell></row><row><cell>PointMLP [15]</cell><cell>Point</cell><cell>85.4 ? 0.3</cell><cell>60.4</cell><cell>169.0</cell></row><row><cell>PointMLP-elite [15]</cell><cell>Point</cell><cell>83.8 ? 0.6</cell><cell>240.1</cell><cell>632.8</cell></row><row><cell>APP-Net(Exp+AW)</cell><cell>Point</cell><cell>84.3 ? 0.3</cell><cell>1633.0</cell><cell>2442.0</cell></row><row><cell>APP-Net(Cos+AW)</cell><cell>Point</cell><cell>84.2 ? 0.1</cell><cell>1377.3</cell><cell>2343.1</cell></row><row><cell>APP-Net(Cos+PW)</cell><cell>Point</cell><cell>84.4 ? 0.2</cell><cell>1405.4</cell><cell>2395.7</cell></row><row><cell>APP-Net(Sin+AW)</cell><cell>Point</cell><cell>84.7 ? 0.1</cell><cell>1394.2</cell><cell>2387.7</cell></row><row><cell>APP-Net(Sin+PW)</cell><cell>Point</cell><cell>84.4 ? 0.1</cell><cell>1359.4</cell><cell>2471.7</cell></row><row><cell>APP-Net(Cos+AW) ?</cell><cell>Point</cell><cell>86.1 ? 0.2</cell><cell>1239.7</cell><cell>2069.9</cell></row><row><cell>APP-Net(Sin+AW) ?</cell><cell>Point</cell><cell>87.0 ? 0.2</cell><cell>1153.9</cell><cell>2023.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI THE</head><label>VI</label><figDesc>REDUCIBLE OPERATION. WE EXPLORE WHETHER THE REDUCIBLE ATTRIBUTE IS NECESSARY AND A NEW INSTANTIATION OF THE REDUCIBLE PHILOSOPHY. THE BEST ONE IS COLORED WITH Bold. TESTED WITH THE "EXP+AW" AGGREGATOR.</figDesc><table><row><cell></cell><cell>OA(%)</cell></row><row><cell>Reducible</cell><cell>84.3</cell></row><row><cell>Not Reducible [Different Mapping Function]</cell><cell>82.8</cell></row><row><cell>Not Reducible [Non-linear Mapping Function]</cell><cell>83.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII DIFFERENT</head><label>VII</label><figDesc>INPUT TO THE NETWORK. TABLE VIIICOMBINATIONS OF THE APP LAYER AND POINTNET++ LAYER. 'A' DENOTES USING APP LAYER AND 'P' DENOTES USING POINTNET++ LAYER. TESTED WITH THE "EXP+AW" AGGREGATOR.</figDesc><table><row><cell></cell><cell cols="2">Sin+AW</cell><cell cols="2">Cos+AW</cell><cell>Sin+PW</cell><cell>Cos+PW</cell></row><row><cell>xyz</cell><cell></cell><cell>76.1</cell><cell></cell><cell>78.2</cell><cell>79.8</cell><cell>79.4</cell></row><row><cell cols="2">normal</cell><cell>83.8</cell><cell></cell><cell>83.0</cell><cell>83.3</cell><cell>83.7</cell></row><row><cell cols="2">normal+ curvature</cell><cell>84.7</cell><cell></cell><cell>84.2</cell><cell>84.4</cell><cell>84.4</cell></row><row><cell>Layer 0</cell><cell>Layer 1</cell><cell cols="2">Layer 2</cell><cell>OA (%)</cell><cell>Speed (samples/s)</cell><cell>Memory (MB)</cell></row><row><cell>A</cell><cell>A</cell><cell>A</cell><cell></cell><cell>84.3</cell><cell>2442.0</cell><cell>1393</cell></row><row><cell>A</cell><cell>A</cell><cell>P</cell><cell></cell><cell>83.7</cell><cell>2058.6</cell><cell>3361</cell></row><row><cell>A</cell><cell>P</cell><cell>P</cell><cell></cell><cell>83.7</cell><cell>1762.4</cell><cell>5535</cell></row><row><cell>P</cell><cell>P</cell><cell>P</cell><cell></cell><cell>84.1</cell><cell>1353.3</cell><cell>5535</cell></row><row><cell cols="7">improve the performance. Meanwhile, when fed with the xyz,</cell></row><row><cell cols="7">the APP-Net achieves PointNet++'s [8] and DGCNN's</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Farthest Point Sample versus Random Sample We use a random sample in the blockdown sample module. According to Table IX, although the farthest point sample produces a more uniform subset of the input point cloud, it does not lead to better accuracy. And it is slightly slower than the random sample in our experiments.</figDesc><table /><note>Whether to use feature difference in the Point-wise MLP aggregator To eliminate the calculation for the point-wise MLP style aggregator, we reuse the concatenation of feature and position encoding for the push and pull steps. This induces the aggregator to model the feature difference. To explore</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX DOWN</head><label>IX</label><figDesc>SAMPLE METHODS. 'RS' DENOTES RANDOM SAMPLE AND 'FPS' DENOTES FARTHEST POINT SAMPLE.the effect of modeling the feature difference and the original feature, we decouple the calculation of the push and pull steps. According toTable X, the modeling of the original feature marginally hits a better accuracy. However, it slows down the network. Taking the results inTable Vinto account, the speed around 2000 samples/s can be also met through enlarging the channels, and the enlarged version achieved even better accuracy. To keep the architecture simple, we model the feature difference as the standard version. Different Relation Modeling Methods We compare different ways of encoding points' relations in this part. The variants include using the global position encoding, local position encoding, or directly computing the spatial relation without using position encoding. The local position is computed by subtracting the corresponding center point. Considering that the transformer measures the relation between features, we also try to combine the feature relation in the APP. Results in</figDesc><table><row><cell>Configs</cell><cell cols="2">OA(%)</cell><cell></cell><cell cols="2">Speed (samples/s)</cell></row><row><cell cols="2">Exp+AW</cell><cell cols="2">Sin+AW</cell><cell>Exp+AW</cell><cell>Sin+AW</cell></row><row><cell>RS</cell><cell>84.3</cell><cell cols="2">84.7</cell><cell>2442.0</cell><cell>2387.7</cell></row><row><cell>FPS</cell><cell>83.8</cell><cell cols="2">84.1</cell><cell>2320.4</cell><cell>2235.2</cell></row><row><cell></cell><cell cols="3">TABLE X</cell><cell></cell></row><row><cell cols="6">POINTWISE MLP STYLE ANALYSIS</cell></row><row><cell>Pulls</cell><cell></cell><cell></cell><cell cols="2">OA(%)</cell><cell>Speed (samples/s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Sin</cell><cell>Cos</cell><cell>Sin</cell><cell>Cos</cell></row><row><cell></cell><cell></cell><cell></cell><cell>+PW</cell><cell>+PW</cell><cell>+PW</cell><cell>+PW</cell></row><row><cell cols="3">W ? [f i , ?(A(p i ) ? ?(p i ))]</cell><cell>84.4</cell><cell>84.4</cell><cell>2471.7</cell><cell>2395.7</cell></row><row><cell cols="2">W ? [0, ?(A(p i )) ? ?(p i )]</cell><cell></cell><cell>84.7</cell><cell>85.0</cell><cell>2143.2</cell><cell>1992.0</cell></row><row><cell></cell><cell cols="3">TABLE XI</cell><cell></cell></row><row><cell cols="6">DIFFERENT WAYS TO MODEL THE RELATIONS AMONG NEIGHBORS.</cell></row><row><cell cols="6">TESTED WITH THE "EXP+AW" AGGREGATOR.</cell></row><row><cell></cell><cell cols="2">Configs</cell><cell></cell><cell></cell><cell>OA(%)</cell></row><row><cell cols="4">Global Position Encoding ?(p i )</cell><cell></cell><cell>84.3</cell></row><row><cell cols="4">Local Position Encoding ?(p i ? A(p i ))</cell><cell></cell><cell>82.8</cell></row><row><cell></cell><cell cols="3">No Position Encoding p i</cell><cell></cell><cell>82.5</cell></row><row><cell cols="5">Feat+Global Position Encoding f i + ?(p i )</cell><cell>83.6</cell></row><row><cell cols="5">Concat[Feat, Global Position] Encoding [f i , ?(p i )]</cell><cell>82.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XII DIFFERENT</head><label>XII</label><figDesc>WAYS TO UPDATE THE FEATURE AFTER THE PULL-STEP. TESTED WITH THE "EXP+AW" AGGREGATOR.Concat g i = ?([g i , f i ]) Not Concat g i = ?(g i ) Res Feature g i = ?(g i ) + f iTABLE XIII THE INFLUENCE OF THE NETWORK DEPTH. WITH DIFFERENT LAYERS, THE RATE WILL BE ADJUSTED ADAPTIVELY.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Identity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>g i = g i</cell></row><row><cell>OA (%)</cell><cell>84.3</cell><cell>77.8</cell><cell>83.7</cell><cell>79.3</cell></row><row><cell></cell><cell></cell><cell>Configs</cell><cell></cell><cell>OA(%)</cell></row><row><cell></cell><cell cols="2">2 layers, r d =[8,8], ra=[64,64]</cell><cell></cell><cell>83.0</cell></row><row><cell></cell><cell cols="2">3 layers, r d =[8,8,8], ra=[64,64,64]</cell><cell></cell><cell>84.3</cell></row><row><cell></cell><cell cols="3">4 layers, r d =[4,4,8,8], ra=[64,64,16,16]</cell><cell>83.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XIV DIFFERENT</head><label>XIV</label><figDesc>RATE CONFIGURATIONS FOR THE APP-NET. Rate OA(%) r d =[8,8,8],ra=[64,64,64] 84.3 r d =[4,4,4],ra=[64,64,64] 84.1 r d =[16,16,16],ra=[64,64,64] 83.5 r</figDesc><table /><note>d =</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XV</head><label>XV</label><figDesc></figDesc><table><row><cell cols="2">DIFFERENT POOLING POLICIES.</cell></row><row><cell>Configs</cell><cell>OA(%)</cell></row><row><cell>AVG+MAX Pooling</cell><cell>84.3</cell></row><row><cell>MAX Pooling</cell><cell>82.1</cell></row><row><cell>AVG Pooling</cell><cell>79.7</cell></row><row><cell>Position Adaptive Pooling</cell><cell>79.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-view harmonized bilinear network for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">View-gcn: View-based graph convolutional network for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1850" to="1859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNI-TION (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="326" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03739</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pct: Point cloud transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="199" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking network design and local geometry in point cloud: A simple residual MLP framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-view harmonized bilinear network for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning relationships for multi-view 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7505" to="7514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Drcnn: Dynamic routing convolutional neural network for multi-view 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="868" to="877" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gvcnn: Group-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Veram: Viewenhanced recurrent attention model for 3d shape classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3244" to="3257" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning relationships for multi-view 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7505" to="7514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mvtn: Multi-view transformation network for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Equivariant multi-view networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1568" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fine-grained 3d shape classification with hierarchical part-view attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2020.3048623</idno>
		<ptr target="https://doi.org/10.1109/TIP.2020.3048623" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1744" to="1758" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spidernet: An integrated peerto-peer service composition framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nahrstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 13th IEEE International Symposium on High performance Distributed Computing</title>
		<meeting>13th IEEE International Symposium on High performance Distributed Computing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3173" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Point2spatialcapsule: Aggregating features and spatial relationships of local regions on point clouds using spatial-aware capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8855" to="8869" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">L3doc: Lifelong 3d object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="7486" to="7498" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-dataset point cloud recognition using deep-shallow domain adaptation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="7364" to="7377" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Estimating surface normals in a pointcloud</title>
		<ptr target="https://pcl.readthedocs.io/projects/tutorials/en/latest/normalestimation.html" />
	</analytic>
	<monogr>
		<title level="m">?highlight=normal% 20estimation Accessed</title>
		<imprint>
			<date type="published" when="2022-07-31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pointnet2 pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<ptr target="https://github.com/erikwijmans/Pointnet2PyTorchAccessed" />
		<imprint>
			<date type="published" when="2022-07-31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="820" to="830" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5239" to="5248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="134" to="826" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Geometric back-projection network for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning geometry-disentangled representation for complementary understanding of 3d object point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.10921</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Point cloud learning with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Q</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13636</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Curvenet: Curvature-based multitask learning deep networks for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muzahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1177" to="1187" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1588" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dense-resolution network for point cloud classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3813" to="3822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Revisiting point cloud shape classification with a simple and effective baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2021</title>
		<imprint>
			<biblScope unit="page" from="3809" to="3820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pra-net: Point relationaware network for 3d point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4436" to="4448" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
