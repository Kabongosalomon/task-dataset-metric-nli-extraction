<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-En</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology(BNRist)</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
							<email>xuhua@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology(BNRist)</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology(BNRist)</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer and Information Technology</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying new user intents is an essential task in the dialogue system. However, it is hard to get satisfying clustering results since the definition of intents is strongly guided by prior knowledge. Existing methods incorporate prior knowledge by intensive feature engineering, which not only leads to overfitting but also makes it sensitive to the number of clusters. In this paper, we propose constrained deep adaptive clustering with cluster refinement (CDAC+), an end-toend clustering method that can naturally incorporate pairwise constraints as prior knowledge to guide the clustering process. Moreover, we refine the clusters by forcing the model to learn from the high confidence assignments. After eliminating low confidence assignments, our approach is surprisingly insensitive to the number of clusters. Experimental results on the three benchmark datasets show that our method can yield significant improvements over strong baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Discovering new user intents that have not been met is an important task in the dialogue system. By grouping similar utterances into clusters, we may identify new business opportunities and decide the future direction of system development. Since most conversational data is unlabelled, an effective clustering method can help us automatically find a reasonable taxonomy and identify potential user needs.</p><p>However, it is not as easy as we think. On the one hand, it is difficult to estimate the exact number of new intents. On the other hand, it is hard to get desired clustering results since the taxonomy of intents is usually determined by the heuristic <ref type="bibr" target="#b11">(Lin and Xu 2019)</ref>. For example, suppose we want to partition the data according to the technical problems encountered by users, we may end up with clustering results partitioned by question types <ref type="bibr">(e.g., what, how, why)</ref>.</p><p>Recently, this problem has attracted the attention of researchers. For example, <ref type="bibr" target="#b6">Hakkani-T?r et al. (2015)</ref> use semantic parsing to decompose user utterances into graphs  <ref type="figure">Figure 1</ref>: An example of new intent discovery. Our goal is to find out the underlying new intents by utilizing the limited labeled data to guide the clustering process. and prune it into subgraphs based on frequency and entropy. Padmasundari and Bangalore (2018) combine the results of different clustering methods and sentence representations through an ensemble approach. AutoDial <ref type="bibr" target="#b17">(Shi et al. 2018</ref>) extracts all kinds of features, such as POS tags and keywords, and then uses the hierarchical clustering method to group the sentences. <ref type="bibr" target="#b7">Haponchyk et al. (2018)</ref> use predefined structured outputs to guide the clustering process. However, all of the above methods require intensive feature engineering. Besides, those methods perform representation learn and cluster assignments in a pipeline manner, which may result in poor performance.</p><p>In reality, as shown in <ref type="figure">Figure 1</ref>, we may have limited labeled data and a vast amount of unlabeled data, and we do not know all the intent categories in advance. Besides, the training data is noisy because unlabeled data contain both known and unknown intents. The key is to take advantage of labeled data to improve clustering performance effectively.</p><p>To address these issues, we propose an end-to-end clustering method that optimizes the intent representation within the clustering process. Also, we leverage the pre-trained language model, BERT <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref>, and the labeled data to aid the clustering process. As shown in <ref type="figure">Figure 2</ref>, we divide our method into three steps. First, we obtain intent representations from BERT. Second, we construct a Step 1: Extract intent representations</p><p>Step 2.2b: Select similar and dissimilar sentence pairs by dynamic upper and lower thresholds</p><p>Step Step 3.1 Generate auxiliary target distribution P Probabilities (Q)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering Results</head><p>Self-supervised (Labeled + Unlabeled) Step 2.3: Train on BERT <ref type="figure">Figure 2</ref>: The model architecture of CDAC+. We repeat step 1 and 2 iteratively until the upper and lower thresholds overlap. Then we go to step 3 to refine the clustering results further. The figure is best viewed in color. We use blue blocks to represent frozen network parameters.</p><p>pairwise-classification task as the surrogate for clustering by determining whether the sentence pair is similar or not. We use intent representations to calculate the similarity matrix of sentence pairs. Then, we train the network with similar or dissimilar labels, which are generated by either labeled data or dynamic similarity thresholds. We treat the pairwise constraints provided by labeled data as prior knowledge and use it to guide the clustering process. Finally, we use the auxiliary target distribution and Kullback-Leibler divergence (KLD) loss to encourage the model to learn from the high confidence assignments. We refine the intent representation and cluster assignment jointly in an end-to-end fashion. By eliminating low confidence assignments, our approach is insensitive to the number of clusters.</p><p>We summarize our contribution as follows. First, we propose an end-to-end clustering method that does not require intensive feature engineering and is insensitive to the number of clusters. Second, we demonstrate how to leverage the pre-trained language model and limited labeled data to aid the clustering process. Finally, extensive experiments conducted on three datasets show that our method can yield significant improvements compared with strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Transfer Learning</head><p>Transfer learning uses knowledge in the source domain to help the learning process in the target domain. Types of transferred knowledge include training instance, feature representation, model parameter, and the relation among data <ref type="bibr" target="#b14">(Pan and Yang 2010)</ref>. <ref type="bibr" target="#b8">Hsu, Lv, and Kira (2018)</ref> propose to transfer the pairwise similarity for cross-domain clustering. However, an extra similarity prediction model must be trained in advance. Our method transfers the relations among data through pairwise similarity and model parameters of the pre-trained language model <ref type="bibr" target="#b4">(Devlin et al. 2019</ref>), but does not require an extra similarity prediction model. We use the transferred knowledge to guide the clustering process.</p><p>Few-shot learning Few-shot learning also requires knowledge transfer from existing classes to the new classes. It focuses on classification problems, and one of the most popular methods is to transfer knowledge via the clustering approach <ref type="bibr" target="#b18">(Snell, Swersky, and Zemel 2017)</ref>. Besides, its test set contains only new classes.</p><p>In contrast, our work focuses on clustering problems, and we transfer knowledge via classification approach. When we try to discover new intents, the test set usually contains both known and unknown classes, and the samples could be easily misclassified as known classes. Our setting is not only closer to reality but also more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Clustering</head><p>In the literature, various algorithms can be used for intent clustering such as K-means (MacQueen and others 1967) and agglomerative clustering <ref type="bibr" target="#b5">(Gowda and Krishna 1978)</ref>. However, these traditional methods are ineffective in highdimensional data due to the limitations of feature space and the choice of predefined distance metrics.</p><p>We can resolve this problem by using neural networks to optimize the feature space in advance. For example, STCC <ref type="bibr" target="#b23">(Xu et al. 2015</ref>) construct a self-taught objective to learn the compressed representation and then perform K-means on it. With the development of deep learning, researchers start studying how to use the neural network to learn feature representations and cluster assignments simultaneously.</p><p>Deep Neural Network-based Clustering Clustering with deep neural networks is an emerging topic, and deep embedding clustering (DEC) <ref type="bibr" target="#b22">(Xie, Girshick, and Farhadi 2016)</ref> opens up the possibility for it. DEC compress the TF-IDF of documents into low-dimensional representations through a stacked autoencoder. Then, they iteratively optimize the clustering objective with a self-training target distribution by KLD loss. Deep clustering network (DCN) <ref type="bibr" target="#b24">(Yang et al. 2017</ref>) follow the idea of DEC and add penalty term on reconstruction during the process of optimizing the clustering objective. However, these methods merely compress representations and unable to capture the context effectively. <ref type="bibr" target="#b3">Chang et al. (2017)</ref> propose deep adaptive clustering (DAC), which uses a pairwise-classification framework and recasts image clustering into a binary classification problem. They use convolutional neural network (CNN) to determine whether the sentence pair is similar or not. Then, they perform adaptive clustering in a self-supervised manner. The key is that the filters of CNN can naturally provide discriminative power even they are randomly initialized <ref type="bibr" target="#b2">(Caron et al. 2018)</ref>, so the similarity between samples can be measured. However, the assumption does not work for text data. Instead, we replace CNN with the pre-trained language model and use it to measure the similarity between samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constrained Clustering</head><p>Constrained clustering uses a small amount of labeled data to aid the clustering process. A paradigm is to modify the clustering objective function to satisfy the pairwise constraints. For example, COP-KMeans <ref type="bibr" target="#b19">(Wagstaff et al. 2001)</ref> use must-link and cannot-link between samples as hard constraints. PCK-Means <ref type="bibr" target="#b0">(Basu, Banerjee, and Mooney 2004)</ref> introduce the soft constraints by allowing the constraints to be violated with violation cost. Based on PCK-Means, MPCK-Means <ref type="bibr" target="#b1">(Bilenko, Basu, and Mooney 2004)</ref> use the constraints to optimize the distance metric simultaneously. <ref type="bibr" target="#b20">Wang, Mi, and Ittycheriah (2016)</ref> extend the idea to neural networks with instance-level constraints. <ref type="bibr" target="#b8">Hsu, Lv, and Kira (2018)</ref> use an extra similarity prediction model to incorporate pairwise constraints into the clustering process. We use pairwise constraints for optimizing clustering objective and metric learning in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constrained Deep Adaptive Clustering with Cluster Refinement</head><p>We divide the proposed method into three steps: intent representation, pairwise-classification, and cluster refinement. The model architecture is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intent Representation</head><p>First, we use the pre-trained BERT language model to obtain intent representations. Given the i th sentence x i in the corpus, we take all token embeddings [C, T 1 , ? ? ? , T N ] ? R (N +1)?H in the last hidden layer of BERT and apply mean-pooling on it to get the average representation e i ? R H :</p><formula xml:id="formula_0">e i = mean-pooling([C, T 1 , ? ? ? , T N ])<label>(1)</label></formula><p>where N is the sequence length and H is the hidden layer size. Then, we feed e i to clustering layer g and obtain intent representation</p><formula xml:id="formula_1">I i ? R k : g(e i ) = I i = W 2 (Dropout(tanh(W 1 e i )))<label>(2)</label></formula><p>where W 1 ? R H?H and W 2 ? R H?k are learnable parameters, and k is the number of clusters. We use clustering layer to group the high-level features and extract intent representation I i for next steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pairwise-Classification with Similarity Loss</head><p>The essence of clustering is to measure the similarity between samples <ref type="bibr" target="#b7">(Haponchyk et al. 2018;</ref><ref type="bibr" target="#b16">Poddar et al. 2019)</ref>. Inspire by DAC <ref type="bibr" target="#b3">(Chang et al. 2017)</ref>, we reframe the clustering problem as a pairwise-classification task. By determining whether the sentence pair is similar or not, our model can learn clustering-friendly intent representation. We use intent representation I to compute the similarity matrix S:</p><formula xml:id="formula_2">S ij = I i I T j I i I j (3)</formula><p>where ? is L2 norm and i, j ? {1, . . . , n}. We denote batch size as n. S ij indicates the similarity the between sentence x i and x j . Then, we iteratively go through supervised and self-supervised step to optimize the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised</head><p>Step Given a small amount of labeled data, we can construct the label matrix R:</p><formula xml:id="formula_3">R ij := 1, if y i = y j , 0, if y i = y j<label>(4)</label></formula><p>where i, j ? {1, . . . , n}. Then, we use the similarity matrix S and the label matrix R to compute the similarity loss L sim :</p><formula xml:id="formula_4">L sim (R ij , S ij ) = ?R ij log(S ij ) ?(1 ? R ij ) log(1 ? S ij ).<label>(5)</label></formula><p>Here we treat labeled data as priori knowledge and use it to guide the clustering process. It implies how the model should partition the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-supervised</head><p>Step First, by applying dynamic thresholds on similarity matrix S, we get the self-labeled matrix R:R In each run of the experiment, we randomly select 25% intents as unknown. Taking SNIPS dataset as an example, we randomly select 2 intents as unknown and treat the remaining 5 intents as known.</p><formula xml:id="formula_5">ij := ? ? ? 1, if S ij &gt; u(?) or y i = y j , 0, if S ij &lt; l(?) or y i = y j , Not selected , otherwise<label>(6)</label></formula><p>where i, j ? {1, . . . , n}. The dynamic upper threshold u(?) and the dynamic lower threshold l(?) are used to determine whether the sentence pair is similar or dissimilar. Note that the sentence pairs with similarities between u(?) and l(?) do not participate in the training process. In this step, we mix labeled and unlabeled data to train the model. The labeled data can provide ground truth for noisy self-labeled matrix S and reduce the error. Second, we add u(?) ? l(?) as the penalty term for the number of samples.</p><formula xml:id="formula_6">min ? E(?) = u(?) ? l(?)<label>(7)</label></formula><p>where ? is an adaptive parameter that controls the sample selection, and we iteratively update the value of ? with:</p><formula xml:id="formula_7">? := ? ? ? ? ?E(?) ??<label>(8)</label></formula><p>where ? denotes the learning rate of ?. Since u(?) ? ?? and l(?) ? ?, we can gradually increase ? during the training process to decrease u(?) and increase l(?). It allows us to gradually select more sentence pairs to participate in the training process. It may also introduce more noise toR. Finally, we use the similarity matrix S and the self-labeled matrixR to compute the similarity lossL sim :</p><formula xml:id="formula_8">L sim (R ij , S ij ) = ?R ij log(S ij ) ?(1 ?R ij ) log(1 ? S ij ).<label>(9)</label></formula><p>As the thresholds change, we train the model from easily classified sentences pair to hardly classified sentences pair iteratively to obtain the clustering-friendly representation. When u(?) ? l(?), we stop the iterative process and move to the refinement stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cluster Refinement with KLD loss</head><p>We adopt the idea of <ref type="bibr" target="#b22">Xie, Girshick, and Farhadi (2016)</ref> and refine the cluster assignments via an expectationmaximization approach iteratively. The intuition is to encourage the model to learn from the high confidence assignments. First, given the initialized cluster centroids U ? R k?k saved in the refinement layer, we calculate the soft assignment between intent representations and cluster centroids. Specifically, we use Students t-distribution as a kernel to estimate the similarity between intent representation I i and cluster centroid U j :</p><formula xml:id="formula_9">Q ij = (1+ I i ? U j 2 ) ?1 j (1+ I i ? U j 2 ) ?1<label>(10)</label></formula><p>where Q ij represents the probability (soft assignment) that the sample i belongs to the cluster j. Second, we use the auxiliary target distribution P to force the model to learn from the high confidence assignments, thereby refining the model parameters and cluster centroids. We define target distribution P as follows:</p><formula xml:id="formula_10">P ij = Q 2 ij /f i j Q 2 ij /f j<label>(11)</label></formula><p>where f i = i Q ij denotes the soft cluster frequencies. Finally, we minimize the KLD loss between P and Q:</p><formula xml:id="formula_11">L KLD = KL(P Q) = i j P ij log P ij Q ij<label>(12)</label></formula><p>Then, we repeat the above two steps until the cluster assignment changes less than ? label % in two consecutive iterations. Finally, we inference cluster c i results as follows:</p><formula xml:id="formula_12">c i = arg max k Q ik<label>(13)</label></formula><p>where c i is the cluster assignment for sentence x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>We conduct experiments on three publicly available short text datasets. The detailed statistics are shown in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>SNIPS It is a personal voice assistant dataset which contains 14484 utterances with 7 types of intents.</p><p>DBPedia <ref type="bibr" target="#b25">(Zhang and LeCun 2015)</ref> It contains 14 nonoverlapping classes of ontology selected from DBPedia 2015 <ref type="bibr" target="#b10">(Lehmann et al. 2015)</ref>. We follow <ref type="bibr" target="#b20">Wang, Mi, and Ittycheriah (2016)</ref> and randomly select 1,000 samples for each classes.</p><p>StackOverflow Originally released on Kaggle.com, it contains 3,370,528 title of technical questions across 20 different classes. We use the dataset processed by <ref type="bibr" target="#b23">Xu et al. (2015)</ref> who randomly select 1,000 samples for each classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compare our method with both unsupervised and semisupervised clustering methods.   Unsupervised We compare our method with K-means (KM) (MacQueen and others 1967), agglomerative clustering (AG) <ref type="bibr" target="#b5">(Gowda and Krishna 1978)</ref>, SAE-KM and DEC <ref type="bibr" target="#b22">(Xie, Girshick, and Farhadi 2016)</ref> , DCN <ref type="bibr" target="#b24">(Yang et al. 2017)</ref> and DAC <ref type="bibr" target="#b3">(Chang et al. 2017)</ref>. For KM and AG, we encode the sentence as a 300-dimensional embedding by averaging the pre-trained GloVe <ref type="bibr" target="#b15">(Pennington, Socher, and Manning 2014)</ref> word embeddings. We also run K-means on sentences encoded with averaging embeddings of all output tokens of the last hidden layer of the pre-trained BERT (BERT-KM).</p><p>Semi-unsupervised For semi-unsupervised methods, we compare with PCK-means (Basu, Banerjee, and Mooney 2004) , BERT-Semi <ref type="bibr" target="#b20">(Wang, Mi, and Ittycheriah 2016)</ref> and BERT-KCL <ref type="bibr" target="#b8">(Hsu, Lv, and Kira 2018)</ref>. For a fair comparison, we change the backbone network of these methods to the same BERT model as ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We follow previous studies and choose three metrics that are widely used to evaluate clustering results: Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and clustering accuracy (ACC). To calculate clustering accuracy, we use the Hungarian algorithm <ref type="bibr" target="#b9">(Kuhn 1955)</ref> to find the best alignment between the predicted cluster label and the ground-truth label. All metrics range from 0 to 1. The higher the score, the better the clustering performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>For each run of experiments, we randomly select 25% of classes as unknown and 10% of training data as labeled.</p><p>We set the number of clusters as the ground-truth. Besides, we divide all dataset into training, validation, and test sets. First, we train the model by limited labeled data (containing known intents) and unlabeled data (containing all intents) in the training set. Second, we tune the model on the validation set, which only contains known intents. Finally, we evaluate the results on the test set. We report the average performance of each algorithm over ten runs. We build our model on top of the pre-trained BERT model (base-uncased, with 12-layer transformer) implemented in PyTorch <ref type="bibr" target="#b21">(Wolf et al. 2019</ref>) and adopt most of its hyperparameter settings. To speed up the training process and avoid over-fitting, we freeze all the parameters of BERT except the last transformer layer. The training batch size is 256, and the learning rate is 5e ?5 . We use the same dynamic thresholds as DAC <ref type="bibr" target="#b3">(Chang et al. 2017</ref>) and set u(?) = 0.95 ? ?, l(?) = 0.455 + 0.1 ? ?, and ? = 0.009.</p><p>During the refinement stage, we perform K-means on intent representation I to obtain the initial cluster centroids U and set the stop criteria ? label as 0.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>The results are shown in <ref type="table" target="#tab_4">Table 2</ref>. The proposed CDAC+ method outperforms other baselines by a significant mar-   gin in all datasets and evaluation metrics. It shows that our method effectively groups sentences based on the intent representations learned with pairwise classification and constraints, and even can generalize to new intents that we do not know in advance.</p><p>The performance of unsupervised methods is particularly poor on DBPedia and StackOverflow, which may be related to the number of intents and the difficulty of the dataset. Semi-supervised methods are not necessarily better than unsupervised methods. If the constraints are not used correctly, it can not only lead to overfitting but also fail to group new intents into clusters.</p><p>Among these baselines, BERT-KM performed the worst, even worse than running K-means on sentences encoded with Glove. Our results suggest that fine-tuning is necessary for BERT to perform downstream tasks. Next, we will discuss the robustness and effectiveness of the proposed method from different aspects.</p><p>Ablation study To investigate the contribution of constraints and cluster refinement, we compare CDAC+ with its variant methods, such as performing K-means clustering with representation learned by DAC (DAC-KM) or CDAC (CDAC-KM), CDAC+ without constraints (DAC+), and CDAC+ without cluster refinement (CDAC). The results are shown in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>Most methods have better performance when constraints are added. Compared with DAC+ on StackOverflow, CDAC+ can even increase clustering accuracy by up to 50%. It shows the effectiveness of constraints. For cluster refinement, DAC+ and CDAC+ consistently perform better than DAC-KM and CDAC-KM. DAC+ even outperforms other baselines on SNIPS and DBPedia. It implies that learning representation only through DAC or CDAC is not enough, and cluster refinement is necessary to get better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Number of Clusters</head><p>To study whether our method is sensitive to the number of clusters or not, we increase the number of predefined clusters from its ground truth number to four times of it. The results are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. As the number of clusters increases, the performance of almost all methods except CDAC+ drops dramatically. Besides, our method consistently performs better than CDAC-KM, which demonstrates the robustness of cluster refinement. In <ref type="figure" target="#fig_5">Figure 7</ref>, we use the confusion matrix to analyze the results further. It shows that our method not only maintains excellent performance but is also insensitive to the number of clusters.  The predefined number of clusters is twice of its ground truth. The values along the diagonal represent how many samples are correctly classified into the corresponding class. The larger the number, the deeper the color. We hide empty clusters for better visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Labeled Data</head><p>We vary the ratio of labeled data in training set in the range of 0.001, 0.01, 0.03, 0.05 and 0.1, and show the results in <ref type="figure" target="#fig_2">Figure 4</ref>. First, even if the ratio of labeled data is much lower than 0.1, CDAC+ still performs better than most baselines. Second, the performance changes the most on the StackOverflow dataset. The reason is that the taxonomy of it can be divided by technical subjects or question types (e.g., what, how, why). It requires the labeled data as prior knowledge to guide the clustering process. The unsupervised methods fail since there is no prior knowledge to guide the clustering process. Finally, the NMI score of BERT-Semi is slightly better than CDAC+ when the labeled ratio is 0.01 and 0.03 on StackOverflow. The reason is that BERT-Semi uses instancelevel constraints as prior knowledge. It can easily group known intents but fail to group the unknown intents into new clusters. We will discuss it in the next paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Unknown Classes</head><p>We vary the ratio of unknown classes in training set in the range of 0.25, 0.5 and 0.75, and show the results in <ref type="figure" target="#fig_3">Figure 5</ref>. The higher the ratio of unknown classes, the more new intent classes in the training set. Our method is still robust compared with baselines. In this case, the performance of BERT-Semi drops dramatically. The instance-level constraints they use will cause overfitting and will not be able to group new intents into clusters.</p><p>Performance on Imbalanced Dataset We follow previous works <ref type="bibr" target="#b3">(Chang et al. 2017</ref>) and randomly sample subsets of datasets with different minimum retention probability ?. Given a dataset with N-classes, samples of class 1 will be kept with probability ? and class N with probability 1. The lower the ?, the more imbalanced the dataset is. The results are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. Our method is not only robust to imbalanced classes but also outperform other baselines trained with balanced classes. The performance of other baselines drops around 3% to 10% under different ?.</p><p>Error Analysis We further analyze whether CDAC+ can discover new intents on the test set. In <ref type="figure" target="#fig_5">Figure 7</ref>, we set BookRestaurant and SearchCreativeWork as unknown in training set. Our method is still able to find out these intents.</p><p>Note that some samples of SearchCreativeWork are incorrectly assigned to cluster of SearchScrrenEvent since they are semantically similar.</p><p>In <ref type="figure" target="#fig_4">Figure 6</ref>, we use the t-SNE (Maaten and Hinton 2008) to visualize the intent representation. Compared with other methods, the representation learned by CDAC+ is compact within the class and separable between classes. It shows that our method does learn cluster-friendly representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, we propose an end-to-end clustering method that uses limited labeled data to guide the clustering process for discovering new intents and further refine the cluster results by forcing the model to learn from the high confidence assignments. Extensive experiments show that our method not only yields significant improvements compared with strong baselines but is also insensitive to the number of clusters. In the future, we will try to combine different kinds of prior knowledge to guide the clustering process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Influence of the number of clusters on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Influence of the labeled ratio on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Influence of the unknown class ratio on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of intent representation learned on StackOverflow dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Confusion matrix for the clustering results of CDAC+ on SNIPS datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Influence of imbalanced subset on StackOverflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of SNIPS, DBPedia, and StackOverflow dataset. # indicates the total number of sentences.</figDesc><table><row><cell>Dataset</cell><cell cols="6">#Classes (Known + Unknown) #Training #Validation #Test Vocabulary Length (max / mean)</cell></row><row><cell>SNIPS</cell><cell>7 (5 + 2)</cell><cell>13,084</cell><cell>700</cell><cell>700</cell><cell>11,971</cell><cell>35 / 9.03</cell></row><row><cell>DBPedia</cell><cell>14 (11 + 3)</cell><cell>12,600</cell><cell>700</cell><cell>700</cell><cell>45,077</cell><cell>54 / 29.97</cell></row><row><cell>StackOverflow</cell><cell>20 (15 + 5)</cell><cell>18,000</cell><cell>1,000</cell><cell>1,000</cell><cell>17,182</cell><cell>41 / 9.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The clustering results on three datasets. We evaluate both unsupervised and semi-supervised methods.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>SNIPS</cell><cell></cell><cell></cell><cell>DBPedia</cell><cell></cell><cell cols="3">StackOverflow</cell></row><row><cell></cell><cell>Method</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell></row><row><cell></cell><cell>DAC</cell><cell>79.97</cell><cell>69.17</cell><cell>76.29</cell><cell>75.37</cell><cell>56.30</cell><cell>63.96</cell><cell>14.71</cell><cell>2.76</cell><cell>16.30</cell></row><row><cell>Unsup.</cell><cell>DAC-KM</cell><cell>86.29</cell><cell>82.58</cell><cell>91.27</cell><cell>84.79</cell><cell>74.46</cell><cell>82.14</cell><cell>20.28</cell><cell>7.09</cell><cell>23.69</cell></row><row><cell></cell><cell>DAC+</cell><cell>86.90</cell><cell>83.15</cell><cell>91.41</cell><cell>86.03</cell><cell>75.99</cell><cell>82.88</cell><cell>20.26</cell><cell>7.10</cell><cell>23.69</cell></row><row><cell></cell><cell>CDAC</cell><cell>77.57</cell><cell>67.35</cell><cell>74.93</cell><cell>80.04</cell><cell>61.69</cell><cell>69.01</cell><cell>29.69</cell><cell>8.00</cell><cell>23.97</cell></row><row><cell>Semi-sup.</cell><cell>CDAC-KM</cell><cell>87.96</cell><cell>85.11</cell><cell>93.03</cell><cell>93.42</cell><cell>87.55</cell><cell>89.77</cell><cell>67.71</cell><cell>45.65</cell><cell>71.49</cell></row><row><cell></cell><cell>CDAC+</cell><cell>89.30</cell><cell>86.82</cell><cell>93.63</cell><cell>94.74</cell><cell>89.41</cell><cell>91.66</cell><cell>69.84</cell><cell>52.59</cell><cell>73.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The clustering results of CDAC+ and its variant methods.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active semisupervision for pairwise constrained clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 SIAM international conference on data mining</title>
		<meeting>the 2004 SIAM international conference on data mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="333" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrating constraints and metric learning in semi-supervised clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5879" to="5887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Agglomerative clustering using the concept of mutual nearest neighbourhood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clustering novel intents in a conversational interaction system with semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised clustering of questions into intents for dialog system applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Haponchyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2310" to="2321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to cluster in order to transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="195" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep unknown intent detection with margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5491" to="5496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA. Padmasundari, and Bangalore, S</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="606" to="610" />
		</imprint>
	</monogr>
	<note>Intent discovery through unsupervised semantic text clustering</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Train one get one free: Partially supervised neural network for bug report duplicate detection and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Poddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auto-dialabel: Labeling dialogue data with unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Constrained k-means clustering with background knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schr?dl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised clustering for short text via deep representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv abs/1910.03771</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Short text clustering via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="62" to="69" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01710</idno>
		<title level="m">Text understanding from scratch</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
