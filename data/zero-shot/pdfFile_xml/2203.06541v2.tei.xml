<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xia</surname></persName>
							<email>jiahao.xia@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Qu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Technology and Science</orgName>
								<address>
									<addrLine>3 CalmCar</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjian</forename><surname>Huang</surname></persName>
							<email>huangwj@sustech.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Technology and Science</orgName>
								<address>
									<addrLine>3 CalmCar</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
							<email>zhangjg@sustech.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Technology and Science</orgName>
								<address>
									<addrLine>3 CalmCar</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
							<email>xi.wang@calmcar.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
							<email>min.xu@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heatmap regression methods have dominated face alignment area in recent years while they ignore the inherent relation between different landmarks. In this paper, we propose a Sparse Local Patch Transformer (SLPT) for learning the inherent relation. The SLPT generates the representation of each single landmark from a local patch and aggregates them by an adaptive inherent relation based on the attention mechanism. The subpixel coordinate of each landmark is predicted independently based on the aggregated feature. Moreover, a coarse-to-fine framework is further introduced to incorporate with the SLPT, which enables the initial landmarks to gradually converge to the target facial landmarks using fine-grained features from dynamically resized local patches. Extensive experiments carried out on three popular benchmarks, including WFLW, 300W and COFW, demonstrate that the proposed method works at the state-of-the-art level with much less computational complexity by learning the inherent relation between facial landmarks. The code is available at the project website 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face alignment is aimed at locating a group of predefined facial landmarks from images. Robust face alignment based on deep learning technology has attracted increasing attention in recent years and it is the fundamental algorithm in many face-related applications such as face reenactment <ref type="bibr" target="#b40">[40]</ref>, face swapping <ref type="bibr" target="#b20">[21]</ref> and driver fatigue detection <ref type="bibr" target="#b0">[1]</ref>. Despite recent progress, it still remains a challenging problem, especially for images with heavy occlusion, profile view and illumination variation.</p><p>The inherent relation between facial landmarks play an important role in face alignment since human face has a regular structure. Although heatmap regression methods <ref type="figure">Figure 1</ref>. The proposed coarse-to-fine framework leverages the sparse local patches for robust face alignment. The sparse local patches are cropped according to the landmarks in the previous stage and fed into the same SLPT to predict the facial landmarks. Moreover, the patch size narrows down with the increasing of stages to enable the local features to evolve into a pyramidal form.</p><p>achieve impressive performance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">[33]</ref><ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref> in recent years, they still ignore the inherent relation because convolutional neural network (CNN) kernels focus locally, thus failed to capture the relations of landmarks farther away in a global manner. In particular, they consider the pixel coordinate with highest intensity of the output heatmap as the optimal landmark, which inevitably introduces a quantization error, especially for common downsampled heatmap. Coordinate regression methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b42">42]</ref> have an innate potential to learn the relation since it regresses the coordinates from global feature directly via fully-connected layers (FC). Nevertheless, a coherent relation should be learned together with local appearance while coordinate regression methods lose the local feature by projecting the global feature into FC layers.</p><p>To address the aforementioned problems, we propose a Sparse Local Patch Transformer (SLPT). Instead of predicting the coordinates from the full feature map like DETR <ref type="bibr" target="#b4">[5]</ref>, the SLPT firstly generates the representation for each landmark from a local patch. Then, a series of learnable queries, which are called landmark queries, are used to aggregate the representations. Based on the cross-attention mechanism of transformer, the SPLT learns an adaptive adjacency matrix in each layer. Finally, the subpixel coordinate of each landmark in their corresponding patch is predicted independently by a MLP. Due to the use of sparse local patches, the number of the input token decreases significantly compared to other vision transformer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>To further improve the performance, a coarse-to-fine framework is introduced to incorporate with the SLPT, as shown in <ref type="figure">Fig.1</ref>. Similar to cascaded shape regression method <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">44]</ref>, the proposed framework optimizes a group of initial landmarks to the target landmarks by several stages. The local patches in each stage are cropped based on the initial landmarks or the landmarks predicted in the former stage, and the patch size for a specific stage is 1/2 of its former stage. As a result, the local patches evolve in a pyramidal form and get closer to the target landmarks for the fine-grained local feature.</p><p>To verify the effectiveness of the SLPT and the proposed framework, we carry out experiments on three popular benchmarks, WFLW <ref type="bibr" target="#b36">[36]</ref>, 300W <ref type="bibr" target="#b27">[28]</ref> and COFW <ref type="bibr" target="#b3">[4]</ref>. The results show the proposed method significantly outperforms other state-of-the-art methods in terms of diverse metrics with much lower computational complexity. Moreover, we also visualize the attention map of SLPT and the inner product matrix of landmark queries to demonstrate the SLPT can learn the inherent relation of facial landmarks.</p><p>The main contributions of this work can be summarized as:</p><p>? We introduce a novel transformer, Sparse Local Patch Transformer, to explore the inherent relation between facial landmarks based on the attention mechanism. The adaptive inherent relation learned by SLPT enables the model to achieve SOTA performance with much less computational complexity.</p><p>? We introduce a coarse-to-fine framework to incorporate with the SLPT, which enables the local patch to evolve in a pyramidal form and get closer to the target landmark for the fine-grained feature.</p><p>? Extensive experiments are conducted on three popular benchmarks, WFLW, 300W and COFW. The result illustrates the proposed method learns the inherent relation of facial landmarks by the attention mechanism and works at the SOTA level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the early stage of face alignment, the mainstream methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44]</ref> regress facial landmarks directly from the local feature with classical machine learning algorithms like random forest. With the development of CNN, the CNN-based face alignment methods have achieved impressive performance. They can be roughly divided into two categories: heatmap regression method and coordinate regression method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Coordinate Regression Method</head><p>Coordinate regression methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref> regress the coordinates of landmarks from feature map directly via FC layers. To further improve the robustness, diverse cascaded networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref> and recurrent networks <ref type="bibr" target="#b38">[38]</ref> are proposed to achieve face alignment with multi stages. Despite coordinate regression methods have an innate potential to learn the inherent relation, it commonly requires a huge number of samples for training. To address the problem, Qian et al. <ref type="bibr" target="#b25">[26]</ref> and Dong et al. <ref type="bibr" target="#b8">[9]</ref> expand the number of training samples by style transfer; Browatzki et al. <ref type="bibr" target="#b2">[3]</ref> and Dong et al. <ref type="bibr" target="#b9">[10]</ref> leverage the unlabeled dataset to train the model. In recent years, state-of-the-art works employ the structure information of face as the prior knowledge for better performance. Lin et al. <ref type="bibr" target="#b23">[24]</ref> and Li et al. <ref type="bibr" target="#b21">[22]</ref> model the interaction between landmarks by a graph convolutional network (GCN). However, the adjacency matrix of GCN is fixed during inference and cannot adjust case by case. Learning an adaptive inherent relation is crucial for robust face alignment. Unfortunately, there is no work yet on this topic, and we propose a method to fill this gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Heatmap Regression Method</head><p>Heatmap regression methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">34]</ref> output an intermediate heatmap for each landmark and consider the pixel with highest intensity as the optimal output. Therefore, it leads to quantization errors since the heatmap is commonly much smaller than the input image. To eliminate the error, Kumar et al. <ref type="bibr" target="#b17">[18]</ref> estimate the uncertainty of predicted landmark locations; Lan et al <ref type="bibr" target="#b18">[19]</ref> adopt an additional decimal heatmap for subpixel estimation; Huang et al. <ref type="bibr" target="#b14">[15]</ref> further regress the coordinate from an anisotropic attention mask generated from heatmaps. Moreover, heatmap regression methods also ignore the relation between landmarks. To construct the relation between neighboring points, Wu et al. <ref type="bibr" target="#b36">[36]</ref> and Wang et al. <ref type="bibr" target="#b35">[35]</ref> take advantage of facial boundaries as the prior knowledge; Zou et al. <ref type="bibr" target="#b47">[47]</ref> cluster landmarks with a graph model to provide structural constraints. However, they still cannot explicitly model an inherent relation between the landmarks with long distance.</p><p>The vision transformer <ref type="bibr" target="#b10">[11]</ref> proposed recently enables the model to attend the area with a long distance. Besides, the attention mechanism in transformer can generate an adaptive global attention for different tasks, such as object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">46]</ref> and human pose estimation <ref type="bibr" target="#b22">[23]</ref>, and in principle, we envision that it can also learn an adaptive inherent relation for face alignment. In this paper, we demonstrate the capability of SLPT for learning the relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sparse Local Patch Transformer</head><p>As shown in <ref type="figure" target="#fig_0">Fig.2</ref>, Sparse Local Patch Transformer (SLPT) consists of three parts, the patch embedding &amp; structure encoding, inherent relation layers and prediction heads.</p><p>Patch embedding &amp; structure encoding: ViT <ref type="bibr" target="#b10">[11]</ref> divides an image or a feature map I ? R H I ?W I ?C into a grid of H I P h ? W I Pw with each patch of size P h ?P w and maps it into a d-dimension vector as the input. Different from ViT, for each landmark, the SLPT crops a local patch with the fixed size (P h , P w ) from the feature map as its supporting patch, whose center is located at the landmark. Then, the patches are resized to K ? K by linear interpolation and mapped into a series of vectors by a CNN layer. Hence, each vector can be viewed as the representation of the corresponding landmark. Besides, to retain the relative position of landmarks in a regular face shape (structure information), we supplement the representations with a series of learnable parameters called structure encoding. As shown in <ref type="figure" target="#fig_1">Fig.3</ref>, the SLPT learns to encode the distance between landmarks within the regular facial structure in the similarity of encod-ings. Each encoding has high similarity with the encoding of neighboring landmark (eg. left eye and right eye).</p><p>Inherent relation layer: Inspired by Transformer <ref type="bibr" target="#b32">[32]</ref>, we propose inherent relation layers to model the relation between landmarks. Each layer consists of three blocks, multi-head self-attention (MSA) block, multi-head crossattention (MCA) block, and multilayer perceptron (MLP) block, and an additional Layernorm (LN) is applied before every block. Based on the self-attention mechanism in MSA block, the information of queries interact adaptively for learning a query ? query inherent relation. Supposing the l-th MSA block obtains H heads, the input T l and landmark queries Q with C I -dimension are divided into H sequences equally (T l is a zero matrix in 1st layer). The self-attention weight of the h-th head A h is calculated by: </p><formula xml:id="formula_0">A h = sof tmax T l h + Q h W q h T l h + Q h W k h T ? C h ,<label>(1)</label></formula><formula xml:id="formula_1">M SA T l = A 1 T l 1 W v 1 ; ...; A H T l H W v H W P ,<label>(2)</label></formula><p>where W v h ? R C h ?C h and W P ? R C I ?C I are also the learnable parameters of linear layers. The MCA block aggregates the representations of facial landmarks based on the cross-attention mechanism for learning an adaptive representation ? query relation. As shown in the rightmost images of <ref type="figure" target="#fig_0">Fig.2</ref>, by taking advantage of the cross attention, each landmark can employ neighboring landmarks for coherent prediction and the occluded landmark can be predicted according to the representations of visible landmarks. Similar to MSA, MCA also has H heads and the attention weight in the h-th head A h can be calculated by:</p><formula xml:id="formula_2">A h = sof tmax T l h + Q h W q h (R h + P h ) W k h T ? C h . (3) Where W q h and W k h ? R C h ?C h are learnable parameters of two linear layers in the h-th head. T l h ? R N ?C h is the input l-th MCA block; P h ? R N ?C h is the structure encod- ings; R h ? R N ?C h is the landmark representations.</formula><p>MCA block can be formulated as:</p><formula xml:id="formula_3">M CA T l = A 1 T l 1 W v 1 ; ...; A H T l H W v H W P ,<label>(4)</label></formula><p>where W v h ? R C h ?C h and W P ? R C I ?C I are also the learnable parameters of linear layers in MCA block.</p><p>Supposing predicting N pre-defined landmarks, the computational complexity of the MCA that employ sparse local patches ?(S) and full feature map ?(F ) is: for i ? 1 to N stage do 5:</p><formula xml:id="formula_4">?(S) = 4HN C 2 h + 2HN 2 C h ,<label>(5)</label></formula><formula xml:id="formula_5">?(F ) = 2N + 2 W I H I P w P h HC 2 h + 2N H W I H I P w P h C h . (6)</formula><p>Crop local pactes P from F according to former landmarks S i?1 ;</p><p>6:</p><p>Resize patches from (P w , P h ) to K ? K; <ref type="bibr">7:</ref> Forward T for landmarks by S i = T (P ); <ref type="bibr">8:</ref> Reduce the patch size (P w , P h ) by half; <ref type="bibr">9:</ref> end for 10:</p><p>Minimize L S gt , S 1 , S 2 , ? ? ? , S Nstage 11: end while Compared to using the full feature map, the number of representations decreases from <ref type="bibr" target="#b4">[5]</ref>), which decreases the computational complexity significantly. For a 29 landmark dataset <ref type="bibr" target="#b3">[4]</ref>, ?(S) is only 1/5 of ?(F ) (H = 8 and C h = 32 in the experiment).</p><formula xml:id="formula_6">H I P h ? W I Pw to N (with the same input size, H I P h ? W I Pw is 16 ? 16 in the related frame- work</formula><p>Prediction head: the prediction head consists of a layernorm to normalize the input and a MLP layer to predict the result. The output of the inherent relation layer is the local position of the landmark with respect to its supporting patch. Based on the local position on the i-th patch t i x , t i y , the global coordinate of the i-th landmark x i , y i can be calculated by:</p><formula xml:id="formula_7">x i = x i lt + w i t i x , y i = y i lt + h i t i y ,<label>(7)</label></formula><p>where (w i , h i ) is the size of the supporting patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Coarse-to-fine locating</head><p>To further improve the performance and robustness of SLPT, we introduce a coarse-to-fine framework trained in an end-to-end method to incorporate with the SLPT. The pseudo-code in Algorithm 1 shows the training pipeline of the framework. It enables a group of initial facial landmarks S 0 calculated from the mean face in the training set to converge to the target facial landmarks gradually with several stages. Each stage takes the previous landmarks as center to crop a series of patches. Then, the patches are resized into a fixed size K ? K and fed into the SLPT to predict the local point on the supporting patches. Large patch size in the initial stage enables the SLPT to obtain a large receptive filed that prevents the patch from deviating from the target landmark. Then, the patch size in the following stages is its former stage, which enables the local patches to extract fine-grained features and evolve into a pyramidal form. By taking advantage of the pyramidal form, we can observe a significant improvement for SLPT. (see Section 4.5).</p><formula xml:id="formula_8">1/2 of Method NME(%)? FR 0.1 (%)? AUC 0.1 ? LAB [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>We employ the normalized L2 loss to provide the supervision for stages of the coarse-to-fine framework. Moreover, similar to other works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>, providing additional supervision for the intermediate output during the training is also helpful. Therefore, we feed the intermediate output of each inherent relation layer into a shared prediction head. The loss function is written as:</p><formula xml:id="formula_9">L = 1 SDN S i=1 D j=1 N k=1 x k gt , y k gt ? x ijk , y ijk 2 d ,<label>(8)</label></formula><p>where S and D indicate the number of coarse-to-fine stage and inherent relation layer respectively. x k gt , y k gt is the labeled coordinate of the k-th point. x ijk , y ijk is the coordinate of k-th point predicted by j-th inherent relation layer in i-th stage. d is the distance between outer eye corners that acts as a normalization factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Experiments are conducted on three popular benchmarks, including WFLW <ref type="bibr" target="#b36">[36]</ref>, 300W <ref type="bibr" target="#b27">[28]</ref> and COFW <ref type="bibr" target="#b3">[4]</ref>.</p><p>WFLW dataset is a very challenging dataset that consists of 10,000 images, 7,500 for training and 2,500 for testing. It provides 98 manually annotated landmarks and rich</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Inter-Ocular NME (%) ? Common Challenging Fullset SAN <ref type="bibr" target="#b8">[9]</ref> 3.34 6.60 3.98 Coord <ref type="bibr" target="#b34">[34]</ref> 3 attribute labels, such as profile face, heavy occlusion, makeup and illumination. 300W is the most commonly used dataset that includes 3,148 images for training and 689 images for testing. The training set consists of the fullset of AFW <ref type="bibr" target="#b45">[45]</ref>, the training subset of HELEN <ref type="bibr" target="#b19">[20]</ref> and LFPW <ref type="bibr" target="#b1">[2]</ref>. The test set is further divided into a challenging subset that includes 135 images (IBUG fullset <ref type="bibr" target="#b27">[28]</ref>) and a common subset that consists of 554 images (test subset of HELEN and LFPW). Each image in 300W is annotated with 68 facial landmarks.</p><p>COFW mainly consists of the samples with heavy occlusion and profile face. The training set includes 1,345 images and each image is provided with 29 annotated landmarks. The test set has two variants. One variant presents 29 landmarks annotation per face image (COFW), The other is provided with 68 annotated landmarks per face image (COFW68 <ref type="bibr" target="#b13">[14]</ref>). Both contains 507 images. We employ the COFW68 set for cross-dataset validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>Referring to other related work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">35]</ref>, we evaluate the proposed methods with standard metrics, Normalized Mean Error (NME), Failure Rate (FR) and Area Under Curve (AUC). NME is defined as:</p><formula xml:id="formula_10">N M E (S, S gt ) = 1 N N i=1 p i ? p i gt 2 d ? 100%,<label>(9)</label></formula><p>where S and S gt denote the predicted and annotated coordinates of landmarks respectively. p i and p i gt indicate the coordinate of i-th landmark in S and S gt . N is the number of landmarks, d is the reference distance to normalize the error. d could be the distance between outer eye corners (interocular) or the distance between pupil centers (inter-pupils).</p><p>FR indicates the percentage of images in the test set whose NME is higher than a certain threshold. AUC is calculated based on Cumulative Error Distribution (CED) curve. It indicates the fraction of test images whose NME(%) is less or equal to the value on the horizontal axis. AUC is the area under CED curve, from zero to the threshold for FR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Each input image is cropped and resized to 256 ? 256. We train the proposed framework with Adam <ref type="bibr" target="#b7">[8]</ref>, setting the initial learning rate to 1 ? 10 ?3 . Without specifications, the size of the resized patch is set to 7 ? 7 and the framework has 6 inherent relation layers and 3 coarse-to-fine stages. Besides, we augment the training set with random horizontal flipping (50%), gray (20%), occlusion (33%), scaling (?5%), rotation (?30 ? ), translation (?10px). We implement our method with two different backbone: a light HRNetW18C <ref type="bibr" target="#b34">[34]</ref> (the modularized block number in each stage is set to 1) and Resnet34 <ref type="bibr" target="#b15">[16]</ref>. For the HRNetW18Clite, the resolution of feature map is 64 ? 64, and for the Resnet34, we extract representations from the output feature maps of stages C2 through C5. (see Appendix A.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Inter-Ocular Inter-Pupil NME(%)?FR(%)? NME(%)?FR(%)? DAC-CSR <ref type="bibr" target="#b12">[13]</ref> 6.03 4.73 --LAB <ref type="bibr" target="#b36">[36]</ref> 3.92 0.39 --Coord <ref type="bibr" target="#b34">[34]</ref> 3.73 0.39 --SDFL <ref type="bibr" target="#b23">[24]</ref> 3.63 0.00 --Heatmap <ref type="bibr" target="#b34">[34]</ref> 3   <ref type="table">Table 1</ref> (more detailed results on the subset of WFLW are in Appendix A.2), SLPT demonstrates impressive performance. With the increasing of inherent layers, the performance of SLPT can be further improved and outperforms the ADNet (see Appendix A.5). Referring to DETR, we also implement a Transformer based method that employs the full feature map for face alignment. The number of the input tokens is 16 ? 16. With the same backbone (HRNetW18C-lite), we observe an improvement of 12.10% in NME, and the number of training epoch is 8? less than the DETR (see Appendix A.3). Moreover, the SLPT also outperforms the coordinate regreesion and heatmap regression methods significantly. Some qualitative results are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. It is evident that our method could localize the landmarks accurately, in partic-  <ref type="table">Table 5</ref>. Performance comparison of the SLPT with different number of coarse-to-fine stages on WFLW. The normalization factor for NME is inter-ocular and the threshold for FR and AUC is set to 0. ular for face images with blur (2nd row in <ref type="figure" target="#fig_2">Fig.4</ref>), profile view (1st row in <ref type="figure" target="#fig_2">Fig.4</ref>) and heavy occlusion (3rd and 4th row in <ref type="figure" target="#fig_2">Fig.4)</ref>. 300W: the comparison result is shown in <ref type="table">Table 2</ref>. Compared to the coordinate and heatmap regression methods (HRNetW18C <ref type="bibr" target="#b34">[34]</ref>), SLPT still achieves an impressive improvement of 9.69% and 4.52% respectively in NME on the fullset. However, the improvement on 300W is not as significant as WFLW since learning an adaptive inherent relation requires a large number of annotated samples. With limited training samples, the methods with prior knowledge, such as facial boundaries (Awing and ADNet) and affined mean shape (SDL), always achieve better performance.</p><p>COFW: We conduct two experiments on COFW for comparsion, the within-dataset validation and cross-dataset validation. For the within-dataset validation, the model is trained with 1,345 images and validated with 507 images on COFW. The inter-ocular and inter-pupil NME of SLPT and the state-of-the-art methods are reported in <ref type="table" target="#tab_5">Table 3</ref> respectively. In this experiment, the number of training sample is quite small, which leads to the significant degradation of the coordinate regression methods, such as SDFL, LAB. Nevertheless, SLPT still maintains excellent performance and yields the second best performance. It improves the metric by 3.77% and 11.00% in NME over the heatmap regression and coordinate regression methods respectively.</p><p>For the cross-dataset validation, the training set includes the complete 300W dataset (3,837 images) and the test set is COFW68 (507 images with 68 landmark annotation). Most samples of COFW68 are under heavy occlusion. The interocular NME and FR of SLPT and the state-of-the-art methods are reported in <ref type="table">Table 4</ref>. Compared to the methods based on GCN (SDL and SDFL), the SLPT (HRNet) achieves impressive result, as low as 4.10% in NME. The result illustrates that the adaptive inherent relation of SLPT works better than the fixed adjacency matrix of GCN for robust face alignment, especially for the condition of heavy occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>Evaluation on different coarse-to-fine stages: to explore the contribution of the coarse-to-fine framework, we train the SLPT with different number of coarse-to-fine stages on the WFLW dataset. The NME, AUC 0.1 and FR 0.1 of each intermediate stage and the final stage are shown in <ref type="table">Table 5</ref>. Compared to the model with only one stage, the local patches in multi-stages model evolve into a pyramidal form, which improves the performance of intermediate stages and final stage significantly. When the stage increases from 1 to 3, the NME of the first stage decreases dramatically from 4.79% to 4.38%. When the number of stages is more than 3, the performance converges and additional stages cannot bring any improvement to the model.</p><p>Evaluation on MSA and MCA block: To explore the influence of query-query inter relation (eq.1) and representation-query inter relation (eq.3) created by MSA and MCA blocks, we implement four different models with/without MSA and MCA, ranging from 1 to 4. For the models without MCA block, we utilize the landmark representations as the queries input. The performance of the four models are tabulated in <ref type="table">Table 6</ref>. Without MSA and MCA, each landmark is regressed merely based on the feature of the supporting patches in model 1. Nevertheless, it still outperforms other coordinate regression methods because of the coarse-to-fine framework. When selfattention or cross-attention is introduced into the model, the performance is boosted significantly, reaching at 4.20% and 4.17% respectively in terms of NME. Moreover, the self- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>FLOPs(G) Params(M) HRNet <ref type="bibr" target="#b34">[34]</ref> 4.75 9.66 LAB <ref type="bibr" target="#b36">[36]</ref> 18.85 12.29 AVS + SAN <ref type="bibr" target="#b25">[26]</ref> 33.87 35.02 AWing <ref type="bibr" target="#b35">[35]</ref> 26.8 24.15 DETR ? (98 landmarks) <ref type="bibr" target="#b4">[5]</ref> 4.26 11.00 DETR ? (68 landmarks) <ref type="bibr" target="#b4">[5]</ref> 4.06 11.00 DETR ? (29 landmarks) <ref type="bibr" target="#b4">[5]</ref> 3 attention and cross-attention can be combined to improve the performance of model further.</p><p>Evaluation on structure encoding: we implement two models with/without structure encoding to explore the influence of structural information. With structural information, the performance of SLPT is improved, as shown in <ref type="table">Table 7</ref>.</p><p>Evaluation on computational complexity: the computational complexity and parameters of SLPT and other SOTA methods are shown in <ref type="table">Table 8</ref>. The computational complexity of SLPT is only 1/8 to 1/5 FLOPs of the previous SOTA methods (AVS and AWing), demonstrating that learning inherent relation is more efficient than other methods. Although SLPT runs three times for coarse-to-fine localization, patch embedding and linear interpolation procedures, we do not observe a significant increasing of computational complexity, especially for 29 landmarks, because the sparse local patches lead to less tokens.</p><p>Besides, the influence of patch size and inherent layer number are shown in the Appendix A.4 and A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visualization</head><p>We calculate the mean attention weight of each MCA and MSA block on the WFLW test set, as shown in <ref type="figure" target="#fig_3">Fig.5</ref>. We find out that the MCA block tends to aggregate the representation of the supporting and neighboring patches to generate the local feature, while MSA block tends to pay attention to the landmarks with a long distance to create the global feature. That is why the MCA block can incorporate with the MSA block for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we find out that the inherent relation between landmarks is significant to the performance of face alignment while it is ignored by the most state-of-the-art methods. To address the problem, we propose a sparse local patch transformer for learning a query-query and a representation-query relation. Moreover, a coarse-to-fine framework that enables the local patches to evolve into pyramidal former is proposed to further improve the performance of SLPT. With the adaptive inherent relation learned by SLPT, our method achieves robust face alignment, especially for the faces with blur, heavy occlusion and profile view, and outperforms the state-of-the-art methods significantly with much less computational complexity. Ablation studies verify the effectiveness of the proposed method. In future work, the inherent relation learning will be studied further and extended to other tasks.  <ref type="figure">Figure 6</ref>. Constructing multi-level feature maps for SLPT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Details of comparison on WFLW</head><p>The comparison results on WFLW test set and its subsets are tabulated in <ref type="table" target="#tab_10">Table 9</ref>. SLPT yields the best performance in NME and works at SOTA level on all subsets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Convergence curves of SLPT and DETR</head><p>The convergence curves of SLPT and DETR is shown in <ref type="figure">Fig.7</ref>. The DETR achieves 4.71% NME at 391 epochs on WFLW test set. The SLPT achieves better performance with around 8? less training epochs. With the increasing of training epochs, the performance of SLPT is improved further, achieving 4.14% NME at 140 epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Evaluation on the input patch size</head><p>Each local patch is resized to K ? K and then projected into a vector by a CNN layer with K ? K kernel size. In this section, we explore the influence of the patch size on WFLW test set, as tabulated in <ref type="table" target="#tab_12">Table 10</ref>. Compared to 7 ? 7 patches, the 5 ? 5 patches lose more information because of the lower resolution, which leads to degradation of the performance. When the patch size is extended from 7 ? 7 to 9 ? 9, the parameters of the CNN layer is doubled, which leads to the overfitting on the training set. Therefore, we can also observe a slight degradation with 9 ? 9 patch size, from 4.14% to 4.16% in NME.</p><p>Patch size NME(%) FR 0.1 (%) AUC A.5 Evaluation on the number of inherent relation layers <ref type="table">Table 11</ref> demonstrates the influence of inherent relation layer number. The performance of SLPT relies on the inherent relation layer heavily. When the number of inherent relation layers increases from 2 to 12, We can observe a significant improvement, from 4.19% to 4.12% in NME. Nevertheless, too many inherent relation layers also increase the parameters and computational complexity dramatically. Considering the real-time capability, we choose the model with 6 inherent relation layers as the optimal model. Layer number NME(%) FR 0.1 (%) AUC </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Further example predicted results and inherent relation maps</head><p>We visualize the predicted results and adaptive inherent relation maps for the samples of COFW, 300W and WFLW, as shown in <ref type="figure">Fig.8, Fig.9</ref> and <ref type="figure">Fig.10</ref> respectively. In the inherent relation maps, we connect each point to the point with highest cross-attention weight. The SLPT tends to utilize the visible landmarks to localize the landmarks with heavy occlusion for robust face alignment. For other landmark, it relies more on its neighboring landmark. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>An overview of the SLPT. The SLPT crops local patches from the feature map according to the facial landmarks in the previous stage. Each patch is then embedded into a vector that can be viewed as the representation of the corresponding landmark. Subsequently, they are supplemented with the structure encoding to obtain the relative position in a regular face. A fixed number of landmark queries are then input into the decoder, attending the vectors to learn the inherent relation between landmarks. Finally, the outputs are fed into a shared MLP to estimate the position of each facial landmark independently. The rightmost images demonstrate the adaptive inherent relation of different samples. We connect each point to the point with highest cross-attention weight in the first inherent relation layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Cosine similarity for structure encodings of SLPT learned from a dataset with 98 landmark annotations. High cosine similarities are observed for the corresponding points which are close in the regular face structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of the ground truth and face alignment result of SLPT, heatmap regression (HRNetW18C) and coordinate regression (HRNetW18C) method on the faces with blur, heavy occlusion and profile face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The statistical attention interactions of MCA and MSA in the final stage on the WFLW test set. Each row indicates the attention weight of the landmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .Figure 9 .Figure 10 .</head><label>8910</label><figDesc>Further example predicted results and attention maps on COFW (random selection) Further example predicted results and attention maps on 300W (random selection) Further example predicted results and attention maps on WFLW (random selection)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where W q h and W k h ? R C h ?C h are the learnable parameters of two linear layers. T l h ? R N ?C h and Q h ? R N ?C h are the input and landmark queries respectively of the h-th head with the dimension C h = C I /H. Then, MSA block can be formulated as:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Training pipeline of the coarse-to-fine framework Require: Training image I, initial landmarks S 0 , backbone network B, SLPT T , loss function L, ground truth S gt , Stage number N stage 1: while the training epoch is less than a specific number do</figDesc><table><row><cell>2:</cell><cell>Forward B for feature map by F = B (I);</cell></row><row><cell>3:</cell><cell>Initialize the local patch size (P w , P h ) ? W 4 , H 4</cell></row><row><cell>4:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Inter-Pupil NME(%)?</cell><cell>FR 0.1 (%)?</cell></row><row><cell>TCDCN [42]</cell><cell>7.66</cell><cell>16.17</cell></row><row><cell>CFSS [44]</cell><cell>6.28</cell><cell>9.07</cell></row><row><cell>ODN [43]</cell><cell>5.30</cell><cell>-</cell></row><row><cell>AVS+SAN [26]</cell><cell>4.43</cell><cell>2.82</cell></row><row><cell>LAB [36]</cell><cell>4.62</cell><cell>2.17</cell></row><row><cell>SDL [22]</cell><cell>4.22</cell><cell>0.39</cell></row><row><cell>SDFL [24]</cell><cell>4.18</cell><cell>0.00</cell></row><row><cell>SLPT  ?</cell><cell>4.11</cell><cell>0.59</cell></row><row><cell>SLPT  ?</cell><cell>4.10</cell><cell>0.59</cell></row><row><cell cols="3">Table 4. Inter-ocular NME and FR0.1 comparisons on 300W-</cell></row><row><cell cols="3">COFW68 cross-dataset evaluation. Key: [Best, Second Best,</cell></row><row><cell cols="3">=HRNetW18C,  ? =HRNetW18C-lite,  ? =ResNet34]</cell></row><row><cell cols="3">4.4. Comparison with State-of-the-Art Method</cell></row></table><note>NME and FR0.1 comparisons under Inter-Ocular nor- malization and Inter-Pupil normalization on within-dataset vali- dation. The threshold for failure rate (FR) is set to 0.1. Key: [Best, Second Best, =HRNetW18C, ? =HRNetW18C-lite, ?=ResNet34]WFLW: as tabulated in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>N?P W5 ?P H5 ?C I /4 N?P W4 ?P H4 ?C I /4 N?P W3 ?P H3 ?C I /4 N?P W2 ?P H2 ?C I /4</figDesc><table><row><cell>1?1 Conv</cell><cell>H 5 ?W 5 ?C I /4</cell><cell>Patches Crop Local</cell><cell>Resize</cell><cell>N?K?K?C I /4</cell><cell></cell></row><row><cell>1?1 Conv</cell><cell>H 4 ?W 4 ?C I /4</cell><cell>Crop Local Patches</cell><cell>Resize</cell><cell>N?K?K?C I /4</cell><cell>Concatenate</cell></row><row><cell>3 rd stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>C</cell></row><row><cell>1?1 Conv</cell><cell>H 3 ?W 3 ?C I /4</cell><cell>Patches Crop Local</cell><cell>Resize</cell><cell>N?K?K?C I /4</cell><cell></cell></row><row><cell>2 nd stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1?1 Conv</cell><cell>H 2 ?W 2 ?C I /4</cell><cell>Patches Crop Local</cell><cell>Resize</cell><cell>N?K?K?C I /4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Performance comparison of the SLPT and the state-of-the-art methods on WFLW and its subsets. The normalization factor is inter-ocular and the threshold for FR is set to 0.1. Key: [Best, Second Best, =HRNetW18C, ? =HRNetW18C-lite, ? =ResNet34]</figDesc><table><row><cell>Metric</cell><cell>Method</cell><cell>Testset</cell><cell>Pose</cell><cell cols="4">Expression Illumination Make-up Occlusion</cell><cell>Blur</cell></row><row><cell></cell><cell>LAB [36]</cell><cell>5.27</cell><cell>10.24</cell><cell>5.51</cell><cell>5.23</cell><cell>5.15</cell><cell>6.79</cell><cell>6.32</cell></row><row><cell></cell><cell>SAN [9]</cell><cell>5.22</cell><cell>10.39</cell><cell>5.71</cell><cell>5.19</cell><cell>5.49</cell><cell>6.83</cell><cell>5.80</cell></row><row><cell></cell><cell>Coord [34]</cell><cell>4.76</cell><cell>8.48</cell><cell>4.98</cell><cell>4.65</cell><cell>4.84</cell><cell>5.83</cell><cell>5.49</cell></row><row><cell></cell><cell>DETR  ? [5]</cell><cell>4.71</cell><cell>7.91</cell><cell>4.99</cell><cell>4.60</cell><cell>4.52</cell><cell>5.73</cell><cell>5.33</cell></row><row><cell></cell><cell>Heatmap [34]</cell><cell>4.60</cell><cell>7.94</cell><cell>4.85</cell><cell>4.55</cell><cell>4.29</cell><cell>5.44</cell><cell>5.42</cell></row><row><cell></cell><cell>AVS + SAN [26]</cell><cell>4.39</cell><cell>8.42</cell><cell>4.68</cell><cell>4.24</cell><cell>4.37</cell><cell>5.60</cell><cell>4.86</cell></row><row><cell>NME(%)?</cell><cell>LUVLi [18] AWing [45]</cell><cell>4.37 4.36</cell><cell>7.56 7.38</cell><cell>4.77 4.58</cell><cell>4.30 4.32</cell><cell>4.33 4.27</cell><cell>5.29 5.19</cell><cell>4.94 4.96</cell></row><row><cell></cell><cell>SDFL [24]</cell><cell>4.35</cell><cell>7.42</cell><cell>4.63</cell><cell>4.29</cell><cell>4.22</cell><cell>5.19</cell><cell>5.08</cell></row><row><cell></cell><cell>SDL [22]</cell><cell>4.21</cell><cell>7.36</cell><cell>4.49</cell><cell>4.12</cell><cell>4.05</cell><cell>4.98</cell><cell>4.82</cell></row><row><cell></cell><cell>HIH [19]</cell><cell>4.18</cell><cell>7.20</cell><cell>4.19</cell><cell>4.45</cell><cell>3.97</cell><cell>5.00</cell><cell>4.81</cell></row><row><cell></cell><cell>ADNet [15]</cell><cell>4.14</cell><cell>6.96</cell><cell>4.38</cell><cell>4.09</cell><cell>4.05</cell><cell>5.06</cell><cell>4.79</cell></row><row><cell></cell><cell>SLPT  ?</cell><cell>4.20</cell><cell>7.18</cell><cell>4.52</cell><cell>4.07</cell><cell>4.17</cell><cell>5.01</cell><cell>4.85</cell></row><row><cell></cell><cell>SLPT  ?</cell><cell>4.14</cell><cell>6.96</cell><cell>4.45</cell><cell>4.05</cell><cell>4.00</cell><cell>5.06</cell><cell>4.79</cell></row><row><cell></cell><cell>LAB</cell><cell>7.56</cell><cell>28.83</cell><cell>6.37</cell><cell>6.73</cell><cell>7.77</cell><cell>13.72</cell><cell>10.74</cell></row><row><cell></cell><cell>SAN</cell><cell>6.32</cell><cell>27.91</cell><cell>7.01</cell><cell>4.87</cell><cell>6.31</cell><cell>11.28</cell><cell>6.60</cell></row><row><cell></cell><cell>Coord</cell><cell>5.04</cell><cell>23.31</cell><cell>4.14</cell><cell>3.87</cell><cell>5.83</cell><cell>9.78</cell><cell>7.37</cell></row><row><cell></cell><cell>DETR  ?</cell><cell>5.00</cell><cell>21.16</cell><cell>5.73</cell><cell>4.44</cell><cell>4.85</cell><cell>9.78</cell><cell>6.08</cell></row><row><cell></cell><cell>Heatmap</cell><cell>4.64</cell><cell>23.01</cell><cell>3.50</cell><cell>4.72</cell><cell>2.43</cell><cell>8.29</cell><cell>6.34</cell></row><row><cell></cell><cell>AVS + SAN</cell><cell>4.08</cell><cell>18.10</cell><cell>4.46</cell><cell>2.72</cell><cell>4.37</cell><cell>7.74</cell><cell>4.40</cell></row><row><cell>FR 0.1 (%)?</cell><cell>LUVLi AWing</cell><cell>3.12 2.84</cell><cell>15.95 13.50</cell><cell>3.18 2.23</cell><cell>2.15 2.58</cell><cell>3.40 2.91</cell><cell>6.39 5.98</cell><cell>3.23 3.75</cell></row><row><cell></cell><cell>SDFL</cell><cell>2.72</cell><cell>12.88</cell><cell>1.59</cell><cell>2.58</cell><cell>2.43</cell><cell>5.71</cell><cell>3.62</cell></row><row><cell></cell><cell>SDL</cell><cell>3.04</cell><cell>15.95</cell><cell>2.86</cell><cell>2.72</cell><cell>1.45</cell><cell>5.29</cell><cell>4.01</cell></row><row><cell></cell><cell>HIH</cell><cell>2.96</cell><cell>15.03</cell><cell>1.59</cell><cell>2.58</cell><cell>1.46</cell><cell>6.11</cell><cell>3.49</cell></row><row><cell></cell><cell>ADNet</cell><cell>2.72</cell><cell>12.72</cell><cell>2.15</cell><cell>2.44</cell><cell>1.94</cell><cell>5.79</cell><cell>3.54</cell></row><row><cell></cell><cell>SLPT  ?</cell><cell>3.04</cell><cell>15.95</cell><cell>2.86</cell><cell>1.86</cell><cell>3.40</cell><cell>6.25</cell><cell>4.01</cell></row><row><cell></cell><cell>SLPT  ?</cell><cell>2.76</cell><cell>12.27</cell><cell>2.23</cell><cell>1.86</cell><cell>3.40</cell><cell>5.98</cell><cell>3.88</cell></row><row><cell></cell><cell>LAB</cell><cell>0.532</cell><cell>0.235</cell><cell>0.495</cell><cell>0.543</cell><cell>0.539</cell><cell>0.449</cell><cell>0.463</cell></row><row><cell></cell><cell>SAN</cell><cell>0.536</cell><cell>0.236</cell><cell>0.462</cell><cell>0.555</cell><cell>0.522</cell><cell>0.456</cell><cell>0.493</cell></row><row><cell></cell><cell>Coord</cell><cell>0.549</cell><cell>0.262</cell><cell>0.524</cell><cell>0.559</cell><cell>0.555</cell><cell>0.472</cell><cell>0.491</cell></row><row><cell></cell><cell>DETR  ?</cell><cell>0.552</cell><cell>0.285</cell><cell>0.520</cell><cell>0.558</cell><cell>0.563</cell><cell>0.471</cell><cell>0.497</cell></row><row><cell></cell><cell>Heatmap</cell><cell>0.524</cell><cell>0.251</cell><cell>0.510</cell><cell>0.533</cell><cell>0.545</cell><cell>0.459</cell><cell>0.452</cell></row><row><cell></cell><cell>AVS + SAN</cell><cell>0.591</cell><cell>0.311</cell><cell>0.549</cell><cell>0.609</cell><cell>0.581</cell><cell>0.516</cell><cell>0.551</cell></row><row><cell>AUC 0.1 ?</cell><cell>LUVLi AWing</cell><cell>0.557 0.572</cell><cell>0.310 0.312</cell><cell>0.549 0.515</cell><cell>0.584 0.578</cell><cell>0.588 0.572</cell><cell>0.505 0.502</cell><cell>0.525 0.512</cell></row><row><cell></cell><cell>SDFL</cell><cell>0.576</cell><cell>0.315</cell><cell>0.550</cell><cell>0.585</cell><cell>0.583</cell><cell>0.504</cell><cell>0.515</cell></row><row><cell></cell><cell>SDL</cell><cell>0.589</cell><cell>0.315</cell><cell>0.566</cell><cell>0.595</cell><cell>0.604</cell><cell>0.524</cell><cell>0.533</cell></row><row><cell></cell><cell>HIH</cell><cell>0.597</cell><cell>0.342</cell><cell>0.590</cell><cell>0.606</cell><cell>0.604</cell><cell>0.527</cell><cell>0.549</cell></row><row><cell></cell><cell>ADNet</cell><cell>0.602</cell><cell>0.344</cell><cell>0.523</cell><cell>0.580</cell><cell>0.601</cell><cell>0.530</cell><cell>0.548</cell></row><row><cell></cell><cell>SLPT  ?</cell><cell>0.588</cell><cell>0.327</cell><cell>0.563</cell><cell>0.596</cell><cell>0.595</cell><cell>0.514</cell><cell>0.528</cell></row><row><cell></cell><cell>SLPT  ?</cell><cell>0.595</cell><cell>0.348</cell><cell>0.574</cell><cell>0.601</cell><cell>0.605</cell><cell>0.515</cell><cell>0.535</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Figure 7. Convergence curves of SLPT and DETR on WFLW test set. The learning rate of SLPT is reduced at 120 and 140 epochs; the learning rate of DETR is reduced at 320 and 360 epochs.</figDesc><table><row><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SLPT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ViT</cell></row><row><cell></cell><cell>0.09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Normalized Mean Error</cell><cell>0.06 0.07 0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>NME: 4.66% Epochs: 55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NME: 4.71% Epochs: 391</cell></row><row><cell></cell><cell>0.04</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell><cell>400</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epochs</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>NME(?), FR0.1(?) and AUC0.1(?) with different patch sizes K ? K on WFLW test set. Key:[Best]    </figDesc><table><row><cell>.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Table 11. NME(?), FR0.1(?) and AUC0.1(?) with different patch sizes K ? K on WFLW test set. Key:[Best]    </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>.1</cell></row><row><cell>2</cell><cell>4.19%</cell><cell>2.88%</cell><cell>0.592</cell></row><row><cell>4</cell><cell>4.17%</cell><cell>2.84%</cell><cell>0.593</cell></row><row><cell>6</cell><cell>4.14%</cell><cell>2.76%</cell><cell>0.595</cell></row><row><cell>12</cell><cell>4.12%</cell><cell>2.72%</cell><cell>0.596</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A A.1 Contructing Multi-scale Feature Maps for SLPT</head><p>As discussed in Section 4.3, we construct multi-level feature maps for ResNet34, as shown in <ref type="figure">Fig.6</ref>. Supposing the feature map size of k-th stage in ResNet34 is W k ? H k ? d k , we firstly adopt a 1 ? 1 CNN layer to reduce the channels from d k to C I /4. Then, the SLPT crops N patches whose size is P W k ? P Hk from each level and resizes these patches to K ? K. Note that P W k ? P Hk is W k /4 ? H k /4 in the initial coarse-to-fine stage and is reduced by half in each following stage. Finally, the resized patches from different levels are concatenated on the channel dimension which is C I . As the result, the SLPT can utilize both high level and low level features for face alignment. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multi-stage, multi-feature machine learning approach to detect driver sleepiness in naturalistic road driving conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartosz</forename><surname>Zab?ocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Riethmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Anund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christer</forename><surname>Ahlstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3fabrec: Fast few-shot face alignment by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Browatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wallraven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6109" to="6119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="929" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decafa: Deep convolutional cascade for face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="6892" to="6900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kingma</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ba</forename><surname>Jimmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervision by registration and triangulation for landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3681" to="3694" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic attention-controlled cascaded shape regression exploiting training data augmentation and fuzzy-set sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Occlusion coherence: Localizing occluded faces with a hierarchical deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1899" to="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adnet: Leveraging error-bias towards normal direction in face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep alignment network: A convolutional neural network for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Trzcinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2034" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Luvli face alignment: Estimating landmarks&apos; location, uncertainty, and visibility likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Koike-Akino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revisting quantization error in face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ICCVW</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Vuong Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Advancing high fidelity identity swapping for forgery detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5073" to="5082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structured landmark detection via topology-adapting deep graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chihung</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Tung</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Fu</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publish</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>ing. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tokenpose: Learning keypoint tokens for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structure-coherent deep feature learning for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunze</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beier</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Shengju Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards efficient u-nets: A coupled and quantized approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolaou</surname></persName>
		</author>
		<title level="m">Epameinondas Antonakos, and Stefanos Zafeiriou</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A deeply-initialized coarse-to-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust face alignment by multi-order high-precision hourglass network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="133" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive wing loss for robust face alignment via heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6970" to="6980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Leveraging intra and interdataset variations for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2096" to="2105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentive-refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengtao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Freenet: Multi-identity face reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianfang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via occlusionadaptive deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meilu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Sadiq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3481" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning robust facial landmark detection via hierarchical structured ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
