<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Medical Code Prediction from Discharge Summary: Document to Sequence BERT using Sequence Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Sung</forename><surname>Heo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongcheol</forename><surname>Jo</surname></persName>
							<email>byeongcheol7674@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongmin</forename><surname>Yoo</surname></persName>
							<email>yooyongmin91@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyounguk</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongjoon</forename><surname>Park</surname></persName>
							<email>yeongjoon1227@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsun</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">AI R&amp;D Group NHN Diquest Korea</orgName>
								<address>
									<settlement>Seoul</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">AI R&amp;D Group NHN Diquest Korea</orgName>
								<address>
									<settlement>Seoul</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">AI R&amp;D Group NHN Diquest Korea</orgName>
								<address>
									<settlement>Seoul</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">AI R&amp;D Group NHN Diquest Korea</orgName>
								<address>
									<settlement>Seoul</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">AI R&amp;D Group NHN Diquest Korea</orgName>
								<address>
									<settlement>Seoul</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">AI R&amp;D Group NHN Diquest Korea</orgName>
								<address>
									<settlement>Seoul</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Medical Code Prediction from Discharge Summary: Document to Sequence BERT using Sequence Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>*Equal contribution</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clinical notes are unstructured text generated by clinicians during patient encounters. Clinical notes are usually accompanied by a set of metadata codes from the international classification of diseases (ICD). ICD code is an important code used in various operations, including insurance, reimbursement, medical diagnosis, etc. Therefore, it is important to classify ICD codes quickly and accurately. However, annotating these codes is costly and time-consuming. So we propose a model based on bidirectional encoder representations from transformers (BERT) using the sequence attention method for automatic ICD code assignment. We evaluate our approach on the medical information mart for intensive care III (MIMIC-III) benchmark dataset. Our model achieved performance of macro-averaged F1: 0.62898 and micro-averaged F1: 0.68555 and is performing better than a performance of the state-of-the-art model using the MIMIC-III dataset. The contribution of this study proposes a method of using BERT that can be applied to documents and a sequence attention method that can capture important sequence information appearing in documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keywords-medical information mart for intensive care III, international classification of diseases, prediction, bidirectional encoder representations from transformer, sequence attention, multi-label classification</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Clinical notes are unstructured text generated by clinicians during patient encounters. Clinical notes are usually accompanied by a set of metadata codes from the international classification of diseases (ICD), which present a standardized way of indicating diagnoses and procedures that were performed during the encounter with the patient. ICD is essentially a hierarchical classification that defines unique codes for patient conditions, diseases, infections, symptoms, causes of injury, and others. These unique diagnostic codes are assigned to patient records to facilitate clinical and financial decisions made by the hospital management for various tasks, including billing, insurance claims, and reimbursements <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. For these reasons, labeling the ICD code in the clinical description is very important. However, manually coding requires a lot of effort. It is not only errorprone but also time-consuming because it is what a person does. In addition, coding technical terms such as clinical records requires hiring people who can understand technical terms, which is costly to employ. In particular, the problem of multi-label classification, which is the task of automatically classifying multiple ICD codes in one clinical note, has received a lot of attention from the past to the present, and many studies have been conducted <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Recently, in natural language processing (NLP), many studies have been conducted using machine learning algorithms and deep learning algorithms to predict various classification tasks such as disease, prognosis, and ICD code using clinical notes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Kim et al. <ref type="bibr" target="#b5">[6]</ref> used logistic regression, na?ve Bayesian classification, single decision tree, and support vector machine to predict acute ischemic stroke using magnetic resonance imaging text reports of the brain. Heo et al. <ref type="bibr" target="#b6">[7]</ref> used extreme gradient boosting, light gradient boosting machines, convolutional neural networks (CNN), and long short-term memory (LSTM) to predict stroke prognosis using magnetic resonance imaging text reports of the brain. Heo et al. <ref type="bibr" target="#b7">[8]</ref> used CNN to predict the occurrence possibility of future atrial fibrillation using output texts of electrocardiogram extracted through an electrocardiography machine. Huang et al. <ref type="bibr" target="#b8">[9]</ref> used feed-forward neural networks, CNN, LSTM, and gated recurrent units to predict ICD codes using clinical notes.</p><p>The medical information mart for intensive care III (MIMIC-III) is a large, single-center database comprising information relating to patients admitted to intensive care units (ICU) at a hospital and is a freely available benchmark dataset <ref type="bibr" target="#b9">[10]</ref>. The dataset includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, imaging reports, hospital length of stay, survival data, procedure and diagnoses codes providing ICD 9th revision (ICD-9) codes, and more. Some research papers have shown good performance by performing multi-label classification tasks using the MIMIC-III dataset <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> In this study, we automatically assign ICD-9 codes through a deep learning model using clinical notes in the MIMIC-III data. Our model is based on bidirectional encoder representations from transformers (BERT) <ref type="bibr" target="#b13">[14]</ref> which shows good performance in various tasks of NLP. We propose document-to-sequence BERT (D2SBERT), which solves the max sequence length problem that occurs when applying a transformer-based model to document classification tasks. And, we use sequence attention to capture important sequences for a specific ICD-9 code among many sequences appearing in the MIMIC-III dataset. As a result, our approach outperforms previous results on medical code prediction on the MIMIC-III dataset.</p><p>The contributions of this study are as follows:</p><p>? Since BERT has a maximum sequence length limitation when using pre-trained models, it is difficult to apply directly to documents having relatively long sequence lengths. To overcome these shortcomings, we introduce D2SBERT, which is BERT that can be applied to documents.</p><p>? We propose a sequence attention method that processes sequence representations extracted through D2SBERT. This method has the advantage of being able to capture important sequences among many sequences appearing in the document.</p><p>This study is composed as follows. Section 2 mentions related works about document classification. Section 3 describes the dataset and preprocessing method for the experiment. Section 4 explains the proposed methodology and details of the main components. Section 5 describes the hyperparameters used in the experiment and the experimental results. Finally, Section 6 mentions the summary of this research and discusses future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Automatic ICD coding is a classification task for documents. Document classification tasks have been carried out in many studies and shown good performance using deep learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Mullenbach et al. <ref type="bibr" target="#b10">[11]</ref> proposed the convolutional attention for multi-label classification (CAML) and predicted several ICD-9 codes using 2,500 word tokens that appeared in the discharge summaries of the MIMIC-III dataset. Convolutional attention extracts feature maps, combined information of adjacent words, via CNN. It is also a neural network that predicts the target label using an important feature map corresponding to the target label among feature maps. Finally, they predicted ICD-9 codes by using sigmoid, an activation function, in the linear layer.</p><p>Hu and Teng <ref type="bibr" target="#b11">[12]</ref> proposed the shallow and wide attention convolutional mechanism (SWAM), which broadened the network of CAML. SWAM is a neural network such as CAML, but it extracts more feature maps by increasing the number of filters on CNN used. They predicted multiple ICD-9 codes using 2,500 word tokens that appeared in the discharge summaries of the MIMIC-III dataset. Finally, they predicted ICD-9 codes by using sigmoid, an activation function, in the linear layer.</p><p>Mayya et al. <ref type="bibr" target="#b12">[13]</ref> proposed the enhanced CAML (EnCAML), which consists of multi-channel. EnCAML is a method of utilizing CNNs with multi-Channel, which has the advantage of utilizing various information. They predicted multiple ICD-9 codes using 2,500 word tokens that appeared in the discharge summaries of the MIMIC-III dataset. Finally, they predicted ICD-9 codes by using an activation function named sigmoid in the linear layer.</p><p>Sun et al. <ref type="bibr" target="#b14">[15]</ref> classified documents using BERT, which shows excellent performance in NLP. Since BERT has a max sequence length limitation, they truncated the text in three ways to fine-tune BERT with a maximum sequence length of 512 (510 sequence tokens and 2 special tokens) in the document. The first method is the BERT-head method, which uses 510 sequence tokens at the front, the second method is the BERT-tail method, which uses 510 sequence word tokens at the end, and the third method is the BERT-head-tail method, which uses 128 sequence tokens at the front and 382 sequence tokens at the end. In their experiments through BERT, the third method used both the front and the end parts of the document showed the best performance. Words in the longest discharge summary 10,500</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASET</head><p>Words in the shortest discharge summary 51 ICD-9 codes (diagnoses and procedure) 6,918 and 2,003 The MIMIC-III is a database comprising information relating to patients admitted to ICU at a hospital and consisted of 26 table data. Our study uses the noteevents table composed of text and the diagnoses_icd and procedure_icd tables containing ICD-9 code. The number of data used in this study is 52,726, as shown in <ref type="table" target="#tab_0">Table I</ref>, and the experiment is conducted by dividing it into training set: validation set: test set = 8: 1: 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preprocessing</head><p>The texts that appear in the noteevents table only use the discharge summaries corresponding to the first hospitalization, not all texts. And, all letters are changed to lowercase letters, and letters that did not consist of the alphabet are removed (e.g., removing '200' but keeping '200cc'). Although our proposed model can use all word tokens, we truncated the documents to a maximum length of 2,500 word tokens at the front part rather than using all tokens in the document, such as in previous studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. In this way, we can ensure fairness in comparing the performance of the proposed model with previous studies.</p><p>The ICD-9 codes shown in the diagnoses_icd and procedure_icd tables are merged into one table and the 50 most frequent ICD-9 codes are used among the entire ICD-9 codes. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the distribution of the 50 most frequent ICD-9 codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>In this study, we propose D2SBERT, which solves the max sequence length problem that arises when applying a transformer-based model to document classification tasks. Also, we propose sequence attention that recognizes the relationship contents of the document. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the model is divided into D2SBERT, which extracts and processes all information in a sentence as much as possible, and sequence attention classifier (SAC), which predicts labels by reflecting only important information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Document-to-Sequence BERT (D2SBERT)</head><p>In the case of the text classification task, BERT adds 2 special tokens, which are [CLS] token located at the beginning of a sentence and [SEP] token located at the end of a sentence to the existing text and uses them as input values <ref type="bibr" target="#b13">[14]</ref>. Existing BERT has a limit on length because of their specified max sequence length <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Generally, input sequences are truncated when fine-tuning BERT with long sequences. However, it results in missing data and is especially inappropriate for medical code predication, which requires a detailed understanding of the sequence.</p><p>D2SBERT first divides the entire document into sequences having a constant size of equal lengths to solve this problem. Each divided sequence is used as an input for the BERT model. To extract information about sequence, we use the [CLS] token extracted through BERT. We define the [CLS] token extracted through each sequence as . When has the dimension of ? ? and the number of sequences is , the document representation vector ( ) containing all sequence information has the dimension of ? ?? . The is expressed as equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= [ 1 ; 2 ; ? ; ]</head><p>(1)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sequence Attention Classifier (SAC)</head><p>Sequence attention decides important parts in the document by utilizing the relationship of the sequences. In the sequence attention classifier, the model extracts the important information from by using the sequence attention mechanism. When the total number of labels to be classified is , and sequence attention matrix ( ) having the dimension of ? ?? are utilized to generate attention weights ( ). is obtained as in equation <ref type="formula">(2)</ref>.</p><formula xml:id="formula_0">= (tanh ( )) (2)</formula><p>In equation <ref type="formula">(2)</ref>, is calculated in the form of a score using . The label representation vectors ( = [ 1 , 2 , 3 , ? , ]) are calculated using attention weight, which extracts the important information from a document. has the dimension of ? ??1 . Equation <ref type="formula">(3)</ref> shows the process of calculating .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= ? =1</head><p>(3)</p><p>Information needed to determine whether the target label is true or false is contained in . After each is connected to a different fully-connected layer ( ), the final score of each target label is calculated using sigmoid function ( ). The final score about each target label is calculated as equation <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b4">5)</ref>.</p><formula xml:id="formula_1">( ) = ? ? =1 + (4) = ( ( ))<label>(5)</label></formula><p>has between 0 and 1. Finally, true or false of the label is determined by equation <ref type="formula">(6)</ref>.</p><formula xml:id="formula_2">ICD?9 = { True if &gt; 0.5</formula><p>False otherwise</p><formula xml:id="formula_3">(6) V. EXPERIMENT</formula><p>The performance of the proposed model in this study is compared with CAML, SWAM, EnCAML, BERT-head, BERT-tail, and BERT-head-tail models mentioned in related works. CAML, SWAM, and EnCAML, which are models that apply word embedding, used BioWordVec pre-trained with clinical documents and clinical notes <ref type="bibr" target="#b17">[18]</ref>. And, BERT-head, BERT-tail, BERT-head-tail, and the proposed model used BioBERT pre-trained with clinical documents <ref type="bibr" target="#b18">[19]</ref>.  <ref type="table" target="#tab_0">Table II</ref> shows the hyperparameters of BERT used in this experiment, and the hyperparameters used in this experiment are the same as the optimal value(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hyperparameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>We use two standard metrics to measure and compare the performance of our models: macro-averaged F1 and microaveraged F1. The two standard metrics are calculated using precision and recall. And precision and recall are calculated using true-positive (TP), false-positive (FP), and falsenegative (FN).</p><p>Macro-averaged F1 is a standard metric that calculates the metric independently for each class and then averages it. Macro-averaged F1 is calculated as the following equation <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b7">8)</ref>.</p><formula xml:id="formula_4">Precision = 1 ? TP TP + FP =1 (7) Recall = 1 ? TP TP + FN =1 (8)</formula><p>Micro-averaged F1 calculates an average metric by aggregating the contributions of all classes. Micro-averaged F1 is calculated as the following equation <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b9">10)</ref>.</p><formula xml:id="formula_5">Precision = ? TP =1 ? (TP =1 + FP )<label>(9)</label></formula><formula xml:id="formula_6">Recall = ? TP =1 ? (TP =1 + FN )<label>(10)</label></formula><p>Finally, macro-averaged F1 and micro-averaged F1 are calculated as the harmonic mean of precision and recall, such as equation <ref type="bibr" target="#b10">(11)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1 = 2 ? Precision ? Recall Precision + Recall</head><p>C. Results  <ref type="table" target="#tab_0">Table III</ref> is the performance comparison of the model experimented under the same conditions except performance marked with an asterisk for 50 most frequent ICD-9 codes.</p><p>And, EnCAML marked with an asterisk (*) is the performance shown in the paper by Mayya et al. <ref type="bibr" target="#b12">[13]</ref>. The experiment of Mayya et al. <ref type="bibr" target="#b12">[13]</ref> marked with an asterisk corrected typos appearing in the MIMIC-III text data and also used the Fisher-Jenks Natural Breaks algorithm to further improve the performance of the EnCAML model. And, in <ref type="table" target="#tab_0">Table III</ref>, the models labeled BERT use 510 sequence tokens and 2 special tokens. BERT-head uses 510 sequence tokens at the front part, BERT-tail uses 510 sequence tokens at the end part, and BERT-head-tail uses 128 sequence tokens at the front part and 382 sequence tokens at the end part.</p><p>For BERT, the BERT-head method showed the highest performance. This indicates that there is more important content about ICD-9 codes in the front part than the end part of the document. However, BERT, which generally shows good performance in NLP, showed significantly lower performance than other models. This is because it has the max sequence length limit of BERT, so the method using BERT cannot fully understand all contents of the document because it uses only a part of the document.</p><p>In the model using CNN-based attention, EnCAML showed high performance compared to CAML and SWAM models because it uses various semantic information.</p><p>It can be seen that the proposed model in this study has the higher performance of macro-averaged F1: 0.13522 to 0.17445 and micro-averaged F1: 0.11928 to 0.14544 compared to BERT. This result shows that we can overcome the disadvantage of BERT with max sequence length limitation. And, we can see that the proposed model has a higher performance of macro-averaged F1: 0.03245 to 0.05974 and micro-averaged F1: 0.01961 to 0.03562 compared to the models using CNN-based attention. Moreover, although we do not use additional typo correction and other algorithms, the proposed model outperforms EnCAML, the state-of-the-art model, that uses the typo correction and the Fisher-Jenks Natural Breaks algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>ICD code is an important code used in a variety of operations, including insurance, reimbursement, medical diagnosis, etc. Therefore, it is important to classify ICD codes from clinical notes quickly and accurately. However, manual coding is time-consuming and costly. Therefore, we propose a deep learning model that automatically assigns ICD-9 codes using clinical notes from MIMIC-III data in this research. Because BERT has max sequence length limitations, we used D2SBERT, which divides the document into sequences and applies BERT to each sequence. Also, sequence attention is applied to features with sequence information extracted via D2SBERT, and finally, ICD-9 code is predicted through a fully connected layer. Experiments show that the proposed model has the highest performance compared to the previous research models.</p><p>We have achieved useful results in the clinical field using clinical notes called MIMIC-III. Although this study aims at the clinical field, our method can be applied to various multilabel classification fields, such as the cooperative patent classification, the international patent classification for management strategy, and sentiment analysis to predict complex sentiments. Based on the experimental results, we expect that it will show good performance on other datasets.</p><p>Since the clinical note of MIMIC-III is a document written by several clinicians, the sentences have a structure that is not segmented correctly. So, we found a proper sequence length per document throughout the experiment. However, obtaining the proper sequence length is time-consuming because it is obtained through many experiments. Therefore, if we find a more efficient way to split the sentence, not the sequence split way, we can save time, and the performance of the model is expected to increase. Thus, in the future study, we will explore proper sentence segmentation methods and use various language models that outperform BERT to improve performance more. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Statistics of 50 most frequent ICD-9 codes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ACKNOWLEDGMENT</head><label></label><figDesc>This work was supported by the ICT R&amp;D By the Institute for ) [Project Number : 2020-0-00113, Project Name : Development of data augmentation technology by using heterogeneous information and data fusions] and the Industrial Technology Innovation Program funded by the ministry of Trade, Industry &amp; Energy(MOTIE, Korea) [Project Number : 20008625, Project Name : Development of deep tagging and 2D virtual try on for fashion online channels to provide mixed reality visualized service based on fashion attributes]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>STATISTICS OF THE DISCHARGE SUMMARIES</figDesc><table><row><cell>Description</cell><cell>Total</cell></row><row><cell>Discharge summaries</cell><cell>52,726</cell></row><row><cell>Words in the discharge summary</cell><cell>79,801,402</cell></row><row><cell>Average word count per discharge summary</cell><cell>1513.51</cell></row><row><cell>Unique words in the discharge summary</cell><cell>150,853</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II .</head><label>II</label><figDesc>HYPERPARAMETER USED IN THE EXPERIMENT</figDesc><table><row><cell>Description</cell><cell>Range</cell><cell>Optimal Value(s)</cell></row><row><cell>The number of Sequences</cell><cell>{10; 25; 50; 100; 250}</cell><cell>25</cell></row><row><cell>Word per sequence</cell><cell>{250; 100; 50; 25; 10}</cell><cell>100</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Learning rate</cell><cell>{3e-5; 1e-5; 5e-6}</cell><cell>1e-5</cell></row><row><cell>Exponential decay rates</cell><cell>1 =0.9, 2 =0.999</cell><cell>1 =0.9, 2 =0.999</cell></row><row><cell>Loss</cell><cell>Binary Cross Entropy</cell><cell>Binary Cross Entropy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell>.</cell><cell cols="3">PERFORMANCE COMPARISON OF THE MODEL</cell></row><row><cell>Model</cell><cell>Macro</cell><cell>F1</cell><cell>Micro</cell></row><row><cell>CAML [10]</cell><cell>0.56924</cell><cell></cell><cell>0.64993</cell></row><row><cell>SWAM [11]</cell><cell>0.58025</cell><cell></cell><cell>0.65994</cell></row><row><cell>EnCAML [12]</cell><cell>0.59653 0.6109 *</cell><cell></cell><cell>0.66594 0.6764 *</cell></row><row><cell>BERT-head [14]</cell><cell>0.49376</cell><cell></cell><cell>0.56627</cell></row><row><cell>BERT-tail [14]</cell><cell>0.45453</cell><cell></cell><cell>0.54011</cell></row><row><cell>BERT-head-tail [14]</cell><cell>0.49362</cell><cell></cell><cell>0.56566</cell></row><row><cell>Proposed model</cell><cell>0.62898</cell><cell></cell><cell>0.68555</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining electronic health records: towards better research applications and clinical care</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brunak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="395" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated ICD-9 coding via a deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol Bioinform</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1193" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving palliative care with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Avati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Downing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Inform. Decis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accuracy of ICD-9-CM codes for identifying cardiovascular and stroke risk factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Birman-Deych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Nilasena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Care</title>
		<imprint>
			<biblScope unit="page" from="480" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Enhancing automatic icd-9-cm code assignment for medical texts with pubmed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<editor>BioNLP. Canada</editor>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="263" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing and machine learning algorithm to identify brain MRI reports with acute ischemic stroke</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Obeid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lenert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">212778</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Various Approaches for Predicting Stroke Prognosis using Magnetic Resonance Imaging Text Records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Clinical NLP</title>
		<meeting>3rd Clinical NLP</meeting>
		<imprint>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prediction of Atrial Fibrillation Cases: Convolutional Neural Networks Using the Output Texts of Electrocardiography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sens. Mater</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="393" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep learning for ICD-9 code assignment using MIMIC-III clinical notes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Sy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Prog. Bio</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="141" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explainable Prediction of Medical Codes from Clinical Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mullenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL. United States</title>
		<meeting>HLT-NAACL. United States</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1101" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An Explainable CNN Approach for Medical Codes Prediction from Clinical Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Teng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11430</idno>
		<imprint/>
	</monogr>
	<note>arXiv preprint.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multichannel, convolutional attention based neural model for automated diagnostic coding of unstructured patient discharge summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mayya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gangavarapu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="374" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL. United States</title>
		<meeting>HLT-NAACL. United States</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How to fine-tune BERT for text classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCL. China</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="194" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL. Italy</title>
		<meeting>ACL. Italy</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Longformer: The longdocument transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BioWordVec, improving biomedical word embeddings with subword information and MeSH</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
