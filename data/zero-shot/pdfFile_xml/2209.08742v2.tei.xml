<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Integrative Feature and Cost Aggregation with Transformers for Dense Correspondence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Cho</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Integrative Feature and Cost Aggregation with Transformers for Dense Correspondence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel architecture for dense correspondence. The current state-of-theart are Transformer-based approaches that focus on either feature descriptors or cost volume aggregation. However, they generally aggregate one or the other but not both, though joint aggregation would boost each other by providing information that one has but other lacks, i.e., structural or semantic information of an image, or pixel-wise matching similarity. In this work, we propose a novel Transformer-based network that interleaves both forms of aggregations in a way that exploits their complementary information. Specifically, we design a self-attention layer that leverages the descriptor to disambiguate the noisy cost volume and that also utilizes the cost volume to aggregate features in a manner that promotes accurate matching. A subsequent cross-attention layer performs further aggregation conditioned on the descriptors of both images and aided by the aggregated outputs of earlier layers. We further boost the performance with hierarchical processing, in which coarser level aggregations guide those at finer levels. We evaluate the effectiveness of the proposed method on dense matching tasks and achieve state-of-the-art performance on all the major benchmarks. Extensive ablation studies are also provided to validate our design choices.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Finding visual correspondence between images is a central problem in computer vision, with numerous applications including simultaneous localization and mapping (SLAM) <ref type="bibr" target="#b2">[3]</ref>, augmented reality (AR) <ref type="bibr" target="#b52">[53]</ref>, object tracking, structure from motion (SfM) <ref type="bibr" target="#b61">[62]</ref>, optical flow <ref type="bibr" target="#b17">[18]</ref>, and image editing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref>. Given visually or semantically similar images, unlike sparse correspondence approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b54">55]</ref> that first detect a set of sparse points and extract corresponding descriptors to find matches across them, dense correspondence <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b27">28]</ref> aims at finding matches for all pixels. Dense correspondence approaches typically follow the classical matching pipeline <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b53">54]</ref> of feature extraction, cost aggregation, and flow estimation.</p><p>Much research has designed a means to address either the feature extraction or the cost aggregation, as shown in <ref type="figure" target="#fig_5">Fig. 1 (a)</ref> and <ref type="bibr">(b)</ref>. Feature aggregation aims to not only integrate self-similar features within an image and but also align similar features between the two images for matching, such as by using deep dense feature descriptors <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b31">32]</ref>. The advantages of feature aggregation are particularly evident in attention and Transformer-based methods <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b76">77]</ref> thanks to their attention layers with global receptive fields and adaptability to input tokens, which previous works with convolutions <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46]</ref> lack. These methods, however, solely aggregate feature descriptors without consideration of cost aggregation. Numerous works <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref> on dense correspondence proposed methods for cost aggregation stage instead and demonstrate its importance. During cost aggregation, pair-wise interactions between pixels of the two <ref type="figure" target="#fig_5">Figure 1</ref>: Intuition of the proposed method: (a) feature aggregation methods <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b31">32]</ref> that aggregate feature descriptors, (b) cost aggregation methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref> that aggregate a cost volume, and (c) our integrative feature and cost aggregation method, which jointly performs both aggregations in a complementary manner.</p><p>images are considered by first computing a cost volume between descriptors and then suppressing noise to promote accurate correspondence estimation. Transformer-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref> are found to benefit significantly from cost aggregation, but they disregard aggregation of feature descriptors even though an improved cost volume constructed using less noisy features would ease the subsequent cost aggregation.</p><p>We argue that both feature aggregation and cost aggregation should ideally be performed in dense correspondence, as they serve different purposes and the benefits of each are well-established. Although there have been a few approaches <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b24">25]</ref> that attempt to aggregate both, no approaches utilized transformers due to its expensive computational complexity, and we believe that directly performing the two aggregations independently in a sequential manner only allows one aggregation to benefit the other but not vice versa, thus limiting the synergy between these two processes.</p><p>In this work, we present a method, which we call Integrative Feature and Cost Aggregation with Transformers (IFCAT), that jointly aggregates feature descriptors and the cost volume in a manner that leverages their complementarity, as shown in <ref type="figure" target="#fig_5">Fig. 1 (c)</ref>. This goal is accomplished in two steps, the first of which employs a self-attention layer to jointly aggregate the descriptors and cost volume. In this stage, the descriptors help to disambiguate the noisy cost volume similarly to cost volume filtering <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b66">67]</ref>, and the cost volume enhances feature aggregation by introducing matching similarities as a factor for aggregation. The cost volume explicitly represents the similarity of features in one image with respect to the features in the other, and accounting for it drives the features in each image to become more compatible with those of the other. In the subsequent step, we design a cross-attention layer that performs further aggregation aided by both the feature descriptors and the cost volume from earlier aggregations. By constructing better cross-attention maps with both the feature descriptors and the aggregated cost volume, the aggregated features of both images can be mutually improved more effectively. These self-and cross-attention layers are interleaved to facilitate convergence. We further boost performance through hierarchical processing that enhances this complementary aggregation by providing coarser outputs to guide finer-level aggregation.</p><p>We evaluate the proposed method on semantic and geometric matching tasks. In the experiments, we show that IFCAT outperforms prior works on all the major benchmarks, including SPair-71k <ref type="bibr" target="#b48">[49]</ref>, PF-PASCAL <ref type="bibr" target="#b19">[20]</ref>, PF-WILLOW <ref type="bibr" target="#b18">[19]</ref> and HPatches <ref type="bibr" target="#b3">[4]</ref>, by a significant margin, establishing a new state-of-the-art for all of them. We also conduct an extensive ablation study to validate our approach and the architectural design choices. The pre-trained weights and codes will be made available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Feature Extraction. Various methods have been proposed to extract feature descriptors for robust sparse matching. This process involves detecting interest points and extracting the descriptors of corresponding points. In traditional methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b68">69]</ref>, the matching performance mostly relies on the quality of the feature detection and the description method, and outlier rejection across matched points is typically determined by RANSAC <ref type="bibr" target="#b16">[17]</ref>. These methods focus on the problem of identifying more meaningful feature points and extracting feature descriptors given an image. Despite their solid performance, they often struggle in cases of extreme appearance or viewpoint changes.</p><p>To overcome such issues, several methods <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b54">55]</ref> extract dense deep features used to obtain descriptors tailored for matching. These works have demonstrated that the quality of feature descriptors contributes substantially to matching performance. In accordance with this, recent matching networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b76">77]</ref> proposed effective means for feature aggregation. Notable works include SuperGlue <ref type="bibr" target="#b59">[60]</ref>, which employs graph self-and cross-attention to aggregate deep feature maps. SFNet <ref type="bibr" target="#b35">[36]</ref> and DMP <ref type="bibr" target="#b24">[25]</ref> introduce an adaptation layer subsequent to feature extraction in order to learn feature maps well-suited to matching.</p><p>Recent state-of-the-art works utilize Transformer <ref type="bibr" target="#b73">[74]</ref> for feature aggregation. COTR <ref type="bibr" target="#b31">[32]</ref> uses transformers by taking input coordinates and feature maps to infer the correspondence of a given pixel coordinate, with self-attention computed for feature aggregation. LOFTR <ref type="bibr" target="#b67">[68]</ref> also uses self-and cross-attention, but leaves the cost aggregation to a handcrafted method, i.e., optimal transport <ref type="bibr" target="#b65">[66]</ref>. Very recently, GMFlow <ref type="bibr" target="#b76">[77]</ref> also utilized a transformer for feature aggregation in optical flow estimation. Despite its state-of-the-art performance, its disregard of cost aggregation may lead to sub-optimal solutions, a problem we address in this paper.</p><p>Cost Aggregation. In the correspondence literature, recent works carefully design their architecture to effectively aggregate a cost volume. Some works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b71">72]</ref> use 2D convolutions to establish correspondence while aggregating the cost volume with learnable kernels that have a local receptive field. Although 2D convolutions are used for flow estimation, they in fact also aggregate costs during the process, making them suitable for both the cost aggregation and flow estimation stages. Some works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b39">40]</ref> utilize handcrafted methods including RHM <ref type="bibr" target="#b7">[8]</ref> and the OT solver <ref type="bibr" target="#b65">[66]</ref>. These works have inherent limitations as their use of handcrafted techniques do not take advantage of learning and are susceptible to severe deformations. NC-Net <ref type="bibr" target="#b58">[59]</ref> proposes to use 4D convolutions for cost aggregation in order to identify sets of consistent matches by exploring neighborhood consensus. Inspired by this, numerous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> either adopted or extended 4D convolutions. For example, DCCNet <ref type="bibr" target="#b26">[27]</ref> used it for cost embedding; Sparse NC-Net <ref type="bibr" target="#b36">[37]</ref> designed adaptive 4D convolutions, and <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b46">47]</ref> proposed efficient versions. However, they are all limited in the sense that they inherit the limitations of CNN-based architectures, for which the receptive fields are local.</p><p>Recently, CATs <ref type="bibr" target="#b8">[9]</ref> proposed to use Transformer <ref type="bibr" target="#b73">[74]</ref> as a means for cost aggregation, and an extension <ref type="bibr" target="#b9">[10]</ref> combined convolutions with Transformer for enhanced cost aggregation. Although they benefit from the global receptive field of self-attention operations, they disregard feature aggregation even though the cost volume they use is constructed from feature maps. FlowFormer <ref type="bibr" target="#b27">[28]</ref> takes an approach that utilizes Transformer, but it is designed specifically for the optical flow task and does not aggregate features. By disregarding feature aggregation, these methods may limit their performance due to the resultant noise in the cost volume which then hampers cost aggregation.</p><p>3 Preliminaries: Self-and Cross-Attention Self-and cross-attention are the core elements of Transformer <ref type="bibr" target="#b73">[74]</ref> for their ability to globally model relationships and interactions among input tokens and their adaptability to input tokens. As a general description, given a sequence of tokens as an input, Transformer <ref type="bibr" target="#b73">[74]</ref> first linearly projects tokens to obtain query, key and value embeddings. These are then fed into a scaled dot product attention layer, followed by layer normalization (LN) <ref type="bibr" target="#b0">[1]</ref> and a feed-forward network or MLP, to produce an output with the same shape as the input. Each token is attended to by all the other tokens. This attention process can be formulated as:</p><formula xml:id="formula_0">Q = P Q (X), K = P K (X), V = P V (X),<label>(1)</label></formula><p>where P Q , P K and P V denote query, key and value projections, respectively, and X denotes a token with a positional embedding. The obtained query, key and value embeddings then pass through an attention layer:</p><formula xml:id="formula_1">Attention(X) = softmax( QK T ? d k )V,<label>(2)</label></formula><p>where d k is the dimension of key embeddings. Note that the Attention(?) function can be defined in various ways <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b75">76]</ref>. A key factor that distinguishes self-and cross-attention is the</p><formula xml:id="formula_2">! " # " Self-Attention Cross-Attention " ? Conv4d Conv4d Linear Linear # $ ! $ Self-Attention Cross-Attention ? Conv4d Conv4d ?2 ?2 Linear Linear # % ! % Self-Attention Cross-Attention ? Conv4d Conv4d $ % * Conv4d Conv4d Conv4d Cost Volume Feature Correlation Upsample Correlation Upsample ?2 ?2 ?4 ?2</formula><p>Linear Linear : Upsampling <ref type="figure">Figure 2</ref>: Overall architecture of the proposed method. Given feature maps D s and D t , and cost volume C as inputs, our method employs self-and cross-attention specifically designed to conduct joint feature aggregation and cost aggregation in a coarse-to-fine manner.</p><p>input to the key and value projections. Given a pair of input tokens, e.g., X s and X t , the input to the key and value projections when performing self-attention with X s is the same input, X s , but for cross-attention across X s and X t , the inputs to the key and value projection are X t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Formulation</head><p>Let us denote a pair of visually or semantically similar images, i.e., the source and target, as I s and I t , the feature descriptors extracted from I s and I t as D s and D t , respectively, and the cost volume computed between the feature maps as C. Given I s and I t , we aim to establish a correspondence field F (i) that is defined at all pixels i and warps I s towards I t .</p><p>Recent learning-based networks <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b8">9]</ref> accomplish dense correspondence by extracting features from deep CNNs <ref type="bibr" target="#b21">[22]</ref> or Transformers <ref type="bibr" target="#b13">[14]</ref> for D s and D t . The extracted features subsequently undergo l-2 normalization. A cost volume that consists of all pair-wise feature similarities C ? R h?w?h?w with height h and width w is then computed and stored:</p><formula xml:id="formula_3">C(i, j) = D s (i) ? D t (j),</formula><p>where i and j index the source and target features, respectively. To improve the matching performance, existing state-of-the-art methods perform either feature aggregation <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b76">77]</ref> or cost aggregation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref> with Transformer <ref type="bibr" target="#b73">[74]</ref> </p><formula xml:id="formula_4">such that {D s , D t } = T (D s , D t ) or C = T (C),</formula><p>where T (?) denotes Transformer. Then, F (i) is determined from C(i, j) considering all j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Motivation and Overview</head><p>We argue that solely focusing on either feature or cost aggregation may lead to sub-optimal solutions. While feature aggregation <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b76">77]</ref> is a process of aligning similar features based on the rich structural or semantic information present in dense feature maps, cost aggregation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9]</ref> is a process of suppressing noises based on matching similarities. The information the two aggregations consider are different, but they can enhance matching by improving each other's aggregation with the help of information that the other has.</p><p>In this work, we aim to jointly learn feature and cost aggregation modules by establishing a complementary relationship between them. To this end, we first concatenate a cost volume and feature descriptors, which we call a feature cost volume, and feed it to the self-attention layer. Within the self-attention layer, both feature and cost aggregation are performed, where the feature descriptors and the cost volume benefit from one another during the aggregation. Subsequently, we leverage the aggregated features and cost volume for cross-attention, which performs further aggregation aided by the aggregated inputs to the cross-attention layer. These self-and cross-attention layers are interleaved to facilitate convergence. Finally, we formulate our architecture in a coarse-to-fine manner, where the outputs of coarser attention blocks are used to guide the aggregation of finer-level blocks. In the following, we will explain each module in detail. ["]</p><p>Integrative Self-Attention</p><p>Integrative Self-Attention ["]</p><p>Integrative Self-Attention</p><p>Integrative Self-Attention </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Integrative Feature and Cost Aggregation</head><p>Self-Attention for Integrative Feature and Cost Aggregation. Subsequent to feature extraction and cost computation, we feed both feature descriptors D s , D t and cost volume C into our proposed self-attention layer. We first embed these inputs with linear projection for channel reduction prior to self-attention, and positional embeddings <ref type="bibr" target="#b73">[74]</ref> are added after the query and key projections, as shown in <ref type="figure">Fig. 2</ref>. As done in <ref type="bibr" target="#b58">[59]</ref>, to ensure input order invariance, we consider the bidirectional nature of cost volumes and feed a pair {D s , C} and {D t , C T } into the proposed self-attention layer independently, where C T (i, j) = C(j, i), and add the outputs.</p><p>Specifically, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, within the self-attention layer, we first obtain a feature cost volume [D, C] by concatenating D and C, where [ ? ] denotes concatenation. To compute self-attention, we need to define the query, key and value embeddings. Unlike other works <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b8">9]</ref> that solely aggregate either the feature descriptors or cost volume, we jointly aggregate both. To this end, we exploit the feature cost volume in computing self-attention and define two independent value embeddings, specifically one for feature projection and the other for cost volume projection. Note that we do not use the feature cost volume for value embeddings to ensure disentangled aggregation that is targeted for one and not the other. Formally, we define the query, key and values as:</p><formula xml:id="formula_5">Q = P Q ([D, C]), K = P K ([D, C]), V D = P V D (D), V C = P V C (C),<label>(3)</label></formula><p>where V D and V C denote the value embeddings of feature descriptors and the cost volume, respectively. After computing an attention map by applying softmax over the query and key dot product, we use it to aggregate feature D and cost volume C with V D and V C using Eq. 2 as follows:</p><formula xml:id="formula_6">Attention self?D (C, D) = softmax( QK T ? d k )V D , Attention self?C (C, D) = softmax( QK T ? d k )V C .</formula><p>(4) Note that any type of attention computation can be utilized, i.e., additive <ref type="bibr" target="#b1">[2]</ref> or dot product <ref type="bibr" target="#b73">[74]</ref>, while in practice we use the linear kernel dot product with the associative property of matrix products <ref type="bibr" target="#b32">[33]</ref>. The outputs of this self-attention are denoted as D s , D t , and C .</p><p>In this way, we benefit from two perspectives. From the cost aggregation point of view, the feature map of the feature cost volume can disambiguate the noisy cost volume as proven in the stereo matching literature <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21]</ref>, i.e., cost volume filtering, which aids the cost aggregation process. From the feature aggregation point of view, the cost volume explicitly represents the similarity of features in one image with respect to the features in the other, and accounting for it drives the features in each image to become more compatible with those of the other.</p><p>Cross-Attention with Matching Distribution. In the proposed cross-attention layer, the aggregated feature and cost volume are explicitly used for further aggregation, and we condition both feature descriptors on both input images via this layer. By exploiting the outputs of the self-attention Concretely, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we first treat the input cost volume as a cross-attention map, since applying a softmax function over the cost volume is tantamount to obtaining an attention map. In this way, we could perform more enhanced aggregation as the input cost map to the cross-attention layer is guided by residual connections that contain information from improved features and an enhanced cost volume as further explained in the next paragraph. Note that features D s and D t undergo average pooling followed by layer normalization <ref type="bibr" target="#b0">[1]</ref> and a linear projection layer in order to adjust the spatial resolution to C , as will be further explained in Section 4.4. Formally, we first define a cross-attention map and value for attention score as QK T = C and V D = P V D (D ), respectively. The attention process for cross-attention is then defined as follows:</p><formula xml:id="formula_7">(a) Source (b) Target (c) C 1 (d) C 2 (e) C 3 (f) D s ? D t (g) C (h) D s ? D t</formula><formula xml:id="formula_8">Attention cross (C , D ) = softmax( C ? d k )V D .<label>(5)</label></formula><p>The outputs of this cross-attention are denoted as D s , D t , and C .</p><p>Enhanced Aggregation with Improved Features and Cost Volume. Within the proposed attention block, it is shown in <ref type="figure">Fig. 2</ref> that the outputs of the self-and cross-attention layers, which include aggregated feature maps and a cost volume, are connected to the next layer or added to the input cost volume. Specifically, the feature maps are used in two ways: for cost volume construction and as inputs to aggregation in subsequent layers. For cost volume construction, the aggregated features are processed according to Eq. 3 and the result is added to the cost volume input of the next layer by a residual connection. This is repeated for both self-attention and cross-attention layers across N attention blocks, where the aggregated pairs of one block are fed as input to the following attention block. Similar to the feature maps, the cost volume is progressively improved from aggregation and better features as it passes through the attention blocks. Through these mechanisms of the proposed joint aggregation, the feature maps and cost volume help each other in self-attention and also facilitate further aggregation by providing an enhanced cross-attention map and improved features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Coarse-to-Fine Formulation</head><p>To improve the robustness of fine-scale estimates, we extend our architecture to a coarse-to-fine approach through pyramidal processing, as done in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b24">25]</ref>. Specifically, we use feature maps of the last index at each pyramid level, specifically feature maps from conv3 x to conv5 x when ResNet <ref type="bibr" target="#b21">[22]</ref> is used. We first use a coarse pair of refined feature maps and aggregated cost volume, and similar to <ref type="bibr" target="#b80">[81]</ref> that learns complementary correspondence by adding the cost volume of the previous scale, we progressively learn complementary descriptors and correspondences through adding the previous-level outputs to those of the next level.</p><p>The overall architecture is shown in <ref type="figure">Fig. 2</ref>. A straightforward solution would be to upsample all the outputs of the cross-attention layer, following a pyramidal structure. However, the increasing computational and memory burden with respect to the cost volume resolution makes this infeasible.</p><p>To alleviate such issues, we fix the resolution of the cost volume to the coarsest resolution and utilize 4D convolutions to downsample the spatial resolution when finer cost volumes are added to the coarsest cost volume. Finally, we sum up the output cost volumes after each attention block computed using the enhanced feature maps across all levels for the final output of the network. Formally, given the outputs of the attention block at each level, D ,l s , D ,l t and C ,l , where l denotes the l-th level,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training Objective</head><p>To train the networks, we first compute the correlation map between D s and D t at each level and then transform it into a dense flow field F pred using the soft-argmax operator <ref type="bibr" target="#b35">[36]</ref>. Then, we compare the predicted dense flow field with the ground-truth flow field F GT . Specifically, we use average end-point error (AEPE), computed by averaging the Euclidean distance between the ground-truth and estimated flow, for the objective function. We then sum up the AEPE loss across all levels. We thus formulate the objective function as L = F GT ? F pred 2 . Flow fields may instead be obtained from C , but we empirically find that slightly better results are obtained when feature maps are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>For the backbone network, we use VGG-16 <ref type="bibr" target="#b64">[65]</ref> for dense alignment and ResNet-101 <ref type="bibr" target="#b21">[22]</ref> for dense semantic correspondence, which are both pretrained on ImageNet <ref type="bibr" target="#b11">[12]</ref>. We use N= (2, 2, 2) for efficiency in training. We use data augmentation used in <ref type="bibr" target="#b8">[9]</ref> for dense semantic correspondence, while no augmentation is used for dense alignment. Our networks are trained with the input images resized to 512?512. We implemented our network using PyTorch <ref type="bibr" target="#b51">[52]</ref>, and the AdamW <ref type="bibr" target="#b41">[42]</ref> optimizer is employed with an initial learning rate of 1e?4 for the IFCAT layers, which we gradually decrease using step learning rate decay. We train our networks for 50 epochs on SPair-71k <ref type="bibr" target="#b48">[49]</ref> and DPED-CityScape-ADE <ref type="bibr" target="#b70">[71]</ref>, and 300 epochs for PF-PASCAL <ref type="bibr" target="#b19">[20]</ref>. More details can be found in the supplementary material. The code and pretrained weights will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dense Semantic Correspondence</head><p>In this section, we evaluate the effectiveness of the proposed method for dense correspondence. For a fair comparison, following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b8">9]</ref>, when evaluating on SPair-71k <ref type="bibr" target="#b48">[49]</ref> we train the proposed method on SPair-71k <ref type="bibr" target="#b48">[49]</ref>, and when evaluating on PF-PASCAL <ref type="bibr" target="#b19">[20]</ref> and PF-WILLOW <ref type="bibr" target="#b18">[19]</ref> we train on PF-PASCAL <ref type="bibr" target="#b19">[20]</ref>. <ref type="table">Table 1</ref>: Quantitative evaluation on standard semantic correspondence benchmarks <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Higher PCK is better. The best results are in bold, and the second best results are underlined. Reso.: Resolution, F.A.: Feature Aggregation, C.A.: Cost Aggregation.   <ref type="table">I  II  III  IV  V  Avg.  I  II  III  IV  V  Avg.</ref> COTR <ref type="bibr" target="#b31">[32]</ref> Trans.  <ref type="bibr" target="#b18">[19]</ref> is composed of 900 image pairs from 4 categories. We use percentage of correct keypoints (PCK) for the evaluation metric, for which higher values are better. Concretely, given predicted keypoint k pred and ground-truth keypoint k GT , we count the number of predicted keypoints that satisfy the following condition: d(k pred , k GT ) ? ? ? max(H, W ), where d( ? ) denotes Euclidean distance; ? denotes a threshold value; H and W denote height and width of the object bounding box or the entire image. Note that as confirmed in <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b9">10]</ref>, using different ground-truth resolutions for evaluation leads to different results, so we set our evaluation on the original resolution.</p><formula xml:id="formula_9">- - - - - - - - - - - - 7.</formula><p>The results are summarized in <ref type="table">Table 1</ref>. As shown, IFCAT clearly sets a new state-of-the-art for the three dense semantic correspondence benchmarks. This demonstrates the effectiveness of joint aggregation by outperforming methods that focus on either feature or cost aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dense Alignment</head><p>We further evaluate our method on the dense alignment dataset HPatches <ref type="bibr" target="#b3">[4]</ref>. For each scene, there is a source image and five target images from different viewpoints, along with corresponding ground-truth flows. The resolutions of HPatches range from 450 ? 600 to 1,613 ? 1,210. As in <ref type="bibr" target="#b44">[45]</ref>, we also evaluate on downsampled HPatches <ref type="bibr" target="#b3">[4]</ref>, where the images are resized to a low resolution (240 ? 240).</p><p>For the evaluation metric, we use the average end-point error (AEPE), computed by averaging the Euclidean distance between the ground-truth and estimated flow, and percentage of correct keypoints (PCK), computed as the ratio of estimated keypoints within a threshold of the ground truth to the total number of keypoints. When evaluating on HPatches <ref type="bibr" target="#b3">[4]</ref>, following <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b69">70]</ref> we train our networks on the DPED-CityScape-ADE <ref type="bibr" target="#b70">[71]</ref> dataset. <ref type="table" target="#tab_2">Table 2</ref> summarizes the quantitative results. As shown, IFCAT outperforms existing works by a large margin, clearly achieving state-of-the-art performance. Note that a fair comparison to COTR <ref type="bibr" target="#b31">[32]</ref> is not feasible because its use of the zoom-in technique with a multiple inference strategy dramatically boosts its performance as a trade-off to speed. It is shown that RANSAC-Flow <ref type="bibr" target="#b63">[64]</ref> and RANSAC-DMP <ref type="bibr" target="#b24">[25]</ref> exceeds the performance of our method, but they adopt two-stage inference where the first stage aims to find the homographic transformation between an image pair, which give them an advantage on the HPatches dataset. On its own, IFCAT is demonstrated to be effective, highlighting the importance of integrative aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>Aggregation Strategy. In this ablation study, we compare the performance of different aggregation strategies. <ref type="table" target="#tab_4">Table 3</ref> summarizes the results. There are seven components we evaluate. From (I) to (II), we report the results for sole aggregation on either feature or cost volume. We investigate the effectiveness of conditioning features on both images and performing both aggregations in (III and (IV). Lastly, we progressively add the proposed components in (V) to (VII) to demonstrate their significance. For a fair comparison, we trained all strategies with a single level except for (VII). As shown, solely aggregating either the feature or cost volume yields limited performance, where (II) can be seen as a simplified version of CATs <ref type="bibr" target="#b8">[9]</ref>. However, we observe that combining self-and cross-attention, which is highly similar to LoFTR <ref type="bibr" target="#b67">[68]</ref>, for feature aggregation helps to boost the performance by conditioning features on both images. This highlights the importance of providing information from the other image, which implies that providing a cost volume that explicitly represents similarity information with respect to each image would help to establish a more accurate correspondence field. Interestingly, (IV) shows that performing feature and cost aggregation yields a large performance boost, demonstrating that cost aggregation benefits from powerful feature representations tailored to matching. From (V) to (VII), each component contributes appreciably to the improvement of performance, clearly showing the effectiveness of our proposed components. This confirms that, as the result of (III) implies, leveraging the complementarity of features and cost volume is of prime importance.</p><p>Depth of Attention Block. As shown in <ref type="figure">Fig. 2</ref>, we can stack the attention block at each level to increase model capacity and allow further aggregation. In this ablation study, we show the effects of varying the hyperparameter N . We additionally show comparisons of memory consumption, run time, and number of learnable parameters to indicate their efficiency. The results are summarized in <ref type="table" target="#tab_5">Table 4</ref>. We consistently observe that as the attention block depth increases, the performance is boosted, demonstrating that the earlier aggregations help the subsequent aggregations. We generally observe increasing memory consumption, run-time and number of parameters as N increases, which are trade-offs to improve performance. Note that during training, the number of parameters has a direct influence on memory consumption, which is a limitation of increasing the attention block depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a novel transformer-based architecture, called Integrative Feature and Cost Aggregation with Transformer (IFCAT), that interleaves feature refinement and cost aggregation by establishing a complementary relationship. Our design based on self-and cross-attention is tailored for matching and joint enhancement of feature descriptors and the cost volume. This method is formulated in a coarse-to-fine manner, yielding an appreciable performance boost. We have shown that our method surpasses all other existing works on several benchmarks for semantic and geometric matching, establishing new state-of-the-art performance. We also conducted an extensive ablation study to validate our choices.  (a) DHPF <ref type="bibr" target="#b49">[50]</ref> (b) CHM <ref type="bibr" target="#b45">[46]</ref> (c) CATs <ref type="bibr" target="#b8">[9]</ref> (d) IFCAT <ref type="figure">Figure 2</ref>: Qualitative results on PF-PASCAL <ref type="bibr" target="#b19">[20]</ref> (a) DHPF <ref type="bibr" target="#b49">[50]</ref> (b) CHM <ref type="bibr" target="#b45">[46]</ref> (c) CATs <ref type="bibr" target="#b8">[9]</ref> (d) IFCAT <ref type="figure" target="#fig_2">Figure 3</ref>: Qualitative results on PF-WILLOW <ref type="bibr" target="#b18">[19]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the proposed self-and cross-attention: (a) self-attention layer that performs joint feature aggregation and cost aggregation, and (b) cross-attention layer with matching distribution for enhanced feature aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of attention maps: (a) source image, (b) target image, (c)-(e) raw correlations, (f) a cost volume constructed with features aggregated with self-attention, (g) a cost volume aggregated with self-attention, and (h) a cost volume constructed with features aggregated with cross-attention. layer, the cross-attention layer performs cross-attention between feature descriptors for further feature aggregation using the improved feature descriptors D s , D t and enhanced cost volume C from earlier aggregations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on SPair-71k<ref type="bibr" target="#b48">[49]</ref> (top) and HPatches<ref type="bibr" target="#b3">[4]</ref> (bottom).we upsample the aggregated features using bilinear interpolation, and add them to the raw feature descriptors extracted from I s and I t defined at the next level: D l+1s = D l+1 s + up(D ,l s ), where D l+1t is defined similarly. In addition, C l+1 = C l+1 + Conv4d(C ,l ). The visualizations of cost volumes are shown inFig. 4. Finally, given the features D s and D t at each level, we compute the correlation map between D s and D t , and the sum of cost volumes across all levels are added up to obtain the final output C * that is used to estimate the final flow field, as shown in the bottom ofFig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1</head><label>1</label><figDesc>Pseudo-Code, PyTorch-like class TransformerLayer: def forward(corr, src_feat, trg_feat): corr_src, src_feat_refined = integrative_self_attention(corr, src_feat) corr_trg, trg_feat_refined = integrative_self_attention(transpose(corr), trg_feat) corr = corr_src + transpoze(corr_trg) corr = corr + conv4d_1(cost_computation(src_feat_refined, trg_feat_refined)) corr = corr + conv4d_2(corr) src_feat_refined, trg_feat_refined = integrative_cross_attention(corr, src_feat_refined, trg_feat_refined) corr = corr + conv4d_3(cost_computation(src_feat_refined, trg_feat_refined)) corr = corr + conv4d_4(corr) return corr, src_feat_refined, trg_feat_refined class IFCAT: def forward(trg_img, src_img): src_feat_list = feature_backbone(src_img) trg_feat_list = feature_backbone(trg_img) src_feats = projection(src_feat_list) trg_feats = projection(trg_feat_list) correlations = [] corr_1 = correlation(src_feat_list[0], src_feat_list[0]) corr_1 = conv4d_1(corr_1) src_feat_1, trg_feat_1 = src_feats[0], trg_feats[0] corr_1, src_feat_1, trg_feat_1 = transformer_layer[0](corr_1, src_feat_1, trg_feat_1) correlations.append(cost_computation(src_feat_1, trg_feat_1))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Qualitative results on HPatches<ref type="bibr" target="#b3">[4]</ref>.18 (a) Source (b) Target (c) C 1 (d) C 2 (e) C 3 (f) D s ? D t (g) C (h) D s ? D tVisualization of attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation on HPatches<ref type="bibr" target="#b3">[4]</ref>. We evaluate on both HPatches-240 and HPatches original. Lower AEPE is better. We divide methods into two groups: multiple feed-forward and single feed-forward.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>HPatches (240 ? 240)</cell><cell></cell><cell>HPatches</cell><cell></cell></row><row><cell>Methods</cell><cell>F.A.</cell><cell>C.A.</cell><cell>AEPE ?</cell><cell>PCK</cell><cell>AEPE ?</cell><cell>PCK</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>96.28 4.32 11.21 22.80 31.34 33.64 20.65 75.35 .12 12.31 13.68 16.12 9.69 79.21 3.21 15.54 32.54 38.62 63.43 30.64 63.21 .27 90.90 1.90 10.72 18.95 24.36 31.40 17.59 80.41 SPair-71k [49] consists of 70,958 image pairs with extreme and diverse viewpoints, scale variations, and rich annotations for each image pair. PF-PASCAL [19] contains 1,351 image pairs over 20 object categories with keypoint annotations, and PF-WILLOW</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">75 91.10</cell></row><row><cell>RANSAC-Flow [64]</cell><cell>2D Conv.</cell><cell cols="2">0.51 2.36</cell><cell>2.91</cell><cell>4.41</cell><cell>5.12</cell><cell>3.06</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="8">RANSAC-DMP [25] 3.05 CNNGeo [56] 2D Conv. 2D Conv. 0.53 2.21 2.76 4.62 5.14 2D Conv. 9.59 18.55 21.15 27.83 35.19 22.46</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DGC-Net [45]</cell><cell>2D Conv.</cell><cell cols="2">1.74 5.88</cell><cell cols="12">9.07 12.14 16.50 9.07 50.01 5.71 20.48 34.15 43.94 62.01 33.26 58.06</cell></row><row><cell>GLU-Net [71]</cell><cell>2D Conv.</cell><cell cols="2">0.59 4.05</cell><cell>7.64</cell><cell cols="11">9.82 14.89 7.40 83.47 1.55 12.66 27.54 32.04 52.47 25.05 78.54</cell></row><row><cell>GOCOR-GLU-Net [70]</cell><cell>Hand-crafted</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="7">1.29 10.07 23.86 27.17 38.41 20.16 81.43</cell></row><row><cell cols="4">DMP [25] 1.21 5IFCAT (Ours) 2D Conv. 2D Conv. Trans. Trans. 0.65 3.33</cell><cell>5.41</cell><cell cols="3">6.91 10.09 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Aggregation strategies for IFCAT.</figDesc><table><row><cell></cell><cell>Components</cell><cell cols="2">SPair-71k ? bbox = 0.1 ? AEPE ? HPatches</cell></row><row><cell>(I)</cell><cell>Feature self-att.</cell><cell>36.1</cell><cell>84.08</cell></row><row><cell>(II)</cell><cell>Cost self-att.</cell><cell>28.7</cell><cell>54.80</cell></row><row><cell>(III)</cell><cell>Feature self-att. + cross-att.</cell><cell>38.5</cell><cell>81.72</cell></row><row><cell>(IV)</cell><cell>Sequential (III) + cost self-att.</cell><cell>56.5</cell><cell>49.00</cell></row><row><cell>(V)</cell><cell>Integrative self-att.</cell><cell>54.7</cell><cell>34.85</cell></row><row><cell>(VI)</cell><cell>Integrative self-and cross-att.</cell><cell>58.5</cell><cell>24.41</cell></row><row><cell cols="2">(VII) (VI) + hierarchical processing</cell><cell>64.4</cell><cell>17.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablations on varying depth of attention block.</figDesc><table><row><cell># of N</cell><cell cols="5">SPair-71k ? bbox = 0.1 ? AEPE ? HPatches Memory Runtime # of param. [MB] [ms] [M]</cell></row><row><cell>(1,0,0)</cell><cell>55.4</cell><cell>35.02</cell><cell>335.09</cell><cell>35.09</cell><cell>0.58</cell></row><row><cell>(2,0,0)</cell><cell>58.5</cell><cell>24.41</cell><cell>354.57</cell><cell>47.23</cell><cell>1.04</cell></row><row><cell>(1,1,1)</cell><cell>63.7</cell><cell>20.49</cell><cell>845.06</cell><cell>91.96</cell><cell>0.83</cell></row><row><cell>(2,2,2)</cell><cell>64.4</cell><cell>17.59</cell><cell>874.25</cell><cell>138.05</cell><cell>1.55</cell></row><row><cell>(3,3,3)</cell><cell>64.7</cell><cell>17.43</cell><cell>903.40</cell><cell>188.43</cell><cell>2.26</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this document, we provide more implementation details and psuedo-code of IFCAT, and more results on SPair-71k <ref type="bibr" target="#b48">[49]</ref>, PF-PASCAL <ref type="bibr" target="#b19">[20]</ref>, PF-WILLOW <ref type="bibr" target="#b18">[19]</ref> and HPatches <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Details</head><p>Training Details. For training, we employ the same augmentation strategy introduced in <ref type="bibr" target="#b8">[9]</ref>. To implement 4D convolutions, we use separable 4D convolutions for efficient computation, which is introduced in VCN <ref type="bibr" target="#b77">[78]</ref>. All separable 4D convolutions are followed by ReLU activation and Layer Normalization <ref type="bibr" target="#b0">[1]</ref>. We set the weight decay to 0.05 and the learning rate to 1e ?4 for IFCAT and to 1e ?6 for the backbone, which is halved at epochs 30 and 40.</p><p>Psuedo-code. We present Pytorch-like Psuedo-code of the proposed method in Alg. 1.</p><p>More Qualitative Results. We provide more qualitative results for SPair-71k <ref type="bibr" target="#b48">[49]</ref> in <ref type="figure">Fig. 1</ref>, PF-PASCAL <ref type="bibr" target="#b19">[20]</ref> in <ref type="figure">Fig. 2</ref>, PF-WILLOW <ref type="bibr" target="#b19">[20]</ref> in <ref type="figure">Fig. 3</ref>, and HPatches <ref type="bibr" target="#b3">[4]</ref> in <ref type="figure">Fig. 4</ref>. We also present more visualization of the attention maps in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>An apparent limitation of the proposed method is that as IFCAT acts on the cost volume, it is not feasible to perform cost aggregation at higher resolutions. The maximum resolution that would be accessible to users is 64 <ref type="bibr" target="#b3">4</ref> . This makes the use of standard transformer for attention computation infeasible in terms of memory consumption. Another limitation may be when given a pair of images that are not relevant or show completely different objects, such that there are no correspondences between views, the proposed method lacks an ability to prevent establishing correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our network can be beneficial in a wide range of applications, including simultaneous localization and mapping (SLAM) <ref type="bibr" target="#b2">[3]</ref>, augmented reality (AR) <ref type="bibr" target="#b52">[53]</ref>, object tracking, structure from motion (SfM) <ref type="bibr" target="#b61">[62]</ref>, optical flow <ref type="bibr" target="#b17">[18]</ref>, and image editing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref>. As a future work, we could apply the proposed method to different tasks, including feature matching, segmentation and optical flow. Our work would not pose significantly malicious threats on its own.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simultaneous localization and mapping (slam): Part ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Durrant-Whyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE robotics &amp; automation magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="108" to="117" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5173" to="5182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Brief: Binary robust independent elementary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Calonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1201" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cats: Cost aggregation transformers for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cats++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06817</idno>
		<title level="m">Boosting cost aggregation with convolutions and transformers</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="224" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">D2-net: A trainable cnn for joint description and detection of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/cvf conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8092" to="8101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of mathematical models in computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="237" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Proposal flow: Semantic correspondences from object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cost aggregation is all you need for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisu</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11685</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cost aggregation with 4d convolutional swin transformer for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisu</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.10866</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep matching prior: Test-time optimization for dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asmaa</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic context correspondence network for semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Flowformer: A transformer architecture for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Chun</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16194</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8981" to="8989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parn: Pyramidal affine regression networks for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Sangryul Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="351" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Guided semantic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Sangryul Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cotr: Correspondence transformer for matching across images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6207" to="6217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sang Ryul Jeon, Dongbo Min, and Kwanghoon Sohn. Recurrent transformer networks for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Patchmatch-based neighborhood consensus for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Yong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Degol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fragoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipta N</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13153" to="13163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Correspondence networks with adaptive neighbourhood consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><forename type="middle">W</forename><surname>Costain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Howard-Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10196" to="10205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Probabilistic model distillation for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7505" to="7514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="978" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic correspondence as an optimal transport problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11945</idno>
		<title level="m">Soft: Softmax-free transformer with linear complexity</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dgc-net: Dense geometric correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iaroslav</forename><surname>Melekhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Tiulpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Convolutional hough matching networks for robust and efficient visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05221</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Convolutional hough matching networks for robust and efficient visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05221</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Spair-71k: A large-scale benchmark for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10543</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to compose hypercolumns for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XV 16</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Lf-net: Learning local features from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Gan-supervised dense visual alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05143</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?sar</forename><forename type="middle">De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Humenberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06195</idno>
		<title level="m">R2d2: repeatable and reliable detector and descriptor</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">End-to-end weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficient neighbourhood consensus networks via submanifold sparse convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10510</idno>
		<title level="m">Neighbourhood consensus networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Attentive semantic alignment with offset-aware correlation kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deunsol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="349" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Ransac-flow: generic two-stage image alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Darmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="618" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Diagonal equivalence to matrices with prescribed row and column sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sinkhorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Mathematical Monthly</title>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Loftr: Detector-free local feature matching with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8922" to="8931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Daisy: An efficient dense descriptor applied to wide-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="815" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Gocor: Bringing globally optimized correspondence volumes into your neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14278" to="14290" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Glu-net: Global-local universal network for dense flow and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6258" to="6268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning accurate dense correspondences and when to trust them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Probabilistic warp consistency for weakly-supervised semantic correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04279</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Belinda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Fastformer: Additive attention can be all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.09084</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gmflow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13680</idno>
		<title level="m">Learning optical flow via global matching</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Adaptive support-weight approach for correspondence search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Kuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="650" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Multiscale matching networks for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3354" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankun</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00330</idno>
		<title level="m">Dip: Deep inverse patchmatch for high-resolution optical flow</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>2022. (a) DHPF [50] (b) CHM [46] (c) CATs [9] (d) IFCAT</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
