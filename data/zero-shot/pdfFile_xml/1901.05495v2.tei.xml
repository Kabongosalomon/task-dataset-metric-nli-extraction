<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 An Underwater Image Enhancement Benchmark Dataset and Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
							<email>lichongyi25@gmail.com.chunleguoiswiththe</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunle</forename><surname>Guo</surname></persName>
							<email>guochunle@tju.edu.cn.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
							<email>rwq.renwenqi@gmail.com.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Runmin</forename><surname>Cong</surname></persName>
							<email>rmcong@bjtu.edu.cn.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Junhui</forename><surname>Hou</surname></persName>
							<email>jh.hou@cityu.edu.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Sam</forename><surname>Kwong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kwong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<postCode>999077</postCode>
									<settlement>Kowloon</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Information Security</orgName>
								<orgName type="institution">Hong Kong SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institute of Information Engineering, Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Wenqi Ren is with State Key Laboratory of Information Security</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<postCode>999077</postCode>
									<settlement>Kowloon, Hong Kong SAR</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">City University of Hong Kong Shenzhen Research Institute</orgName>
								<address>
									<postCode>51800</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">Faculty of Engineering and Information Technologies</orgName>
								<orgName type="laboratory">UBTECH Sydney Artificial Intelligence Centre and the School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<addrLine>6 Cleveland St</addrLine>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 An Underwater Image Enhancement Benchmark Dataset and Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>This work was supported in part by CCF-Tencent Open Fund, in part by Zhejiang Lab&apos;s International Talent Fund for Young Professionals, in part by Fundamental Research Funds for the Central Universities under Grant 2019RC039, in part by National Natural Science Foundation of China under Grant 61802403, Grant 61771334, and Grant 61871342, in part by Hong Kong RGC General Research Funds under Grant 9042038 (CityU 11205314) and Grant 9042322 (CityU 11200116), and in part by Hong Kong RGC Early Career Schemes under Grant 9048123. (Corresponding author: Wenqi Ren) Chongyi Li is with the Runmin Cong is with the Institute of Information Science, Beijing Jiaotong University, Dacheng Tao is with the</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-underwater image enhancement</term>
					<term>real-world un- derwater images</term>
					<term>comprehensive evaluation</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Underwater image enhancement has been attracting much attention due to its significance in marine engineering and aquatic robotics. Numerous underwater image enhancement algorithms have been proposed in the last few years. However, these algorithms are mainly evaluated using either synthetic datasets or few selected real-world images. It is thus unclear how these algorithms would perform on images acquired in the wild and how we could gauge the progress in the field. To bridge this gap, we present the first comprehensive perceptual study and analysis of underwater image enhancement using large-scale real-world images. In this paper, we construct an Underwater Image Enhancement Benchmark (UIEB) including 950 realworld underwater images, 890 of which have the corresponding reference images. We treat the rest 60 underwater images which cannot obtain satisfactory reference images as challenging data. Using this dataset, we conduct a comprehensive study of the stateof-the-art underwater image enhancement algorithms qualitatively and quantitatively. In addition, we propose an underwater image enhancement network (called Water-Net) trained on this benchmark as a baseline, which indicates the generalization of the proposed UIEB for training Convolutional Neural Networks (CNNs). The benchmark evaluations and the proposed Water-Net demonstrate the performance and limitations of state-of-the-art algorithms, which shed light on future research in underwater image enhancement. The dataset and code are available at https://li-chongyi.github.io/proj benchmark.html.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D URING the past few years, underwater image enhancement has drawn considerable attention in both image processing and underwater vision <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Due to the complicated underwater environment and lighting conditions, enhancing underwater image is a challenging problem. Usually, an underwater image is degraded by wavelength-dependent absorption and scattering including forward scattering and backward scattering <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b6">[7]</ref>. In addition, the marine snow introduces noise and increases the effects of scattering. These adverse effects reduce visibility, decrease contrast, and even introduce color casts, which limit the practical applications of underwater images and videos in marine biology and archaeology <ref type="bibr" target="#b7">[8]</ref>, marine ecological <ref type="bibr" target="#b8">[9]</ref>, to name a few <ref type="bibr" target="#b9">[10]</ref>. To solve this problem, earlier methods rely on multiple underwater images or polarization filters, while recent algorithms deal with this problem by using only information from a single image.</p><p>Despite the prolific work, both the comprehensive study and insightful analysis of underwater image enhancement algorithms remain largely unsatisfactory due to the lack of a publicly available real-world underwater image dataset. Additionally, it is practically impossible to simultaneously photograph a real underwater scene and the corresponding ground truth image for different water types. Lacking sufficient and effective training data, the performance of deep learningbased underwater image enhancement algorithms does not match the success of recent deep learning-based high-level and low-level vision problems <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b14">[15]</ref>. To advance the development of underwater image enhancement, we construct a large-scale real-world Underwater Image Enhancement Benchmark (UIEB). Several sampling images and the corresponding reference images from UIEB are presented in <ref type="figure" target="#fig_0">Fig. 1</ref>. As shown, the raw underwater images in the UIEB have diverse color ranges and degrees of contrast decrease. In contrast, the corresponding reference images are color castsfree (at least relatively genuine color) and have improved visibility and brightness. With the proposed UIEB, we carry out a comprehensive study for several state-of-the-art single underwater image enhancement algorithms both qualitatively and quantitatively, which enables insights into their performance and sheds light on future research. In addition, with the constructed UIEB, CNNs can be easily trained to improve the visual quality of an underwater image. To demonstrate this application, we propose an underwater image enhancement model (Water-Net) trained by the constructed UIEB.</p><p>The main contributions of this paper are summarized as arXiv:1901.05495v2 [cs.CV] 26 Nov 2019 follows.</p><p>? We construct a large-scale real-world underwater image enhancement benchmark (i.e., UIEB) which contains 950 real underwater images. These underwater images are likely taken under natural light, artificial light, or a mixture of natural light and artificial light. Moreover, the corresponding reference images for 890 images are provided according to laborious, time-consuming, and well-designed pairwise comparisons. UIEB provides a platform to evaluate, at least to some extent, the performance of different underwater image enhancement algorithms. It also makes supervised underwater image enhancement models which are out of the constraints of specific underwater scenes possible. ? With the constructed UIEB, we conduct a comprehensive study of the state-of-the-art single underwater image enhancement algorithms ranging from qualitative to quantitative evaluations. Our evaluation and analysis provide comprehensive insights into the strengths and limitations of current underwater image enhancement algorithms, and suggest new research directions. <ref type="bibr">?</ref> We propose a CNN model (i.e., Water-Net) trained by the UIEB for underwater image enhancement, which demonstrates the generalization of the constructed UIEB and the advantages of our Water-Net, and also motivates the development of deep learning-based underwater image enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXISTING METHODOLOGY, EVALUATION METRIC, AND DATASET: AN OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Underwater Image Enhancement Method</head><p>Exploring underwater world has become an active issue in recent years <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>. Underwater image enhancement as an indispensable step to improve the visual quality of recorded images has drawn much attention. A variety of methods have been proposed and can be organized into four groups: supplementary information-based, non-physical model-based, physical model-based, and data-driven methods. Supplementary Information-based Methods. In the earlier stage, supplementary information from multiple images <ref type="bibr" target="#b18">[19]</ref> or specialized hardware devices (e.g., polarization filtering <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref>, range-gated imaging <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and fluorescence imaging <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>) were utilized to improve the visibility of underwater images. Compared to supplementary informationbased methods, single underwater image enhancement has been proven to be more suitable for challenging situations such as dynamic scenes, and thus, gains extensive attention. Non-physical Model-based Methods. Non-physical modelbased methods aim to modify image pixel values to improve visual quality. Iqbal et al. <ref type="bibr" target="#b27">[28]</ref> stretched the dynamic pixel range in RGB color space and HSV color space to improve the contrast and saturation of an underwater image. Chani and Isa <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> modified the work of <ref type="bibr" target="#b27">[28]</ref> to reduce the over-/underenhanced regions by shaping the stretching process following the Rayleigh distribution. Ancuti et al. <ref type="bibr" target="#b30">[31]</ref> proposed an underwater image enhancement method by blending a contrastenhanced image and a color-corrected image in a multi-scale fusion strategy. In <ref type="bibr" target="#b31">[32]</ref>, a two-step approach for underwater image enhancement was proposed, which includes a color correction algorithm and a contrast enhancement algorithm.</p><p>Another line of research tries to enhance underwater images based on the Retinex model. Fu et al. <ref type="bibr" target="#b32">[33]</ref> proposed a retinex-based method for underwater image enhancement, which consists of color correction, layer decomposition, and enhancement. Zhang et al. <ref type="bibr" target="#b33">[34]</ref> proposed an extended multiscale retinex-based underwater image enhancement method. In this work, the underwater Physical Model-based Methods. Physical model-based methods regard the enhancement of an underwater image as an inverse problem, where the latent parameters of an image formation model are estimated from a given image. These methods usually follow the same pipeline: 1) building a physical model of the degradation; 2) estimating the unknown model parameters; and 3) addressing this inverse problem.</p><p>One line of research is to modify the Dark Channel Prior (DCP) <ref type="bibr" target="#b34">[35]</ref> for underwater image enhancement. In <ref type="bibr" target="#b35">[36]</ref>, DCP was combined with the wavelength-dependent compensation algorithm to restore underwater images. In <ref type="bibr" target="#b36">[37]</ref>, an Underwater Dark Channel Prior (UDCP) was proposed based on the fact that the information of the red channel in an underwater image is undependable. Based on the observation that the dark channel of the underwater image tends to be a zero map, Liu and Chau <ref type="bibr" target="#b37">[38]</ref> formulated a cost function and minimized it so as to find the optimal transmission map, which is able to maximize the image contrast. Instead of the DCP, Li et al. <ref type="bibr" target="#b38">[39]</ref> employed the random forest regression model to estimate the transmission of the underwater scenes. Recently, Peng et al. <ref type="bibr" target="#b39">[40]</ref> proposed a Generalized Dark Channel Prior (GDCP) for image restoration, which incorporates adaptive color correction into an image formation model.</p><p>Another line of research try to employ the optical properties of underwater imaging. Carlevaris-Bianca et al. <ref type="bibr" target="#b40">[41]</ref> proposed a prior that exploits the difference in attenuation among three color channels in RGB color space to predict the transmission of an underwater scene. The idea behind this prior is that the red light usually attenuates faster than the green light and the blue light in an underwater scenario. Galdran et al. <ref type="bibr" target="#b41">[42]</ref> proposed a Red Channel method, which recovers the lost contrast of an underwater image by restoring the colors associated with short wavelengths. According to the findings that the background color of underwater images has relations with the inherent optical properties of water medium, Zhao et al. <ref type="bibr" target="#b42">[43]</ref> enhanced the degraded underwater images by deriving inherent optical properties of water from the background color. Li et al. <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> proposed an underwater image enhancement method based on the minimum information loss principle and histogram distribution prior. Peng et al. <ref type="bibr" target="#b45">[46]</ref> proposed a depth estimation method for underwater scenes based on image blurriness and light absorption, which is employed to enhance underwater images. Berman et al. <ref type="bibr" target="#b46">[47]</ref> took multiple spectral profiles of different water types into account and reduced the problem of underwater image restoration to single image dehazing. Wang et al. <ref type="bibr" target="#b47">[48]</ref> combined the adaptive attenuation-curve prior with the characteristics of underwater light propagation for underwater image restoration. More recently, Akkaynak and Treibitz <ref type="bibr" target="#b48">[49]</ref> proposed an underwater image color correction method based on a revised underwater image formation model <ref type="bibr" target="#b5">[6]</ref> which is physically accurate. The existing physical model-based methods, except for the recent work <ref type="bibr" target="#b48">[49]</ref>, follow the simplified image formation models that assume the attenuation coefficients are only properties of the water and are uniform across the scene per color channel. This assumption leads to the unstable and visually unpleasing results as demonstrated in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Data-driven Methods. Recent years have witnessed the significant advance of deep learning in low-level vision problems. These methods can be trained using synthetic pairs of degraded images and high-quality counterparts. However, underwater image formation models depend on specific scenes and lighting conditions, and even are related to temperature and turbidity. Thus, it is difficult to synthesize realistic underwater images for CNNs training. Further, the learned distribution by CNNs trained on synthetic underwater images does not always generalize to real-world cases. Therefore, the performance and the amount of deep learning-based underwater image enhancement methods do not match the success of recent deep learning-based low-level vision problems <ref type="bibr" target="#b49">[50]</ref>.</p><p>Recently, Li et al. <ref type="bibr" target="#b50">[51]</ref> proposed a deep learning-based underwater image enhancement model, called WaterGAN. WaterGAN first simulates underwater images from the inair image and depth pairings in an unsupervised pipeline. With the synthetic training data, the authors use a two-stage network for underwater image restoration, especially for color casts removal. Underwater image enhancement models (i.e., UWCNNs) trained by ten types of underwater images was proposed in <ref type="bibr" target="#b51">[52]</ref>, where underwater images are synthesized based on a revised underwater image formation model <ref type="bibr" target="#b5">[6]</ref> and the corresponding underwater scene parameters. More recently, a weakly supervised underwater color transfer model <ref type="bibr" target="#b52">[53]</ref> (i.e., Water CycleGAN) was proposed based on Cycle-Consistent Adversarial Networks <ref type="bibr" target="#b53">[54]</ref>. Benefiting from the adversarial network architecture and multi-term loss function, this network model relaxes the need for paired underwater images for training and allows the underwater images being taken in unknown locations. However, it tends to produce inauthentic results in some cases due to the nature of multiple possible outputs. Guo et al. <ref type="bibr" target="#b54">[55]</ref> proposed a multiscale dense GAN (i.e., Dense GAN) for underwater image enhancement. The authors combined the non-saturating GAN loss with the 1 loss and gradient loss to learn the distribution of ground truth images in the feature domain. However, this method still cannot avoid the limitations of multiple possible outputs from GANs. Therefore, the robustness and generalization of deep learning-based underwater enhancement methods still fall behind conventional state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Underwater Image Quality Evaluation</head><p>In the following, we will give a brief introduction of the image quality evaluation metrics which are widely used for underwater image enhancement methods. Full-reference Metrics. For an underwater image with ground truth image, the full-reference image quality evaluation metrics (e.g., MSE, PSNR, and SSIM <ref type="bibr" target="#b55">[56]</ref>) were employed for evaluations. Such underwater images usually are a few color checker images or color image patches taken in the simulated or real underwater environment. For example, Zhao et al. <ref type="bibr" target="#b42">[43]</ref> treated a plastic color disk as ground truth image and captured its underwater image in a water pool as the testing image. Non-reference Metrics. Different from other low-level vision problems where the ground truth images can be easily obtained (e.g., image super-resolution), it is challenging to achieve a large amount of paired underwater images and the corresponding ground truth images. For a real-world underwater image where the ground truth image was unavailable, nonreference image quality evaluation metrics, such as image entropy, visible edges <ref type="bibr" target="#b56">[57]</ref>, and dynamic range independent image quality assessment <ref type="bibr" target="#b57">[58]</ref>, were utilized. In addition, some authors employed specific applications, like feature point matching, edge detection, and image segmentation, to evaluate their results. Besides, several specific non-reference metrics were proposed for underwater image quality evaluation. Yang and Sowmya <ref type="bibr" target="#b58">[59]</ref> proposed an underwater color image quality evaluation metric (i.e., UCIQE). UCIQE first quantifies the non-uniform color casts, blurring, and low contrast, and then combines these three components in a linear manner. In <ref type="bibr" target="#b59">[60]</ref>, the authors proposed a non-reference underwater image quality measure, called UIQM, which comprises three attribute measures: colorfulness measure, sharpness measure, and contrast measure. Each presented attribute measure is inspired by the properties of human visual system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Underwater Image Datasets</head><p>There are several real-world underwater image datasets such as Fish4Knowlege dataset for underwater target detection and recognition 1 , underwater images in SUN dataset for scene recognition and object detection 2 <ref type="bibr" target="#b60">[61]</ref>, MARIS dataset for marine autonomous robotics 3 , Sea-thru dataset including 1100 underwater image with range maps 4 <ref type="bibr" target="#b48">[49]</ref>, and Haze-line dataset providing raw images, TIF files, camera calibration files, and distance maps 5 <ref type="bibr" target="#b61">[62]</ref>. However, existing datasets usually have monotonous content and limited scenes, few degradation characteristics, and insufficient data. Moreover, these datasets did not provide the corresponding ground truth images or reference results since it is difficult or even impractical to simultaneously obtain a real underwater image and the corresponding ground truth image of the same scene due to the diverse water types and lighting conditions as well as expensive and logistically complex imaging devices. In recent years, several underwater image synthesis methods have been proposed. Li et al. <ref type="bibr" target="#b50">[51]</ref> proposed a GAN-based method <ref type="bibr" target="#b5">6</ref> while Duarte et al. <ref type="bibr" target="#b62">[63]</ref> simulated underwater image degradation using milk, chlorophyll, or green tea in a tank. Blasinski et al. <ref type="bibr" target="#b63">[64]</ref> provided an open-source underwater image simulation tool and a three parameter underwater image formation model <ref type="bibr" target="#b64">[65]</ref>. Li et al. <ref type="bibr" target="#b51">[52]</ref> proposed a synthetic underwater image dataset including ten subsets for different types of water 7 . However, there still exists a gap between synthetic and realworld underwater images. Therefore, it is challenging to evaluate the state-of-the-art methods fairly and comprehensively, and is hard to develop effective deep learning-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED BENCHMARK DATASET</head><p>After systematically reviewing previous work, we found the main issue existing in the community of underwater image enhancement is lacking a large-scale real-world underwater image dataset with reference images. In what follows, we introduce the constructed dataset in detail, including data collection and reference image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Collection</head><p>There are three objectives for underwater image collection: 1) a diversity of underwater scenes, different characteristics of quality degradation, and a broad range of image content should be covered; 2) the amount of underwater images should be large; and 3) the corresponding high-quality reference images should be provided so that pairs of images enable fair image quality evaluation and end-to-end learning. To achieve the first two objectives, we first collect a large number of underwater images, and then refine them.  These underwater images are collected from Google, YouTube, related papers <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b41">[42]</ref>, and our self-captured videos. We mainly retain the underwater images which meet the first objective. After data refinement, most of the collected images are weeded out, and about 950 candidate images are remaining. We provide a statistic of image resolutions and the scene/main object categories of the UIEB in <ref type="figure" target="#fig_1">Fig. 2</ref> and present some examples of the images in <ref type="figure" target="#fig_2">Fig. 3</ref>. As shown in Figs. 2 and 3, the UIEB contains a large range of image resolutions and spans diverse scene/main object categories including coral (e.g., fringing reefs and barrier reefs), marine life (e.g., turtles and sharks), etc. To achieve the third objective, we introduce a high-quality reference image generation method in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reference Image Generation</head><p>With the candidate underwater images, the potential reference images are generated by 12 image enhancement methods, including 9 underwater image enhancement methods (i.e., fusion-based <ref type="bibr" target="#b30">[31]</ref>, two-step-based <ref type="bibr" target="#b31">[32]</ref>, retinex-based <ref type="bibr" target="#b32">[33]</ref>, UDCP <ref type="bibr" target="#b36">[37]</ref>, regression-based <ref type="bibr" target="#b38">[39]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, and blurriness-based <ref type="bibr" target="#b45">[46]</ref>), 2 image dehazing methods (i.e., DCP <ref type="bibr" target="#b34">[35]</ref> and MSCNN <ref type="bibr" target="#b65">[66]</ref>), and 1 commercial application for enhancing underwater images (i.e., dive+ <ref type="bibr" target="#b7">8</ref> ). We exclude the recent deep learning-based methods due to their limited generalization capability to the diverse real-world underwater images and the fixed size of network output <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b54">[55]</ref>. The source codes of all the employed methods except the fusion-based <ref type="bibr" target="#b30">[31]</ref> are provided by their authors. We reimplement the fusion-based <ref type="bibr" target="#b30">[31]</ref> since the source code is unavailable. For the dive+, we tune its parameter settings to generate satisfactory results. At last, we totally generate 12 ? 950 enhanced results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw Underwater Image</head><p>Method A Method B <ref type="figure">Fig. 4</ref>. An example of pairwise comparisons. Treating the raw underwater image as a reference, a volunteer needs to independently decide which one is better between the results of method A and method B.</p><p>With raw underwater images and the enhanced results, we invite 50 volunteers (25 volunteers with image processing experience; 25 volunteers without related experience) to perform pairwise comparisons among the 12 enhanced results of each raw underwater image under the same monitor.</p><p>Specifically, each volunteer is shown a raw underwater image and a set of enhanced result pairs. The enhanced image pairs are drawn from all the competitive methods randomly, and the result winning the pairwise comparisons will be compared again in the next round, until the best one is selected. There is no time constraint for volunteers and zoomin operation is allowed. An example of pairwise comparisons is shown in <ref type="figure">Fig. 4</ref>. For each pair of enhanced results, taking the raw underwater image as a reference, a volunteer first needs to independently decide which one is better than the other. For each volunteer, the best result will be selected after 11 pairwise comparisons.</p><p>Additionally, the volunteer needs to inspect the best result again and then label the best result as being satisfactory or dissatisfactory. The reference image for a raw underwater image is first selected by majority voting after pairwise comparisons. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Percentage (%) fusion-based <ref type="bibr" target="#b30">[31]</ref> 24.72 two-step-based <ref type="bibr" target="#b31">[32]</ref> 7.30 retinex-based <ref type="bibr" target="#b32">[33]</ref> 0.22 DCP <ref type="bibr" target="#b34">[35]</ref> 2.58 UDCP <ref type="bibr" target="#b36">[37]</ref> 0.00 regression-based <ref type="bibr" target="#b38">[39]</ref> 1.80 GDCP <ref type="bibr" target="#b39">[40]</ref> 0.34 Red Channel <ref type="bibr" target="#b41">[42]</ref> 0.90 histogram prior <ref type="bibr" target="#b44">[45]</ref> 13.37 blurriness-based <ref type="bibr" target="#b45">[46]</ref> 3.48 MSCNN <ref type="bibr" target="#b65">[66]</ref> 0.90 dive+ <ref type="bibr">43.93</ref> After that, if the selected reference image has greater than half the number of votes labeled dissatisfaction, its corresponding raw underwater image is treated as a challenging image and the reference image is discarded. We totally achieve 890 available reference images which have higher quality than any individual methods and a challenging set including 60 underwater images. To visualize the process of reference image generation, we present some cases that the results of some methods are shown and indicate which one is the final reference image in <ref type="figure">Fig. 5</ref>. Furthermore, the percentage of the reference images from the results of different methods is presented in <ref type="table" target="#tab_1">Table I</ref>. In this paper, we highlight the top one performance in red, whereas the second top one is in blue. In summary, the results with improved contrast and genuine color are most favored by observers while the over-/underenhancement, artifacts, and color casts lead to visually unpleasing results. Finally, the constructed UIEB includes two subsets: 890 raw underwater images with the corresponding high-quality reference images; 60 challenging underwater images. To the best of our knowledge, it is the first real-world underwater image dataset with reference images so far. The UIEB has various potential applications, such as performance evaluation and CNNs training. Next, we will introduce these two applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION AND DISCUSSION</head><p>A comprehensive and fair evaluation of underwater image enhancement methods has long been missing from the literatures. Using the constructed UIEB, we evaluate the state-ofthe-art underwater image enhancement methods (i.e., fusionbased <ref type="bibr" target="#b30">[31]</ref>, two-step-based <ref type="bibr" target="#b31">[32]</ref>, retinex-based <ref type="bibr" target="#b32">[33]</ref>, UDCP <ref type="bibr" target="#b36">[37]</ref>, regression-based <ref type="bibr" target="#b38">[39]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, blurriness-based <ref type="bibr" target="#b45">[46]</ref>) both qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Qualitative Evaluation</head><p>We first select several underwater images from the UIEB, and then divide these images into five categories: greenish and bluish images, downward looking images, forward looking images, low backscatter scenes, and high backscatter scenes. The results of different methods and the corresponding reference raws fusion-based retinex-based UDCP Red Channel histogram prior blurriness-based GDCP dive+ <ref type="figure">Fig. 5</ref>. Results generated by different methods. From left to right are raw underwater images, and the results of fusion-based <ref type="bibr" target="#b30">[31]</ref>, retinex-based <ref type="bibr" target="#b32">[33]</ref>, UDCP <ref type="bibr" target="#b36">[37]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, blurriness-based <ref type="bibr" target="#b45">[46]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>  In open water, the red light first disappears because of its longest wavelength, followed by the green light and then the blue light <ref type="bibr" target="#b6">[7]</ref>. Such selective attenuation in open water results in bluish or greenish underwater images, such as the raw underwater images in <ref type="figure">Fig. 6</ref>. Color deviation seriously affects the visual quality of underwater images and is difficult to be removed. As shown, the fusion-based <ref type="bibr" target="#b30">[31]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, and regression-based <ref type="bibr" target="#b38">[39]</ref> introduce reddish color deviation due to the inaccurate color correction algorithms used in <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b38">[39]</ref> and the histogram distribution prior <ref type="bibr" target="#b44">[45]</ref>. The retinex-based <ref type="bibr" target="#b32">[33]</ref> removes the color deviation well, while the UDCP <ref type="bibr" target="#b36">[37]</ref> and GDCP <ref type="bibr" target="#b39">[40]</ref> aggravate the effect of color casts. The two-step-based <ref type="bibr" target="#b31">[32]</ref> can effectively increase the contrast of underwater images. The Red Channel <ref type="bibr" target="#b41">[42]</ref> and blurrinessbased <ref type="bibr" target="#b45">[46]</ref> have less positive effect on these two images on account of the limitations of the priors used in these two methods.</p><p>For the downward looking images and forward looking images shown in <ref type="figure" target="#fig_3">Figs. 7 and 8</ref>, the fusion-based <ref type="bibr" target="#b30">[31]</ref>, retinexbased <ref type="bibr" target="#b32">[33]</ref>, and histogram prior <ref type="bibr" target="#b44">[45]</ref> significantly remove the effect of haze on the underwater images while the twostep-based <ref type="bibr" target="#b31">[32]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, and blurriness-based <ref type="bibr" target="#b45">[46]</ref> remain some haze in the results. We note that UDCP <ref type="bibr" target="#b36">[37]</ref>, regression-based <ref type="bibr" target="#b38">[39]</ref>, and GDCP <ref type="bibr" target="#b39">[40]</ref> tend to bring in color deviation in the enhanced results. In terms of the physicalmodel based methods (e.g., UDCP <ref type="bibr" target="#b36">[37]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, regression-based <ref type="bibr" target="#b38">[39]</ref>, blurriness-based <ref type="bibr" target="#b45">[46]</ref>, and GDCP <ref type="bibr" target="#b39">[40]</ref>), it is hard to estimate the veiling light from the downward looking images accurately. In addition, the physical-model based methods may incorrectly estimate the veiling light from the RGB values of water in textures regions of forward looking images</p><p>Backscatter reduces the contrast and produces the foggy veiling in an underwater image. The effect of backscatter is related to the distance between the camera and the scene. For the low backscatter scenes (short distance between the camera and the scene) in <ref type="figure">Fig. 9</ref>, the effect of backscatter is relatively easy to be removed. In contrast, the high backscatter (long distance) significantly degrades the visual quality of underwater images. As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, all the physical model-based methods (e.g., UDCP <ref type="bibr" target="#b36">[37]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, regression-based <ref type="bibr" target="#b38">[39]</ref>, blurriness-based <ref type="bibr" target="#b45">[46]</ref>, and GDCP <ref type="bibr" target="#b39">[40]</ref>) cannot remove the high backscatter due to the inaccurate physical models and assumptions used in these methods.</p><p>In summary, the fusion-based <ref type="bibr" target="#b30">[31]</ref> has relatively decent performance on a variety of underwater images. The method of UDCP <ref type="bibr" target="#b36">[37]</ref> tends to produce artifacts on enhanced results in some cases. Other competitors are effective to some extent. In fact, it is almost impossible for a color correction algorithm or a kind of prior effective for all types of underwater images. Moreover, the effect of high backscatter is challenging for underwater image enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative Evaluation</head><p>To quantitatively evaluate the performance of different methods, we perform the full-reference evaluation, nonreference evaluation, and runtime evaluation.</p><p>1) Full-reference Evaluation: We first conduct a fullreference evaluation using three commonly-used metrics (i.e., MSE, PSNR, and SSIM). The results of full-reference image quality evaluation by using the reference images can provide realistic feedback of the performance of different methods to some extent, although the real ground truth images might be different from the reference images. A higher PSNR score and a lower MSE score denote the result is closer to the reference image in terms of image content, while a higher SSIM score means the result is more similar to the reference image in terms of image structure and texture. We present the average scores of different methods on the 890 images with reference images in the UIEB. As shown in <ref type="table" target="#tab_1">Table II</ref>, the dive+ stands out as the best performer across all metrics. In addition, the fusion-based <ref type="bibr" target="#b30">[31]</ref> ranks the second best in terms of the fullreference metrics. It is reasonable for such results, since most reference images are selected from the results generated by dive+ and fusion-based <ref type="bibr" target="#b30">[31]</ref>.</p><p>2) Non-reference Evaluation: We employ two nonreference metrics (i.e., UCIQE <ref type="bibr" target="#b58">[59]</ref> and UIQM <ref type="bibr" target="#b59">[60]</ref>) which are usually used for underwater image quality evaluation <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. A higher UCIQE score indicates the result has better balance among the chroma, saturation, and raws fusion-based retinex-based UDCP Red Channel histogram prior blurriness-based GDCP reference images two-step-based regression-based <ref type="figure">Fig. 6</ref>. Subjective comparisons on bluish and greenish underwater images. From left to right are raw underwater images, and the results of fusion-based <ref type="bibr" target="#b30">[31]</ref>, retinex-based <ref type="bibr" target="#b32">[33]</ref>, two-step-based <ref type="bibr" target="#b31">[32]</ref>, UDCP <ref type="bibr" target="#b36">[37]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, regression-based <ref type="bibr" target="#b38">[39]</ref>, blurriness-based <ref type="bibr" target="#b45">[46]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>, and reference images. , retinex-based <ref type="bibr" target="#b32">[33]</ref>, two-step-based <ref type="bibr" target="#b31">[32]</ref>, UDCP <ref type="bibr" target="#b36">[37]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, regression-based <ref type="bibr" target="#b38">[39]</ref>, blurriness-based <ref type="bibr" target="#b45">[46]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>, and reference images. , retinex-based <ref type="bibr" target="#b32">[33]</ref>, two-step-based <ref type="bibr" target="#b31">[32]</ref>, UDCP <ref type="bibr" target="#b36">[37]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, regression-based <ref type="bibr" target="#b38">[39]</ref>, blurriness-based <ref type="bibr" target="#b45">[46]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>, and reference images. raws fusion-based retinex-based UDCP Red Channel histogram prior blurriness-based GDCP reference images two-step-based regression-based <ref type="figure">Fig. 9</ref>. Subjective comparisons on low backscatter scenes. From left to right are raw underwater images, and the results of fusion-based <ref type="bibr" target="#b30">[31]</ref>, retinex-based <ref type="bibr" target="#b32">[33]</ref>, two-step-based <ref type="bibr" target="#b31">[32]</ref>, UDCP <ref type="bibr" target="#b36">[37]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, regression-based <ref type="bibr" target="#b38">[39]</ref>, blurriness-based <ref type="bibr" target="#b45">[46]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>, and reference images. <ref type="figure" target="#fig_0">Fig. 10</ref>. Subjective comparisons on high backscatter scenes. From left to right are raw underwater images, and the results of fusion-based <ref type="bibr" target="#b30">[31]</ref>, retinex-based <ref type="bibr" target="#b32">[33]</ref>, two-step-based <ref type="bibr" target="#b31">[32]</ref>, UDCP <ref type="bibr" target="#b36">[37]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, regression-based <ref type="bibr" target="#b38">[39]</ref>, blurriness-based <ref type="bibr" target="#b45">[46]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>, and reference images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>raws fusion-based retinex-based UDCP Red Channel histogram prior blurriness-based GDCP reference images two-step-based regression-based</head><p>contrast, while a higher UIQM score indicates the result is more consistent with human visual perception. The average scores are shown in <ref type="table" target="#tab_1">Table III</ref>.</p><p>In <ref type="table" target="#tab_1">Table III</ref>, the histogram prior <ref type="bibr" target="#b44">[45]</ref> and UDCP <ref type="bibr" target="#b36">[37]</ref> obtain the highest scores of UCIQE and UIQM, respectively. The dive+ and fusion-based <ref type="bibr" target="#b30">[31]</ref> are no more the best performers. It  <ref type="bibr" target="#b30">[31]</ref> 0.6414 1.5310 two-step-based <ref type="bibr" target="#b31">[32]</ref> 0.5776 1.4002 retinex-based <ref type="bibr" target="#b32">[33]</ref> 0.6062 1.4338 UDCP <ref type="bibr" target="#b36">[37]</ref> 0.5852 1.6297 regression-based <ref type="bibr" target="#b38">[39]</ref> 0.5971 1.2996 GDCP <ref type="bibr" target="#b39">[40]</ref> 0.5993 1.4301 Red Channel <ref type="bibr" target="#b41">[42]</ref> 0.5421 1.2147 histogram prior <ref type="bibr" target="#b44">[45]</ref> 0.6778 1.5440 blurriness-based <ref type="bibr" target="#b45">[46]</ref> 0.6001 1.3757 dive+ 0.6227 <ref type="bibr">1.3410</ref> is interesting that the good performers in terms of UCIQE and UIQM metrics are not consistent with the subjective pairwise comparisons, though both UCIQE and UIQM claim that they take the human visual perception into account. Such a result provides evidence that the current image quality evaluation metrics designed for underwater image are inconsistent with human visual perception in some cases. This is because humans have not evolved to see in aquatic habitats. When they are shown an underwater photo, they are most likely to pay attention to objects in the center of the scene, or whatever seems to be colorful or interesting (e.g., diver, fish, coral, etc). Thus, human visual perception may be a totally inaccurate way of color correcting underwater images. It might be fine for visually pleasing images, but not to learn attenuation and backscatter. Furthermore, Figs. 6-10 show the results generated by histogram prior <ref type="bibr" target="#b44">[45]</ref> and UDCP <ref type="bibr" target="#b36">[37]</ref> still suffer from color casts and over-enhancement. Through further analyzing, we found these two non-reference metrics might be biased to some characteristics (not entire image) and did not take the color shift and artifacts into account. For example, the results with high contrast (e.g., the results of histogram prior <ref type="bibr" target="#b44">[45]</ref>) are usually favored by the UICQE metric. To illustrate this phenomenon, we present an example in <ref type="figure" target="#fig_0">Fig. 11</ref>.</p><p>In <ref type="figure" target="#fig_0">Fig. 11</ref>, the results generated by histogram prior <ref type="bibr" target="#b44">[45]</ref> have obvious reddish color shift and artifacts; however, they obtain better quantitative scores in terms of UCIQE and UIQM metrics than the results of dive+. Thus, we believe there is a gap between the quantitative scores of non-reference metrics and the subjectively visual quality. In other words, the current image quality evaluation metrics designed for underwater image have limitations in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Runtime Evaluation:</head><p>We compare the average runtime for the images of different sizes. Experiments are conducted by using MATLAB R2014b on a PC with an Intel(R) i7-6700 CPU, 32GB RAM. The average runtime is shown in <ref type="table" target="#tab_1">Table IV</ref>. The two-step-based <ref type="bibr" target="#b31">[32]</ref> is the fastest across different image sizes, while the retinex-based <ref type="bibr" target="#b32">[33]</ref> ranks the second fastest. The regression-based <ref type="bibr" target="#b38">[39]</ref> is the slowest method due to the time-consuming random forest-based transmission prediction, especially for images with large sizes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>500 ? 500 640 ? 480 1280 ? 720 fusion-based <ref type="bibr" target="#b30">[31]</ref> 0.6044 0.6798 1.8431 two-step-based <ref type="bibr" target="#b31">[32]</ref> 0.2978 0.4391 1.0361 retinex-based <ref type="bibr" target="#b32">[33]</ref> 0.6975 0.8829 2.1089 UDCP <ref type="bibr" target="#b36">[37]</ref> 2.2688 3.3185 9.9019 regression-based <ref type="bibr" target="#b38">[39]</ref> 138.6138 167.1711 415.4935 GDCP <ref type="bibr" target="#b39">[40]</ref> 3.2676 3.8974 9.5934 Red Channel <ref type="bibr" target="#b41">[42]</ref> 2.7523 3.2503 9.7447 histogram prior <ref type="bibr" target="#b44">[45]</ref> 4.6284 5.8289 16.9229 blurriness-based <ref type="bibr" target="#b45">[46]</ref> 37.0018 47.2538 146.0233</p><p>After reviewing and evaluating the state-of-the-art underwater image enhancement methods, we found that the fusionbased <ref type="bibr" target="#b30">[31]</ref> is the relatively best performer in most cases, while other compared methods have obvious disadvantages. However, there is no method which always wins when facing a large-scale real-world underwater image dataset (i.e., UIEB). All in all, due to neglecting the underwater imaging physical models, the non-physical model-based methods, such as two-step-based <ref type="bibr" target="#b31">[32]</ref> and retinex-based <ref type="bibr" target="#b32">[33]</ref>, produce over-/under-enhanced results. Physical model-based methods, such as UDCP <ref type="bibr" target="#b36">[37]</ref>, employ an outdoor haze formation based model to predict the medium transmission which is not wellsuited for the underwater scenario. Inaccurate physical models and assumptions result in color casts and remaining haze in the results such as regression-based <ref type="bibr" target="#b38">[39]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>, Red Channel <ref type="bibr" target="#b41">[42]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, and blurriness-based <ref type="bibr" target="#b45">[46]</ref>. Some methods, such as retinex-based <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReLU</head><p>ReLU ReLU <ref type="figure" target="#fig_0">Fig. 12</ref>. An overview of the proposed Water-Net architecture. Water-Net is a gated fusion network, which fuses the inputs with the predicted confidence maps to achieve the enhanced result. The inputs are first transferred to the refined inputs by the Feature Transformation Units (FTUs) and then the confidence maps are predicted. At last, the enhanced result is achieved by fusing the refined inputs and the corresponding confidence maps.</p><p>prior <ref type="bibr" target="#b44">[45]</ref>, tend to introduce noise and artifacts, which leads to visually unpleasing results. The runtime of some methods seriously limits their practical applications.</p><p>In the future, a comprehensive method that can robustly deal with a variety of underwater image degradation is expected. The non-reference metrics which are more effective and consistent with human visual perception are desired in the community of underwater image enhancement. More discussions will be provided in Sec. VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PROPOSED MODEL</head><p>Despite the remarkable progress of underwater image enhancement methods, the generalization of deep learning-based underwater image enhancement models still falls behind the conventional state-of-the-art methods due to the lack of effective training data and well-designed network architectures. With the UIEB, we propose a CNN model for underwater image enhancement, called Water-Net. The purpose of the proposed Water-Net as a baseline is to call for the development of deep learning-based underwater image enhancement, and demonstrate the generalization of the UIEB for training CNNs. Note that the proposed Water-Net is only a baseline model which can be further improved by well-designed network architectures, task-related loss functions, and the like.</p><p>In this section, we first present input generation from an underwater image and the architecture of the proposed Water-Net. Then we present the training and implementation details. At last, we perform experiments to demonstrate its advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Input Generation</head><p>As discussed in Sec. IV, there is no algorithm generalized to all types of underwater images due to the complicated underwater environment and lighting conditions. In general, the fusion-based <ref type="bibr" target="#b30">[31]</ref> achieves decent results, which benefits from the inputs derived by multiple pre-processing operations and a fusion strategy. In the proposed Water-Net, we also employ such a manner. Based on the characteristics of underwater image degradation, we generate three inputs by respectively applying White Balance (WB), Histogram Equalization (HE) and Gamma Correction (GC) algorithms to an underwater image. Specifically, WB algorithm is used to correct the color casts, while HE and GC algorithms aim to improve the contrast and lighten up dark regions, respectively. We directly employ the WB algorithm proposed in <ref type="bibr" target="#b30">[31]</ref>, whose effectiveness has been turned out. For the HE algorithm, we apply the adapthisteq function <ref type="bibr" target="#b66">[67]</ref> provided by MATLAB to the L component in Lab color space, and then transform back into RGB color space. We set the Gamma value of GC algorithm to 0.7 empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>Water-Net employs a gated fusion network architecture to learn three confidence maps which will be used to combine the three input images into an enhanced result. The learned confidence maps determine the most significant features of inputs remaining in the final result. The impressive performance of the fusion-based underwater image enhancement method <ref type="bibr" target="#b30">[31]</ref> also encourages us to explore the fusion-based networks.</p><p>The architecture of the proposed Water-Net and parameter settings are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. As a baseline model, the Water-Net is a plain fully CNN. We believe that the widely used backbones such as the U-Net architecture <ref type="bibr" target="#b67">[68]</ref> and the residual network architecture <ref type="bibr" target="#b68">[69]</ref> can be incorporated to improve the performance. We feed the three derived inputs and original input to the Water-Net to predict the confidence maps. Before performing fusion, we add three Feature Transformation Units (FTUs) to refine the three inputs. The purpose of the FTU is to reduce the color casts and artifacts introduced by the WB, HE, and GC algorithms. At last, the refined three inputs are multiplied by the three learned confidence maps to achieve the final enhanced result:</p><formula xml:id="formula_0">I en = R W B C W B + R HE C HE + R GC C GC , (1)</formula><p>where I en is the enhanced result;</p><p>indicates the elementwise production of matrices; R W B , R HE , and R GC are the refined results of input after processing by WB, HE, and GC algorithms, respectively; C W B , C HE , and C GC are the learned confidence maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementations</head><p>A random set of 800 pairs of the images extracted from the UIEB is used to generate the training set. We resize the training data to size 112?112 due to our limited memory. Flipping and rotation are used to obtain 7 augmented versions of original training data. Resizing the training data to a fixed size is widely used in deep learning, such as image dehazing <ref type="bibr" target="#b65">[66]</ref>, salient object detection <ref type="bibr" target="#b69">[70]</ref>, etc. In contrast to image super-resolution (e.g., <ref type="bibr" target="#b70">[71]</ref>) that has the same degradation in different regions of an input image, different regions of an underwater image may have different degradation. Thus, the contextual information of an underwater image is important for network optimization. This is the main reason why we do not use image patches to train our network. The rest 90 pairs of the images in the UIEB are treated as the testing set.</p><p>To reduce the artifacts induced by pixel-wise loss functions such as 1 and 2 , we minimize the perceptual loss function to learn the mapping function of underwater image enhancement. The perceptual loss can produce visually pleasing and realistic results, which has been widely used in image restoration and synthesis networks, such as image super-resolution <ref type="bibr" target="#b71">[72]</ref>, photographic image synthesis <ref type="bibr" target="#b72">[73]</ref>, etc.</p><p>Inspired by <ref type="bibr" target="#b71">[72]</ref>, we define the perceptual loss based on the ReLU activation layers (i.e., layer relu5 4) of the pretrained 19 layers VGG network <ref type="bibr" target="#b73">[74]</ref>. Let ? j (x) be the jth convolution layer (after activation) of the VGG19 network ? pretrained on the ImageNet dataset <ref type="bibr" target="#b74">[75]</ref>. The perceptual loss is expressed as the distance between the feature representations of the enhanced image I en and the reference image I gt :</p><formula xml:id="formula_1">L ? j = 1 C j H j W j N i=1 ? j (I i en ) ? ? j (I i gt ) ,<label>(2)</label></formula><p>where N is the number of each batch in the training procedure; C j H j W j represents the dimension of the feature maps of the jth convolution layer within the VGG19 network. C j , H j , and W j are the number, height, and width of the feature map. We implemented the proposed Water-Net with TensorFlow on a PC with an Nvidia 1080Ti GPU. During training, a batch-mode learning method with a batch size of 16 was applied. The filter weights of each layer were initialized by standard Gaussian distribution. Bias was initialized as a constant. We used ADAM with default parameters for our network optimization. We initialized the learning rate to 1e ?3 and decreased the learning rate by 0.1 every 10,000 iterations until the Water-Net converges. Our Water-Net can process an image with a size of 640 ? 480 within 0.128s (8FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments</head><p>To demonstrate the advantages achieved by the proposed Water-Net, we compare it against several state-of-the-art underwater image enhancement methods. The experiments are conducted on the testing set which includes 90 underwater images and the challenging set including 60 underwater images. We show several results in <ref type="figure" target="#fig_0">Figs. 13 and 14</ref>.</p><p>In <ref type="figure" target="#fig_0">Fig. 13</ref>, the proposed Water-Net effectively removes the haze on the underwater images and remits color casts, while the competing methods introduce unexpected colors (e.g., fusion-based <ref type="bibr" target="#b30">[31]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, Water CycleGAN <ref type="bibr" target="#b52">[53]</ref>, and Dense GAN <ref type="bibr" target="#b54">[55]</ref>) and artifacts (e.g., fusion-based <ref type="bibr" target="#b30">[31]</ref>, retinex-based <ref type="bibr" target="#b32">[33]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, Water CycleGAN <ref type="bibr" target="#b52">[53]</ref>, and Dense GAN <ref type="bibr" target="#b54">[55]</ref>) or have little effect on inputs (e.g., blurriness-based <ref type="bibr" target="#b45">[46]</ref>). In addition, it is interesting that our results even achieve better visual quality than the corresponding reference images (e.g., more natural appearance and better details). This is because that the perceptual loss optimized Water-Net can learn the potential attributes of good visual quality from the large-scale real-world underwater image dataset. For the results on challenging set shown in <ref type="figure" target="#fig_0">Fig. 14, the</ref> proposed Water-Net produces visually pleasing results. By contrast, other methods tend to introduce artifacts, over-enhancement (e.g., foregrounds), and color casts (e.g., reddish or greenish color).  <ref type="table" target="#tab_7">Table V</ref> reports the quantitative results of different methods in terms of MSE, PSNR, and SSIM on the testing set. The quantitative results are obtained by comparing the result of each method with the corresponding reference image. We discard the non-reference metrics designed for underwater image enhancement based on the conclusion drawn in Sec. IV. Our Water-Net achieves the best performance in terms of full-reference image quality assessment. In addition, instead of pairwise comparison, we conduct a user study to score the visual quality of the results on challenging set. This is because some images in the challenging set are too difficult to obtain satisfactory results following the procedure of reference image generation. Thus, we invited 50 participants (the same volunteers with the reference image generation) to score results. The scores have five scales ranging from 5 to 1 which represent "Excellent", "Good", "Fair", "Poor" and "Bad", respectively. The average scores of the results by each fusion-based retinex-based histogram prior GDCP Water-Net raws blurriness-based reference images Dense GAN Water CycleGAN <ref type="figure" target="#fig_0">Fig. 13</ref>. Subjective comparisons on underwater images from testing set. From left to right are raw underwater images, and the results of fusion-based <ref type="bibr" target="#b30">[31]</ref>, retinex-based <ref type="bibr" target="#b32">[33]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, blurriness-based <ref type="bibr" target="#b45">[46]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>, Water CycleGAN <ref type="bibr" target="#b52">[53]</ref>, Dense GAN <ref type="bibr" target="#b54">[55]</ref>, the proposed Water-Net, and reference images. , retinex-based <ref type="bibr" target="#b32">[33]</ref>, histogram prior <ref type="bibr" target="#b44">[45]</ref>, blurriness-based <ref type="bibr" target="#b45">[46]</ref>, GDCP <ref type="bibr" target="#b39">[40]</ref>, Water CycleGAN <ref type="bibr" target="#b52">[53]</ref>, Dense GAN <ref type="bibr" target="#b54">[55]</ref>, and the proposed Water-Net. method on challenging set are shown in <ref type="table" target="#tab_1">Table VI</ref>. Besides, we also provide the standard deviation of the results by each method on challenging set. We exclude the scores of Water CycleGAN <ref type="bibr" target="#b52">[53]</ref> and Dense GAN <ref type="bibr" target="#b54">[55]</ref> due to their obviously unpleasing results as shown in <ref type="figure" target="#fig_0">Fig. 14.</ref> Our Water-Net receives the highest average score and lowest standard deviation, which indicates our method produces better results from a subjective perspective and has more robust performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average Score ? Standard Deviation ? fusion-based <ref type="bibr" target="#b30">[31]</ref> 2.28 0.8475 retinex-based <ref type="bibr" target="#b32">[33]</ref> 2.23 0.8720 GDCP <ref type="bibr" target="#b39">[40]</ref> 1.90 0.8099 histogram prior <ref type="bibr" target="#b44">[45]</ref> 2.08 0.7897 blurriness-based <ref type="bibr" target="#b45">[46]</ref> 2.02 0.7762 Water-Net 2.57 0.7280</p><p>Qualitative and quantitative experiments demonstrate the effectiveness of the proposed Water-Net and also indicate the constructed dataset can be used for training CNNs. However, there is room for the improvement of underwater image enhancement. Besides, underwater images in the challenging set still cannot be enhanced well.</p><p>VI. CONCLUSION, LIMITATIONS, AND FUTURE WORK In this paper, we have constructed an underwater image enhancement benchmark dataset which offers large-scale real underwater images and the corresponding reference images. This benchmark dataset enables us to comprehensively study the existing underwater image enhancement methods, and easily train CNNs for underwater image enhancement. As analyzed in qualitative and quantitative evaluations, there is no method which always wins in terms of full-and no-reference metrics. In addition, effective non-reference underwater image quality evaluation metrics are highly desirable. To promote the development of deep learning-based underwater image enhancement methods, we proposed an underwater image enhancement CNN trained by the constructed dataset. Experimental results demonstrate the proposed CNN model performs favorably against the state-of-the-art methods, and also verify the generalization of the constructed dataset for training CNNs.</p><p>Although our reference image generation strategy can select visually pleasing results, there is a problem that affects the selection of reference images and further limits the performance of our network. Specifically, the effect of backscatter is difficult to be completely removed, especially for the backscatter in far distances. We use the state-of-the-art image enhancement algorithms to process the raw underwater images; however, the backscatter still cannot be completely removed in some cases. Despite our constructed dataset is not dominated by such underwater images, the backscatter is always discouraged in the reference images. Moreover, some invited volunteers cannot identify the exponentially increasing effect of backscatter in scenes with large ranges. In summary, the main reasons for the shortcomings of our reference image generation are: 1) the existing algorithms follow inaccurate image formation models or assumptions which inherently limit the performance of underwater image enhancement; and 2) some volunteers do not understand the underwater imaging physical model, thus they may ignore the effect of the presence of backscatter in far ranges. Note that the use of inaccurate imaging models is a major problem which keeps the field of underwater computer vision at standstill. Fortunately, the recent work <ref type="bibr" target="#b48">[49]</ref> sheds light on the future research of underwater vision.</p><p>In future work, we will extend the constructed dataset towards more challenging underwater images and underwater videos. Moreover, we will try to design a range map estimation network. The provided 1100 underwater images with range maps in <ref type="bibr" target="#b48">[49]</ref> could be used for the range map estimation network training. With the estimated range maps, we will make full use of such key prior information to further improve the performance of underwater image enhancement network. Besides, inspired by recent work <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b48">[49]</ref>, we believe that more physically reasonable underwater image enhancement algorithms will arise. At that time, we will re-organize the selection of the reference images from more reliable results and also further train the volunteers on what the degrading effects of attenuation and backscatter are, and what it looks like when either is improperly corrected. Additionally, the main purpose of constructing the real-world underwater dataset in this paper is to evaluate the state-of-the-art underwater image enhancement methods and provide paired training data for deep models. Since the full-reference metrics and training a deep model only need a single reference, we do not select multiple references or define the image quality level. However, the image quality level of multiple reference images does help in underwater image enhancement. Thus, we will provide multiple reference images for an underwater image and define the image quality level of their reference images when we re-organize the selection of the reference images. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Sampling images from UIEB. Top row: raw underwater images taken in diverse underwater scenes; Bottom row: the corresponding reference results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Statistics of the constructed UIEB. (a) Image resolutions. (b) Scene/main object categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of the images in UIEB. These images have obvious characteristics of underwater image quality degradation (e.g., color casts, decreased contrast, and blurring details) and are taken in a diversity of underwater scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Subjective comparisons on downward looking images. From left to right are raw underwater images, and the results of fusion-based<ref type="bibr" target="#b30">[31]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Subjective comparisons on forward looking images. From left to right are raw underwater images, and the results of fusion-based<ref type="bibr" target="#b30">[31]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 14 .</head><label>14</label><figDesc>Subjective comparisons on underwater images from challenging set. From left to right are raw underwater images, and the results of fusion-based<ref type="bibr" target="#b30">[31]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Chunle</head><label></label><figDesc>Guo received his B.S. degree from School of Electronic Information Engineering in Tianjin University. He is currently pursuing his Ph.D. degree with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China. His current research focuses on image processing and computer vision, particularly in the domains of deep learning-based image restoration and enhancement. Wenqi Ren is an assistant professor in Institute of Information Engineering, Chinese Academy of Sciences, China. He received his Ph.D. degree from Tianjin University in 2017. During 2015 to 2016, he was a joint-training Ph.D. student in Electrical Engineering and Computer Science at the University of California, Merced, CA, USA. His research interest includes image/video analysis and enhancement, and related vision problems. Runmin Cong (M'19) received the Ph.D. degree in information and communication engineering from Tianjin University, Tianjin, China, in June 2019. He is currently an Associate Professor with the Institute of Information Science, Beijing Jiaotong University, Beijing, China. He was a visiting student at Nanyang Technological University (NTU), Singapore, from Dec. 2016 to Feb. 2017. Since May 2018, he has spent one year as a Research Associate at the Department of Computer Science, City University of Hong Kong (CityU), Hong Kong. His research interests include computer vision and intelligent video analysis, multimedia information processing, saliency detection and segmentation, remote sensing image interpretation, and deep learning. He is a Reviewer for the IEEE TIP, TMM, and TCSVT, etc, and is a Guest Editor of special issues for the Signal Processing: Image Communication. Dr. Cong was a recipient of the Best Student Paper Runner-Up at IEEE ICME in 2018, the First Prize for Scientific and Technological Progress Award of Tianjin Municipality, and the Excellent Doctoral Degree Dissertation Award from BSIG. Junhui Hou (S'13-M'16) received the B.Eng. degree in information engineering (Talented Students Program) from the South China University of Technology, Guangzhou, China, in 2009, the M.Eng. degree in signal and information processing from Northwestern Polytechnical University, Xian, China, in 2012, and the Ph.D. degree in electrical and electronic engineering from the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, in 2016. He has been an Assistant Professor with the Department of Computer Science, City University of Hong Kong, since 2017. His research interests fall into the general areas of visual signal processing, such as image/video/3D geometry data representation, processing and analysis, semisupervised/unsupervised data modeling for clustering/classification, and data compression and adaptive transmission. Dr. Hou was the recipient of several prestigious awards, including the Chinese Government Award for Outstanding Students Study Abroad from China Scholarship Council in 2015, and the Early Career Award from the Hong Kong Research Grants Council in 2018. He is serving/served as an Associate Editor for The Visual Computer, an Area Editor for Signal Processing: Image Communication, the Guest Editor for the IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, the Journal of Visual Communication and Image Representation, and Signal Processing: Image Communication, and an Area Chair of ACM International Conference on Multimedia (ACM MM) 2019 and IEEE International Conference on Multimedia &amp; Expo (IEEE ICME) 2020. Sam Kwong (M'93-SM'04-F'13) received the B.S. degree in electrical engineering from The State University of New York at Buffalo, Buffalo, NY, USA, in 1983, the M.S. degree from the University of Waterloo, Waterloo, ON, Canada, in 1985, and the Ph.D. degree from the University of Hagen, Germany, in 1996. From 1985 to 1987, he was a Diagnostic Engineer with Control Data Canada, Mississauga, ON, Canada. He was a Member of Scientific Staff with Bell Northern Research Canada, Ottawa. In 1990, he became a Lecturer with the Department of Electronic Engineering, City University of Hong Kong, where he is currently a Chair Professor with the Department of Computer Science. His research interests are video and image coding and evolutionary algorithms.Dacheng Tao (F'15) is Professor of Computer Science and ARC Laureate Fellow in the School of Computer Science and the Faculty of Engineering, and the Inaugural Director of the UBTECH Sydney Artificial Intelligence Centre, at The University of Sydney. His research results in artificial intelligence have expounded in one monograph and 200+ publications at prestigious journals and prominent conferences, such as IEEE T-PAMI, IJCV, JMLR, AAAI, IJCAI, NIPS, ICML, CVPR, ICCV, ECCV, ICDM, and KDD, with several best paper awards. He received the 2018 IEEE ICDM Research Contributions Award and the 2015 Australian Scopus-Eureka prize. He is a Fellow of the IEEE and the Australian Academy of Science.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PERCENTAGE</head><label>I</label><figDesc>OF THE REFERENCE IMAGES FROM THE RESULTS OF DIFFERENT METHODS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and dive+. Red boxes indicate the final reference images.images are shown in Figs. 6-10. Best viewed with zoom-in on a digital display. Note that these underwater images cannot cover the entire UIEB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II FULL</head><label>II</label><figDesc>-REFERENCE IMAGE QUALITY EVALUATION.</figDesc><table><row><cell>Method</cell><cell cols="3">MSE (?10 3 ) ? PSNR (dB) ? SSIM ?</cell></row><row><cell>fusion-based [31]</cell><cell>0.8679</cell><cell>18.7461</cell><cell>0.8162</cell></row><row><cell>two-step-based [32]</cell><cell>1.1146</cell><cell>17.6596</cell><cell>0.7199</cell></row><row><cell>retinex-based [33]</cell><cell>1.3531</cell><cell>16.8757</cell><cell>0.6233</cell></row><row><cell>UDCP [37]</cell><cell>5.1300</cell><cell>11.0296</cell><cell>0.4999</cell></row><row><cell>regression-based [39]</cell><cell>1.1365</cell><cell>17.5751</cell><cell>0.6543</cell></row><row><cell>GDCP [40]</cell><cell>3.6345</cell><cell>12.5264</cell><cell>0.5503</cell></row><row><cell>Red Channel [42]</cell><cell>2.1073</cell><cell>14.8935</cell><cell>0.5973</cell></row><row><cell>histogram prior [45]</cell><cell>1.6282</cell><cell>16.0137</cell><cell>0.5888</cell></row><row><cell>blurriness-based [46]</cell><cell>1.5826</cell><cell>16.1371</cell><cell>0.6582</cell></row><row><cell>dive+</cell><cell>0.5358</cell><cell>20.8408</cell><cell>0.8705</cell></row><row><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="3">NO-REFERENCE IMAGE QUALITY EVALUATION.</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">UCIQE [59] ? UIQM [60] ?</cell></row><row><cell>fusion-based</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV AVERAGE</head><label>IV</label><figDesc>RUNTIME FOR DIFFERENT IMAGE SIZES (IN SECOND).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V FULL</head><label>V</label><figDesc>-REFERENCE IMAGE QUALITY ASSESSMENT IN TERMS OF MSE, PSNR, AND SSIM ON TESTING SET.</figDesc><table><row><cell>Method</cell><cell cols="3">MSE (?10 3 ) ? PSNR (dB) ? SSIM ?</cell></row><row><cell>fusion-based [31]</cell><cell>1.1280</cell><cell>17.6077</cell><cell>0.7721</cell></row><row><cell>retinex-based [33]</cell><cell>1.2924</cell><cell>17.0168</cell><cell>0.6071</cell></row><row><cell>GDCP [40]</cell><cell>4.0160</cell><cell>12.0929</cell><cell>0.5121</cell></row><row><cell>histogram prior [45]</cell><cell>1.7019</cell><cell>15.8215</cell><cell>0.5396</cell></row><row><cell>blurriness-based [46]</cell><cell>1.9111</cell><cell>15.3180</cell><cell>0.6029</cell></row><row><cell>Water CycleGAN [53]</cell><cell>1.7298</cell><cell>15.7508</cell><cell>0.5210</cell></row><row><cell>Dense GAN [55]</cell><cell>1.2152</cell><cell>17.2843</cell><cell>0.4426</cell></row><row><cell>Water-Net</cell><cell>0.7976</cell><cell>19.1130</cell><cell>0.7971</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI THE</head><label>VI</label><figDesc>AVERAGE SCORES AND STANDARD DEVIATION OF DIFFERENT METHODS ON CHALLENGING SET.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://itunes.apple.com/us/app/dive-video-color-correction/ id1251506403?mt=8</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Underwater optical imaging: The past, the present, and the prospects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jaffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Oceanic Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="683" to="700" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The next best underwater view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sheinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3764" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A computer model for underwater camera systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcglamery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ocean Optics VI</title>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computer modeling and the design of optimal underwater imaging systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jaffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Oceanic Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="111" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optical turbulence on underwater image degradation in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jarosz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="2678" to="2686" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A revised underwater image formation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Akkaynak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What is the space of attenuation coefficients in underwater computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Akkaynak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shlesinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. Pattern Rec</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. Pattern Rec</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Applications of georeferenced underwater photo mosaics in marine biology and archaeology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ludvigsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sortland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johnsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Oceanography</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="140" to="149" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognition of fish species by colour and shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strachan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="10" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An in-depth survey of underwater image enhancement and restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123638" to="123657" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PDR-Net: Perception-inspired single image dehazing network with refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09146</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nested network with two-stream pyramid for salient object detection in optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="9156" to="9166" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An iterative co-saliency framework for RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybernetics</title>
		<imprint>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="233" to="249" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical features driven residual learning for depth map super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2545" to="2557" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Underwater image processing: State of the art of restoration and image enhancement methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Corchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Advan. Signal Process</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A review on intelligence dehazing and color restoration for underwater images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Man, Cybern., Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
			<publisher>Early Access</publisher>
		</imprint>
	</monogr>
	<note>IEEE Trans. Syst.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extended state observer-based integral sliding mode control for an underwater robot with unknown disturbances and uncertain nonlinearities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="6785" to="6795" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contrast restoration of weather degraded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="713" to="724" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Polarization vision helps detect transparent prey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shashar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hanlon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Dem</forename><surname>Petz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">6682</biblScope>
			<biblScope unit="page" from="222" to="223" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clear underwater vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="536" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recovery of underwater visibility and structure by polarization analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Oceanic Eng</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="570" to="587" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Active polarization descattering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A novel applications of range-gated underwater laser imaging system in near target turbid medium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sluzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opticas and Lasers Eng</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="995" to="1009" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spoeckle noise suppression of range gated underwater imaging system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applid Optics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3937" to="3944" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Turbid scene enhancement using multidirectional illumination fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4662" to="4667" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Photometric stereo in a scattering medium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3415" to="3423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enhancing the low quality images using unsupervised colour correction method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Odetayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Syst</title>
		<meeting>of IEEE Int. Conf. Syst</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1703" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Underwater image quality enhancement through integrated color model with Rayleigh distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Isa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="230" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enhancement of low quality underwater image through integrated global and local contrast correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Isa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="332" to="344" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enhancing underwater images and videos by fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bekaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>2, 4, 5, 6, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-step approach for single underwater image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symposium. of IEEE Intell. Signal Process. Commun. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="789" to="794" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>2, 4, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A retinex-based enhancing approach for single underwater image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>of IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>2, 4, 5, 6, 7, 8</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Underwater image enhancement via extended multi-scale Retinex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomput</title>
		<imprint>
			<biblScope unit="volume">245</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Underwater image enhancement by wavelength compensation and dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1756" to="1769" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Underwater depth estimation and image restoration based on single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drews-Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Botelho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="35" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Underwater image restoration based on contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int</title>
		<meeting>of IEEE Int</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="584" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A hybrid method for underwater image correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Rec. Lett</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="62" to="67" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalization of the dark channel prior for single image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Initial results in underwater single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlevaris-Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Oceans, 2010</title>
		<meeting>of IEEE Int. Conf. Oceans, 2010</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automatic Red-Channel underwater image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Picn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commu. and Image Repre</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="132" to="145" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deriving inherent optical properites from background color and underwater image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ocean Eng</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="163" to="172" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Underwater image restoration based on minimum information loss principle and optical properties of underwater imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>of IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Underwater image enhancement by dehazing with minimum information loss and histogram distribution prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Underwater image restoration based on image blurriness and light absorption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Diving into haze-lines: Color restoration of underwater images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Briti. Mach. Vis. Conf. (BMVC</title>
		<meeting>of Briti. Mach. Vis. Conf. (BMVC</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Single underwater image restoration using adaptive attenuation-curve prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="992" to="1002" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sea-thru: A method for removing water from underwater images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Akkaynak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Diving deeper into underwater image enhancement: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07863</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">WaterGAN: Unsupervised generative network to enable real-time color correction of monocular underwater images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Underwater scene prior inspired deep underwater image and video enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Rec</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Emerging from water: Underwater image color correction based on weakly supervised colro transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-coinsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Par</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis.(ICCV</title>
		<meeting>of IEEE Int. Conf. Comput. Vis.(ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Underwater image enhancement using a multiscale dense generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Oceanic. Eng</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Blind contrast enhancement assessment by gradient ratioing at visible edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Anal. Stereo</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dynamic range independetn image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="69" to="79" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An underwater color image quality evaluation metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sowmya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6062" to="6071" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Human-visual-system-inspired underwater image quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Panetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agaian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Ocean Eng</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recgonition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Underwater single image color restoration using haze-lines and a new quantitative dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01343</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A dataset to evaluate underwater image restoration methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Oceans</title>
		<meeting>of IEEE Int. Conf. Oceans</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Underwater image systems simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blasinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Farrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Imaging and Applied Optics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A three parameter underwater image formation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blasinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Farrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Single image dehazing via multiscale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>of Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Contrast limited adaptive histogram equalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="474" to="485" />
		</imprint>
	</monogr>
	<note>Graphics Gems</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Med</title>
		<meeting>of Med</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>of Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>of Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. (ICCV</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. Machine Learn. (ICML)</title>
		<meeting>of Int. Conf. Machine Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. Comput. Vis. Pattern Rec</title>
		<meeting>of IEEE Int. Conf. Comput. Vis. Pattern Rec</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
