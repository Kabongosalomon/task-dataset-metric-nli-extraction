<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grounded Situation Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-26">26 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Pratt</surname></persName>
							<email>sarahp@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Weihs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Grounded Situation Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-26">26 Mar 2020</date>
						</imprint>
					</monogr>
					<note>2 S. Pratt et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Grounded Situation Recognition (GSR), a task that requires producing structured semantic summaries of images describing: the primary activity, entities engaged in the activity with their roles (e.g. agent, tool), and bounding-box groundings of entities. GSR presents important technical challenges: identifying semantic saliency, categorizing and localizing a large and diverse set of entities, overcoming semantic sparsity, and disambiguating roles. Moreover, unlike in captioning, GSR is straightforward to evaluate. To study this new task we create the Situations With Groundings (SWiG) dataset which adds 278,336 bounding-box groundings to the 11,538 entity classes in the im-Situ dataset. We propose a Joint Situation Localizer and find that jointly predicting situations and groundings with end-to-end training handily outperforms independent training on the entire grounding metric suite with relative gains between 8% and 32%. Finally, we show initial findings on three exciting future directions enabled by our models: conditional querying, visual chaining, and grounded semantic aware image retrieval. Code and data available at https://prior.allenai.org/projects/gsr. Fig. 1. A Two examples from our dataset: semantic frames describe primary activities and relevant entities. Groundings are bounding-boxes colored to match roles. B Output of our model (dev set image). C Top-4 nearest neighbors to B using model predictions.</p><p>Beyond visual similarity, these images are clearly semantically similar. D Output of the conditional model: given a bounding-box (yellow-dashed), predicts a relevant frame. E Example of grounded semantic chaining: given query boxes we are able to chain situations together. E.g. the teacher teaches students so they may work on a project</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. We introduce Grounded Situation Recognition (GSR), a task that requires producing structured semantic summaries of images describing: the primary activity, entities engaged in the activity with their roles (e.g. agent, tool), and bounding-box groundings of entities. GSR presents important technical challenges: identifying semantic saliency, categorizing and localizing a large and diverse set of entities, overcoming semantic sparsity, and disambiguating roles. Moreover, unlike in captioning, GSR is straightforward to evaluate. To study this new task we create the Situations With Groundings (SWiG) dataset which adds 278,336 bounding-box groundings to the 11,538 entity classes in the im-Situ dataset. We propose a Joint Situation Localizer and find that jointly predicting situations and groundings with end-to-end training handily outperforms independent training on the entire grounding metric suite with relative gains between 8% and 32%. Finally, we show initial findings on three exciting future directions enabled by our models: conditional querying, visual chaining, and grounded semantic aware image retrieval. Code and data available at https://prior.allenai.org/projects/gsr. Beyond visual similarity, these images are clearly semantically similar. D Output of the conditional model: given a bounding-box (yellow-dashed), predicts a relevant frame. E Example of grounded semantic chaining: given query boxes we are able to chain situations together. E.g. the teacher teaches students so they may work on a project 1 Introduction Situation Recognition [60] is the task of recognizing the activity happening in an image, the actors and objects involved in this activity, and the roles they play. The structured image descriptions produced by situation recognition are drawn from FrameNet <ref type="bibr" target="#b4">[5]</ref>, a formal verb lexicon that pairs every verb with a frame of semantic roles, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. These semantic roles describe how objects in the image participate in the activity described by the verb.</p><p>As such, situation recognition generalizes several computer vision tasks such as image classification, activity recognition, and human object interaction. It is related to the task of image captioning, which also typically describes the salient objects and activities in an image using natural language. However, in contrast to captioning, it has the advantages of always producing a structured and complete (with regards to semantic roles) output and it does not suffer from the well known challenges of evaluating natural language captions.</p><p>While situation recognition addresses what is happening in an image, who is playing a part in this and what their roles are, it does not address a critical aspect of visual understanding: where the involved entities lie in the image. We address this shortcoming and present Grounded Situation Recognition (GSR), a task that builds upon situation recognition and requires one to not just identify the situation observed in the image but also visually ground the identified roles within the corresponding image. GSR presents the following technical challenges. Semantic saliency: in contrast to recognizing all entities in the image, it requires identifying the key objects and actors in the context of the primary activity being presented. Semantic sparsity: grounded situation recognition suffers from the problem of semantic sparsity <ref type="bibr">[59]</ref>, with many combinations of roles and groundings rarely seen in training. This challenge requires models to learn from limited data. Ambiguity: grounding roles into images often requires disambiguating between multiple observed entities of the same category. Scale: the scales of the grounded entities vary vastly with some entities also being absent in the image (in which case models are responsible for detecting this absence). Halucination: labeling semantic roles and grounding them often requires halucinating the presence of objects since they may be fully occluded or off screen.</p><p>To train and benchmark models on GSR, we present the Situations With Groundings dataset (SWiG) that builds upon the large imSitu dataset by adding 278,336 bounding-box-based visual groundings to the annotated frames. SWiG contains groundings for most of the more than 10k entity classes in imSitu and exhibits a long tail distribution of grounded object classes. In addition to the aforementioned technical challenges of GSR, the diversity of activities, images, and grounded classes, makes SWiG particularly challenging for existing approaches.</p><p>Training neural networks for grounded situation recognition using the challenging SWiG dataset requires localizing roughly 10k categories; a task that modern object detection models like RetinaNet <ref type="bibr">[34]</ref> struggle to scale to out of the box. We first propose modifications to RetinaNet that enables us to train large-class-cardinality object detectors. Using these modifications, we then create a strong baseline, the Independent Situation Localizer (ISL), that independently predicts the situation and groundings and uses late fusion to produce the desired outputs. Our proposed model, the Joint Situation Localizer (JSL), jointly predicts the situation and grounding conditioned on the context of the image. During training, JSL backpropagates gradients through the the entire network. JSL demonstrates the effectiveness of joint structured semantic prediction and grounding by improving both semantic role prediction and grounding and obtaining huge relative gains of between 8% and 32% points over ISL on the entire suite of grounding metrics.</p><p>Grounded situation recognition opens up several exciting avenues for future research. First, it enables us to build a Conditional Situation Localizer (CSL); a model that outputs a grounded situation conditioned on an input image and a specified region of interest within the image. CSL allows us to query what is happening in an image in regards to a specified query object or region. This is particularly revealing when entities are involved in multiple situations within an image or when an image consists of a large number of visible entities. Second, we show that such pointed conditioning models enable us to tackle higher order semantic relations amongst activities in images via visual chaining. Third, we show that grounded situation recognition models can serve as effective image retrieval mechanisms that can condition on linguistic as well as visual inputs and are able to retrieve images with the desired semantics.</p><p>In summary our contributions include: (i) proposing Grounded Situation Recognition, a task to identify the observed salient situation and ground the corresponding roles within the image, (ii) presenting the SWiG dataset towards building and benchmarking models for this task, (iii) showing that joint structured semantic prediction and grounding models improve both semantic role prediction and grounding by large margins, but also noting that there is still considerable ground for future improvements; (iv) revealing several exciting avenues for future research that exploit grounded situation recognition data to build models for semantic querying, visual chaining, and image retrieval. Our new dataset, code, and trained model weights will be publicly released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Grounded Situation Recognition is related to several areas of research at the intersection of vision and language and we now present a review of these below.</p><p>Describing Activities in Images. While recognizing actions in videos has been a major focus area <ref type="bibr" target="#b28">[50,</ref><ref type="bibr">25,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b26">48,</ref><ref type="bibr" target="#b25">47]</ref>, describing activities from images has also received a lot of attention (see Gella et al. <ref type="bibr" target="#b14">[15]</ref> for a more detailed overview).</p><p>Early works <ref type="bibr">[23,</ref><ref type="bibr">19,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">57,</ref><ref type="bibr">58,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b12">13]</ref> framed this as a classification problem amongst a few verbs (running/walking/etc.) or few verb-object tuples (riding bike/riding horse/etc.). More recent work has focused on human object interactions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">30,</ref><ref type="bibr">61,</ref><ref type="bibr" target="#b23">45]</ref> with more classes; but the classes are either arbitrarily chosen or obtained by starting with a set of images and then labeling them with actions. Also, the relationships include Subject-Verb-Object triples or subsets thereof. In contrast, the imSitu dataset for situation recognition uses linguistic resources to define a large and more comprehensive space of possible situations, ensuring a fairly balanced datasets despite the large number of verbs (roughly 500) and modeling a detailed set of semantic roles per verb obtained from FrameNet <ref type="bibr" target="#b4">[5]</ref>.</p><p>Image captioning is another popular setup to describe the salient actions taking place in an image with several datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">46,</ref><ref type="bibr" target="#b0">1]</ref> and many recent neural models that perform well <ref type="bibr" target="#b31">[53,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">24]</ref>. One serious drawback to image captioning is the well known challenge of evaluation which has led to a number of proposed metrics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">52,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr">32,</ref><ref type="bibr">38]</ref>; but these problems continue to persist. Situation recognition does not face this issue and has clearly established metrics for evaluation owing to its structured frame output.</p><p>Other relevant works include visual sense disambiguation <ref type="bibr" target="#b15">[16]</ref>, visual semantic role labelling <ref type="bibr">[20]</ref>, and scene graph generation [28] with the latter two described in more detail below.</p><p>Visual Grounding. In contrast to associating full images with actions or captions, past works have also associated regions to parts of captions. This includes visual grounding i.e. associating words in a caption to regions in an image and referring expression generation i.e. producing a caption to unambiguously describe a region of interest; and there are several interesting datasets here.</p><p>Flickr30k-Entities <ref type="bibr" target="#b18">[40]</ref> is a large dataset for grounded captioning. v-COCO [20] is more focused on semantic role labeling for human interactions with human groundings, action labels and relevant object groundings. Compared to SWiG, the verbs (26 vs 504) and semantic roles per verb (up to 2 vs up to 6) are fewer. HICO-Det <ref type="bibr" target="#b6">[7]</ref> has 117 actions, but they only involve 80 objects, compared to nearly 10,000 objects in SWiG. In addition to these human centric datasets, SWiG also contains actions by animals and objects.</p><p>Large referring expression datasets include RefClef [26], RefCOCO [37] and RefCOCO+ collected using a two person game, RefCOCOg collected by standard crowdsourcing and GuessWhat?! <ref type="bibr" target="#b32">[54]</ref> that combines dialog and visual grounding.</p><p>An all encompassing vision and language dataset is Visual Genome (VG) [28] containing scene graphs: dense structured representations for images with objects, attributes, relations, groundings and QA. VG differs from SWiG in a few ways. Scene graphs are dense while situations capture salient activities. Also, relations in scene graphs are binary and tend to favor part and positional relations (the top 10 relations in VG are of this nature and cover 66% of the total) while SWiG contains more roles per verb, has 504 verbs drawn from language and has a good coverage of data per verb. Finally, dense annotations are notoriously hard to obtain; and it is well known that VG suffers from missing relations, rendering evaluation tricky.  <ref type="bibr" target="#b29">[51]</ref> achieved state of the art accuracy using attention graph neural nets. Our pro-posed grounded models build upon the RNN based approach of [36] owing to its simplicity and high accuracy; but our methods to combine situation recognition models with detectors can be applied to any of the aforementioned approaches. Large-Class-Cardinality Object Detection. While most popular object detectors are built and evaluated on datasets <ref type="bibr">[35,</ref><ref type="bibr" target="#b13">14]</ref> with few classes, some past works have addressed the problem of building detectors for thousands of classes. This includes YOLO-9000 <ref type="bibr" target="#b21">[43]</ref>, DLM-FA <ref type="bibr" target="#b34">[56]</ref>, R-FCN-3000 <ref type="bibr" target="#b27">[49]</ref>, and CS-R-FCN <ref type="bibr" target="#b17">[18]</ref>. Our modifications to RetinaNet borrow some ideas from these works. Task. Grounded Situation Recognition (GSR) builds upon situation recognition and requires one to identify the salient activity, the entities involved, the semantic roles they play and the locations of each entity in the image. The frame representation is drawn from the linguistic resource FrameNet and the visual groundings are akin to bounding boxes produced by object detectors. More formally, given an input image, the goal is to produce three outputs. (a) Verb: classifying the salient activity into one of 504 visually groundable verbs (one in which it is possible to view the action, for example, talking is visible, but thinking is not). (b) Frame: consists of 1 to 6 semantic role values i.e. nouns associated with the verb (each verb has its own pre-defined set of roles). For e.g., <ref type="figure" target="#fig_1">Fig. 2</ref> shows that kneading consists of 3 roles: Agent, Item, and Place. Every image labeled with the verb kneading will have the same roles but may have different nouns filled in at each role based on the contents of the image. A role value can also be ? indicating that a role does not exist in an image ( <ref type="figure" target="#fig_1">Fig. 2c</ref>). (c) Groundings: each grounding is described with coordinates [x 1 , y 1 , x 2 , y 2 ] if the noun in grounded in the image. It is possible for a noun to be labeled in the frame but not grounded, for example in cases of occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Situation Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GSR and SWiG</head><p>Data. SWiG builds on top of imSitu [60]. SWiG retains the original images, frame annotations and splits from imSitu with a total of 126,102 images spanning 504 verbs. For each image, there are three frames by three different annotators, with a length between 1 and 6 roles and an average frame length of 3.55.</p><p>Bounding-box annotations were obtained using Amazon's Mechanical Turk framework with each role annotated by three workers and the resulting boxes combined by averaging their extents. In total, SWiG contains 451,916 noun slots across all images. Of these 435,566 are non-?. Of these 278,336 (63.9%) have bounding boxes. The missing bounding boxes correspond to objects that are not visible or to 'Place' which is never annotated with a bounding box as the location of an action is always the entire image.</p><p>SWiG exhibits a huge variability in the number of groundings per noun (see <ref type="figure" target="#fig_2">Fig. 3a</ref>). For instance 'man' appears over 100k times while others occur only once. Unlike other detection datasets such as MS-COCO <ref type="bibr" target="#b8">[9]</ref>, SWiG contains a long-tail distribution of grounded objects similar to the real world.  <ref type="figure" target="#fig_2">Fig. 3b</ref> shows the frequency with which different roles are grounded in the image. Note that, like nouns, roles also have an uneven distribution. Almost all situations are centered around an 'Agent' but very few situations use a 'Firearm'. This plot shows how often each role is grounded invariant to its absolute frequency. Some roles are much more frequently salient, demonstrating the linguistic frame's ability to capture both concrete and abstract concepts related to situations. Objects filling roles like 'Firearm'/'Teacher' are visible nearly every time they are relevant to a situation. However, the noun taking on the role of the 'Event' cannot usually be described by a particular object in the image. Only one role ('Place') is never grounded in the image. <ref type="figure" target="#fig_2">Fig. 3c</ref> shows the distribution of grounding scale and aspect ratio for a sample of nouns. Many nouns exhibit high variability across the dataset (1st column), but some nouns have strong priors that may be used by models (2nd column). <ref type="figure" target="#fig_3">Fig. 4</ref> shows the variability of appearance of groundings across verbs. <ref type="figure" target="#fig_3">Fig. 4a</ref> indicates the scale and aspect ratio of every occurrence of the noun 'Rope' for verbs where this noun occurs at least 60 times. Each point is an instance and the color represents the verb label for the image where that instance appears. A large scale indicates that at least one side of the bounding box is large in relation to the image. A large aspect ratio indicates that the height of the bounding box is much greater than the width. This plot shows that the verb associated with an image gives a strong prior towards the physical attributes of an object. In this case, knowing that a rope appears in a situation with the verb 'drag' or 'pull', indicates that it is likely to have a horizontal alignment. If the situation is 'hoisting' or 'climbing' then the rope is likely to have a vertical alignment. <ref type="figure" target="#fig_3">Fig. 4b</ref> shows the scale and aspect ratio of the role 'Agent', invariant to the noun, for a variety of verbs. The clustering of colors in the plot indicates that the verb gives a strong prior to the size and aspect ratio of the 'Agent'. However, this also demonstrates the non-triviality of the task. This is especially evident in the images depicting the agent for 'Mowing' compared to 'Harvesting'. While knowing the verb gives a strong indication as to the appearance of the 'Agent', it is not trivial to distinguish between the two verbs given just the agent. The correlation between object appearance and actions demonstrates the importance of combining situation understanding with groundings, but we must still maintain the entire context to complete the task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>Grounded situation recognition involves recognizing the salient situation and grounding the associated role values via bounding boxes; indicating that a model for this task must perform the roles of situation recognition and object detection. We present a novel method Joint Situation Localization (JSL) with a strong baseline, the Independent Situation Localization (ISL). Situation Recognition Model. The proposed ISL and JSL models represent techniques to combine situation recognition and detection models and can be applied to all past situation recognition models. In this work, we select the RNN without fusion from [36] since: (i) it achieves a high accuracy while having a simple architecture <ref type="table">(Table 1</ref>). (ii) We were able to upgrade it with a reimplementation, new backbone, label smoothing, and hyper-parameter tuning resulting in huge gains <ref type="table">(Table 1)</ref> over the reported numbers, beating graph nets [31] and much closer to attention graph nets <ref type="bibr" target="#b29">[51]</ref> (the current state-of-the-art on im-Situ). (iii) Code and models for attention graph nets are not released, rendering reproducibility challenging, especially given the complexity of the method.</p><p>As in the top of <ref type="figure" target="#fig_4">Fig. 5</ref>, ResNet-50 embeddings are used for verb prediction and then an LSTM [22] sequentially predicts the noun for each role in the frame. The order of the roles is dictated by the dataset. The loss is a sum of cross entropy with label smoothing on the predicted nouns, and cross entropy on the verb.</p><p>Large-Class-Cardinality Object Detection. We use a modified version of RetinaNet [34] as our baseline detector within the proposed models. RetinaNet is a single stage detector with a Feature Pyramidal Network (FPN) [33] for multiscale features, multiple anchors to account for varied aspect ratios and two heads: a classification head that assigns each anchor to a class and a regression head that modifies the anchor to better localize any object in that location. RetinaNet does not scale well to the 10k classes in SWiG out of the box.</p><p>We make 3 modifications to RetinaNet to scale it to 10,000 classes, as seen in the middle of <ref type="figure" target="#fig_4">Fig. 5</ref>. (i) Objectness: instead of each anchor predicting a score for each class, each anchor now predicts an "objectness score". Non-Maximum Suppression (NMS) is performed on the boxes, the top 100 are chosen and featurized using RoI Align <ref type="bibr" target="#b22">[44]</ref>. These local features combined with global ResNet features are classified into the ?10,000 noun categories. The resulting memory savings are huge. In RetinaNet, the classification branch output tensor has dimensions</p><formula xml:id="formula_0">n i=1 (W i ? H i ? A ? K) where W i , H i indicate</formula><p>the spatial dimensional of the features for the i th output of the FPN, A indicates the number of anchor boxes, and K indicates the number of classes. This does not fit on a single TI-TAN RTX GPU for K = 10, 000 for any reasonable batch size. In contrast, our modification reduces the tensor dimension to</p><formula xml:id="formula_1">n i=1 (W i ? H i ? A ? P )</formula><p>where P is the number of image regions we consider and is set to 100. With these modifications we are able to train with a batch size of 64 on 4 TITAN RTX GPUs with 24GB of memory. (ii) Drop fine scale: we exclude the finest grain of features from FPN since anchors at this scale do not overlap with a significant portion of our data leading to computation savings. (iii) Anchor selection: anchor box aspect ratios are assigned using aspect ratio clustering on our training data, as in <ref type="bibr" target="#b21">[43]</ref>. As in [34], we use a focal loss for classification and an L 1 loss for regression, with a binary cross entropy loss for noun prediction.</p><p>Independent Situation Localizer (ISL). The ISL independently runs the situation recognizer and detector and combines their results. The RNN model produces a prediction for each noun in the frame. The detector obtains a distribution over all possible object categories for each of the top 100 bounding boxes. Then for each noun in the frame, we assign the grounding with the highest score for that noun. This allows an object that is assigned to one class by the detector to eventually get assigned to another class as long as the score for the latter class is high enough. If all of the box scores for a noun are below a threshold or the role is 'Place', it is considered ungrounded.</p><p>Joint Situation Localizer (JSL). We propose JSL as a method to simultaneously classify a situation and locate objects in that situation. This allows for a role's noun and grounding to be conditioned on the nouns and groundings of previous roles and the verb. It also allows features to be shared potential patterns between nouns and positions (like in <ref type="figure" target="#fig_3">Fig. 4</ref>) to be exploited. We refer the reader to the appendix and our code for model details, but point out key differences between JSL and ISL here.</p><p>JSL (shown in the bottom of <ref type="figure" target="#fig_4">Fig. 5</ref>) uses similar backbones as ISL but with key differences: (i) rather than predicting localization for every object in the image at the same time (as is done in object detection), JSL predicts the location of the objects recurrently (as is done for predicting nouns in situation recognition). (ii) In contrast to the RNN model in ISL, the LSTM accepts as input the verb embedding, global ResNet features of the image, embedding of the noun predicted at the previous time step and local ResNet features of the bounding box predicted at the previous time step. (iii) In contrast to the detector in ISL, the localization is now conditioned on the situation and current role being predicted. In ISL, FPN features feed directly into the classification and regression branches, but in JSL the FPN features are combined with the LSTM hidden state and then fed in. (iv) The JSL also uses the classification branch to produce an explicit score indicating the likelihood that an object is grounded.</p><p>(v) Only one noun needs to be localized at each time step, which means that only anchor boxes relevant to that one grounding will be marked as positive during training, given that the noun is visible and grounded in the training data.</p><p>The loss includes focal loss and L 1 loss from the detector and cross entropy loss for the grounding and verb. Additionally, we use cross entropy with label smoothing for the noun loss, and sum this over all three annotator predictions. This results in the following total loss: <ref type="bibr" target="#b29">51]</ref>, we found that using a separate ResNet backbone to predict the verb achieved a boost in accuracy. However, the JSL architecture with this additional ResNet backbone still maintains the same number of parameters and ResNet backbones as the ISL model.</p><formula xml:id="formula_2">L = L 1 (reg) + F L 0.25,2 (class) + CE(verb) + CE(ground) + 3 i=1 CE 0.2 (noun i ) Similar to previous works [36,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Implementation Details. ISL and JSL use two ResNet-50 backbones and maintain an equal number of parameters (?108 million). We train our models via gradient descent using the Adam Optimizer [27] with momentum parameters of ? = (0.9, 0.999). We use 4 24GB TITAN RTX GPUs for approximately 20 hours. For comprehensive training and model details, including learning rate schedules, batch sizes, and layer sizes, please see our appendix. Metrics. We report five metrics, three standard ones from prior situation recognition work: (i) verb to measure verb prediction accuracy, (ii) value to measure accuracy when predicting a noun for a given role, (iii) value-all to measure the accuracy in correctly predicting all nouns in a frame simultaneously; and introduce two new grounding metrics: (iv) grounded-value to measure accuracy in predicting the correct noun and grounding for a given role. A grounding is considered correct if it has an IoU of at least 0.5 with the ground truth. (v) grounded-value-all to measures how frequently both the noun and the groundings are predicted correctly for the entire frame. Note that if a noun does not have a grounding, the model must also predict this correctly. All these metrics are calculated for each verb and then averaged across verbs so as to not unfairly bias this metric toward verbs with more annotations or longer semantic frames.</p><p>Since these metrics are highly dependent on verb accuracy, they have the potential to obfuscate model differences with regards to noun prediction and grounding. Hence we report them in 3 settings: Ground-Truth-Verb: the ground truth verb is assumed to be known. Top-1-Verb: verb reports the accuracy of the top 1 predicted verb and all noun and groundings are considered incorrect if the verb is incorrect. Top-5-Verb: verb corresponds to the top-5 accuracy of verb prediction. Noun and grounding predicitons are taken from the model conditioning on the correct verb having been predicted. Results. The top section of <ref type="table">Table 1</ref> shows past imSitu models for the dev set while the lower section illustrates the efficacy of jointly training a model for grounding and situation recognition. The yellow rows indicate the base RNN model used in this work and the green row shows the large upgrades to this model across all metrics. ISL achieves reasonable results, especially for ground truth verbs. However, JSL improves over ISL across every metric while using an equal number of parameters. This includes substantial improvements on all grounding metrics (ranging from relative improvements of 8.6% for Ground-Truth-Verbground-value to 32.9% for Top-1-Verb-grounded-value-all ).</p><p>The ability to improve across both the grounding and non-grounding scores demonstrate the value in combining grounding with the task of situation recognition. Not only can the context of the situation improve the models ability to locate objects, but locating these objects improves the models ability to understand them. This is further emphasized by the models ability to predict the correct noun under the GroundTruthVerb setting.</p><p>Importantly, in spite of using the simpler RNN based backbone for situation recognition, JSL achieves state of the art numbers on the GroundTruthVerb-Value metric, beating the more complex Kernel GraphNet model demonstrating the benefits of joint prediction. This indicates that further improvements may be obtained by incorporating better backbones. Additionally, it is interesting to note that the model achieves this high value even though it does not achieve state of the art in value-all. This indicates another potential benefit of the model. While more total frames contain a mistake, JSL is still able to recover some partial information and recognize more total objects. One explanation is that grounding may contribute to the ability to have a partial understanding of more complicated images where other models fail completely. Finally, test set metrics are shown in <ref type="table">Table 2</ref> and qualitative results in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Grounded situation recognition and SWiG open up several exciting directions for future research. We present initial findings for some of these explorations. Grounded Semantic aware Image Retrieval. Over the past few years, large improvements have been obtained in content based image retrieval (CBIR) by employing visual representations from CNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">41,</ref><ref type="bibr" target="#b20">42,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">55]</ref>. CNN features work well particularly when evaluated on datasets requiring instance retrieval [39] or category retrieval <ref type="bibr" target="#b10">[11]</ref>, but unsurprisingly do not do well when the intent of the query is finding a matching situation. We perform a small study for image retrieval using the dev set in SWiG. We partition this set into a query set and a retrieval set and perform retrieval using four representations: (i) ResNet-50 embeddings, (ii) bag of objects obtained from our modified RetinaNet object detector, (iii) situations obtained from our baseline RNN model, and (iv) grounded situations obtained from JSL. Details regarding the setup and distance functions for each are presented in the appendix. <ref type="figure" target="#fig_6">Fig. 6</ref> shows a qualitative result. Resnet-50 retrieves images that look similar (all have water) but have the wrong situations. The same goes for object detection. Situation based retrieval gets the semantics correct (most of the retrieved images contain surfing). Grounded situations provide the additional detail of not just similar semantics but also similar arrangement of objects, since the locations of the entities are also taken into account. Furthermore, the proposed method also produces explainable outputs via the grounded situations in the retrieved images; arguably more useful than CBIR explanations via heatmaps <ref type="bibr" target="#b11">[12]</ref>. This approach can also be extended to structured queries (obtained from text) and a mix of text and image based queries.</p><p>Conditional Grounded Situation Recognition. JSL accepts the entire image as an input and produces groundings for the salient situation. But images may contain entities in multiple situations (a person sitting and discussing and drinking coffee) or multiple entities. Conditioning on a localized object or region can enable us to query an image regarding the entity of interest. Note that a query entity may be an actor (what is this person doing? ) or an object (What situation is this object involved in? ) or a location (What is happening at this specific location? ) in the scene. A small modification to JSL results in a Conditional Situation Localizer (CSL) model (details in appendix), which enables this exploration. <ref type="figure" target="#fig_7">Fig. 7a</ref> shows that a query box around the cellphone invokes calling while the baby invokes feeding. <ref type="figure" target="#fig_7">Fig. 7b</ref> shows that a query box may have 1 or more entities within it. <ref type="figure">Fig. 8</ref>. Grounded semantic chaining. When a person looks at this image, they may infer several things. A father is teaching his son to use the grill. They are barbecuing some meat with the intent of feeding friends and family who are sitting at a nearby table. Using the conditional localizer followed by spatial and semantic chaining produces situations and relationships-between-situations. These are shown via colored boxes, text and arrows. Conditional inputs are shown with dashed yellow boxes. Notice the similarity between the higher level semantics output by this chaining model and the inferences about the image that you may draw Grounded Semantic Chaining. Pointed conditional models such as CSL, when invoked on a set of bounding boxes (obtained via object detection), enable us to chain together situations across multiple parts of image. While a situation addresses local semantics, chaining of situations enables us to address higher order semantics across an entire image. Visual chaining can be obtained using <ref type="figure">Fig. 9</ref>. Qualitative results for the proposed JSL model. First two rows show examples with correctly classified situations and detected groundings; and demonstrates the diversity of situations in the data. Third row shows classification errors. Note that some of them are perfectly plausible answers. Fourth row shows incorrect groundings; some of which are only partially wrong but get counted as errors nonetheless spatial and semantic proximity between groundings in different situations. While scene graphs are a formalism towards this, they only enable binary relations between entities; and this data in Visual Genome [28] has a large focus on part and spatial relations. Since SWiG contains a diverse set of verbs with comprehensive semantic roles, visual chains obtained by CSL tend to be very revealing and are an interesting direction to pursue in future work. <ref type="figure">Fig. 8</ref> shows an interesting example of querying multiple persons and then chaining the results using groundings, revealing: a man is helping his son while barbecuing meat and the people are dining on the hamburger that is being grilled by the man.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We introduce Grounded Situation Recognition (GSR) and the SWiG dataset. Our experiments reveal that simultaneously predicting the semantic frame and groundings results in huge gains over independent prediction. We also show exciting directions for future research. Here we provide a more detailed explanation of methods introduced in this work and provide additional qualitative results demonstrating the efficacy of our proposed model. In Section A we discuss the details of Semantic Image Retrieval as mentioned in Section 6. In Section B we provide the implementation details of our baseline model (ISL) and proposed model (JSL). In Section C we discuss the model changes we make to JSL in order to create the Conditional Situation Localizer as discussed in Section 6. Finally, in Section D we provide qualitative results comparing the localization of ISL and JSL as well as qualitative results visualizing the situations generated for the top-5 verbs predicted by JSL.</p><p>A Semantic Image Retrieval In <ref type="figure" target="#fig_6">Fig. 6</ref> and <ref type="figure" target="#fig_0">Fig. 10</ref> we show qualitative examples of semantic image retrieval implemented with nearest neighbor computations and a collection of different similarity functions. In particular, we divide our validation set into a query set (1008 images, 2 images per verb) and search set (24192 images, 48 images per verb). For each of the images in our query set, we compute the similarity of the query image with all images in search set and save the top-5 most similar images to the query. We now describe how we compute image similarity using ResNet-50 features, bag-of-words object detections, situation predictions, and grounded situation predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 ResNet-50</head><p>We compute a featurization of each image using a ResNet-50 model pretrained on the ImageNet dataset. Similarity between images is then computed as the negative of the L2 distance between these featurizations (so that images with nearer featurizations are more similar).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Object Detections</head><p>For each image I we compute object detections using the modified RetinaNet described in Section 4. We find these detections by computing the maximum likelihood category for each box. If the logits corresponding to probability of the maximum category is greater than -1 we consider it a valid detection. To prevent multiple detections of the same object we use NMS to remove any overlapping boxes of the same object category. We save the predicted class labels {c I 1 , ..., c I N I } and bounding-boxes {b I 1 , ..., b I N I }. Similarity between two images I, J is then computed as</p><formula xml:id="formula_3">ObjSim(I, J) = 1 N N i=1 max{1 [c I i =c J j ] ? (1 + IoU(b I i , b J j )) | 1 ? j ? M } (1)</formula><p>so that ObjSim(I, J) will be maximal when the objects detected in I have the same classes and bounding-boxes as those in J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Situation Recognition</head><p>For each image I in our validation set, we compute v I 1 , ..., v I 5 the top-5 predicted activities (verbs) associated with I. For each of these verbs v I i , we additionally predict the entities associated with the roles of that verb, e I i,1 , ..., e I i,N v I i . We then compute the situation similarity between two images I, J as</p><formula xml:id="formula_4">SitSim(I, J) = max{ 1 [v I i =v J j ] i ? j ? N v N v I i k=1 1 [e I i,k =e J j,k ] | 1 ? i, j ? 5}.<label>(2)</label></formula><p>Notice that SitSim(I, J) is only non-zero if there is at least one verb shared in the top-5 verb predictions of I and J. Moreover, the similarity will be at its maximum value of 1 if any only if both I and J have the same top-1 verb and, for that verb, all predicted entities (conditioned on that top-1 verb) for both images are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Grounded Situation Recognition</head><p>As above, we have, for an image I top-5 verb predictions v I 1 , ..., v I 5 and entity</p><formula xml:id="formula_5">predictions {e I i,ki | 1 ? i ? 5, 1 ? k i ? N v I i }.</formula><p>For grounded situation predictions we also have, for each entity e I i,k a bounding-box prediction b I i,k . We then compute similarity between two images I, J as</p><formula xml:id="formula_6">GrSitSim(I, J) = max{ 1 [v I i =v J j ] i ? j ? N v N v I i k=1 1 [e I i,k =e J j,k ] ? (1 + IoU(b I i,k , b J j,k )) | 1 ? i, j ? 5}.<label>(3)</label></formula><p>Notice that GrSitSim is nearly identical to SitSim except that GrSitSim will be larger when predicted entities have similar bounding boxes, as measured by their intersection over union.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 RNN</head><p>Architecture We use a ResNet-50 backbone pretrained on ImageNet. The embedding size for nouns is 512 and the embedding size for verbs is 256. We use a single layer LSTM as the the RNN with a hidden size of 1024 and an input size of 2816 (2048 image features, 512-dimensional embedding of the previous noun, 256-dimensional embedding of the verb). The LSTM is initialized with orthogonal weights. The 512-dimensional noun vector is initialized with zeros for the first noun prediction. The LSTM predicts a sequence length of 6 as this is the maximum length frame. Frames with less than this length are padded to length 6. The ground truth verb embedding is used as input to the LSTM for all of training, as incorrect verb predictions are always marked as having incorrect noun predictions, so there is no benefit to training with incorrect verb predictions.</p><p>Training We train the RNN using the Adam Optimizer [27] with ? = (0.9, 0.999). The initial learning rate is set to 1e-4 which is decreased by a factor of 10 at epoch 12 and 24. Additionally, we begin training by freezing the ResNet weights and only begin to propagate the gradients through ResNet at epoch 14.</p><p>We train with a batch size of 32 for 100 epochs, which takes 40 hours on one 12GB TITAN V GPU and use the weights from the best performing epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Object Detector</head><p>Architecture The majority of this architecture is unchanged from the original RetinaNet architecture. We ResNet-50 backbone pretrained on ImageNet. The majority of the differences from the original RetinaNet take place in the adjustments to the network which allow for detection of 10,000 categories. As mentioned in in Section 4, we adjust the network by predicting the likelihood that each anchor box contains an object, rather than predicting a distribution over all object categories for each anchor box. We then perform NMS to remove low scoring boxes which have a high overlap with other boxes. We take the top 100 boxes most likely to contain an object and obtain the features corresponding to these boxes in the final spatial layer of ResNet using RoI align. We then linearly transform the feature vectors into a vector the size of the noun vocabulary to obtain a predicted distribution. For training, if these boxes overlap with a ground truth annotation with an IoU of at least 0.5, they are labeled will all the categories attributed to the ground truth box. A ground truth box may have multiple categories as there are multiple annotators. If it does not overlap with any ground truth box it is not labeled with any category. If it overlaps with multiple ground truth boxes, we duplicate the predicted box and each one is considered to overlap with one ground truth box. We then use binary cross entropy on these labels and the predicted distribution. When combining the RNN output and RetinaNet outputs, a box is assigned to a noun category if it has the highest predicted value for that noun category out of all 100 boxes. If none of the boxes reach a certain threshold for that noun category, then the noun is label as ungrounded in the image. We tune this threshold to be -4 for our model, so if none of the logits are above this value for the desired category, it is ungrounded.</p><p>Training We train with a batch size of 64 using the Adam Optimizer [27] with ? = (0.9, 0.999). We use a learning rate of 1e-4 for all of training. We train until convergence and then use the weights from the epoch (26) which achieved the highest accuracy on the dev set. We train the network for ?72 hours on eight 12GB TITAN V GPUs. Despite the modifications we made to the RetinaNet model, training is still relatively slow as we must still perform 100 classifications for every image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 JSL</head><p>Architecture As with the RNN, we use a single layer LSTM with hidden size 1024, noun embeddings of size 512 and verb embeddings of size 256. We initialize the ResNet-50 backbone with imagenet weights and initialize the LSTM with orthogonal weights. Additionally we pad shorter frames to be of length 6 and label all of the pad symbols to be ungrounded. Like the RNN model, we always use the embedding of the ground truth verb during training, as the nouns are always considered incorrect if the verb prediction is incorrect so there is no benefit to training with the incorrect verb prediction. Additionally, for the first 5 epochs of training, we use the ground truth bounding boxes when obtaining the local features for noun classification and we use the previous ground truth noun when embedding the previous noun for the LSTM. When combining the output of the LSTM with the features from the FPN before the classification and regression branches (see <ref type="figure" target="#fig_4">Figure 5</ref>) we concatenate the FPN features with a linear projection of size 256 of the hidden state of the LSTM. Additionally we concatenate an element wise product of these two vectors, resulting in a final input vector with a channel dimension of 768.</p><p>Training We train with a batch size of 64 using the Adam Optimizer [27] with ? = (0.9, 0.999). We use an initial learning rate of 6e-4 which we decrease by a factor of 10 at epochs 10 and 20. Like with the RNN, we begin by freezing the ResNet weights and only begin to propagate the weights to the ResNet backbone at epoch 12. We train until convergence and use the weights which have the highest performance on the validation set (epoch 27). Training takes ?20 hours on four 24GB TITAN RTX GPUs.</p><p>Verb Prediction Network As mentioned in Section 4, we find using a separate network to predict the verb increases the accuracy of verb prediction, while keeping the total number of parameters equal to that of the independent model. To train the verb classifier, we use a ResNet backbone with a linear layer on top of the final feature vector of size 2048, just after the final average pooling. We use the Adam Optimizer with an initial learning rate of 1e-4 which we decrease by a factor of 10 at epoch 18. We train just the final linear layer for the first 5 epochs, then just the linear layer and final block for the next 5 epochs. We continue this pattern, unfreezing one additional ResNet block every 5 epochs until epoch 15.</p><p>We never propagate through the first block as we find this decreases the overall accuracy, likely due to overfitting. We use standard cross entropy loss and a batch size of 256. Training takes ?1 hour on eight 12GB TITAN V GPUs. <ref type="figure" target="#fig_0">Fig. 11</ref>. Model schematics for the CSL model. Differences between JSL and CSL are highlighted in yellow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Conditional Situation Recognition</head><p>The Conditional Situation Localizer (CSL) is a modification of JSL which conditions its output on a specific bounding-box, as illustrated by <ref type="figure" target="#fig_7">Figure 7</ref>. The network architecture of CSL is illustrated by <ref type="figure" target="#fig_0">Figure 11</ref>, with differences from JSL highlighted in yellow. Rather than predicting the verb via a separate network, the verb prediction is done from the local features inside the bounding-box. As in JSL, these local features are obtained by performing RoI Align on the last spatial features of ResNet. Then the verb prediction and these local features are used to predict the role that the object within the box plays with respect to the verb. For example in <ref type="figure" target="#fig_0">Figure 7A1</ref>, the local features surrounding the query were first used to predict the action as 'Calling' and then this verb prediction and those local features where used to predict that the object in the bounding-box fills the second role for this verb, which corresponds to 'Tool' in this case.</p><p>CSL then works exactly as JSL except the input bounding-box is used for the predicted role. So if the model predicts that the bounding-box corresponds to the second role for the predicted verb, then on the second pass of the LSTM, the bounding-box prediction made by the classification and regression branches are overwritten by the position of the input bounding-box. This is demonstrated by the "check role" portion of <ref type="figure" target="#fig_0">Figure 11</ref>. At each pass, the network checks if the current iteration is equal to the role predicted by the input bounding-box. If it is, then that bounding-box is used, otherwise the predicted bounding-box is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Qualitative</head><p>We present additional qualitative results further demonstrating the efficacy of JSL. <ref type="figure" target="#fig_0">Figure 12</ref> shows a comparison between groundings generated by JSL and ISL for the same image. We illustrate these differences on a sample of images where both ISL and JSL are able to classify the nouns correctly, but ISL fails to correctly locate the entities in the frame. Here we show two common reasons that ISL fails to locate the correct object. The first 2 rows of <ref type="figure" target="#fig_0">Figure 12</ref> demonstrate the case where there are multiple people in the scene and ISL is unable to pick the correct one for a given role. Because ISL cannot condition its detection on situation, it is often unable to select the correct object when there are multiple objects of the same category present in an image. The bottom 2 rows of <ref type="figure" target="#fig_0">Figure 12</ref> show cases where ISL correctly locates the object, but fails to create an accurate bounding box around that object. This demonstrates a potential advantage of JSL as predicting the objects in sequence may allow for more accurate localization.</p><p>Additionally, <ref type="figure" target="#fig_0">Figure 13</ref> shows the generated situations for different verbs given the same image. For each image we obtain the top 5 most probable verbs and then generate the grounded situations for each of these verbs. The top two rows of <ref type="figure" target="#fig_0">Figure 13</ref> are examples where the top verb guess is correct. The first row demonstrates the model's ability to describe the scene in terms of the interaction between two participants as well as what actions they are doing together. In this case, it is clear that both girls are studying, but one is explaining something to the other. Looking at multiple possible verbs captures these complexities. The following two rows are examples where the correct verb is in the top 5 and the bottom two rows show examples where the correct verb is not in the top 5. This tends to happen when the action is very unusual or occurring in a strange context, such as purposefully spilling a cup of water on a keyboard. <ref type="figure" target="#fig_0">Fig. 12</ref>. For all images, the detections generated by JSL are shown first followed by the detections generated by ISL. Incorrect detections are shown with dotted lines and boxes are colored to correspond with roles. <ref type="figure" target="#fig_0">Fig. 13</ref>. Top-5 predictions for a sample of images in the SWiG dev set. The ground truth verb is indicated on the left of each row.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A Two examples from our dataset: semantic frames describe primary activities and relevant entities. Groundings are bounding-boxes colored to match roles. B Output of our model (dev set image). C Top-4 nearest neighbors to B using model predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Grounded situations from the SWiG dataset. This figure showcases the variability of images, situations and groundings across the dataset. Some challenges seen in this figure are absent roles (first image), animals as agents (second image) contrasting datasets that only focus on human interactions, ambiguity resolution (two female children in the third image), matching groundings for two roles (sofa in the third image) and partial occlusion (person only partially visible in the fourth image)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Dataset visualizations. (A) Number of groundings per noun. Note the log scale and the fact that this only shows a small sample. (B) Frequency with which different roles are grounded in the image. (C) Distribution of grounding scale (y-axis) and aspect ratio (x-axis) conditioned on some nouns</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Scale and aspect ratio distributions across nouns and roles. (A) Every occurrence of the noun 'Rope' for verbs -showing that verb gives a strong prior towards the physical attributes of an object. (B) The role 'Agent', invariant to the noun -shows priors but also the challenges of the task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Model schematics for the proposed ISL and JSL models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative results for semantic image retrieval. For the query figure of a surfer in action, ResNet and Object Detection based methods struggle to match the fine semantics. Grounded situation based retrieval leads to the correct semantics with matching viewpoints</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative results using the Conditional Situation Localizer. A1 &amp; A2: The woman is taking part in multiple situations with different entities in the scene. These situations are invoked via different queries. B1 &amp; B2: Querying the person with a guitar vs querying the group of people also reveals their corresponding situations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>19. Gupta, A., Kembhavi, A., Davis, L.S.: Observing human-object interactions: Using spatial and functional compatibility for recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 31, 1775-1789 (2009) 20. Gupta, S., Malik, J.: Visual semantic role labeling. arXiv preprint arXiv:1505.04474 (2015) 21. Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.: Activitynet: A large-scale video benchmark for human activity understanding. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 961-970 (2015) 22. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation 9, 1735-1780 (1997) 23. Ikizler, N., Cinbis, R.G., Pehlivan, S., Sahin, P.D.: Recognizing actions from still images. 2008 19th International Conference on Pattern Recognition pp. 1-4 (2008) 24. Karpathy, A., Li, F.F.: Deep visual-semantic alignments for generating image descriptions. In: CVPR (2015) 25. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, A., Suleyman, M., Zisserman, A.: The kinetics human action video dataset. ArXiv abs/1705.06950 (2017) 26. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.L.: Referitgame: Referring to objects in photographs of natural scenes. In: EMNLP (2014) 27. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Bengio, Y., LeCun, Y. (eds.) International Conference on Learning Representations (2015) 28. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., Bernstein, M.S., Fei-Fei, L.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision 123, 32-73 (2016) 29. Le, D.T., Bernardi, R., Uijlings, J.R.R.: Exploiting language models to recognize unseen actions. In: ICMR '13 (2013) 30. Le, D.T., Uijlings, J.R.R., Bernardi, R.: Tuhoi: Trento universal human object interaction dataset. In: VL@COLING (2014) 31. Li, R., Tapaswi, M., Liao, R., Jia, J., Urtasun, R., Fidler, S.: Situation recognition with graph neural networks. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 4173-4182 (2017) 32. Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: ACL 2004 (2004) 33. Lin, T.Y., Dollr, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection (2016) 34. Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollr, P.: Focal loss for dense object detection (2017) 35. Lin, T.Y., Maire, M., Belongie, S.J., Hays, J., Perona, P., Ramanan, D., Doll?r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014) 36. Mallya, A., Lazebnik, S.: Recurrent models for situation recognition (2017) 37. Mao, J., Huang, J., Toshev, A., Camburu, O.M., Yuille, A.L., Murphy, K.: Generation and comprehension of unambiguous object descriptions. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 11-20 (2015) 38. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: ACL (2001) 39. Philbin, J., Chum, O., Isard, M., Sivic, J., Zisserman, A.: Object retrieval with large vocabularies and fast spatial matching. 2007 IEEE Conference on Computer Vision and Pattern Recognition pp. 1-8 (2007) 58. Yao, B., Li, F.F.: Grouplet: A structured image representation for recognizing human and object interactions. 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition pp. 9-16 (2010) 59. Yatskar, M., Ordonez, V., Zettlemoyer, L., Farhadi, A.: Commonly uncommon: Semantic sparsity in situation recognition (2016) 60. Yatskar, M., Zettlemoyer, L.S., Farhadi, A.: Situation recognition: Visual semantic role labeling for image understanding. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 5534-5542 (2016) 61. Zhuang, B., Wu, Q., Shen, C., Reid, I.D., van den Hengel, A.: Hcvrd: A benchmark for large-scale human-centered visual relationship detection. In: AAAI (2018)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Additional qualitative results for semantic image retrieval. For the query figure of a baker kneading dough or multiple hikers walking, ResNet and Object Detection based methods struggle to match the semantics of the image. Grounded situation based retrieval leads to the correct semantics with matching viewpoints</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Models. Yatskar et al. [60] present a conditional random field model fed by CNN features and extend it with semantic sparsity augmentation [59]. Mallya et al. [36] improve the accuracy by using a specialized verb predictor and an RNN for noun prediction. Li et al. [31] use Graph Neural Nets to capture joint dependencies between roles. Most recently, Suhail et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Evaluation of models on the SWiG dev set. * indicates our implementation. Yellow rows indicate the base RNN model architecture with numbers from the paper. Green shows the upgraded version of this RNN model used in our proposed models -all value value-all verb value value-all value value-all value value-all value value-all Evaluation of models on the SWiG test set. * indicates our implementation</figDesc><table><row><cell></cell><cell cols="3">top-1 predicted verb</cell><cell></cell><cell cols="3">top-5 predicted verbs</cell><cell></cell><cell></cell><cell cols="3">ground truth verbs</cell></row><row><cell></cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell></row><row><cell>Method</cell><cell cols="7">verb value valuePrior Models for Situation Recognition</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CRF [60]</cell><cell cols="2">32.25 24.56 14.28</cell><cell>-</cell><cell>-</cell><cell cols="2">58.64 42.68 22.75</cell><cell>-</cell><cell>-</cell><cell cols="2">65.90 29.50</cell><cell>-</cell><cell>-</cell></row><row><cell>CRF+Aug [59]</cell><cell cols="2">34.20 25.39 15.61</cell><cell>-</cell><cell>-</cell><cell cols="2">62.21 46.72 25.66</cell><cell>-</cell><cell>-</cell><cell cols="2">70.80 34.82</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">RNN w/o Fusion[36] 35.35 26.80 15.77</cell><cell>-</cell><cell>-</cell><cell cols="2">61.42 44.84 24.31</cell><cell>-</cell><cell>-</cell><cell cols="2">68.44 32.98</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">RNN w/ Fusion[36] 36.11 27.74 16.60</cell><cell>-</cell><cell>-</cell><cell cols="2">63.11 47.09 26.48</cell><cell>-</cell><cell>-</cell><cell cols="2">70.48 35.56</cell><cell>-</cell><cell>-</cell></row><row><cell>GraphNet [31]</cell><cell cols="2">36.93 27.52 19.15</cell><cell>-</cell><cell>-</cell><cell cols="2">61.80 45.23 29.98</cell><cell>-</cell><cell>-</cell><cell cols="2">68.89 41.07</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Kernel GraphNet[51] 43.21 35.18 19.46</cell><cell>-</cell><cell>-</cell><cell cols="2">68.55 56.32 30.56</cell><cell>-</cell><cell>-</cell><cell cols="2">73.14 41.48</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RNN based models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">RNN w/o Fusion [36] 35.35 26.80 15.77</cell><cell>-</cell><cell>-</cell><cell cols="2">61.42 44.84 24.31</cell><cell>-</cell><cell>-</cell><cell cols="2">68.44 32.98</cell><cell>-</cell><cell>-</cell></row><row><cell>Updated RNN*</cell><cell cols="2">38.83 30.47 18.23</cell><cell>-</cell><cell>-</cell><cell cols="2">65.74 50.29 28.59</cell><cell>-</cell><cell>-</cell><cell cols="2">72.77 37.49</cell><cell>-</cell><cell>-</cell></row><row><cell>ISL*</cell><cell cols="12">38.83 30.47 18.23 22.47 7.64 65.74 50.29 28.59 36.90 11.66 72.77 37.49 52.92 15.00</cell></row><row><cell>JSL*</cell><cell cols="12">39.60 31.18 18.85 25.03 10.16 67.71 52.06 29.73 41.25 15.07 73.53 38.32 57.50 19.29</cell></row><row><cell></cell><cell cols="3">top-1 predicted verb</cell><cell></cell><cell cols="3">top-5 predicted verbs</cell><cell></cell><cell></cell><cell cols="2">ground truth verbs</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell></row><row><cell>Method</cell><cell cols="12">verb value value-all value value-all verb value value-all value value-all value value-all value value-all</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RNN based models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Updated RNN*</cell><cell>39.36 30.09</cell><cell>18.62</cell><cell>-</cell><cell>-</cell><cell>65.51 50.16</cell><cell>28.47</cell><cell>-</cell><cell>-</cell><cell>72.42</cell><cell>37.10</cell><cell>-</cell><cell>-</cell></row><row><cell>ISL*</cell><cell>39.36 30.09</cell><cell>18.62</cell><cell>22.73</cell><cell>7.72</cell><cell>65.51 50.16</cell><cell>28.47</cell><cell>36.60</cell><cell>11.56</cell><cell>72.42</cell><cell>37.10</cell><cell>52.19</cell><cell>14.58</cell></row><row><cell>JSL*</cell><cell cols="3">39.94 31.44 18.87 24.86</cell><cell>9.66</cell><cell>67.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>51.88 29.39 40.60 14.72 73.21 37.82 56.57 18.45</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1812.08658</idno>
		<title level="m">nocaps: novel object captioning at scale. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1404.1777</idno>
		<title level="m">Neural codes for image retrieval. ArXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.3115/980451.980860</idno>
		<ptr target="https://doi.org/10.3115/980451.980860" />
		<title level="m">Proceedings of the 17th International Conference on Computational Linguistics</title>
		<meeting>the 17th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
	<note>COLING &apos;98</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEvaluation@ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="381" to="389" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Recognizing human actions in still images: a study of bag-of-features and part-based representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explainability for content-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoogs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An analysis of action recognition datasets for language and vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<idno>ArXiv abs/1704.07129</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised visual sense disambiguation for verbs using multimodal embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<idno>ArXiv abs/1603.09188</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cs-r-fcn: Cross-supervised learning for large-scale object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1905.12863</idno>
		<ptr target="http://arxiv.org/abs/1905.12863" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Visual instance retrieval with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno>abs/1412.6574</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolo9000: Better, faster, stronger</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<editor>Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Describing common human visual actions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02203</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Charades-ego: A large-scale dataset of paired third and first person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<idno>ArXiv abs/1804.09626</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno>ArXiv abs/1604.01753</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">R-fcn-3000 at 30fps: Decoupling detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1081" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mixture-kernel graph attention network for situation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10363" to="10372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4466" to="4475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Efficient image retrieval via decoupling diffusion into online and offline processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting 11k classes: Large scale object detection without fine-grained bounding boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9804" to="9812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1331" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
