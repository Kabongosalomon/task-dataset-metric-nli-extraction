<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gotta Go Fast When Generating Data with Score-Based Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Kachman</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Montreal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Montreal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Artificial Intelligence Radboud University &amp; Donders Institute Ioannis Mitliagkas Department of Computer Science</orgName>
								<orgName type="institution">University of Montreal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Gotta Go Fast When Generating Data with Score-Based Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Score-based (denoising diffusion) generative models have recently gained a lot of success in generating realistic and diverse data. These approaches define a forward diffusion process for transforming data to noise and generate data by reversing it (thereby going from noise to data). Unfortunately, current score-based models generate data very slowly due to the sheer number of score network evaluations required by numerical SDE solvers. In this work, we aim to accelerate this process by devising a more efficient SDE solver. Existing approaches rely on the Euler-Maruyama (EM) solver, which uses a fixed step size. We found that naively replacing it with other SDE solvers fares poorly -they either result in low-quality samples or become slower than EM. To get around this issue, we carefully devise an SDE solver with adaptive step sizes tailored to score-based generative models piece by piece. Our solver requires only two score function evaluations, rarely rejects samples, and leads to high-quality samples. Our approach generates data 2 to 10 times faster than EM while achieving better or equal sample quality. For high-resolution images, our method leads to significantly higher quality samples than all other methods tested. Our SDE solver has the benefit of requiring no step size tuning. Code is available on https://github.com/AlexiaJM/score_sde_fast_sampling. ? Equal contribution Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Score-based generative models <ref type="bibr" target="#b4">[Song and Ermon, 2019</ref><ref type="bibr" target="#b6">, Ho et al., 2020</ref><ref type="bibr" target="#b7">, Jolicoeur-Martineau et al., 2020</ref><ref type="bibr" target="#b8">, Song et al., 2020a</ref><ref type="bibr">, Pich?-Taillefer, 2021</ref> have been very successful at generating data from various modalities, such as images <ref type="bibr" target="#b6">[Ho et al., 2020</ref><ref type="bibr" target="#b8">, Song et al., 2020a</ref>, audio <ref type="bibr" target="#b10">[Chen et al., 2020</ref><ref type="bibr" target="#b11">, Kong et al., 2020</ref><ref type="bibr" target="#b12">, Mittal et al., 2021</ref><ref type="bibr" target="#b13">, Kameoka et al., 2020</ref>, and graphs <ref type="bibr" target="#b14">[Niu et al., 2020]</ref>. They have further been used effectively for super-resolution <ref type="bibr">[Saharia et al., 2021, Kadkhodaie and</ref><ref type="bibr" target="#b16">Simoncelli, 2020]</ref>, inpainting <ref type="bibr" target="#b16">[Kadkhodaie and</ref><ref type="bibr">Simoncelli, 2020, Song and</ref>, source separation <ref type="bibr" target="#b17">[Jayaram and Thickstun, 2020]</ref>, and image-to-image translation <ref type="bibr" target="#b18">[Sasaki et al., 2021]</ref>. In most of these applications, score-based models achieved superior performances in terms of quality and diversity than the historically dominant Generative Adversarial Networks (GANs) <ref type="bibr" target="#b19">[Goodfellow et al., 2014]</ref>. <ref type="figure">Figure 1</ref>: Comparison between our novel SDE solver at various values of error tolerance and Euler-Maruyama for an equal computational budget. We measure speed through the Number of Function Evaluations (NFE) and the quality of the generated images through the Fr?chet Inception Distance (FID; lower is better). See <ref type="table" target="#tab_0">Table 1</ref>-2 for more details.</p><p>In Score-based approaches, a diffusion process progressively transforms real data into Gaussian noise. Then, the process is reversed in order to generate real data from Gaussian noise. Reversing the process requires the score function, which is estimated with a neural network (known as a score network). Although very powerful, score-based models generate data through an undesirably long iterative process; meanwhile, other state-of-the-art methods such as GANs generate data from a single forward pass of a neural network. Increasing the speed of the generative process is thus an active area of research. <ref type="bibr" target="#b10">Chen et al. [2020]</ref> and <ref type="bibr" target="#b20">San-Roman et al. [2021]</ref> proposed faster step size schedules for VP diffusions that still yield relatively good quality/diversity metrics. Although fast, these schedules are arbitrary, require careful tuning, and the optimal schedules will vary from one model to another. <ref type="bibr" target="#b21">Block et al. [2020]</ref> proposed generating data progressively from low to high-resolution images and show that the scheme improves speed. Similarly, <ref type="bibr" target="#b22">Nichol and Dhariwal [2021]</ref> proposed generating low-resolution images and then upscale them since generating low-resolution images is quicker. They further suggested to accelerate VP-based models by learning dimension-specific noise rather than assuming equal noise everywhere. Note that these methods do not affect the data generation algorithm and would thus be complementary to our methods. <ref type="bibr" target="#b8">Song et al. [2020a]</ref> and <ref type="bibr" target="#b23">Song et al. [2020b]</ref> proposed removing the noise from the data generation algorithm and solve an Ordinary Differential Equation (ODE) rather than a Stochastic Differential Equation (SDE); they report being able to converge much faster when there is no noise. Although it improves the generation speed, <ref type="bibr" target="#b8">Song et al. [2020a]</ref> report obtaining lower-quality images when using the ODE formulation for the VE process <ref type="bibr" target="#b8">[Song et al., 2020a]</ref>. We will later show that our SDE solver generally leads to better results than ODE solvers at similar speeds. Thus, existing methods for acceleration often require considerable step size/schedule tuning (this is also true for the baseline approach) and do not always work for both VE and VP processes. To improve speed and remove the need for step size/schedule tuning, we propose to solve the reverse diffusion process using SDE solvers with adaptive step sizes.</p><p>It turns out that off-the-shelf SDE solvers are ill-suited for generative modeling and exhibit (1) divergence, (2) slower data generation than the baseline, or (3) significantly worse quality than the baseline (see Appendix A). This can be attributed to distinct features of the SDEs that arise in score-based generative models that set them apart from the SDEs traditionally considered in the numerical SDE solver literature, namely: (1) the codomain of the unknown function is extremely high-dimensional, especially in the case of image generation; (2) evaluating the score function is computationally expensive, requiring a forward pass of a large mini-batch through a large neural network; (3) the required precision of the solution is smaller than usual because we are satisfied as long as the error is not perceptible (e.g., one RGB increment on an image).</p><p>We devise our own SDE solver with these features in mind, resulting in an algorithm that can get around the problems encountered by off-the-shelf solvers. To address high dimensionality, we use the 2 norm rather than the ? norm to measure the error across different dimensions to prevent a single pixel from slowing down the solver. To address the cost of score function evaluations while still obtaining high precision, we (1) take the minimum number of score function evaluations needed for adaptive step sizes (two evaluations), and (2) use extrapolation to get high precision at no extra cost. To take advantage of the reduced requirement for precision, we set the absolute tolerance for the error according to the range of RGB values.</p><p>Our main contribution is a new SDE solver tailored to score-based generative models with the following benefits:</p><p>? Our solver is much faster than the baseline methods, i.e. reverse-diffusion method with Langevin dynamics and Euler-Maruyama (EM);</p><p>? It yields higher quality/diversity samples than EM when using the same computing budget;</p><p>? It does not require any step size or schedule tuning;</p><p>? It can be used to quickly solve any type of diffusion process (e.g., VE, VP) 2 Background 2.1 Score-based modeling with SDEs Let x(0) ? R d be a sample from the data distribution p data . The sample is gradually corrupted over time through a Forward Diffusion Process (FDP), a common type of Stochastic Differential Equation <ref type="table">(SDE)</ref>:</p><formula xml:id="formula_0">dx = f (x, t)dt + g(t)dw,<label>(1)</label></formula><formula xml:id="formula_1">where f (x, t) : R d ? R ? R d is the drift, g(t)</formula><p>: R ? R is the diffusion and w(t) is the Wiener process indexed by t ? [0, 1]. Data points and their probability distribution evolve along the trajectories {x(t)} 1 t=0 and {p t (x)} 1 t=0 respectively, with p 0 ? p data . The functions f and g are chosen such that x(1) be approximately Gaussian and independent from x(0). Inference is achieved by reversing this diffusion, drawing x(1) from its Gaussian distribution and solving the Reverse Diffusion Process (RDP) equal to:</p><formula xml:id="formula_2">dx = f (x, t) ? g(t) 2 ? x log p t (x) dt + g(t)dw,<label>(2)</label></formula><p>where ? x log p t (x) is referred to as the score of the distribution at time t <ref type="bibr" target="#b24">[Hyv?rinen, 2005]</ref> andw(t) is the Wiener process in which time flows backward <ref type="bibr" target="#b25">[Anderson, 1982]</ref>.</p><p>One can observe from Equation 2 that the RDP requires knowledge of the score (or p t ), which we do not have access to. Fortunately, it can be estimated by a neural network (referred to as the score network) by optimizing the following objective:</p><formula xml:id="formula_3">L(?) = E x(t)?p(x(t)|x(0)),x(0)?pdata ?(t) 2 s ? (x(t), t) ? ? x(t) log p t (x(t)|x(0)) 2 2 ,<label>(3)</label></formula><p>where ?(t) : R ? R is a weighting function generally chosen to be inversely proportional to:</p><formula xml:id="formula_4">E ? x(t) log p t (x(t)|x(0)) 2 2 .</formula><p>One can demonstrate that the minimizer of that objective ? * will be such that s ? * (x, t) = ? x log p t (x) <ref type="bibr" target="#b26">[Vincent, 2011]</ref>, allowing us to approximate the reverse diffusion process. As can be seen, evaluating the objective requires the ability to generate samples from the FDP at arbitrary times t. Thankfully, as long as the drift is affine (i.e., f (x, t) = Ax + B), the transition kernel p(x(t)|x(0)) will always be normally distributed <ref type="bibr" target="#b27">[S?rkk? and Solin, 2019]</ref>, which means that we can solve the forward diffusion in a single step. Furthermore, the score of the Gaussian transition kernel is trivial to compute, making the loss an inexpensive training objective.</p><p>There are two primary choices for the FDP in the literature, which we discuss below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Variance Exploding (VE) process</head><p>The Variance Exploding (VE) process consists in the following FDP:</p><formula xml:id="formula_5">dx = d [? 2 (t)] dt dw.</formula><p>Its associated transition kernel is:</p><formula xml:id="formula_6">x(t)|x(0) ? N (x(0), [? 2 (t) ? ? 2 (0)]I) ? N (x(0), ? 2 (t)I).</formula><p>In practice, we let ?(t) = ? min ?max ?min t , where ? min = 0.01 and ? max ? max i N j=1 ||x (i) ? x (j) || is the maximum Euclidean distance between two samples from the dataset {x (i) } N i=1 . Using the maximum Euclidean distance ensures that x(1) does not depend on x(0); thus, x(1) is approximately distributed as N (0, ? 2 (1)I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Variance Preserving (VP) process</head><p>The Variance Preserving (VP) process consists in the following FDP:</p><formula xml:id="formula_7">dx = ? 1 2 ?(t)xdt + ?(t)dw.</formula><p>Its associated transition kernel is:</p><formula xml:id="formula_8">x(t)|x(0) ? N (x(0) e ? 1 2 t 0 ?(s)ds , (1 ? e ? t 0 ?(s)ds ) I).</formula><p>In practice, we let ?(t) = ? min + t (? max ? ? min ), where ? min = 0.1 and ? max = 20. Thus, x(1) is approximately distributed as N (0, I) and does not depend on x(0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Solving the Reverse Diffusion Process (RDP)</head><p>There are many ways to solve the RDP; the most basic one being Euler-Maruyama <ref type="bibr" target="#b28">[Kloeden and Platen, 1992]</ref>, the SDE analog to Euler's method for solving ODEs. <ref type="bibr" target="#b8">Song et al. [2020a]</ref> also proposed Reverse-Diffusion, which consists in ancestral sampling <ref type="bibr" target="#b6">[Ho et al., 2020]</ref> with the same discretization used in the FDP. With the Reverse-Diffusion, <ref type="bibr" target="#b8">[Song et al., 2020a]</ref> obtained poor results unless applying an additional Langevin dynamics step after each Reversion-Diffusion step. They named this approach Predictor-Corrector (PC) sampling, with the predictor corresponding to Reverse-Diffusion and the corrector to Langevin dynamics. Although using a corrector step leads to better samples, PC sampling is only heuristically motivated and the theoretical underpinnings are not yet understood. Nevertheless, <ref type="bibr" target="#b8">[Song et al., 2020a]</ref> report their best results (in terms of lowest Fr?chet Inception Distance <ref type="bibr" target="#b29">[Heusel et al., 2017]</ref>) using the Reverse-Diffusion with Langevin dynamics for VE models. For VP models, they obtain their best results using Euler-Maruyama.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Efficient Method for Solving Reverse Diffusion Processes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setting up the algorithm</head><p>We start with a general algorithm for solving an SDE (similar to most ODE/SDE solvers). We choose the various options/hyper-parameters based on a mixture of theory and experiments; an ablation study of the different hyper-parameters can also be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Integration method</head><p>Solving the RDP to generate data can take an undesirably long time. One would assume that solving SDEs with high-order methods would improve speed over Euler-Maruyama, just like high-order ODE solvers improve speed over Euler's method when solving ODEs. However, this is not always the case: while higher-order solvers may achieve lower discretization errors, they require more function evaluations, and the improved precision might not be worth the increased computation cost <ref type="bibr" target="#b30">[Lehn et al., 2002</ref><ref type="bibr" target="#b31">, Lamba, 2003</ref>].</p><p>Our preliminary attempts at using SDE solvers with the DifferentialEquations.jl Julia package <ref type="bibr" target="#b32">[Rackauckas and Nie, 2017a]</ref> confirmed that higher-order methods were significantly slower (6 to 8 times slower; see Appendix A). Lamba's algorithm <ref type="bibr" target="#b31">[Lamba, 2003]</ref>, a low-order adaptive method, yielded the fastest results, thus motivating us to restrict our search to the space of low-order methods. Still, the resulting images were of lower quality.</p><p>To be able to dynamically adjust the step size over time, thereby gaining speed over a fixed-step size algorithm, two integration methods are employed. Traditionally, for ODEs, a lower-order (x ) method is used conjointly with a higher-order (x ) one. The local error E(x , x ) = x ? x can be used to determine how stable the lower-order method is at the current step size; the closer to zero, the more appropriate the step size is. From this information, we can dynamically adjust the step size and decide whether or not to accept the proposed sample of the lower-order method. Alternatively, one can select x as the proposal, which we will refer to as extrapolating.</p><p>Rather than using Improved Euler <ref type="bibr" target="#b33">[S?li and Mayers, 2003</ref>] as in <ref type="bibr" target="#b31">Lamba [2003]</ref> or a high-order stochastic Runge-Kutta method <ref type="bibr" target="#b34">[R??ler, 2010]</ref> as in <ref type="bibr" target="#b35">Rackauckas and Nie [2017b]</ref> (which did not work well in our preliminary attempts with the Julia package) we instead rely on the stochastic Improved Euler's method <ref type="bibr" target="#b36">[Roberts, 2012]</ref> as our higher order method. This method only requires two score function evaluations and re-use the same score function evaluation used for EM, meaning that it is only twice as expensive as EM. Similarly to Lamba's algorithm, this method, albeit quick, lead to images of poor quality. However, by using extrapolation (taking x instead of x as our proposal), we were able to match and improve over the baseline approach (EM). Thus, using the stochastic Improved Euler was the key to taking bigger steps without sacrificing precision. Note that Lamba's algorithm cannot use extrapolation due to its use of a non-stochastic ODE solver (Improved Euler).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Tolerance</head><p>In ODE/SDE solvers, the local error is divided by a tolerance term. Traditionally, the mixed tolerance ?(x ) : R d ? R d is calculated as the maximum between the absolute and relative tolerance:</p><formula xml:id="formula_9">?(x ) = max( abs , rel |x |),<label>(4)</label></formula><p>where the absolute value | ? | is applied element-wise.</p><p>The DifferentialEquations.jl Julia package instead calculates the mixed tolerance through the maximum of the current and previous sample:</p><formula xml:id="formula_10">?(x , x prev ) = max( abs , rel max(|x |, |x prev |)).<label>(5)</label></formula><p>Having no trivial prior for which approach to use, we tried both and found the latter approach (Equation 5) to converge much faster for VE models (see Appendix B).</p><p>Given our focus on image generation, we can set abs a priori. During training and at the end of the data generation, images are represented as floating-point tensors with range [y min , y max ]. When evaluated, they must be transformed into 8-bit color images; this means that images are scaled to the range [0, 255] and converted to the nearest integer (to represent one of the 256 values per color channel). Given the 8-bit color encoding, an absolute tolerance abs = ymax?ymin 256 corresponds to tolerating local errors of at most one color (e.g., x ij with Red=5 and x ij with Red=6 is accepted, but x ij with Red=5 and x ij with Red=7 is not) channel-wise. One-color differences are not perceptible and should not influence the metrics used for evaluating the generated images. For VP models, which have range [?1, 1], this corresponds to abs = 0.0078 while for VE models, which have range [0, 1], this corresponds to abs = 0.0039.</p><p>To control speed/quality, we vary rel , where large values lead to more speed but less precision, while small values lead to the converse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Norm of the scaled error</head><p>The scaled error (the error scaled by the mixed tolerance) is calculated as</p><formula xml:id="formula_11">E q = x ? x ?(x , x prev ) q .</formula><p>Many algorithms use q = ? <ref type="bibr">[Lamba, 2003, Rackauckas and</ref><ref type="bibr" target="#b35">Nie, 2017b]</ref>, where ||x|| ? = max(x 1 , ..., x k ) over all k elements of x. Although this can work with low-dimensional SDEs, this is highly problematic for high-dimensional SDEs such as those in image-space. The reason is that a single channel of a single pixel (out of 65536 pixels for a 256 ? 256 color image) with a large local error will cause the step size to be reduced for all pixels and possibly lead to a step size rejection. Indeed, our experiments confirmed that using q = ? slows down generation considerably (see Appendix B). To that effect, we instead use a scaled 2 norm:</p><formula xml:id="formula_12">||x|| 2 = 1 n k i=1 (E q ) k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Hyperparameters of the dynamic step size algorithm</head><p>Upon calculating the scaled error, we accept the proposal x if E q ? 1 and increment the time by h Whether or not it is accepted, we update the next step size h in the usual way: h ? min(h max , ?hE ?r q ), where h max is the maximum step size, ? is the safety parameter which determines how strongly we adapt the step size (0 being very safe; 1 being fast, but high rejections rate), and q is an exponentscaling term.</p><p>Although ODE theory tells us that we should let r = 1 p+1 with p being the order of the lowerorder integration method, there is no such theory for SDEs <ref type="bibr" target="#b35">[Rackauckas and Nie, 2017b]</ref>. Thus, as <ref type="bibr" target="#b35">Rackauckas and Nie [2017b]</ref> suggest, we resorted to empirically testing values and found that any r ? [0.5, 1] works well on both VE and VP processes, but that r ? [0.8, 0.9] is slightly faster (see Appendix B). We arbitrarily chose r = 0.9 as the default setting.</p><p>Finally, we defaulted to setting ? = 0.9 for the safety parameter as is common in the literature, and choose h max to be equal to the largest step size possible, namely the remaining time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Handling the mini-batch</head><p>Using the same step size for every sample of a mini-batch means that every images would be slowed down by the other images. Since every image's RDP is independent from one another, we apply a different step size to each data sample; some images may converge faster than others, but we wait for all images to have converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm</head><p>We have now defined every aspect of the algorithm needed to numerically solve the Equation (2) for images. The resulting algorithm is described in Algorithm 1. This algorithm is straightforward to parallelize across the batch dimension. Note that this algorithm is only for solving an RDP; a more general version for solving an arbitrary forward-time diffusion process can be found in Appendix C. Additionally, we present a demonstration in Appendix F showing that the extrapolation step conserves the stability and convergence of the EM step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Dynamic step size extrapolation for solving Reverse Diffusion Processes</head><p>Require: s ? , rel , abs , h init = 0.01, r = 0.9, ? = 0.9 for images: abs = ymax?ymin</p><formula xml:id="formula_13">256 t ? 1 h ? h init Initialize x x prev ? x while t &gt; 0 do Draw z ? N (0, I) x ? x ? hf (x, t) + hg(t) 2 s ? (x, t) + ? hg(t)z Euler-Maruyam? x ? x ? hf (x , t ? h) + hg(t ? h) 2 s ? (x , t ? h) + ? hg(t ? h)z x ? 1 2 (x +x) Improved Euler (SDE version) ? ? max( abs , rel max(|x |, |x prev |)) Element-wise operations E 2 ? 1 ? n (x ? x ) /? 2 if E 2 ? 1 then Accept x ? x Extrapolation t ? t ? h x prev ? x h ? min(t, ?hE ?r 2 ) Dynamic step size update return x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To test Algorithm 1 on RDPs, we apply it to various pre-trained models from <ref type="bibr" target="#b8">Song et al. [2020a]</ref>. To start, we generate low-resolution images (32x32) by testing the VP, VE, VP-deep, and VEdeep models on CIFAR-10 [ <ref type="bibr" target="#b37">Krizhevsky et al., 2009]</ref>. Then, we generate higher-resolutions images (256x256) by testing the VE models on LSUN-Church <ref type="bibr" target="#b38">[Yu et al., 2015]</ref>, and Flickr-Faces-HQ (FFHQ) <ref type="bibr" target="#b39">[Karras et al., 2019]</ref>. See implementation details in Appendix D. We used four or less V100 GPUs to run the experiments.</p><p>To measure the performance of the image generation, we calculate the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b29">[Heusel et al., 2017]</ref> and the Inception Score (IS) <ref type="bibr">[Salimans et al., 2016]</ref>, where low FID and high IS correspond to higher quality/diversity. We compare our method to the three base solvers used in <ref type="bibr" target="#b8">Song et al. [2020a]</ref>: Reverse-Diffusion with Langevin dynamics, Euler-Maruyama (EM), and Probability Flow, where the latter solves an ODE instead of an SDE using Runge-Kutta 45 <ref type="bibr">[Dormand and Prince, 1980]</ref>. We also compare against the fast solver by <ref type="bibr" target="#b23">[Song et al., 2020b]</ref> called denoising diffusion implicit models (DDIM), which is only defined for VP models. We define the baseline approach as the solver used by <ref type="bibr" target="#b8">Song et al. [2020a]</ref> which leads to the lowest FID (EM for VP models and Reverse-Diffusion with Langevin for VE models). For our algorithm, the only free hyperparameter is the relative tolerance which we set to rel ? {0.01, 0.02, 0.05, 0.1, 0.5}.</p><p>The FID and the Number of score Function Evaluations (NFE) are described in <ref type="table" target="#tab_0">Table 1 for lowresolution images and Table 2</ref> for high-resolution images. The Inception Score (IS) is described for CIFAR-10 in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance</head><p>Compared to EM, we observe that our method is immediately advantageous in terms of quality/diversity for high-resolution images, along with 2 to 3? speedups ( rel = 0.02). While this advantage becomes less obvious in terms of the FID on CIFAR-10, our method still offers &gt; 5? computational speedups at no apparent disadvantage ( rel ? {0.02, 0.05}).</p><p>Reverse-Diffusion with Langevin achieves the lowest FID for VE models on CIFAR-10, though at the cost of a 4? computational overhead over our method. Furthermore, their advantage vanishes for VP models and when generating high-resolution images.</p><p>We further compare our SDE solver to EM given the same computational budget and observe that our method is always immensely preferable in high-resolutions and for VP models. For VE models on CIFAR-10, we observe that our algorithm leads to a better FID as long as the NFE is sufficiently   <ref type="formula" target="#formula_2">(270)</ref>. Note that since our algorithm takes two score function evaluations per step, EM has the advantage of doing twice as many steps given the same NFE, which appears to be a factor more important than the order of the method at low budget in low-resolution VE. Nevertheless, comparing for equal number of iterative step, the results still point to our method being always preferable. For high-resolution images, we see that EM cannot converge on moderate to small NFEs due to the high-dimensionality, making of our SDE solver the way to go.</p><p>Generally, we observe that the VE process cannot be solved as fast as the VP process; this is due to the enormous Gaussian noise in the VE process causing larger local errors. This reflects the issue mentioned in Section 3.1.1 regarding high-order SDE solvers not always being beneficial in terms of speed for SDEs with heavy Gaussian noise. In practice, for VE, the algorithm uses a small step size in the beginning to ensure high accuracy and eventually increases the step size as the noise becomes less considerable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Solving an ODE instead of an SDE</head><p>We see that our SDE solver generally does better than Probability Flow, especially in high-resolution, where we obtain greatly lower FIDs with a similar budget. Our algorithm attains the same NFE as Probability Flow when rel = 0.10 for low-resolution images and when 0.05 &lt; rel &lt; 0.10 for high-resolution images. For the same budget, Probability Flow has higher FID than our approach on all but low-resolution VE models. However, in that case, our algorithm achieves a much lower FID when rel ? 0.05, albeit slower. In high-resolution, Probability Flow leads to very poor FIDs, suggesting no convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DDIM</head><p>Contrary to <ref type="bibr" target="#b23">Song et al. [2020b]</ref>,the FID of DDIM worsens significantly when the NFE decreases. This could be due to differences between <ref type="bibr" target="#b8">Song et al. [2020a]</ref> continuous-time score-matching and the DDIM training procedure and architecture. Nevertheless, the FID increase engendered by a reduced budget is much less dramatic than for EM. As of note, DDIM succeeds in maintaining a lower FID than our solver at extremely small NFEs (&lt; 50), albeit with poor performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>Although we tested our approach on a wide range of settings, we nevertheless only tested on continuous-time image generation models. We did so because solving the SDE requires continuoustime and the only such pre-trained models at time of publishing are the one by <ref type="bibr" target="#b8">Song et al. [2020a]</ref>. Future work should train continuous-time versions of models from different data types, model architectures, and the learned-variance approach by <ref type="bibr" target="#b22">Nichol and Dhariwal [2021]</ref>, which generates data inherently faster; our SDE solver could then be used on these models.</p><p>Although our approach removes step size and schedule tuning, we still need to choose a value of the relative tolerance, which indirectly affects the number of steps taken; one could theoretically tune this hyper-parameter to optimize a certain metric, going against the point of removing tuning. Still, letting rel = 0.01 for precise results and rel = 0.05 for fast results are reasonable choices, as all evidence points to the FID being stable w.r.t. rel .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We built an SDE solver that allows for generating images of comparable (or better) quality to Euler-Maruyama at a much faster speed. Our approach makes image generation with score-based models more accessible by shrinking the required computational budgets by a factor of 2 to 5?, and presenting a sensible way of compromising quality for additional speed. Nevertheless, data generation remains slow (a few minutes) compared to other generative models, which can generate data in a single forward pass of a neural network. Therefore, additional work would be needed to find ways to make these models fast enough to be more attractive and useful for real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Broader Impact</head><p>Our work allows generating data from score-based generative models more quickly, taking the technology closer to real-time applications. While generative models have many socially beneficial applications, they can also be used to maliciously deceive humans (e.g. deepfakes). Generative models also carry the risk of reproducing biases in existing datasets.   <ref type="bibr" target="#b34">[R??ler, 2010]</ref> 1.5 Yes 5.92 times slower SRA3 <ref type="bibr" target="#b34">[R??ler, 2010]</ref> 1.5 Yes 6.93 times slower Lamba EM (default) <ref type="bibr" target="#b31">[Lamba, 2003]</ref> 0.5 Yes Did not converge Lamba EM (atol=1e-3) <ref type="bibr" target="#b31">[Lamba, 2003]</ref> 0.5 Yes 2 times faster Lamba EM (atol=1e-3, rtol=1e-3) <ref type="bibr" target="#b31">[Lamba, 2003]</ref> 0.5 Yes 1.27 times faster Euler-Heun 0.5 No 1.86 times slower Lamba Euler-Heun <ref type="bibr" target="#b31">[Lamba, 2003]</ref> 0.5 Yes 1.75 times faster SOSRI <ref type="bibr" target="#b34">[R??ler, 2010]</ref> 1.5 Yes 8.57 times slower RKMil (at various tolerances) <ref type="bibr" target="#b28">[Kloeden and Platen, 1992]</ref> 1.0 Yes Did not converge ImplicitRKMil <ref type="bibr" target="#b28">[Kloeden and Platen, 1992]</ref> 1.0 Yes Did not converge ISSEM 0.5 Yes Did not converge Here, we report the preliminary experiments we ran with the DifferentialEquations.jl Julia package <ref type="bibr" target="#b32">[Rackauckas and Nie, 2017a]</ref> before devising our own SDE solver. As can be seen, most methods either did not converge (with warnings of "instability detected") or converged, but were much slower than Euler-Maruyama. The only promising method was Lamba's method <ref type="bibr" target="#b31">[Lamba, 2003]</ref>. Note that an algorithm has strong-order p when the local error from t to t + h is O(h p+1 )). As can be seen, most chosen settings are lead to better results. However, r seems to have little impact on the FID. Still, using r ? [0.8, 0.9] lead to a little bit less score function evaluations and sometimes lead to lower FID. Variations of <ref type="bibr" target="#b31">Lamba [2003]</ref> Algorithm r = 0.5, Lamba integration 9.08 18.28 2492 r = 0.5, Lamba integration, Extrapolation 3.70 169.78 2252 r = 0.5, Lamba integration, q = ? 9.42 6.80 5886 r = 0.5, Lamba integration, q = ?, ? = 0.8 9.35 6.20 2970 C Dynamic step size algorithm for solving any type of SDE (rather than just Reverse Diffusion Processes) Assume, we have a Diffusion Process of the form:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Effects of modifying Algorithm 1</head><formula xml:id="formula_14">dx = f (x, t)dt + g(x, t)dw.<label>(6)</label></formula><p>The algorithm to solve it is represented in Algorithm 2. The differences are:</p><p>? it is in forward-time ? the range of time must be given ? The diffusion can depend on x, which leads to a slightly more complicated formulation that depends on some random number s = ?1 [Roberts, 2012]. ? we retain the full trajectory instead of only the ending ? we retain the noise after a rejection to ensure that there is no bias in the rejections</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Dynamic step size extrapolation for solving arbitrary (forward-time) Diffusion Processes</head><p>Require: s ? , t begin , t end , rel , abs , h init = 0.01, r = 0.9, ? = 0.9</p><formula xml:id="formula_15">t ? t begin h ? h init Initialize x(t) x prev ? x Draw z ? N (0, I) while t &lt; t end do if Stratonovich SDE or g(x, t) = g(x) then s ? 0 else It? diffusion Draw s ? Uniform({?1, 1}) x ? x(t) + hf (x(t), t) + ? hg(x(t), t)(z ? s) Euler-Maruyam? x ? x(t) + hf (x , t + h) + ? hg(x , t + h)(z + s) x ? 1 2 (x +x) Improved Euler (SDE version) ? ? max( abs , rel max(|x |, |x prev |)) Element-wise operations E 2 ? 1 ? n (x ? x ) /? 2 if E 2 ? 1 then Accept t ? t + h x(t) ? x Extrapolation x prev ? x Draw z ? N (0, I) h ? min(t, ?hE ?r 2 ) Dynamic step size update return x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Implementation Details</head><p>We started from the original code by <ref type="bibr" target="#b8">Song et al. [2020a]</ref> but changed a few settings concerning the SDE solving. This create some very minor difference between their reported results and ours. For the VP and VP-deep models, we obtained 2.55 and 2.49 instead of the original 2.55 and 2.41 for the baseline method (EM). For the VE and VE-deep models, we obtained 2.40 and 2.21 instead of the original 2.38 and 2.20 for the baseline method (Reverse-Diffusion with Langevin).</p><p>When solving the SDE, time followed the sequence t 0 = 1, t i = t i?1 ? 1? N , where N = 1000 for CIFAR-10, N = 2000 for LSUN, = 1e ? 3 for VP models, and = 1e ? 5 for VE models.</p><p>Meanwhile, the actual step size h used in the code for Euler-Maruyama (EM) was equal to 1 N . Thus, there was a negligible difference between the step size used in the algorithm (h = 1 N ) and the actual step size implied by t (h = 1? N ). Note that this has little to no impact. The bigger issue is at the last predictor step was going from t = to t = ? 1 N &lt; 0. Thus, t was made negative. Furthermore the sample was denoised at t &lt; 0 while assuming t = . There are two ways to fix this issue: 1) take only a step from t = to t = 0 and do not denoise (since you cannot denoise with the incorrect t or with t = 0), or 2) stop at t = and then denoise. Since denoising is very helpful, we took approach 2; however, both approaches are sensible.</p><p>Finally, denoising was not implemented correctly before. Denoising was implemented as one predictor step (Reverse-Diffusion or EM) without adding noise. This corresponds to:</p><formula xml:id="formula_16">x ? x ? h f (x, t) ? g(t) 2 ? x log p t (x) .</formula><p>At the last iteration, this incorrect denoising would be:</p><formula xml:id="formula_17">x ? x + d[? 2 (t)] dt 1 N ? x log p t (x) = x + ? min N 2 log ? max ? min ? x log p t (x) ? x</formula><p>for VE and</p><formula xml:id="formula_18">x ? x + ? ? min N ? x log p t (x) ? x for VP.</formula><p>Meanwhile, the correct way to denoise based on Tweedie formula <ref type="bibr">[Efron, 2011]</ref> is:</p><formula xml:id="formula_19">x ? x + Var[x(t)|x(0)]? x log p t (x),</formula><p>where Var[x(t)|x <ref type="formula">(0)</ref>] is the variance of the transition kernel: Var[x(t)|x(0)] = ? min = 0.01 for VE and Var[x(t)|x(0)] = 1. This means that the correct Tweedie formula corresponds to</p><formula xml:id="formula_20">x ? x + 0.01 2 ? x log p t (x) ? x for VE and x ? x + ? x log p t (x)</formula><p>for VP.</p><p>As can be seen, denoising has a very small impact on VE so the difference between the correct and incorrect denoising is minor. Meanwhile, for VP the incorrect denoising lead to a tiny change, while the correct denoising lead to a large change. In practice, we observe that changing the denoising method to the correct one does not significantly affect the FID with VE, but lowers down the FID significantly with VP.</p><p>E Inception Score on CIFAR-10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Stability and Bias of the Numerical Scheme</head><p>The following constructions rely on the underlying assumption of the stochastic dynamics being driven by a wiener process. More so, we also assume that the Brownian motion is time symmetrical. Both assumptions are consistent and widely used in the literature; for example, see <ref type="bibr">[Gardiner, 2009]</ref> [Arnold, 1974].</p><p>The method described in Algorithm 1 gives us a significant speedup in terms of computing time and actions. Albeit the speed up comes from a piece-wise step in the algorithm combining the traditional Euler Maruyama (EM) with a form of adaptive step size predictor-corrector. Here we show that both the stability and the convergence of the EM scheme are conserved by introducing the extra adaptive stepsize of our new scheme. As a first step, we define the stability and bias in a Stochastic Differential Equation (SDE) numerical solution.</p><p>We denote (?) as the real value of a complex-valued ?.</p><p>The linear test SDE is defined in the following way:</p><formula xml:id="formula_21">dx t = ?x t dt + ?dw t<label>(7)</label></formula><p>with its numerical counterpart y n+1 = (h?) y n + z n ,</p><p>where the z n are random variables that do not depend on y 0 , y 1 ......y n or ? and the EM scheme is</p><formula xml:id="formula_22">y n+1 = (1 + h?) y n + z n .</formula><p>A numerical scheme is asymptotically unbiased with step size h &gt; 0 if, for a given linear SDE <ref type="formula" target="#formula_21">(7)</ref> driven by a two-sided Wiener process, the distribution of the numerical solution y n converges as n ? ? to the normal distribution with zero mean and variance ? 2 2|?| [Artemiev and Averina, 2011]. This stems from the fact that a solution of a linear SDE <ref type="formula" target="#formula_21">(7)</ref> is a Gaussian process whenever the initial condition is Gaussian (or deterministic); thus, there are only two moments that control the bias in the algorithm: In what follows, we will trace the criteria for bias through our algorithm and show that it remains unbiased. By construction, the first EM step remains unbiased, while for the RDP, we write down the time reverse Wiener process as? n+1 = (1 + ?h)? n +z n in the reverse time steps h i.e., t ? nh, t ? 2nh,</p><formula xml:id="formula_23">E ? n+1 = (1 + ? (t ? h)) E [? n ] = (1 + ? (t ? h)) E (1 + ? (t ? h))? n?1 . . . = (1 + ? (t ? h)) n+1 E [? 0 ] = (1 + ? (t ? h)) n+1 E [y 0 ] . Thus, if |1 + ? (t ? h)| &lt; 1, then lim n?? E y (h) n = 0.</formula><p>In Algorithm 1, we are performing consecutive steps forward and backwards in time so t = 2h such that |1 + ?h| &lt; 1.</p><p>Thus, the scheme is both numerically stable and unbiased with respect to the mean.</p><p>Next, we focus on the numerical solution in mean square:</p><formula xml:id="formula_24">E ? n+1 2 = |1 + ? (t ? h)| 2 E |? n | 2 + ? 2 h = |1 + ? (t ? h)| 2 |1 + ? (t ? h)| 2 E ? n?1 2 + ? 2 h + ? 2 h . . . = |1 + ? (t ? h)| 2(n+1) E [|y 0 |] + |1 + ? (t ? h)| 2(n+1) ? 1 2 ? + |?| 2 (t ? h) ? 2 .</formula><p>Under the same assumption of consecutive steps, we have that</p><formula xml:id="formula_25">E ? n+1 2 = |1 + ?h| 2(n+1) E [|y 0 |] + |1 + ?h| 2(n+1) ? 1 2 (?) + |?| 2 h ? 2 , lim n?? E ? n+1 2 = ? ? 2 2 (?) + |?| 2 h , lim h?0 lim n?? E ? n+1 2 = ? ? 2 2 (?)</formula><p>.</p><p>Assuming the imaginary part of ? is null, we have that lim h?0 lim n?? E ? n+1 2 = ? ? 2 2 |?| .</p><p>Thus, the numerical scheme is stable and unbiased in the mean square.</p><p>Following the two steps for computation of x andx, the step size decreases and does not change size; thus, all the above statements hold, and the entire algorithm is stable and unbiased with respect to both the mean and square mean. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>with step size h is numerically stable in mean if the numerical solution y (h) n applied to a linear SDE satisfies lim n?? E [y n ] = 0, and is stable in mean square [Saito and Mitsui, 1996] if we have that lim</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FigureFigure 7 :</head><label>7</label><figDesc>Dynamic-step Extrapolation ( = 0.01) (b) Dynamic-step Extrapolation ( = 0.02) (c) Dynamic-step Extrapolation ( = 0.05) (d) Dynamic-step Extrapolation ( = 0.10) VE -FFHQ (256x256)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of score Function Evaluations (NFE) / Fr?chet Inception Distance (FID) on CIFAR-</figDesc><table><row><cell>10 (32x32) from 50K samples</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>VP</cell><cell>VP-deep</cell><cell>VE</cell><cell>VE-deep</cell></row><row><cell cols="5">Reverse-Diffusion &amp; Langevin 1999 / 4.27 1999 / 4.69 1999 / 2.40 1999 / 2.21</cell></row><row><cell>Euler-Maruyama</cell><cell cols="4">1000 / 2.55 1000 / 2.49 1000 / 2.98 1000 / 3.14</cell></row><row><cell>DDIM</cell><cell cols="2">1000 / 2.86 1000 / 2.69</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours ( rel = 0.01)</cell><cell>329 / 2.70</cell><cell>330 / 2.56</cell><cell>738 / 2.91</cell><cell>736 / 3.06</cell></row><row><cell cols="4">Euler-Maruyama (same NFE) 329 / 10.28 330 / 10.00 738 / 2.99</cell><cell>736 / 3.17</cell></row><row><cell>DDIM (same NFE)</cell><cell>329 / 4.81</cell><cell>330 / 4.76</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours ( rel = 0.02)</cell><cell>274 / 2.74</cell><cell>274 / 2.60</cell><cell>490 / 2.87</cell><cell>488 / 2.99</cell></row><row><cell cols="4">Euler-Maruyama (same NFE) 274 / 14.18 274 / 13.67 490 / 3.05</cell><cell>488 / 3.21</cell></row><row><cell>DDIM (same NFE)</cell><cell>274 / 5.75</cell><cell>274 / 5.74</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours ( rel = 0.05)</cell><cell>179 / 2.59</cell><cell>180 / 2.44</cell><cell>271 / 3.23</cell><cell>270 / 3.40</cell></row><row><cell cols="4">Euler-Maruyama (same NFE) 179 / 25.49 180 / 25.05 271 / 3.48</cell><cell>270 / 3.76</cell></row><row><cell>DDIM (same NFE)</cell><cell>179 / 9.20</cell><cell>180 / 9.25</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours ( rel = 0.10)</cell><cell>147 / 2.95</cell><cell>151 / 2.73</cell><cell cols="2">170 / 8.85 170 / 10.15</cell></row><row><cell cols="4">Euler-Maruyama (same NFE) 147 / 31.38 151 / 31.93 170 / 5.12</cell><cell>170 / 5.56</cell></row><row><cell>DDIM (same NFE)</cell><cell cols="2">147 / 11.53 151 / 11.38</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours ( rel = 0.50)</cell><cell>49 / 72.29</cell><cell cols="3">48 / 82.42 52 / 266.75 50 / 307.32</cell></row><row><cell>Euler-Maruyama (same NFE)</cell><cell>49 / 92.99</cell><cell cols="3">48 / 95.77 52 / 169.32 50 / 271.27</cell></row><row><cell>DDIM (same NFE)</cell><cell>49 / 37.24</cell><cell>48 / 38.71</cell><cell>-</cell><cell>-</cell></row><row><cell>Probability Flow (ODE)</cell><cell>142 / 3.11</cell><cell>145 / 2.86</cell><cell>183 / 7.64</cell><cell>181 / 5.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">: Number of score Function Evaluations (NFE) / Fr?chet Inception Distance (FID) on LSUN-</cell></row><row><cell cols="2">Church (256x256) and FFHQ (256x256) from 5K samples</cell></row><row><cell>Method</cell><cell>VE (Church) VE (FFHQ)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Tim Salimans, IanGoodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.   Improved techniques for training gans. In Advances in neural information processing systems, pages 2234-2242, 2016.John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of computational and applied mathematics, 6(1):19-26, 1980.    </figDesc><table><row><cell>Appendices</cell></row><row><cell>A DifferentialEquations.jl</cell></row><row><cell>Bradley Efron. Tweedie's formula and selection bias. Journal of the American Statistical Association,</cell></row><row><cell>106(496):1602-1614, 2011.</cell></row><row><cell>Crispin Gardiner. Stochastic methods, volume 4. Springer Berlin, 2009.</cell></row><row><cell>Ludwig Arnold. Stochastic differential equations. New York, 1974.</cell></row><row><cell>Sergej S Artemiev and Tatjana A Averina. Numerical analysis of systems of ordinary and stochastic</cell></row><row><cell>differential equations. Walter de Gruyter, 2011.</cell></row><row><cell>Yoshihiro Saito and Taketomo Mitsui. Stability analysis of numerical schemes for stochastic differen-</cell></row><row><cell>tial equations. SIAM Journal on Numerical Analysis, 33(6):2254-2267, 1996.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Short experiments with various SDE solvers from DifferentialEquations.jl on the VP model with a small mini-batch.</figDesc><table><row><cell>Method</cell><cell cols="2">Strong-Order Adaptive</cell><cell>Speed</cell></row><row><cell>Euler-Maruyama (EM)</cell><cell>0.5</cell><cell>No</cell><cell>Baseline speed</cell></row><row><cell>SOSRA</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">: Effect of different settings on the [Inception score (IS) / Fr?chet Inception Distance (FID) /</cell></row><row><cell cols="4">Number of score Function Evaluations (NFE)] from 10k samples (with mini-batches of 1k samples)</cell></row><row><cell>with the VP -CIFAR10 model.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Change(s) in Algorithm 1</cell><cell>IS</cell><cell>FID</cell><cell>NFE</cell></row><row><cell>No change q = 2, r = 0.9, ?(x , x prev )</cell><cell cols="2">9.38 4.70</cell><cell>3972</cell></row><row><cell>Small modifications</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?(x )</cell><cell cols="2">9.26 4.69</cell><cell>4166</cell></row><row><cell cols="4">No Extrapolation (thus, using Euler-Maruyama) 9.58 11.73 3978</cell></row><row><cell>q = ?</cell><cell cols="3">9.48 4.90 14462</cell></row><row><cell>r = .5</cell><cell cols="2">9.41 4.69</cell><cell>4104</cell></row><row><cell>r = .8</cell><cell cols="2">9.36 4.68</cell><cell>3938</cell></row><row><cell>r = 1</cell><cell cols="2">9.41 4.69</cell><cell>4048</cell></row><row><cell cols="2">Variations of Lamba [2003] Algorithm</cell><cell></cell><cell></cell></row><row><cell>r = 0.5, Lamba integration</cell><cell cols="3">7.80 52.98 1468</cell></row><row><cell>r = 0.5, Lamba integration, Extrapolation</cell><cell cols="3">7.32 64.65 1438</cell></row><row><cell>r = 0.5, Lamba integration, q = ?</cell><cell cols="3">9.28 21.09 2360</cell></row><row><cell>r = 0.5, Lamba integration, q = ?, ? = 0.8</cell><cell cols="3">9.21 18.82 2346</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">: Effect of different settings on the [Inception score (IS) / Fr?chet Inception Distance (FID) /</cell></row><row><cell cols="4">Number of score Function Evaluations (NFE)] from 10k samples (with mini-batches of 1k samples)</cell></row><row><cell>with the VE -CIFAR10 model.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Change(s) in Algorithm 1</cell><cell>IS</cell><cell>FID</cell><cell>NFE</cell></row><row><cell>No change q = 2, r = 0.9, ?(x , x prev )</cell><cell>9.39</cell><cell>4.89</cell><cell>8856</cell></row><row><cell>Small modifications</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?(x )</cell><cell>9.39</cell><cell>4.99</cell><cell>17514</cell></row><row><cell cols="2">No Extrapolation (thus, using Euler-Maruyama) 9.58</cell><cell>6.57</cell><cell>8802</cell></row><row><cell>q = ?</cell><cell>9.41</cell><cell>5.03</cell><cell>39500</cell></row><row><cell>r = 0.5</cell><cell>9.47</cell><cell>4.87</cell><cell>9594</cell></row><row><cell>r = 0.8</cell><cell>9.45</cell><cell>4.84</cell><cell>8952</cell></row><row><cell>r = 1</cell><cell>9.43</cell><cell>4.93</cell><cell>8784</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Inception Score on CIFAR-10 (32x32) from 50K samples Method VP VP-deep VE VE-deep</figDesc><table><row><cell cols="2">Reverse-Diffusion &amp; Langevin 9.94</cell><cell>9.85</cell><cell>9.86</cell><cell>9.83</cell></row><row><cell>Euler-Maruyama</cell><cell>9.71</cell><cell>9.73</cell><cell>9.49</cell><cell>9.31</cell></row><row><cell>Ours ( rel = 0.01)</cell><cell>9.46</cell><cell>9.54</cell><cell>9.50</cell><cell>9.48</cell></row><row><cell>Ours ( rel = 0.02)</cell><cell>9.51</cell><cell>9.48</cell><cell>9.57</cell><cell>9.50</cell></row><row><cell>Ours ( rel = 0.05)</cell><cell>9.50</cell><cell>9.61</cell><cell>9.64</cell><cell>9.63</cell></row><row><cell>Ours ( rel = 0.10)</cell><cell>9.69</cell><cell>9.64</cell><cell>9.87</cell><cell>9.75</cell></row><row><cell>Probability Flow (ODE)</cell><cell>9.37</cell><cell>9.33</cell><cell>9.17</cell><cell>9.32</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Dynamic-step Extrapolation ( = 0.01) (b) Dynamic-step Extrapolation ( = 0.02)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Euler-Maruyama</surname></persName>
		</author>
		<idno>/ 42.11 2000 / 18.57</idno>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ours</surname></persName>
		</author>
		<idno>rel = 0.01) 1104 / 25.67 1020 /15.68</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Euler-Maruyama</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1104</biblScope>
		</imprint>
	</monogr>
	<note>same NFE</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Euler-Maruyama</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>same NFE</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11918" to="11930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint arxiv</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05475</idno>
		<title level="m">R?mi Tachet des Combes, and Ioannis Mitliagkas. Adversarial score matching and improved sampling for image generation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Hamiltonian monte carlo and consistent sampling for score-based generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
			<pubPlace>Qu?bec</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universit? de Montr?al</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Wavegrad: Estimating gradients for waveform generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Symbolic music generation with diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16091</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Voicegrad: Non-parallel any-to-many voice conversion with annealed langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Kameoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuhiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kou</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobukatsu</forename><surname>Hojo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02977</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Permutation invariant graph generation via score-based generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4474" to="4484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07636</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Solving linear inverse problems using the prior implicit in a denoiser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Kadkhodaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13640</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Source separation with deep generative priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Jayaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Thickstun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4724" to="4735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unit-ddpm: Unpaired image translation with denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05358</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Noise estimation for generative diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San-Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02600</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast mixing of multi-scale langevin dynamics underthe manifold hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Block</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Ross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11166</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Denoising diffusion implicit models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="695" to="709" />
			<date type="published" when="2005-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Brian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Applied stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>S?rkk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Solin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eckhard</forename><surname>Kloeden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Numerical Solution of Stochastic Differential Equations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="103" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive schemes for the numerical solution of sdes-a comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Lehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>R??ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Schein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="308" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An adaptive timestepping algorithm for stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lamba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="417" to="430" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Differentialequations.jl -a performant and feature-rich ecosystem for solving differential equations in julia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Rackauckas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Nie</surname></persName>
		</author>
		<idno type="DOI">10.5334/jors.151</idno>
		<ptr target="https://app.dimensions.aion2019/05/05" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Open Research Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An introduction to numerical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Endre</forename><surname>S?li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mayers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Runge-kutta methods for the strong approximation of solutions of stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>R??ler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="922" to="952" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive methods for stochastic differential equations via natural embeddings and rejection sampling with memory. Discrete and continuous dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Rackauckas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Series B</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2731</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Modify the improved euler scheme to integrate stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aj Roberts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.0933</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
	<note>step Extrapolation ( = 0.01) (b) Dynamic-step Extrapolation ( = 0.02</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<editor>VE -LSUN-Church</editor>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
