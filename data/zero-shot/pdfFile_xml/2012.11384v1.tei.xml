<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document-Level Relation Extraction with Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
							<email>xuwang@hit-mtlab.net</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
							<email>khchen@nict.go.jp</email>
							<affiliation key="aff1">
								<orgName type="department">National Institute of Information and Communications Technology (NICT)</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
							<email>tjzhao@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Document-Level Relation Extraction with Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In document-level relation extraction (DocRE), graph structure is generally used to encode relation information in the input document to classify the relation category between each entity pair, and has greatly advanced the DocRE task over the past several years. However, the learned graph representation universally models relation information between all entity pairs regardless of whether there are relationships between these entity pairs. Thus, those entity pairs without relationships disperse the attention of the encoder-classifier DocRE for ones with relationships, which may further hind the improvement of DocRE. To alleviate this issue, we propose a novel encoder-classifierreconstructor model for DocRE. The reconstructor manages to reconstruct the ground-truth path dependencies from the graph representation, to ensure that the proposed DocRE model pays more attention to encode entity pairs with relationships in the training. Furthermore, the reconstructor is regarded as a relationship indicator to assist relation classification in the inference, which can further improve the performance of DocRE model. Experimental results on a large-scale DocRE dataset show that the proposed model can significantly improve the accuracy of relation extraction on a strong heterogeneous graph-based baseline. The code is publicly available at https://github.com/xwjim/DocRE-Rec.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph structure plays an important role in the document relation extraction (DocRE) <ref type="bibr" target="#b13">Sahu et al. 2019;</ref><ref type="bibr" target="#b10">Nan et al. 2020;</ref><ref type="bibr" target="#b16">Tang et al. 2020)</ref>. Typically, one unstructured input document is first organized as a structure input graph (i.e., homogeneous or heterogeneous graphs) based on syntactic trees, coreference, or heuristics rules, thereby building relationships between entity pairs within and across multiple sentences of the input document. Neural networks (i.e., graph network) are used to iteratively encode the structure input graph as a graph representation to model relation information in the input document. The graph representation is fed into one classifier to classify the relation category between each entity pair, which has achieved the state-of-theart performance in DocRE <ref type="bibr" target="#b10">Nan et al. 2020)</ref>.</p><p>Copyright ? 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>However, during the training of DocRE model, the graph representation universally encodes relation information between all entity pairs regardless of whether there are relationships between these entity pairs. For example, <ref type="figure" target="#fig_0">Figure 1</ref> shows three entities in an input document: X-Files, Chris Carter, and Fox Mulder. Intuitively, they are three entity pairs: {X-Files, Chris Carter}, {X-Files, Fox Mulder}, and {Chris Carter, Fox Mulder}. The DocRE model learns the node representations of each entity pair to classify their relation. As seen, there exists relationship between {Chris Carter, Fox Mulder} in the reference, indicating that there is naturally a reliable reasoning path from Chris Carter to Fox Mulder. In comparison, there do not exist relationships between {X-Files, Chris Carter} and between {X-Files, Fox Mulder}, indicating that there are not reasoning paths between {X-Files-Chris Carter} or {X-Files, Fox Mulder}. However, the learned graph representation models the three path dependencies universally and does not consider whether there is a path dependency between one target entity pair. As a result, {X-Files, Chris Carter} and {X-Files, Fox Mulder} without relationships disperse the attention of the DocRE model for the learning of {Fox Mulder, Chris Carter} with relationship, which may further hinder the improvement of the DocRE model.</p><p>To alleviate this issue, we propose a novel reconstructor method to enable the DocRE model to model path dependency between one entity pair with the ground-truth relationship. To this end, the reconstructor generates a sequence of node representations on the path from one entity node to another entity node and thereby maximizes the probability of its path if there is a ground-truth relationship between one entity pair and minimizes the probability otherwise. This allows the proposed DocRE model to pay more attention to the learning of entity pairs with relationships in the training, thereby learning an effective graph representation for the subsequent relation classification. Furthermore, the reconstructor is regarded as a relationship indicator to assist relation classification in the inference, which can further improve the performance of DocRE model. Experimental results on a large-scale DocRE dataset show that the proposed method gained improvement of 1.7 F1 points over a strong heterogeneous graph-based DocRE model, especially outperformed the recent state-of-  the-art LSR model for DocRE <ref type="bibr" target="#b10">(Nan et al. 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, based on (Christopoulou, Miwa, and Ananiadou 2019)'s work, we used heuristic rules to convert the input document into a heterogeneous graph without external syntactic knowledge. Moreover, a graph attention network is used to encode the heterogeneous graph instead of the edge-oriented graph network , thereby implementing a strong and general baseline for DocRE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Heterogeneous Graph Construction</head><p>Formally, given an input document that consists of L sentences {S 1 , S 2 , ? ? ? , S L }, each of which is a sequence of words {x l 1 , x l 2 , ? ? ? , x l J } with the length J=|S l |. A bidirectional long short-term memory (BiLSTM) reads word by word to generate a sequence of word vectors to represent each sentence in the input document. Also, we apply the heterogeneous graph  to the input document to build relationships between all entity pairs. Specifically, the heterogeneous graph includes three defined distinct types of nodes: Mention Node, Entity Node, and Sentence Node. For example, <ref type="figure" target="#fig_0">Figure 1</ref> shows an input document including two sentences (yellow color index) in which there are four mentions (blue color) and three entities (green color). The representation of each node is the average of the words in the concept, thereby forming a set of node representations {v 1 , v 2 , ? ? ? , v N }, where N is the number of nodes. For edge connections, there are five distinct types of edges between pairs of nodes following (Christopoulou, Miwa, and Ananiadou 2019)'s work, Mention-Mention(MM) edge, Mention-Sentence (MS) edge, Mention-Entity (ME) edge, Sentence-Sentence (SS) edge, Sentence-Sentence (SS) edge, Entity-Sentence (ES) edge respectively. In addition, we add a Mention-Coreference (CO) edges between the two mentions which are referred to the same entity. According to these above definitions, there is a N ?N adjacency matrix E denoting edge connections. Finally, the heterogeneous graph can be denoted as G={V, E}, to keep relation information between all entity pairs in the input document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoder</head><p>To learn an effective graph representation, we used the graph attention network <ref type="bibr" target="#b7">(Guo, Zhang, and Lu 2019)</ref> to encode the feature representation of each node in the heterogeneous graph. Formally, given the outputs of all previous hop reasoning operations {s 1 n , s 2 n , ? ? ? , s l?1 n }, they are concatenated and then transformed to a fixed dimensional vector as the input of the l hop reasoning:</p><formula xml:id="formula_0">z l n = W l e ? [v n : s 1 n : s 2 n : ? ? ? : s l?1 n ],<label>(1)</label></formula><p>where s l?1 n ?R d0 and W l e ?R d0?(l?d0) . Also, according to</p><formula xml:id="formula_1">edge matrix E[n][a c ]=k (0 ? a c &lt; N, k &gt; 0), C direct adjacent nodes of v n are {z l a1 , z l a2 , ? ? ? , z l a C }.</formula><p>We then use the self-attention mechanism <ref type="bibr" target="#b18">(Vaswani et al. 2017)</ref> to capture the feature information of v n between z l n and {z l a1 , z l a1 , ? ? ? , z l a C }:</p><formula xml:id="formula_2">s l n = softmax( z l n K ? d 0 )V,<label>(2)</label></formula><p>where {K, V} are key and value matrices that are transformed from the direct adjacent nodes representations {z l a1 , z l a1 , ? ? ? , z l a C } according to the edge type. After performing L hop reasonings, there is a sequence of annotations {s 1 n , s 2 n , ? ? ? , s L n } to encode relation information in the input document. Finally, another no-linear layer is applied to integrate the reason information {s 1 n , s 2 n , ? ? ? , s L n } and the node information v n :</p><formula xml:id="formula_3">q n = Relu(W o ? [v n : s 1 n : ? ? ? : s L n ]),<label>(3)</label></formula><p>where W o ?R d1?(d0?(L+1)) , q n ?R d1 . As a result, the heterogeneous graph G is represented as {q 1 , q 2 , ? ? ? , q N }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classifier</head><p>Given the heterogeneous graph representation {q 1 , q 2 , ? ? ? , q N }, two node representations of each entity pair are as the input to the classifier to classify their relationship. Specifically, the classifier is a multi-layer perceptron (MLP) layer with sigmoid function to calculate the relationship probability:</p><formula xml:id="formula_4">R(r) = P (r|{e i , e j }) = sigmoid(MLP([q i : q j ])). (4)</formula><p>To train the DocRE model, the binary cross-entropy is used to optimize parameters of neural networks over the triple examples (subject, object, relation) on the training date set (including T documents), that is, {{e1 t n , e2 t n , r t n } Nt n=1 } T t=1 :</p><formula xml:id="formula_5">Loss c = ? 1 T t=0 N t T t=1 Nt n=1 {r t n log(R(r t n )) +(1 ? r t n )log(1 ? R(r t n ))},<label>(5)</label></formula><p>where r t n ? {0, 1} indicates whether the entity pair has relation label r and N t is the number of relations in the tth document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Intuitively, when a human understands a document with relationships, he or she often pays more attention to learn entity pairs with relationships rather than ones without relationships. Motivated by this observation, we proposed a novel DocRE model with reconstruction (See <ref type="figure">Figure 2</ref>) to pay more attention to entity pairs with relationships, thus enhancing the accuracy of relationship classification.  <ref type="figure">Figure 2</ref>: Model overview. The reconstructor manages to reconstruct the ground-truth path dependencies from the graph representation to ensure that the model to pay attention to model entity pairs with relationships. Furthermore, the reconstructor is regarded as a relationship indicator to assist relation classification in the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Meta Path of Entity Pair</head><p>Generally, when there is a relationship between two entities, they should have one strong path dependency in the graph structure (or representation). In comparison, when there is not a relationship between two entities, there is a weak path dependency. 1 Thus, we explore to reconstruct the path dependency between each entity pair from the learned graph representation. To this end, we first define three type paths between two entity nodes in the graph representation as reconstructed candidates according to the meta-path information <ref type="bibr" target="#b15">(Sun and Han 2013)</ref>. 1) Meta Path1 of Pattern Recognition: Two entities are connected through a sentence in this reasoning type. The relation schema is EM ? M M ? EM , for example node sequence {7,3,4,8} in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>2) Meta Path2 of Logical Reasoning: the relation between two entities is indirectly established by a bridge entity. The bridge entity occurs in a sentence with the two entities separately. The relation schema is EM ? M M ? CO ? M M ? EM , for example node sequence {7,3,4,5,6,9} in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>3) Meta Path3 of Coreference Reasoning: Coreference resolution must be performed first to identify target entities. A reference word refers to an entity that appear in the previous sentence. The two entities occur in the same sentence implicitly. The relation schema is ES ?SS ?ES, for example node sequence {7,1,2,9} in <ref type="figure" target="#fig_0">Figure 1</ref>. Actually, all the entity pairs have at least one of the three meta-paths. We select one meta-path type according to the priority, meta-path1 &gt; meta-path2 &gt; meta-path3. Generally, several instance paths may exist corresponding to the meta path, we select the instance path that appears firstly in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Path Reconstruction</head><p>For each entity pair, one instance path is selected as the supervision of the reconstruction of the path dependency. In other words, there is only one supervision</p><formula xml:id="formula_6">path ? n ={v b1 , v b2 , ? ? ? , v b C } between each target pair {e1 n , e2 n }, where b C is the number of nodes.</formula><p>To reconstruct the path dependency of each entity pair, we model the reconstructor as the sequence generation. Specifically, we use a LSTM to compute a path hidden state p bc for each node q bc?1 on the path ? n :</p><formula xml:id="formula_7">p bc = LSTM(p bc?1 , q bc?1 ).<label>(6)</label></formula><p>Note that p b0 is initialized as the transform of o ij , since it plays a key role in classification. p bc is fed into a softmax layer to compute the probability of node v bc on the path:</p><formula xml:id="formula_8">P(v bc |v &lt;bc ) = exp(p bc W r q bc ]) n exp(p bc W r q n ]) ,<label>(7)</label></formula><p>where W r ? R d1?d1 . Also, there is a set of node probabilities {P(v b1 |v &lt;b1 ), P(v b2 |v &lt;b2 ), ? ? ? , P(v b C |v &lt;b C )} for the path ? n . Finally, the probability of this path ? n is computed:</p><formula xml:id="formula_9">N (? n ) = C c=1 (P(v bc |v &lt;bc ).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training with Reconstruction Loss</head><p>We use the reconstructed path probability to compute an additional reconstruction loss over the triple examples of the training data set {{e1 t n , e2 t n , r t n } Nt n=1 } T t=1 :</p><formula xml:id="formula_10">Loss r = ? 1 T t=0 N t T t=1 Nt n=1 {r t n logN (? n ) +(1 ? r t n )log(1 ? N (? n )},<label>(9)</label></formula><p>where r t n is one of {0,1}, that is, we maximize the probability of the path N (? n ) if the entity pair has relation, and minimize the probability otherwise. To simplify the Eq.(9), we use C c=1 (1 ? P bc ) to replace with the (1 ? N (? n )), where P bc =P(v bc |v &lt;bc ). The reconstruction loss is modified as Eq. <ref type="formula" target="#formula_0">(10)</ref>:</p><formula xml:id="formula_11">Loss r = ? 1 T t=0 N t T t=1 Nt n=1 { b C bc=1 {(r t n logP bc ) +(1 ? r t n )log(1 ? P bc )}}.<label>(10)</label></formula><p>Finally, the reconstructor loss and the existing classification loss in Eq. <ref type="formula" target="#formula_5">(5)</ref> is added as the training objective of the proposed DocRE model:</p><formula xml:id="formula_12">Loss = Loss c + Loss r .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference with Path Reconstruction</head><p>Intuitively, the proposed reconstructor encourages the DocRE model to pay more attention to model entity pairs with ground-truth relationships. Furthermore, we maximized the path probability between one entity pair if there is indeed a relation and we minimized it otherwise when computing the reconstruction loss in Eq.(10). In other words, the higher the probability of this path is, the greater the likelihood of a relationship between the entity pair is. Naturally, we treat this path probability as a relational indicator to assist relation classification in the inference:</p><formula xml:id="formula_13">S(r) = log(R(r)) + ? ? 1 C b C bc=1 log(P bc ),<label>(12)</label></formula><p>where ? is a hyper-parameter to control the importance of reconstruction probability in the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>The proposed methods were evaluated on a largescale human-annotated dataset for document-level relation extraction <ref type="bibr" target="#b23">(Yao et al. 2019</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Systems</head><p>According to Section 2, there is a baseline heterogeneousbased graph self-attention network model <ref type="figure">(HeterGSAN)</ref>. Also, there are some recent DocRE methods as our comparison systems:</p><p>? Sequence-based Models: These models used different neural architectures to encode sentences in the document, including including convolution neural networks (CNN) <ref type="bibr" target="#b23">(Yao et al. 2019)</ref>, bidirectional LSTM (BiLSTM) <ref type="bibr" target="#b23">(Yao et al. 2019)</ref> and Context-Aware LSTM <ref type="bibr" target="#b23">(Yao et al. 2019</ref>).</p><p>? Graph-based Models. GCNN <ref type="bibr" target="#b13">(Sahu et al. 2019</ref>), GAT <ref type="bibr" target="#b19">(Veli?kovi? et al. 2018)</ref>, AGGCN <ref type="bibr" target="#b7">(Guo, Zhang, and Lu 2019)</ref> constructed the graph from syntactic parsing and sequential information, or non-local dependencies from coreference resolution and other semantic dependencies, and then uses the GCN based method to calculate the node embedding. EoG  defined several node types and edges to construct a heterogeneous graph of the input document without external syntactic knowledge. EoG uses an iterative algorithm to learn new edge representations between different nodes in the heterogeneous graph and classify relationships between entity pairs. Instead of constructing a static graph representation, LSR <ref type="bibr" target="#b10">(Nan et al. 2020</ref>) empowered the relational reasoning across sentences by automatically inducing the latent document-level graph.</p><p>? BERT. It applied a pre-trained language model to learn the representations of the input document <ref type="bibr" target="#b6">Devlin et al. 2019</ref>). Furthermore, it used a two-phase training process to enhance the performance of DocRE model. Specifically, it first predicts whether a pair of entities has a relation or not and classifies the relation for each entity pair. <ref type="table" target="#tab_2">Table 1</ref> presents the detailed results on the development set and the test set of DocRED. As seen, our baseline HeterGSAN model achieved 53.52 F 1 score on the test set and outperformed the EoG model which is also a heterogeneous-based graph DocRE model by 1.7 points in terms of F 1 . Meanwhile, HeterGSAN is consistently superior to the most of comparison methods, including CNN, BiLSTM, ContextAware, GCNN, GAT, and AGGCN. This indicates that the graph self-attention network can give a strong baseline in the heterogeneous-based methods of DocRE. HeterGSAN+reconstruction achieved 55.23 F 1, which outperformed the baseline HeterGSAN by 1.71 F 1 score. In particular, HeterGSAN+reconstruction outperformed the existing state-of-the-art LSR model by 1.05 F 1 score, which is a new state-of-the-art result on the DocRED dataset without the pre-trained model (BERT). This means that the proposed reconstructor is beneficial to encode relation information in the input document, thereby enhancing the relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>In addition, we evaluated the proposed HeterGSAN model with a pre-trained language model as shown in <ref type="table" target="#tab_2">Table 1</ref>  To valid the effect of reconstruction, <ref type="figure" target="#fig_2">Figure 3</ref> showed learning curves of classification and reconstruction performances (in F1 scores) on the development set during the training. For reconstruction, we used the reconstructor to generate the source path for each entity pair and calculated the probability of the reconstructed path to indicate how much there is a relationship. As seen, the reconstruction F 1 scores went up with the improvement of reconstruction over time. When the classification performance reached a peak at iteration 24K, the proposed model achieved a balance between classification and reconstruction scores. Therefore, we use the trained model at iteration 24K in <ref type="table" target="#tab_2">Table 1</ref>.  To further explore the effect of Reconstructor, we incrementally introduced it into the training and inference phases in turn. <ref type="table" target="#tab_4">Table 2</ref> shows the results of the ablation experiment on the development set. As seen, when Reconstructor was only introduced into the training phase (#2), there was 1.26 F 1 improvement over the baseline HeteGASN model (#1) in which there are not Reconstructor in the training and inference phases. Moreover, Reconstructor was introduced into the inference as a relation indicator to assist relation classification, that is, there are Reconstructor in both training and inference contain the Reconstructor (#3), As a result, there gained 0.56 F 1 further improvement. This shows that the proposed Reconstructor can not only encode relation information of the input document efficient but also indicate how much there is a relationship, to enhance relation classification between entity pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation in Training and Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation of Reconstruction Loss</head><p>In the reconstruction phase, we maximized (max) the path probability if the entity pair has the ground-truth relationship and minimized (min) the path probability otherwise. Therefore, we performed the ablation of the above two reconstruction paths. Specifically, we gradually introduced them into the proposed HeterGSAN with Reconstruction to verify the effect of two reconstruction paths, as shown in <ref type="table" target="#tab_6">Table 3</ref>. Here, "relation" denotes entity pairs with groundtruth relationships while "no-relation" denotes entity pairs without ground-truth relationships. As seen, when one of "no-relation" (#2) and "relation" (#3) entity pairs were used to compute the reconstruction loss, their F 1 scores were better than the baseline HeterGSAN (#1). This means that reconstructing one of two paths is beneficial to improve the performance of DocRE model. Meanwhile, "relation" (#3) was superior to "no-relation" (#2). In particular, both of them can complement each other to further improve F 1 score (#4). This indicates that two path reconstruction methods help the DocRE model capture more diverse useful information from the input document.   In inference, the reconstructor is regarded as a relationship indicator to assist relation classification. The hyper-parameter ? in Eq.(12) keeps a trade-off between the classification scores and the construction scores when classifying the relation of each entity pair. <ref type="figure" target="#fig_3">Figure 4</ref> shows classification F 1 scores of different hyper-parameter ? for the reconstructed path probability of HeterGSAN and +Reconstruction models in inference. As seen, F 1 scores of +Reconstruction model increased with the increasing of ? until 0.4, indicating that the probability of reconstructed path is useful for improving the relation classification. Subsequently, larger values of ? reduced the F 1 scores, suggesting that excessive biased path information may be weak at keeping the gained improvement. Therefore, we set the hyper-parameter ? to 0.4 to control the effect of reconstructed path information in our experiments <ref type="table" target="#tab_2">(Table 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Evaluating Different Meta Path</head><p>To evaluate our defined three candidate meta paths, we divide entity pairs of the same type of meta path in the development set to three groups, for example, "MP1" indicates that the path representation of entity pairs are from the defined Meta Path1 (See section 3.1) during the reconstruction. <ref type="table">Table 4</ref> showed F 1 scores of three groups (MP1, MP2, and MP3) for HeterGSAN and +Reconstruction models. As seen, F 1 scores of +Reconstruction outperformed that of HeterGSAN in all three groups. This means that our defined meta paths can efficient capture path dependency between entity pairs in the reconstruction processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MP1</head><p>MP2 MP3 HeterGSAN (%) 60.67 50.29 46.30 +Reconstruction (%) 61.73 52.19 47.57 <ref type="table">Table 4</ref>: F 1 scores of three groups (MP1, MP2, and MP3) with different meta paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Path Attention Scores</head><p>To study how the reconstructor (Rec) affect the distribution of attention scores along the path in the HeterGSAN, we divided attention scores into five intervals (i.e, 0-0.2, 0.2-0.4, etc) and showed the percent of attention distribution on HeterGSAN and +Reconstruction on the development set as shown in the <ref type="table" target="#tab_8">Table 5</ref>. The attention scores of HeterGSAN are mainly concentrated in interval 0-.2, which may indicate the hypothesis of universally learning relationship information. Thus, +Reconstruction significantly reduced the percent of attention scores in interval 0-.2 and increased the percent of remaining intervals with higher attention scores. This means that the reconstructor guides the DocRE model to pay more attention to model meta-path dependencies for the ground-truth relationships.      <ref type="figure">Figure 5</ref> shows a case study of HeterGSAN and +Reconstruction models. For the entity pair {Lark Force, Australia}, HeterGSAN classified its relation to "NA" which is inconsistent with the Reference "P17" because of its classifier score -1.1271 is less than the threshold ? 1 -0.9828. In comparison, the classifier score of +Reconstruction classified its relation to "P17" which is consistent with the Reference "P17" because of its classifier score -0.9760 was greater than the threshold ? 2 -1.0270. This means that the proposed Reconstructor can better guild the training of DocRE model. For another entity pair {Rabaul, Australia}, the classifier scores of HeterGSAN and +Reconstruction models were greater than ? 1 and ? 2 , respectively. However, they gained a relation category P137 which is inconsistent with the Reference "NA". When the path score -0.6017 was considered in the inference, +Reconstruction classified its relation to "NA" which is consistent with the Reference "NA". This indicates that the inference with Reconstructor can further improve the accuracy of relation classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.11">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>DocRE Early efforts focus on classifying relationships between entity pair within a single sentence or extract entity and relations jointly in a sentence <ref type="bibr" target="#b24">(Zeng et al. 2014;</ref><ref type="bibr" target="#b21">Wang et al. 2016;</ref><ref type="bibr" target="#b22">Wei et al. 2020;</ref><ref type="bibr" target="#b14">Song et al. 2019</ref>). These approaches do not consider interactions across mentions and ignore relations expressed across sentence boundaries. Recently, the extraction scope has been expanded to the entire document in the biomedical domain by only considering a few relations among chemicals <ref type="bibr" target="#b11">(Peng et al. 2017;</ref><ref type="bibr" target="#b12">Quirk and Poon 2017;</ref><ref type="bibr" target="#b8">Gupta et al. 2019;</ref><ref type="bibr" target="#b25">Zhang, Qi, and Manning 2018;</ref><ref type="bibr" target="#b3">Christopoulou, Miwa, and Ananiadou 2019)</ref>. In particular, <ref type="bibr" target="#b23">Yao et al. (2019)</ref> proposed a large-scale human-annotated DocRED dataset. The dataset requires understanding a document and performing multihop reasoning and several works <ref type="bibr" target="#b10">Nan et al. 2020)</ref> have been done on the dataset.</p><p>Reconstruction Reconstructor was used to solve the problem that translations generated by neural network translation (NMT) often lack adequacy <ref type="bibr" target="#b17">(Tu et al. 2016;</ref><ref type="bibr" target="#b2">Cheng et al. 2016)</ref>. <ref type="bibr" target="#b2">(Cheng et al. 2016</ref>) reconstructs the monolingual corpora with two separate source-to-target and target-to-source NMT models. <ref type="bibr" target="#b17">(Tu et al. 2016</ref>) aims at enhancing adequacy of unidirectional (i.e., source-to-target) NMT via a target-to-source objective on parallel corpora. Besides, <ref type="bibr" target="#b9">(Hu et al. 2020)</ref> uses reconstructor to pre-train a graph neural network on the unlabeled data with selfsupervision to reduce the cost of labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposed a novel reconstruction method to guide the DocRE model to pay more attention to the learning of entity pairs with the ground-truth relationships, thereby learning an effective graph representation to classify relation category. In inference, the reconstructor is further regarded as a relation indicator to assist relation classification between entity pair. Experimental results on a large-scale DocRED dataset show that our method can greatly advance the DocRE task. In the future, we will explore more information related to relationship classification in the input document, for example, syntax constraint <ref type="bibr" target="#b0">(Chen et al. 2018</ref>), diverse information <ref type="bibr" target="#b1">(Chen et al. 2020)</ref>, and knowledge reasoning <ref type="bibr">(Cohen et al. 2020)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Heuristic rules are used to convert the input document into a heterogeneous graph. Then graph attention network is applied to learn the graph representation. Finally the node representations of entity pairs are used to classify their relationships.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Learning curves of classification (left y-axis) and reconstruction (right y-axis) performances (in F1 scores) on the development set during the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Classification F 1 scores of different hyperparameter ? for the reconstructed path probability of DocRE models (HeterGSAN and +Reconstruction) in inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure</head><label></label><figDesc>Figure 5: Case Study</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Files was directed by David Nutter, and written by Chris Carter, Frank Spotnitz and Howard Gordon. ... . [2]The show centers on FBI special agents Fox Mulder (David Duchovny) and Dana Scully (Gillian Anderson) who work on cases linked to the paranormal, called X-Files. ...</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Mention node</cell><cell>Entity node</cell><cell>Sentence node</cell><cell></cell><cell></cell></row><row><cell>[1] The X-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L Iterations</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>2</cell><cell>8</cell><cell>7</cell><cell>9</cell><cell>7</cell><cell>9</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>classifier</cell></row><row><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>8</cell><cell>9</cell><cell cols="2">Reference: NA</cell><cell>creator</cell><cell>NA</cell></row><row><cell>Chris Carter</cell><cell>X-Files</cell><cell>Fox Mulder</cell><cell cols="3">Graph Representation</cell><cell></cell><cell cols="2">Relation Classification</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The hop number L of the encoder was set to 2. The learning rate was set to 1e-4 and we trained the model using Adam as the optimizer. For the BERT representations, we used uncased BERT-Based model (768d) as the encoder and the learning rate was set to 1e ?5 For evaluation, we used F 1 and Ign F 1 as the evaluation metrics. Ign F 1 denotes F 1 score excluding relational facts shared by the training and development/test sets. In particular, the predicted results were ranked by their confidence and traverse this list from top to bottom by F 1 score on development set, and the score value corresponding to the maximum F 1 is picked as threshold ?. All hyper-parameters were tuned based on the development set. In addition, the results on the test set were evaluated through CodaLab 2 .</figDesc><table /><note>). DocRED contains 3,053 documents for the training set, 1,000 documents for the development set, and 1,000 documents for the test set, totally with 132,375 entities, 56,354 relational facts, and 96 relation types. More than 40% of the relational facts require the reading and reasoning over multiple sentences. Following settings of (Nan et al. 2020)'s work, we used the GloVe embedding (100d) and BiLSTM (128d) as word embedding and encoder.2 https://competitions.codalab.org/competitions/20717</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on the development set and the test set. Results with * are reported in their original papers. Results with ? are reported in<ref type="bibr" target="#b10">(Nan et al. 2020)</ref>. Bold results indicate the best performance of the current method.</figDesc><table><row><cell>. First, HeterGSAN+BERT model consistently out-</cell></row><row><cell>performed the comparison BERT model, Two-Phase BERT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation of Reconstructor in training and inference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments of reconstruction loss for the proposed HeterGSAN+Reconstruction model.</figDesc><table /><note>4.7 Effect of Path Probability in Inference</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Changes of the distribution of path attention scores 4.10 Ablation of Different Meta-Paths Lark Force was an Australian Army formation established in March 1941 during World War II for service in New Britain and New Ireland.[1] Under the command of Lieutenant Colonel John Scanlan, it was raised in Australia and deployed to Rabaul and Kavieng, aboard SS</figDesc><table><row><cell>we reconstruct one of three meta-paths (MP1, MP2</cell></row><row><cell>and MP3) in each DocRE model and not consider the</cell></row><row><cell>reconstructor in inference. The results are as follows in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="6">. First, reconstruction of each meta-path is beneficial</cell></row><row><cell cols="6">to enhance the DocRE model, confirming our motivation.</cell></row><row><cell cols="6">Thus, the improved range of each meta-path is in descending</cell></row><row><cell cols="6">order: MP1, MP2, MP3, confirming the priority for the</cell></row><row><cell cols="6">reconstruction meta-path in Sec 3.1. It is a statistic that the</cell></row><row><cell cols="6">percentage of MP1, MP2, and MP3 are 22.39%, 23.15%,</cell></row><row><cell cols="6">and 54.46%. Then, when two different meta-paths are</cell></row><row><cell cols="6">considered, their F1 values are higher than the single path</cell></row><row><cell cols="6">which is reconstructed, indicating that more ground-truth</cell></row><row><cell cols="6">path relationships are reconstructed to enhance the training</cell></row><row><cell cols="6">of the DocRE model. Similarly, considering three meta-</cell></row><row><cell cols="5">paths gain the highest F1 on development/test sets.</cell><cell></cell></row><row><cell>type of</cell><cell>Dev</cell><cell>Test</cell><cell>type of</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>meta-path</cell><cell>F1</cell><cell>F1</cell><cell>meta-path</cell><cell>F1</cell><cell>F1</cell></row><row><cell>None</cell><cell cols="5">54.40 53.52 MP1&amp;MP2 55.26 54.40</cell></row><row><cell>MP1</cell><cell cols="5">54.79 54.22 MP1&amp;MP3 55.12 54.37</cell></row><row><cell>MP2</cell><cell cols="5">54.78 54.20 MP2&amp;MP3 54.96 54.28</cell></row><row><cell>MP3</cell><cell cols="2">54.54 53.88</cell><cell>All</cell><cell cols="2">55.66 54.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Ablation experiments of different Meta-Paths.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">If there is no path dependency between two target entities without a relationship, this may weaken the understanding of relationship information in the document.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the anonymous reviewers, senior program Committee and area chair for their insightful comments and suggestions. The corresponding authors are Kehai Chen and Tiejun Zhao. This work is supported by the National Key R&amp;D Program of China (No. 2018YFC0830700) and Huawei Technologies CO., Ltd (No. YBN2019115122).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Syntax-Directed Attention for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16060/16008" />
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>New Orleans, Lousiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4792" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards More Diverse Input Representation for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2020.2996077</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1586" to="1597" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-Supervised Learning for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1185</idno>
		<ptr target="https://www.aclweb.org/anthology/P16-1185" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1965" to="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1498</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1498" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4925" to="4936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siegler</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base</title>
		<ptr target="https://openreview.net/forum?id=BJlguT4YPr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ofthe 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention Guided Graph Convolutional Networks for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<idno>doi:10.18653/ v1/P19-1024</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1024" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural Relation Extraction within and across Sentence Boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Runkler</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016513</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33016513" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="6513" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403237</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403237" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;20</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reasoning with Latent Structure Refinement for Document-Level Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.141</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.141" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-Sentence N-ary Relation Extraction with Graph LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1028" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distant Supervision for Relation Extraction beyond the Sentence Boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/E17-1110" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inter-sentence Relation Extraction with Documentlevel Graph Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno>doi:10.18653/ v1/P19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="19" to="1423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leveraging Dependency Forest for Neural Medical Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1020</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/D19-1020" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mining heterogeneous information networks: a structural analysis approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HIN: Hierarchical Inference Network for Document-Level Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<biblScope unit="volume">12084</biblScope>
			<biblScope unit="page" from="197" to="209" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural Machine Translation with Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1611.01874</idno>
		<ptr target="http://arxiv.org/abs/1611.01874" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fine-tune Bert for DocRED with Two-step Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv abs/1909.11898</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation Classification via Multi-Level Attention CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1123</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.136" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1476" to="1488" />
		</imprint>
	</monogr>
	<note>Online: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DocRED: A Large-Scale Document-Level Relation Extraction Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno>doi:10.18653/ v1/P19-1074</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relation Classification via Convolutional Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C14-1220" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Dublin City University and Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<idno type="DOI">10.18653/v1/D18-1244</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1244" />
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
