<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Representation Learning Beyond Node and Homophily</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binli</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Gui</surname></persName>
						</author>
						<title level="a" type="main">Graph Representation Learning Beyond Node and Homophily</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-graph representation learning</term>
					<term>homophily and heterophily network</term>
					<term>low and high-frequency signals</term>
					<term>self-supervised</term>
					<term>autoencoder !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised graph representation learning aims to distill various graph information into a downstream task-agnostic dense vector embedding. However, existing graph representation learning approaches are designed mainly under the node homophily assumption: connected nodes tend to have similar labels and optimize performance on node-centric downstream tasks. Their design is apparently against the task-agnostic principle and generally suffers poor performance in tasks, e.g., edge classification, that demands feature signals beyond the node-view and homophily assumption. To condense different feature signals into the embeddings, this paper proposes PairE, a novel unsupervised graph embedding method using two paired nodes as the basic unit of embedding to retain the high-frequency signals between nodes to support node-related and edge-related tasks. Accordingly, a multi-self-supervised autoencoder is designed to fulfill two pretext tasks: one retains the high-frequency signal better, and another enhances the representation of commonality. Our extensive experiments on a diversity of benchmark datasets clearly show that PairE outperforms the unsupervised state-of-the-art baselines, with up to 101.1% relative improvement on the edge classification tasks that rely on both the high and lowfrequency signals in the pair and up to 82.5% relative performance gain on the node classification tasks.</p><p>Index Terms-graph representation learning, homophily and heterophily network, low and high-frequency signals, self-supervised, autoencoder !</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure" target="#fig_5">Fig. 1</ref><p>: Inferable relations based on pairs' feature patterns classification and link prediction, since many datasets are indeed homophily networks 1 , i.e., nodes with similar labels and features tend to connect. However, many real-world networks are heterophily networks, i.e., nodes from different classes tend to connect. For example, the chemical interactions in proteins often occur between different types of amino acids <ref type="bibr" target="#b2">[3]</ref>. Even for homophily networks, the actual level of homophily may vary within different pairs of node classes <ref type="bibr" target="#b10">[11]</ref>. This reminds us that the similarity assumption used by existing GRL solutions is distant from optimal for real-world scenarios. Not all the downstream tasks benefit from the akin embeddings between connected nodes.</p><p>The edge classification task, for instance, demands the reservation of feature differences between paired nodes. As shown on the right side of <ref type="figure" target="#fig_5">Fig. 1</ref>, in a multi-layer social network, a node representing a person may have complex relations with other nodes, which is embodied in different types of edges. Although we often only have a desensitized homogeneous graph(left part) of the original graph without any relation information, it is still possible to infer related implicit relationships through the combination of both low and high-frequency signals among features. For instance, an edge between two nodes with the same address(lowfrequency signal) is more likely to have the "live with" relation. The difference in age with the same address might also be a good hint of a father/mother-son relationship. Thus, in the domain of semi-supervised learning, several recent approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> realize the challenges of learning from graphs with high-frequency signals and propose a series of designs: e.g., low and high-frequency filters <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>, and the separation and combination of ego and neighbor embedding training <ref type="bibr" target="#b10">[11]</ref>, to facilitate the learning in heterophily networks. However, these approaches limit the discussion in the node homophily and rely on the node labels for learning. Therefore, with only graph structure, node features, and no labels, effectively distill different information and make the learned representation genuine task-agnostic remains largely unexplored. Capabilities in translating both low and high-frequency signals during embeddings are essential to support diverse downstream tasks.</p><p>Since many existing graph representation learning models either assume strong homophily and fail to generalize to networks with heterophily or rely on the guidance of labels and cannot effectively adapt to unknown downstream tasks. To break these limitations, we propose PairE, a novel multitask unsupervised model to preserve both information for homophily and heterophily by going beyond the localized node view and utilizing higher-level entities with more prosperous expression powers to represent feature signals beyond homophily. The main contributions are summarized as follows: Current Limitations: In the domain of unsupervised GRLs, we first reveal the limitation that exists on the node similarity assumption as their evaluation is limited to a few benchmarks with similar properties. We further point out that they suffer significant performance degradation when downstream tasks demand different entities' low and highfrequency signals. Key Designs: We propose a set of crucial designs to boost the integration of high-frequency signals without trading off accuracy for tasks demanding homophily. (D1) Pair-view embeddings: one natural structure -Pair -existing in the graph is used as the basic unit for embedding. The highlevel entity retains both the commonality and difference of pair features simultaneously and avoids over-smoothing the pair features. (D2) A novel multi-task self-supervised autoencoder integrates both the low-frequency and highfrequency information of a pair and its neighbors into the latent vectors with two different reconstruction pre-tasks. (D3) "Skip Connection" and Concatenation to preserve the integrity and personalization of different inputs. Outstanding experimental results: Comprehensive experimental results show that compared to unsupervised approaches, PairE has unprecedented performance in a variety of downstream tasks: up to 101.1% improvement on the multi-label edge classification tasks, and up to 82.5% performance gain on the node classification task.</p><p>Source code is available at 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Learning under homophily. Early unsupervised methods for learning node representation mainly focus on matrix factorization methods, which calculate losses with handcrafted similarity metrics to build vector representations for each node <ref type="bibr" target="#b14">[15]</ref> with latent features. Later, TADW <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, etc. are proposed the integration of node features into the embedding process, which does not pay attention to the information of different frequencies in the node features. The inspiration for random walks comes from the effective NLP method. The difference between these methods lies in the strategy of walking(e.g., DFS, and BFS) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. A few recent approaches notice the need to encode higherlevel of graph objects, e.g., SEED encodings lines of walks <ref type="bibr" target="#b14">[15]</ref>. ProGAN <ref type="bibr" target="#b19">[20]</ref> encodes the similarity relationship between different nodes by generating the proximity between nodes. Edge2vec <ref type="bibr" target="#b20">[21]</ref> proposes the explicit use of the edge as basic embeddings units. However, they are based on the assumption of neighborhood similarity in the graph topology and are hard to integrate node features.</p><p>Many GNN-based solutions typically adopt the so-called message mechanism to collect neighboring information with general differences on how to design the function of aggregation, combine, and readout functions <ref type="bibr" target="#b21">[22]</ref>. H2GCN <ref type="bibr" target="#b10">[11]</ref> points out; many GNN solutions are optimized for network homophily by propagating features and aggregating them within various graph neighborhoods via different mechanisms (e.g., averaging, LSTM, self-attention) <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b22">[23]</ref>. GraphHeat <ref type="bibr" target="#b23">[24]</ref> designs a more powerful low-pass filter through heat kernel. Another stream is based on autoencoder, e.g., GAE and VGAE <ref type="bibr" target="#b24">[25]</ref> and their follow-up work ARVGE <ref type="bibr" target="#b25">[26]</ref> with an adversarial regularization framework, CAN <ref type="bibr" target="#b26">[27]</ref> embeds each node and attribute with a variational auto-encoder. Several approaches try to keep more complex graph information by learning with higher-level entities, e.g., K-GNN <ref type="bibr" target="#b27">[28]</ref> learns the representation by performing hierarchical pooling combinations from nodes under semisupervised settings. However, those solutions are also based on the assumption that connected nodes should be more similar. Therefore, they fail to generalize to networks with heterophily. Semi-supervised GNN for heterophily. Several recent works have noticed the importance of modeling both the homophily and heterophily of graphs. CS-GNN <ref type="bibr" target="#b12">[13]</ref> has investigated GNN's ability to capture graph useful information by proposing two diagnostic measurements: feature smoothness and label smoothness, to guide the learning process. To better preserve both high-frequency and lowfrequency information, H2GCN <ref type="bibr" target="#b10">[11]</ref> separates the raw features and aggregated features during the learning processes. FAGCN <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref> designs two novel deep GNNs with flexible low-frequency and high-frequency filters, which can well alleviate over-smoothing. Geom-GCN <ref type="bibr" target="#b28">[29]</ref> utilizes the structural similarity to capture the long-range dependencies in heterophily networks. However, the discussion of their work is limited to the discussions of the node-view heterophily. Furthermore, they are semi-supervised and rely on the node labels to guide the learning processes. In the settings of unsupervised GRLs, how to effectively encode different frequency information of different entities into embeddings and make the graph representation can effectively support a wealth of unknown downstream tasks remains largely unexplored. Learning for relations. In the domain of multi-layer networks, TransE <ref type="bibr" target="#b29">[30]</ref> learns vector representations for both entities and relations by inducing a translation function to represent the relations. Later works <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> extend this concept with different relations with different score functions. PairRE <ref type="bibr" target="#b32">[33]</ref> further proposes a model with paired vectors for each relation representation. This approach also adopts the paired vectors for paired nodes. These solutions all explicitly demand the labels of edges, which are not available in the settings of GRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NOTATIONS AND PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations and definitions</head><p>There are two natural entities in the graph: nodes and edges. Let G = (V ; E; X) denote a directed graph, where V is the node set with n nodes and E is the edge set, with node feature matrix X ? R n?d . We denote a general neighborhood centered around u as N (u) that does not include the node u. The d dimensional feature vector of node u is denoted as x u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2: The Node &amp; Pair View</head><p>Definition. 1: (Pair) A pair is an ordered node pair p u,v starting from the node u to the node v, where u, v ? V and (u, v) ? E. Node u and v are called the source node and the target node, respectively. We denote P (G) as the set of all pairs in G. Definition. 2: (Pair neighborhood). Different GNN models have different definitions of neighbor: a general neighborhood centered around p u,v as N (p u,v ) (G may have self-loops), the corresponding neighborhood that does not include the p u,v itself as N (p u,v ), and the general neighbors of pair p u,v at exactly i-hops/steps away (minimum distance) as N i (p u,v ). In this paper, We adopt the typical definition of 1-hop neighbors, and define the neighborhood of p u,v as:</p><formula xml:id="formula_0">N (p u,v ) = {p u,ui ? p v,vj } p u,ui , p v,vj ? P (G), u i ? N (u), v j ? N (v)</formula><p>The bigger circle in <ref type="figure">Fig. 2</ref> shows the neighborhood of p u,v , while the two smaller circles show the 1-hop neighborhood of node u and v respectively. Definition. 3: (Pair embedding) The task of pair-based graph embedding is to learn a low-dimensional vector z u,v to represent the ordered node pair p u,v in the entire graph. In the embedding process, complete feature information of the node pair and surrounding neighboring information N (p u,v ) should be preserved as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Global/Local v.s Node/Edge label assortativity</head><p>Here, we define the metrics to describe the essential characteristics of a graph. There exist multiple metrics, e.g., the label smoothness defined in <ref type="bibr" target="#b12">[13]</ref> or the network assortativity in <ref type="bibr" target="#b33">[34]</ref>. We adopt assortativity as it can be easily generalized to nodes and edges. Here, the assortativity coefficient measures the level of homophily of the graph based on some vertex labeling or values assigned to vertices. If the coefficient is high, connected vertices tend to have the same labels or similar assigned values. Definition 4. (Global Node/Edge label assortativity) For a graph G with node labels, the node label assortativity is defined as follows:</p><formula xml:id="formula_1">r global n = i e ii ? i a i b i 1 ? i a i b i = T r(e) ? (e 2 ) 1 ? (e 2 )<label>(1)</label></formula><p>where e is the matrix whose elements are e ij , e ij is the fraction of edges connecting nodes of label i and label j, a i = j e ij , and b i = i e ij . For multi-label nodes (Ldimensional labels), we can calculate the corresponding assortacity level according to the global distribution of each dimensional label, and finally obtain the assortavity set of all labels:  As mentioned in <ref type="bibr" target="#b34">[35]</ref>, the network performance in the real world often shows diversified mixed modes: the level of assortativity of different nodes or edges is inconsistent. To better represent the diverse patterns in the network and accurately quantify and compare the learnability of pair and other benchmarks in different patterns, we adopt a generalized concept, the local node/edge-level assortativity, one metric based on the label distribution from an individual node/edge-view. Definition 5.(Local Node/Edge label assortativity) We define the r local n (u) as a measure of the local assortativity level for node u:</p><formula xml:id="formula_2">r local n (u) = |{v} : v ? N (u) ? y u = y v | |N (u)| ; u ? V<label>(2)</label></formula><p>where |N (u)| denotes the number of neighbors of node u, y u denotes the label of node u. Due to concise consideration, we only provides node-label assortativity. For edge e, the r local e (e) measure the local label distribution of edge e and its neighborhoods in promximity. Its calculation method is similar to r local n (u). It is worth noting that the definition and calculation of assortativity used in this article are for class-level, not cross-class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>From definition 4, we can see that one graph is a complex structure, and it can be seen from many different perspectives: node and edge. Existing approaches, however, limit their discussions on the node homophily, which is by no means the only important metric for embeddings. Furthermore, the following observations are identified: Observation 1. One graph exists multiple entities and levels of homophily.</p><p>According to the definition, the node and edge label assortativity coefficients are two different metrics to describe the graph to present the overall trends for all the node/edge labels. For many real-world networks with multiple node labels, multiple node assortativity coefficients exist. The same holds for multi-label edges. From our observation, one graph might have discrepancy highly in those assortativity coefficients. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the node/edge assortativity for Cuniform, a graph with 7-dimension node labels and 1dimension edge label(each dimension has two values: 0 or 1). It clearly shows that one graph might display both homophily and heterophily of nodes and edges: the assortativity of different node labels can range from 0.07 to 0.80. Thus, we can not simply classify a network as nodebased homophily or heterophily. It is not appropriate to use the homophily assumption for GRL as it is only one one graph perspective. For different classification tasks, the information required can be totally different.</p><p>Furthermore, as pointed out by <ref type="bibr" target="#b10">[11]</ref>, the actual level of assortativity may also vary within different pairs of node classes, i.e., there is a different tendency of connection between each pair of classes with two different characteristics of the graph. Thus, some pairs of classes may exhibit homophily within the same network, while others exhibit heterophily. In <ref type="figure" target="#fig_1">Fig. 3</ref>(b), we examine various networks from different domains for existence of diverse local patterns using r local n . In the two datasets, we witness skewed and multimodal distributions. We are essentially interested in how GNNs perform under different patterns, and further experimental analysis is provided in section 5.4. Observation 2. The edge classification task relies on the different frequency signals between features of paired nodes.</p><p>As discussed in <ref type="figure" target="#fig_5">Fig. 1</ref>, we believe that the implicit relational knowledge of the edge is related to the low and highfrequency information of the features of the paired nodes. In other words, the representation of the edge strongly depends on different frequency information between the connected nodes. GCN-based aggregation, as pointed out by many references <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, is essentially the smoothing of the features of connected nodes <ref type="bibr" target="#b7">[8]</ref> to capture the similarities of graph structure and features. This causes implicit relationship patterns to become vague and indistinguishable in learned representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODS</head><p>This section introduces the basic embedding units and selfsupervised tasks to keep the original graph information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pair and its neighbors</head><p>To retain commonality and differentiation information between two connected nodes, we use pair as the basic modeling unit and take the feature information of paired nodes as a whole, avoiding smoothing its internal information. To further broaden the field of view of the model and use the correlation between the central unit and its domain information to obtain a more stable and robust effect, we introduce the "context" concept representing the aggregated neighboring feature patterns of a pair. Ego features. To represent p u,v , we use the ego features from paired nodes u and v. The p u,v is expressed with the direct concatenation of feature vector of nodes u and v, X ego</p><formula xml:id="formula_3">u,v = X ego u X ego v , (X ego u = x u , X ego v = x v ? X).</formula><p>Here, " " represents the concatenation operation on the features of paired nodes u and v instead of "mixing" them with sum or average operation to avoid smoothing the high information between paired nodes. Thus, the relationships that highly rely on the feature patterns in the pair can be largely preserved, e.g., the relationships discussed in <ref type="figure" target="#fig_5">Fig. 1</ref>. Aggregated features. For the neighboring information, similar to the GNN-based solutions, we define an aggregation function to represent the surrounding context, denoted as X agg u,v of p u,v . Here, the AGGR function aggregates node features only from the immediate neighbors of node v or node u (defined in the N (v), N (u)) with permutation invariant functions, e.g., mean or sum. As context might involve many different pairs, we use the mean function to acquire low-frequency feature signals of a pair's surrounding context. Thus, the aggregated features represent the commodity around a pair. However, as the two paired nodes might have quite dissimilar contexts, the aggregation operations are taken separately towards each node and then concatenated to keep possible heterophily in between. Eq. 3 defines this process in aggregating features from a pair's context.</p><formula xml:id="formula_4">X agg u,v ? Concat(X agg u = AGGR(x i , ?i ? N (u)), X agg v = AGGR(x j , ?j ? N (v)))<label>(3)</label></formula><p>The dimension of pair ego-features and aggregatedfeatures is F, obviously, F = 2d. Ego-/Agg-feat. vs Low-/High-freq. signals. The differentiation between dimension-wise paired node features represents low-/high-frequency signals. We emphasize that different dimensions of node feature commonly can not be directly compared as they typically have diverse physical meanings, e.g., age and gender. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, for p n1,n2 , the 2nd-dim features of n 1 , n 2 with significant differences(0,1) can be seen as high-frequency, while the signal between the 1st/3rd-dim features are low-frequency. Thus, ego features contain both low and high-frequency signals. To reduce impacts from noise, we use the agg-features, mean aggregated features from n 1 , n 2 neighbors to intensify the low-frequency signals and the mean-smoothed highfrequency signals between n 1 , n 2 contexts. GNNs adopt neighbors aggregation iteratively and gradually smooth out high-frequency signals. It can be clearly seen from <ref type="figure" target="#fig_2">Fig. 4</ref> that it will make the representations of all nodes close to or even completely consistent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning with different types of signals.</head><p>As GRL is to learn embeddings without label supports, and we need to find appropriate loss functions to realize such unsupervised learning. Generally, existing solutions can be divided into the reconstruction loss <ref type="bibr" target="#b6">[7]</ref> or the contrast loss <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref>. As we have two different entities, we defined two different reconstruction tasks: a pair-based ego feature reconstruction task(denoted as ego-task) and a pair-based context reconstruction task(denoted as agg-task). Ego feature reconstruction. This task is designed to recover the pair's ego feature X ego u,v . Instead of reconstructing the actual values of ego features, we adopt the KL-divergence as our loss function by minimizing the Kullbach-Leibler distance D KL between the reconstructed feature distribution Q(X ego u,v ) and the prior distribution P (X ego u,v ). By analogy with information theory, it is called the relative entropy of P concerning Q. D KL (P Q) = 0 if and only if P = Q almost everywhere. Here, our goal is to represent the distribution patterns rather than the actual feature values.</p><p>Here, our goal is to represent the distribution patterns rather than the actual feature values. In order to calculate the KL loss, we calculate the probability distribution of the k-th pair by Eq. 4. Where, Q ego k (i) indicates the i ? th feature's distribution possibility in ego-task, and H ego represents the intermediate result of the output layer of ego-task. It is calculated by applying the standard exponential function to each element. H ego k (i) and the normalized values are divided by the sum of all these exponents.</p><formula xml:id="formula_5">Q ego k (i) = exp (H ego k (i))/ j ? F exp (H ego k (j)) k ? E, i ? F<label>(4)</label></formula><p>The loss function of KL is expressed by Eq. 5 :</p><formula xml:id="formula_6">arg min(D KL ) = M in i ? E j ?F P i (j) ln P i (j) Q i (j)<label>(5)</label></formula><p>Aggregated feature reconstruction. Similar to the pairbased ego feature reconstruction task, this task reconstructs the aggregated context features of the first-order neighbors N (p u,v ) of the pair X agg u,v , and uses a similar equation with Eq. 4 for distribution calculation and a similar loss function based on KL-divergence to reconstruct X agg u,v . Due to the natural smoothness of the mean aggregation operation, X agg u,v represents the information of the pair neighborhood features after smoothing. Therefore, this task provides contextual low-frequency information for the central pair, which can enhance the pair's ego features. When the downstream tasks have high assortativity with similar edges in their neighborhood, the agg-task, by aggregating neighborhood information, essentially enhances the neighborhood's similarity (low frequency). Thus, it achieves better performance than using the pair ego features.</p><p>When the downstream task exhibits disassortativity, most labels of neighboring edges are inconsistent with the pair's label. Thus, the feature distribution of the current pair normally is far away from its neighbors' aggregated ones. That distribution might contain a lot of information unrelated to the pair's label and can be seen as noises from this task's perspective. They can greatly disturb the learning process by making the representation deviate from the ground truth. In comparison, the ego-task guarantees the basic facts and can lead the agg-task with more than 25% absolute improvement in Micro-F1. <ref type="figure">Fig. 5</ref>(a) and 5(b) clearly shows that for different downstream tasks, different types of information are needed. As GRL should be taskagnostic, it is important to integrate all those information into the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The designs of PairE</head><p>According to the analysis in section 4.2, Ego feature reconstruction and Aggregated feature reconstruction capture the similarity (low-frequency) and difference (high-frequency) information from different perspectives in the graph, and the information of a single frequency cannot be adapted to have different assortativity level tasks. Therefore, to retain the information of the entire spectrum of low and high frequencies, we propose a novel multi-self-supervised tasks autoencoder to simultaneously learn a joint embedding space with data from multiple perspectives. <ref type="figure" target="#fig_4">Fig. 6</ref> describes the principal structure of the autoencoder. As shown in the right part of this figure, similar to other AutoEncoder solutions, it can be generally divided into three major parts, the Encoder, Decoder and the Embedding layer. And PairE introduces three peculiar designs to support learning different frequency signals from respective tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seperated En(de)code for respective tasks:</head><p>In this autoencoder, the two reconstruction tasks are encoded/decoded independently. Each is trained in two fully connected layers and only combined in the Embedding layer. The two tasks are also respectively decoded by a fully connected layer with a softmax activation function defined in Eq. 4. Since different tasks focus on different frequency information in the graph, one shared embedding allows different signals to be integrated into one formal representation. Furthermore, this design exploits the mutual support from both tasks as a pair's ego features and its surrounding features are typically mutual-correlated. The impacts of decay or missing features, typical in many real-world networks, can be partially mitigated. Furthermore, learning with multiple tasks can generally improve performance compared to learning from a single task <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenation of intermediate representations:</head><p>Similar to the concatenation of the pair's ego features and agg features, we also separate the ego and context embeddings by encoding them separately. After aggregating the context representations, we combine the ego and the aggregated embeddings by concatenating rather than "mixing" all of them, similar to the GraphSage <ref type="bibr" target="#b22">[23]</ref>. Choosing a concatenate function that separates the representations of pair-ego and pair-agg allows for more expressiveness <ref type="bibr" target="#b10">[11]</ref>.</p><p>In addition to the concatenation, we combine the intermediate representations of ego and aggregated context at the final layer. Similar to the "add skip connections" operation introduced in ResNet <ref type="bibr" target="#b37">[38]</ref>. It is later adopted in jumping knowledge networks for graph representation learning <ref type="bibr" target="#b38">[39]</ref>. By combining all layers of the two tasks simultaneously, this design can preserve the integrity and personalization of the input information as much as possible. The inputs of the final layer are constructed as follows(As <ref type="figure" target="#fig_4">Fig. 6 shows)</ref>:</p><formula xml:id="formula_7">h emb u,v = Concat(h 1 e , h 2 e , h 1 a , h 2 a )<label>(6)</label></formula><p>Linear Activation Function: Using graph-agnostic dense layers to reconstruct the original features of each pair:</p><formula xml:id="formula_8">h self u,v = ?(X self u,v W ),</formula><p>where ? is an activation function, W is a weight matrix. Here, the linear function is used in this paper. By performing an only linear transformation on the input features, our solution will not smooth out highfrequency signals in the ego features of the pair. Discussion: We analyze the expressive power of PairE and GCN-based approaches from the perspective of the similarity between edge embeddings. For any two edges, (u, v), (u, v ) ? E, with a common node u. P (X), P (R), Q(X) are the orignal feature distribution, embedding distribution, and the reconstructed distribution. In order to evalute the similarity between vectors, the Cosine similarity measure is used to measure of similarity between two non-zero vectors of an inner product space. Let S, S P denote the Cosine similarity of edge features, learned edge embeddings by PairE. Values range between -1 and 1, where -1 is diametrically opposed, 0 means independent, and 1 is exactly similar. </p><formula xml:id="formula_9">S = P (X u,v ) ? P (X u,v ) |P (X u,v )||P (X u,v )| , S ? [?1, 1]</formula><formula xml:id="formula_10">argmin(D KL (P Q)) ? 0 ? Q(X u,v ) ? P (X u,v )</formula><p>The Q(X u,v ) are reconstructed with the generated edge embeddings R u,v , thus, R u,v have the same distribution as Q(X u,v ). Thus, we can deduce: P (R u,v ) ? P (X u,v ). It means that the generated representation R u,v will have the similar distribution as the original paired feature distribution if the KL divergence approaching zero. Therefore, embeddings retain a relatively complete relationship pattern in the original feature.</p><p>Therefore, it is easy to calculate:</p><formula xml:id="formula_11">S P = P (R u,v ) ? P (R u,v ) |P (R u,v )||P (R u,v )| ? P (X u,v ) ? P (X u,v ) |P (X u,v )||P (X u,v )| = S</formula><p>Where we can clearly see that S P will gradually approach S during the learning process of PairE, in other words, PairE can be seen as a full-pass filter on the input signals with certain compression. Lemma 1. Most existing GNNs, e.g., GCN, based a lowfrequency filter: (D + I) ?1/2 (A + I)(D + I) ?1/2 , and only have the capability to narrow the distance between node representations, where D, I, A denote the diagonal degree matrix, the identity matrix, and the adjacency matrix of G, respectively:</p><formula xml:id="formula_12">P (R u ) ? P (R v ) |P (R u )||P (R v )| &gt; P (X u ) ? P (X v ) |P (X u )||P (X v )|</formula><p>As this fact has been discussed in many papers, we omitted the detailed proof. Detailed proof can be found in reference <ref type="bibr" target="#b2">[3]</ref>.</p><p>For the edge-related tasks, most GNNs need to obtain edge representations based on node representations with certain conversion operations, e.g, sum, mean, concatenate. For fair comparison, we adopt concatenate operation as an example: P (R u,v ) = P (R u )||P (R v ). Obviously, edge representations still obey Lemma1.</p><formula xml:id="formula_13">S G = P (R u,v ) ? P (R u,v ) |P (R u,v )||P (R u,v )| &gt; P (X u,v ) ? P (X u,v ) |P (X u,v )||P (X u,v )|</formula><p>In addition, as the number of iterative training increases, the discrimination between representations is gradually reduced. Thus, it is easy to cause the so-called over-smoothing problem: S G ? 1. Due to a lack of distinction in the reconstrued edge representations, it isn't easy to make correct inferences about the relationship between nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Edge to node embeddings</head><p>To support node-related downstream tasks, we need to define a translator function T to translate pair embedding to node embeddings. This function should be permutationfree and several operations can be used: min, max, mean, and sum. As pointed out in <ref type="bibr" target="#b39">[40]</ref>, one node is a composite entity with multiple roles. Thus, the sum translator defined in Eq.7. With this translator, the embedding of node u, defined as z u , is the sum of the set of pair embedding z u,vi with u as a starting point. The appendix provides performance comparisons with different types of translators.</p><formula xml:id="formula_14">z u = Sum (z u,vi , v i ? N (u))<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Time and space complexity</head><p>According to <ref type="figure" target="#fig_4">Fig. 6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>The performances of PairE are tested on both edge-related tasks and node-related tasks to answer three questions: Q1: Can PairE effectively encode different types of signals into the embeddings from the perspective of different entities to support various downstream tasks? Q2: What role has the ego and agg tasks played in keeping different types of signals? Q3: Whether PairE is computation efficient and resources scalable to be used in large graphs?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>To investigate the learning ability of PairE towards the complex structures from graphs in different application domains, a variety of real-world graph datasets from benchmarks are selected. The standard benchmark dataset: ? PPI: multi-layer datasets of PPI from biology for the interactions between different proteins.</p><p>? DBLP: from collaboration network. In PPI and DBLP, each node might play multiple roles and have multiple labels <ref type="bibr" target="#b2">3</ref> .</p><p>For the networks with edge labels, four datasets are selected: In those four networks, each edge might have one or multiple types. Since many knowledge graphs have no node features, the node embeddings generated by DeepWalk are used instead. Tab. 1 shows their basic properties. Please note we perform unsupervised learning. Labels of all nodes and edges are hidden during learning.  <ref type="table" target="#tab_1">Cora  2708  5429  7  -1433  CiteSeer  3327  4732  6  -3703  Pubmed  19,717  44,327  3  -500  Cornell  183  280  5  -1703  Wisconsin  251  466  5  -1703</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>PairE is an unsupervised network embedding algorithm. To make a fair comparison, we have selected only the following state-of-the-art node embedding algorithms, which are unsupervised in nature and have publicly available source code. They are classified into two categories: methods based on topology and node features. Furthermore, to best demonstrate the effectiveness in different assortativity of networks, we also compare four semi-supervised solutions optimized for different types of networks. Topology only:</p><p>? DeepWalk(denoted as DW) <ref type="bibr" target="#b17">[18]</ref>, the first approach for learning latent representations of vertices in a network with NLP.</p><p>? TransR <ref type="bibr" target="#b30">[31]</ref> builds entity and relation embeddings in separate entity space and relation spaces to support N-N relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Both topology and node features:</head><p>? ARVGE <ref type="bibr" target="#b25">[26]</ref> is an autoencoder-based solution that enhances GAE to seek robust embedding by adding an adversarial module on the obtained embeddings. This method is known to have cutting-edge performance in the link prediction task.</p><p>? SAGE-U <ref type="bibr" target="#b22">[23]</ref>: an unsupervised GraphSage variant that leverages node feature information to generate node embeddings by sampling a fixed number of neighbors for each node.</p><p>? DGI [36]: a contrastive approach via maximizing mutual information between patch representations and high-level summaries of graphs.</p><p>? CAN <ref type="bibr" target="#b26">[27]</ref>: a variational auto-encoder that embeds each node and attribute with means and variances of Gaussian distributions.</p><p>? NodeE is used for ablation studies, and it works on the node level with the same autoencoder as PairE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-Supervised Algotithms:</head><p>? GCN <ref type="bibr" target="#b5">[6]</ref>: essentially uses a low-pass filter to retain the commonality of node features and fails to generalize to networks with heterophily (or low/medium level of homophily).</p><p>? SAGE-S [23]: a semi-supervised variant of Graph-Sage that uses the same low-pass filter as GCN.</p><p>? PGE <ref type="bibr" target="#b4">[5]</ref>: a graph representation learning framework that incorporates both node and edge properties into the graph embedding procedure.</p><p>? H2GCN <ref type="bibr" target="#b10">[11]</ref>: identifies a set of key designs that can boost learning from the graph structure in heterophily without trading off accuracy in homophily.</p><p>? FAGCN <ref type="bibr" target="#b13">[14]</ref>: frequency adaptation GCN with a selfgating mechanism, which can adaptively integrate both low and high-frequency signals in the process of message passing.</p><p>The semi-supervised solutions are trained in an end-toend fashion and optimized for specific downstream tasks with the support of downstream labels. Thus, it is typical that semi-supervised solutions might have better performance than unsupervised ones.</p><p>Here, only limited baselines on several representative graphs are reported. More comprehensive experiment results with more baselines, e.g., TADW <ref type="bibr" target="#b40">[41]</ref>, ProNE <ref type="bibr" target="#b41">[42]</ref>, and datasets, e.g., Proteins full are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment settings</head><p>In this section, experiment setups for different downstream tasks and baselines are defined. Node/Edge classification task: For these tasks, all the nodes are used in the embedding generation process. For PairE, the node embeddings are generated with Eq. 7. Edge embeddings for all compared baselines are calculated with the L2 norm formula of two-node embeddings. The embeddings of vertices/edges from different solutions are taken to train a classifier with different training ratios from 30%, 50% to 70%, and classification accuracy is evaluated with the remaining data. Due to space limitations, Tab. 2 and Tab. 3 only show the results with the 30% train ratio for both unsupervised and semi-supervised solutions. For each network, we used 10 random splits, and the average performances in Micro-F1 are reported. Link-prediction task: We use a similar prepossessing step in <ref type="bibr" target="#b3">[4]</ref>. Our experiments held out a set of 15% edges for testing and trained all models on the remaining subgraph. Additionally, the testing set also contains an equal number of non-edges. In this task, the edge embeddings for all methods are calculated from the source and target node embeddings with the L2 operator. Experiment settings: For all semi-supervised algorithms and partially unsupervised algorithms(Sage-U, DGI, and ARVGE), we use the most commonly used hyperparameter settings: Implementing in Pytorch with Adam optimizer, we run 10 times and report the mean values with standard deviation. The hidden unit is fixed at 16 in all networks. The hyper-parameter search space is: learning rate in   regression algorithm is used with the one-vs-rest strategy. We report results over 10 runs with different random seeds and train/test splits, and the averaged ROC AUC/Micro-F1 score is reported. The model configurations are fixed across all the experiments. Detailed settings are omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance comparisions</head><p>Edge classification Task: This task aims to answer Q1. As shown in Tab. 2, PairE outperforms by a significant margin in all four tested datasets across all tested ranges. For unsupervised algorithms, PairE achieves more than 13.6%? 36% performance edges in Cuneiform and 43?155% in FB15K-237 and 5?182% WN18RR. On average, PairE achieves up to 101.1% improvement on this task. Although the node features are from deepwalk embeddings, PairE can perform better than all the other baselines, including DW. In comparison, TransR ranks second in Micro-F1 as it is designed to build the relations between nodes with a transformation function. ARVGE is optimized for the linkprediction task focusing on link existence estimation, while this task demands more expression power as edges are with multiple labels. In our experiences, other Trans and ARVGE variants also display similar performance.</p><p>It is particularly worth noting that the classification effect of PairE on multi-label datasets far surpasses the comparison of semi-supervised benchmarks(even for H2GCN and FAGCN) optimized for homophily and heterophily) and achieves the best performance. These results clearly show an interesting fact that the edge has strong correlations with both the node features and the context of the paired nodes. PairE can differentiate those structural differences at a finer granularity and identify heterogeneous information, leading to high-quality pair representations with superior performance.</p><p>Meanwhile, an interesting phenomenon is that all those semi-supervised methods fail on the FB15k-237 dataset. We guess that each edge in the dataset has a 237-dimensional label and the assortativity of most labels is extremely low. The relationships among features and labels are too complicated to be effectively learned by those semi-supervised solutions. Node classification Task: The task provides answers to Q2. The node embeddings of PairE are translated with Eq. 7. As shown in Tab. 3, under unsupervised settings, PairE still outperforms those baselines by a significant margin in all eight tested datasets with different node label assortativity compared with other unsupervised strong baselines. And on average, up to 82.5% performance gain has been achieved.</p><p>Compared to the semi-supervised baselines, PairE achieves comparable performance even with no node labels during embedding. For the datasets with high node label assortativity, the semi-supervised solution GCN and Sage-S achieves good performance as GCN and Sage-S are optimized for homophily networks, e.g., Cora. The FAGCN and H2GCN are generally designed for heterophily networks, and they reach the best performance in Cornell and Wisconsin, whose nodes assortativity are only 0.07 and 0.05, respectively. Sage-S have a strong performance in both kinds of networks as it updates the node ego-embeddings by concatenating the aggregated neighbor-embeddings.</p><p>In the graphs with multiple types of node labels, e.g., DBLP, PPI, and Cuneiform, PairE displays significant performance advantages over the other ten baselines, including both unsupervised and semi-supervised solutions. In PPI, PairE achieves the averaged Micro-F1 score to 0.94 and up to 141.7% relative performance improvement. PairE has a similar performance advantage in Cuneiform as both datasets have very low node label assortativity. As pointed out in <ref type="bibr" target="#b39">[40]</ref>, one node might play multiple roles, which the pair embeddings can partially support. Those complex correlations can be used to support the multi-label node classification task better. However, even for semi-supervised solutions, existing solutions fail to learn a model that can effectively represent the labels with highly different assortativity values. One exception is DBLP. Its labels are highly correlated and have similar high assortativity values(0.58?0.78); thus, different baselines display comparably good performance in this dataset.</p><p>Combined with Tab. 2, we can see that PairE has very consistent performance in both the node and the edge classification task. It is important for an unsupervised embedding solution if its embedding can be used for many different and possible unknown purposes. PairE obviously has such excellent characteristics. Furthermore, results also show that the semi-supervised GNN solutions face significant limitations in learning at datasets with labels of diverse assortativity in both node and edge classification tasks. Link Prediction Task: <ref type="figure">Fig. 8</ref> shows the results for the link prediction task in ROC AUC. Since ARVGE reconstructs graph structure by directly predicting whether there is a link between two nodes, it has outstanding performance in this task. Nevertheless, PairE still beats these very competitive benchmarks in 6 out of 7 datasets in ROC AUC. We observe that two nodes matching neighboring patterns, e.g., close together or matching features, are more likely to form a link. TransR, due to its lack of usage of node features, suffers the most in this task. Results in AP also display similar trends.</p><p>Here, the only exception is PPI. It is a protein interaction network, and different amino acid types are more likely to connect. Thus, there are rich high-frequency signals between the features of paired nodes. As the link prediction task demands fundamental relations between nodes, the rich information might pose complexities to the classifier. The following ablation studies provide further supports for this phenomenon. Node Classification vs. Local Assortativity: In order to better show the difference of nodes with different local assortativity, an analytical experiment is introduced for Pubmed(homophily) and Wisconsin(heterophily). The set-ting of the rask is consistent with the node classification task.</p><p>First of all, we can intuitively see that PairE has a sound and remarkable effect in the three assortativity levels in the two types of datasets from the <ref type="figure" target="#fig_9">Fig. 7</ref>.</p><p>The semi-supervised solutions, e.g., GCN and SAGE-S, have entirely different performances for nodes with different local assortativity: Micro-F1 94.2% and 93.7% in Pubmed for nodes with high local assortativity while 7.9% and 19.7% for nodes with low local assortativity. It clearly shows that GNN-based solutions can only effectively learn nodes with high assortativity metrics and suffers massive performance deterioration due to the lack of guidance from those long-tail nodes. For GCN and SAGE-S, due to their homophily design, their performance for the node with high local assortativity is much better than nodes with low local assortativity.</p><p>However, un-supervised solutions, SAGE-U, DGI, and PairE exhibit much different behavior in and heterophily networks. In Pubmed, they usually are much less impacted from the node-local assortativity, partially due to their unsupervised embeddings design without the miss-lead from node labels. However, their performance difference becomes noticeable in Wisconsin, and the nodes with high local assortativity generally have much better performance than nodes with low local assortativity. To better illustrate the effect of aggregation, we also prove the result of MLP, which only uses features for classification. For both homophily and heterophily networks, MLP has similar performance towards different types of nodes as it only use node features for classification.</p><p>In short, the prediction performance of a wide range of GNN models display high diversity with respect to the different levels of node assortativity. PairE, in comparison, has stable performance in both Pubmed and Wisconsin and on almost all nodes with different local assortativity. Furthermore, we did observe certain performance degradation for nodes with middle assortativity in Wisconsin. We guess since PairE does not have a pretext to support those nodes with mixed local patterns. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation studies</head><p>In this section, we analyze the effect of different design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Node v.s. Pair View</head><p>We compare the performance between the pair-view embeddings and the node-view embeddings with the node <ref type="figure">Fig. 8</ref>: Comparisons in the Link Prediction tasks, evaluated in ROC AUC and edge classification tasks. As can be seen from Tab. 2 and Tab. 3, compared to PairE, NodeE has a significant performance difference in two different tasks. NodeE suffers significant performance deterioration for the edge classification task, about 68.8% in FB15K-237 and 75.9% in WN18RR. The many performances clearly show the importance of pair-view embeddings. It is interesting to see that NodeE achieves excellent performance with only a node's ego and aggregated features for the node classification tasks due to the high assortativity of those networks. In the handwritten dataset Cuneiform, however, node classes are more related to the node positions. Thus, NodeE suffers from the lack of a broader view of the graph topology. In contrast, PairE maintains good performance in all those datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Ego v.s. Agg. tasks for global (dis)assortative labels</head><p>This section focuses on answering the Q2 from two different perspectives: 1) Do the two reconstruction tasks indeed retain different frequency signals? 2) How will the downstream tasks with different assortativity react to the different types of embeddings?</p><p>Please note that the assortativity involved in this experiment is "Global Node/Edge label assortativity". Performance Comparisons: The right part of the <ref type="figure" target="#fig_10">Fig. 9</ref> shows the normalized embedding distribution displays similar generated by different reconstruction tasks or both. As we can see from those figures, all those embeddings follow a Gaussian distribution with different standard deviations ?. Here, we show only distributions of the first dimension while other dimensions also have similar characteristics.</p><p>In both WN18RR(a) and FB15k-237(b), ? ego is much bigger than ? agg . It verifies our remarks that the ego-task keeps more high-frequency signal, while agg-task, due to its average aggregation, the embeddings from agg-task mainly contains low-frequency signals with small ?. Compared to WN18RR, the agg-distribution from FB15k-237 is a multimodal distribution with many peaks. We guess that several major distribution patterns of aggregated features can hardly be represented by the edge assortativity metric for overall trends. <ref type="figure" target="#fig_10">Fig. 9</ref>(c) and <ref type="figure" target="#fig_10">Fig. 9(d)</ref> show the distributions of the node embeddings generated by the sum translator. <ref type="figure" target="#fig_10">Fig. 9</ref>(c) has three almost identical distribution as DBLP has very high node assortativity(0.58 ? 0.78) for almost all labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impacts towards downstream tasks with different assortativity levels:</head><p>To study the impacts of different signals on the tasks with different assortativity, we select the edge/node labels with Top 3 (H) and Lowest 3(L) assortativity for classification. Results are summarized in the left table of <ref type="figure" target="#fig_10">Fig. 9</ref>. As the four labels of DBLP have similar assortativity, its results are omitted. We can observe that in edge classification, H labels benefit the most from the aggregation task. It has comparable performance with all-tasks, even better in WN18RR, since the aggregation of low-frequency information in the neighborhood provides a robust representation of the edges. However, for low-assortativity tasks, embeddings from ego-task can have much better performance as it contains more edge-specific information towards the edge label. At the same time, the agg-task aggregates much unrelated signals. For low-assortativity tasks: Without the constraints of the modeling object's own information, the graph representation guided by the high-frequency information in the neighborhood will deviate from the ground truth. Therefore, the performance of the agg-task is significantly worse than the ego-task.</p><p>PPI is a typical heterophily network as linked nodes with different types are more likely to connect. Therefore, the embeddings from ego-features and agg-features are pretty different. However, it is also remarkable as the neighbor of a node has displayed certain similarities. Thus, ego-task and agg-task provide similar node classification performance. We guess that there are noticeable patterns in the features of nodes and their surrounding nodes due to the law of chemistry inferred from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Comparisons of different loss weight between two pretext tasks</head><p>To check the contribution of different pretext tasks and provide an explicit explanation for Q2. We change the ratios of two different losses for the ego-task and agg-task and check the performance variance. As the ratios of two pretext tasks are normalized to one, we only show the weight of the agg-task. Thus, the ratio of 0.1 denotes that the weight for the ego-task is 0.1 and 0.9 for the agg-task. In this case, the ego-task plays a minor role in the back-propagation during training. <ref type="figure" target="#fig_5">Fig. 11</ref> illustrates that with the change of data homogeneity, the impacts of ego-task and agg-task towards different downstream tasks. <ref type="figure" target="#fig_5">Fig.11(a)</ref> shows the results for both homophily networks, e.g., DBLP and heterophily networks, Cornell and Wisconsin. For DBLP, the change of ratio might not impact the performance of the node classification tasks as the egofeature and the aggregated features might have similar distributions. However, for Cornell and Wisconsin, when the weight of the ego-task increases from 0.1 to 0.9, we can find consistent performance improvements as the aggtask might accumulate unrelated information, which might introduce certain noises in the embeddings. <ref type="figure" target="#fig_5">Fig.11(b)</ref> shows its impact on the edge classification task for three groups of representative edge labels with different assortativity levels in FB15k-237. Each group has three edge labels-the three lines in this figure display totally different behavior with the increase of ego ratio. For the labels with high assortativity(0.31?0.41), the ratio has little impact. The  ratio changes also have similar impacts as node classification in Cornell and Wisconsin for the label with very low assortativity. Especially, for the labels with medium assortativity levels, with the increase of the ego ratio, the edge classification task gradually suffers from the higher ratio of ego-task. For those edge labels, the aggregated features might have more stable and robust signals. Thus, for different tasks, the optimal ratio might not be the same. It means the results for the different tasks in Tab. 3 and Tab. 2 can be further improved as the default ratio of the two tasks is set to (0.5,0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Comparisons of translators</head><p>This section analyzes and evaluates the performance impacts of different translators in PairE, i.e., sum, mean, max, and min, based on the node classification task. As can be seen from <ref type="figure" target="#fig_5">Fig. 10</ref>, the sum translator generally achieves the best performance among the four compared translators. It achieves the best performance in almost all the data ratios expect for the 10% range. The reason that contributes to its good performance, we guess, is that its embedding accumulation operation sums up all pair embeddings starting from the node. To some extent, the "sum" translator keeps certain information on the number of connected edges. The other three types of translators have comparably unstable performance. For instance, the "mean" translator performs well in the Pubmed and PPI dataset while relatively poor in the Cora dataset. However, what operation should be used to best translate pair embeddings to node embeddings with minimal information loss remains a largely unexplored area and could be an exciting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Computation and resource scalability</head><p>Tab. 4 shows the training time of different sizes of graphs in one Nvidia V100-32G for 30 epochs. PairE displays excellent scalability. With the growth of graph scales, the calculation time of PairE increases only linearly. It converges much faster than compared ARVGE, which is also an autoencoderbased solution. For graph achemy full with 2 million nodes (a) Node Classification (b) Edge Classification <ref type="figure" target="#fig_5">Fig. 11</ref>: Node classification(a) and Edge classification(b) results under different weight of two pretext tasks. 0.1 in X axis indicates the weight 0.1 for ego-task and 0.9 for agg-task. and 4 million edges, GraphSage, DGI, TransR, H2GCN, and FAGCN suffer the OOM error. Furthermore, PairE is easy to train. It converges typically in about 20 epochs, even in massive graphs. These results answer Q3. The merit is of paramount importance in many real-world applications.</p><p>In particular, to support learning beyond the homophily network, H2GCN introduces many designs which bring expensive time and space consumption. Therefore it is challenging to train H2GCN and apply it to big datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Real-world graphs usually contain complex types and dimensions of information in both homophily and heterophily networks. To retain both low and high-frequency signals, this paper proposes a novel multi-self-supervised autoencoder called PairE. PairE learns graph embedding from the pair-view, thereby avoiding the loss of high-frequency information represented by edges. The seamless integration of node self-features and nearby structural information is achieved with shared layers in the multi-task autoencoder design to realize the collaborative work of low and highfrequency information. We also provide a solution to translate pair embeddings to node embeddings for node-related graph analysis tasks. Extensive experiments show that PairE outperforms strong baselines in different downstream tasks with significant edges. Our experiment results also point out many directions for improvements. For instance, our current solution is unsupervised and task-agnostic. We are working to extend our solution to the semi-supervised message passing-based GNN. What we want to emphasize is that comparing the performance of a series of models under global assortativity and local assortativity settings, we have observed the limitations of existing GNNs. The most important thing is to discover the interesting and challenging problem of representing graphs from the perspective of local assortativity. This will guide our next steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">ACKNOWLEDGEMENT</head><p>This work is supported by the National Science Foundation of China, NO. 61772473 and 62011530148. Ning Gui is associated professor in the School of Computer Science and Engineering, Central South University, China. His research interests include machine learning, representation learning, information retrieval and their applications in the industry. He is a member of the IEEE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>{r global n (1), ..., r global n (L)}.The global edge label assortativity is denoted as r global e , which uses edges as the unit to measure the global label distribution of edge and edge neighborhoods in the entire graph. Its calculation method is similar to r global n When nodes with similar labels tend to connect with each other, r global n (l) ? 1, the graph exhibits strong homophily from the l st label-view of node. The graph displays node heterophily when r global n (l) ? 0, with dissimilar nodes connected with others. r global e measures the edge homophily of the graph with similar traits.(a) The global label assortativity of Cuniform (b) the local label assortativity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The global/local node/edge assortativity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>PairE v.s. Existing GNNs: i) Different colors and line heights represent different dimensions of information and the strength of low-/high-frequency signals. ii) Explain the main difference between PairE and GNNs from the perspective of retaining the original low-/high-frequency signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) High Global Assortativity (b) Low Global AssortativityFig. 5: Performance comparisons for edge label classification with different global assortativity in FB15k-237 Network assortativity v.s. Reconstruction tasks. Fig.5 shows the performance comparisons with embeddings calculated with the individual tasks with a simple AutoEncoder on edge classification with different label assortativity coefficients (edges with top/bottom three label coefficient).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>A conceptual diagram for multi-self-supervised tasks autoencoder</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Proposition 1 .</head><label>1</label><figDesc>The embeddings learned by PairE can be as close as possible to the original distribution of the feature. Proof. The goal of the feature reconstruction make the final reconstructed feature distributions of pair u,v Q(X u,v ) is nearly identical to the original feature distribution P (X u,v ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?</head><label></label><figDesc>Cora, Citeseer, Pubmed: standard citation networks where nodes represent documents, edges are citation links, and features are the bag-of-words representation of the document.? Cornell, Wisconsin: two webpage datasets, where nodes represent web pages, and edges are hyperlinks between them. Node features are the bag-ofwords representation of web pages. The web pages are manually classified into five categories, student, project, course, staff, and faculty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>??FFtwyt 4 :</head><label>4</label><figDesc>Cuneiform: the Cuneiform for handwriting Hittite cuneiform signs; a friend feed of Twitter and Youtube in the social network;? FB15K-237 and WN18RR 5 : two wildly used knowledge graphs with multi-labe edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>{0.01, 0.005}, dropout in {0.4, 0.5, 0.6}, weight decay in 1E-3, 5E-4, 5E-5, number of layers in {1, 2, ..., 8}, scaling hyperparameter in {0.1, ..., 1.0}. Besides, we run 200 epochs and choose the model with the highest validation accuracy for testing. The embedding dimensions of both pair embeddings in PairE and node embeddings for other unsupervised baselines are set to 128. Other parameters are set as the default settings in the corresponding papers. For the PairE, we set epoch 30 and batch size 1024. A classifier with the logistic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 :</head><label>7</label><figDesc>Performance comparisons for node label classification with different local assortativity in Wisconsin and Pubmed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>Comparison of reconstruction task: performance and embedding distribution. The left table shows the Node/edge classification accuracy(in Micro-F1) with different levels of global assortativity, H/L short for High/Low global assortativity; The (a) ? (d) show embedding distributions of their first dimension. Edge embeddings of WN18RR and FB15k-237, and node embeddings of DBLP and PPI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :</head><label>10</label><figDesc>Translators comparisons on Cora, Pubmed, and PPI under different training ratios</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The space consumption for training is related to the batch size b, and the dimension of pair features F , about O(2 ? b ? F ). As PairE is mini-batch training and we do not need to store intermediate embeddings in the GPU. Thus, PairE can train massive graphs with GPU with small memory.</figDesc><table /><note>, the complexity of PairE mainly depends on the number of pairs |E| and the dimension of pair features F . For the computation complexity, every pair is calculated O(epoch) times, then the total computational complexity here is O(epoch ? |E| ? F ). PairE only needs to put the concatenated features as inputs, and the space occupation is much smaller than adjacent vectors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Datasets Properties, ME denotes Multi-label Edges and MN denotes Multi-label Nodes</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell>Edges</cell><cell>Node T.</cell><cell>Edge T.</cell><cell>Feat.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Edge classification results, trained with 30% ratio, evaluated with average Micro-F1. Analogous trends hold for averaged Macro-F1 scores. Datasets with + are of multi-label edges. Gray background and bold font represent the best results in all algorithms and unsupervised settings, respectively. "-" denotes F1-Score is less than 0.1</figDesc><table><row><cell>Name</cell><cell>Cuneiform</cell><cell>FFtwyt +</cell><cell>FB15K237 +</cell><cell>WN18RR +</cell></row><row><cell>r global e</cell><cell>0.26</cell><cell>0.07 ? 0.53</cell><cell>0.001 ? 0.86</cell><cell>0.13 ? 0.92</cell></row><row><cell>DeepWalk</cell><cell>84.24? 2.05</cell><cell>65.59? 0.52</cell><cell>62.30? 0.49</cell><cell>26.56? 2.00</cell></row><row><cell>ARVGE</cell><cell>72.52?0.62</cell><cell>76.17?0.96</cell><cell>56.92?0.98</cell><cell>64.80?0.63</cell></row><row><cell>TansR</cell><cell>70.29?0.65</cell><cell>80.19?0.42</cell><cell>78.48?0.43</cell><cell>71.65 ? 0.87</cell></row><row><cell>SAGE-U</cell><cell>71.78?0.42</cell><cell>77.54?1.13</cell><cell>56.14?0.78</cell><cell>46.19?2.91</cell></row><row><cell>DGI</cell><cell>71.08?0.11</cell><cell>69.84?5.76</cell><cell>35.04?14.10</cell><cell>36.36 ?9.11</cell></row><row><cell>NodeE</cell><cell>73.34?2.37</cell><cell>70.30?0.65</cell><cell>42.64?0.70</cell><cell>54.88 ? 0.42</cell></row><row><cell>PairE</cell><cell>95.71?1.54</cell><cell>83.20? 0.52</cell><cell>89.36?0.62</cell><cell>74.99 ? 0.53</cell></row><row><cell>GCN</cell><cell>98.52? 0.62</cell><cell>78.71? 0.23</cell><cell>-</cell><cell>46.40?1.17</cell></row><row><cell>SAGE-S</cell><cell>92.17? 1.06</cell><cell>81.45?0.32</cell><cell>-</cell><cell>42.48?1.74</cell></row><row><cell>PGE</cell><cell>71.35?0.66</cell><cell>79.56 ? 0.21</cell><cell>-</cell><cell>54.84?0.83</cell></row><row><cell>FAGCN</cell><cell>95.01?4.90</cell><cell>77.74?1.39</cell><cell>-</cell><cell>35.21?6.09</cell></row><row><cell>H2GCN</cell><cell>80.19?0.27</cell><cell>72.81 ? 0.09</cell><cell>-</cell><cell>29.66?0.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>Node classification results, trained with 30%, evaluated with average Micro-F1. Datasets with * are of multi-label nodes. Different marks have the same meaning as Tab 2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc>Trainning time(in seconds) for different scale of graphs, OOM is the out of memory error</figDesc><table><row><cell>Dataset</cell><cell cols="2">Cuneiform FB15K-237</cell><cell cols="2">PPI alchemy</cell></row><row><cell>ARVGE</cell><cell>5.95</cell><cell>79.84</cell><cell>289.06</cell><cell>1275.42</cell></row><row><cell>TransR</cell><cell>1740</cell><cell>19,188</cell><cell>55,080</cell><cell>OOM</cell></row><row><cell>Sage-U</cell><cell>48.64</cell><cell>188.05</cell><cell>647.94</cell><cell>OOM</cell></row><row><cell>DGI</cell><cell>4.58</cell><cell>11.40</cell><cell>28.41</cell><cell>OOM</cell></row><row><cell>H2GCN</cell><cell>5174.48</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell>FAGCN</cell><cell>22.23</cell><cell cols="2">4064.70 1208.4052</cell><cell>OOM</cell></row><row><cell>PairE</cell><cell>5.20</cell><cell>69.76</cell><cell>167.44</cell><cell>519.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>You Li is currently working toward the Master degree in the School of Computer Science and Engineering, Central South Of University, China. Her research interests include Graph Representation Learning, and Genetic data retrieval. Lin is currently working toward the Master degree in software engineering at the School of Computer Science, Central South University, Changsha, China. Her research interests include graph representation learning and unsupervised learning. Luo was a bachelor student in the School of Computer Science and Engineering, Central South Of University, China. His research interests include Graph Representation Learning.</figDesc><table><row><cell>Bei Binli</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. https://linqs.soe.ucsc.edu/data 4. http://multilayer.it.uu.se/ 5. https://github.com/villmow/datasets knowledge embedding</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Network representation learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Big Data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00797</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A representation learning framework for property graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="65" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv:.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Graph convolutional networks using heat kernel for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16002</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mixhop: Higherorder graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Galstyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measuring and improving the use of graph information in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Interpreting and unifying graph neural networks with an optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11859</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive and unsupervised representation learning on graph structured objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Low-bit quantization for attributed network representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progan: Network embedding via proximity generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1308" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Edge2vec: Edgebased social network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graph convolutional networks using heat kernel for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16002</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04407</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-embedding attributed networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twelfth ACM international conference on web search and data mining</title>
		<meeting>the twelfth ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="393" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Geomgcn: Geometric graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10197</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pairre: Knowledge graph embeddings via paired relation vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03798</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mixing patterns in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26126</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Breaking the limit of graph neural networks by improving the assortativity of graphs with local mixing patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06586</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A survey on multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08114</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Is a single embedding enough? learning node representations that capture multiple social contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Epasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="394" to="404" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="2111" to="2117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Prone: fast and scalable network representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Joint Conf</title>
		<meeting>28th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
