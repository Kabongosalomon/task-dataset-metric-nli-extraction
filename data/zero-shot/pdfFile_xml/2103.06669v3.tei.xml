<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Action Segmentation from Timestamp Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><forename type="middle">Abu</forename><surname>Farha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
							<email>gall@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Action Segmentation from Timestamp Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action segmentation approaches have been very successful recently. However, annotating videos with frame-wise labels to train such models is very expensive and time consuming. While weakly supervised methods trained using only ordered action lists require less annotation effort, the performance is still worse than fully supervised approaches. In this paper, we propose to use timestamp supervision for the temporal action segmentation task. Timestamps require a comparable annotation effort to weakly supervised approaches, and yet provide a more supervisory signal. To demonstrate the effectiveness of timestamp supervision, we propose an approach to train a segmentation model using only timestamps annotations. Our approach uses the model output and the annotated timestamps to generate frame-wise labels by detecting the action changes. We further introduce a confidence loss that forces the predicted probabilities to monotonically decrease as the distance to the timestamps increases. This ensures that all and not only the most distinctive frames of an action are learned during training. The evaluation on four datasets shows that models trained with timestamps annotations achieve comparable performance to the fully supervised approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Analyzing and understanding video content is very important for many applications, such as surveillance or intelligent advertisement. Recently, several approaches have been very successful in analyzing and segmenting activities in videos <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref>. Despite the success of the previous approaches, they rely on fully annotated videos where the start and end frames of each action are annotated.</p><p>This level of supervision, however, is very time consuming and hard to obtain. Furthermore, as the boundaries between action segments are usually ambiguous, this might result in inconsistencies between annotations obtained from different annotators. To alleviate these problems, many researchers start exploring weaker levels of supervision in the form of transcripts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b24">25]</ref> or even sets <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref>. For transcript-level supervision, the videos are annotated with <ref type="bibr">Figure 1</ref>. For fully supervised action segmentation, each frame in the training videos is annotated with an action label (top). This process is time-consuming since it requires an accurate annotation of the start and end frame of each action. To reduce the annotation effort, we propose to use timestamps as supervision (bottom). In this case, only one arbitrary frame needs to be annotated for each action and the annotators do not need to search for the start and end frames, which is the most time-consuming annotation part. an ordered list of actions occurring in the video without the starting and ending time of each action. Whereas for the set-level supervision, only the set of actions are provided without any information regarding the order or how many times each action occurs in the videos.</p><p>While transcript-level and set-level supervision significantly reduce the annotation effort, the performance is not satisfying and there is still a gap compared to fully supervised approaches. In this paper, inspired by the recently introduced timestamp supervision for action recognition <ref type="bibr" target="#b30">[31]</ref>, we propose to use timestamp supervision for the action segmentation task to address the limitations of the current weakly supervised approaches. For timestamp supervision, only one frame is annotated from each action segment as illustrated in <ref type="figure">Fig. 1</ref>. Such timestamps annotations can be obtained with comparable effort to transcripts, and yet it provides more supervision. Besides the ordered list of actions occurring in the video, timestamps annotations give partial information about the location of the action segments, which can be utilized to further improve the performance.</p><p>Given the timestamps annotations, the question is how to train a segmentation model with such level of supervision. A naive approach takes only the sparsely annotated frames for training. This, however, ignores most of the information in the video and does not achieve good results as we will show in the experiments. Another strategy is to iterate the process and consider frames with high confidence scores near the annotations as additional annotated frames and include them during training <ref type="bibr" target="#b30">[31]</ref>. Furthermore, frames that are far away from the annotations can be considered as negative samples <ref type="bibr" target="#b27">[28]</ref>. For temporal action segmentation, which is comparable to semantic image segmentation, however, all frames need to be annotated and there are no large parts of the video that can be used to sample negative examples. Furthermore, relying only on frames with high confidence discards many of the video frames that occur during an action and focuses only on the most distinctive frames of an action, which can be sufficient for action recognition or detection but not for action segmentation.</p><p>In this work, we therefore propose a different approach where all frames of the videos are used. Instead of detecting frames of high confidences, we aim to identify changes of actions in order to divide the videos into segments. Since for each action change the frames before the change should be assigned to the previous timestamp and after the change to the next timestamp, we find the action changes by minimizing the variations of the features within each of the two clusters of frames. While we can then train the model on all frames by assigning the label of the timestamp to the corresponding frames, it does not guarantee that all frames of an action are effectively used. We therefore introduce a loss function that enforces a monotonic decrease in the class probabilities as the distance to the timestamps increases. This loss encourages the model to predict higher probabilities for low confident regions that are surrounded by high confident frames and therefore to use all frames and not only the most distinctive frames.</p><p>Our contribution is thus three folded.</p><p>1. We propose to use timestamp supervision for the temporal action segmentation task, where the goal is to predict frame-wise action labels for untrimmed videos.</p><p>2. We introduce an approach to train a temporal action segmentation model from timestamp supervision. The approach uses the model predictions and the annotated timestamps for estimating action changes.</p><p>3. We propose a novel confidence loss that forces the model confidence to decrease monotonically as the distance to the timestamp increases.</p><p>We evaluate our approach on four datasets: 50Salads <ref type="bibr" target="#b37">[38]</ref>, Breakfast <ref type="bibr" target="#b15">[16]</ref>, BEOID <ref type="bibr" target="#b6">[7]</ref>, and Georgia Tech Egocentric Activities (GTEA) <ref type="bibr" target="#b9">[10]</ref>. We show that training an action segmentation model is feasible with only timestamp supervision without compromising the performance compared to the fully supervised approaches. On the 50Salads dataset, for instance, we achieve 97% of the accuracy compared to fully supervised learning, but at a tiny fraction of the annotation costs. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly discuss the related work for the temporal action segmentation task at different levels of supervision.</p><p>Fully Supervised Action Segmentation. Temporal action segmentation has received an increasing interest recently. In contrast to action recognition where the goal is to classify trimmed videos <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>, temporal action segmentation requires capturing long-range dependencies to classify each frame in the input video. To achieve this goal, many approaches combined frame-wise classifiers with grammars <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b31">32]</ref> or with hidden Markov models (HMMs) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>. Despite the success of these approaches, their performance was limited and they were slow both at training and inference time. Recent approaches, therefore, utilized temporal convolutional networks to capture long-range dependencies for the temporal action segmentation task <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>. While such approaches managed to generate accurate predictions, they suffer from an over-segmentation problem. To alleviate this problem, current state-of-the-art methods follow a multi-stage architecture with dilated temporal convolutions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref>. These approaches rely on fully annotated datasets that are expensive to obtain. On the contrary, we address the temporal action segmentation task in a weakly supervised setup.</p><p>Weakly Supervised Action Segmentation. Weakly supervised action segmentation has been an active research area recently. Earlier approaches apply discriminative clustering to detect actions using movie scripts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>. However, these approaches ignored the action ordering information and only focused on distinguishing action segments from background, which is a common practice for the temporal action localization task <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Bojanowski et al. <ref type="bibr" target="#b2">[3]</ref> extended these ideas to segment actions in videos using transcripts in the form of an ordered list of actions as supervision. Recently, many researchers addressed the task by aligning the video frames and the transcripts using connectionist temporal classification <ref type="bibr" target="#b12">[13]</ref>, dynamic time warping <ref type="bibr" target="#b4">[5]</ref>, or energy-based learning <ref type="bibr" target="#b24">[25]</ref>. Other approaches generate pseudo ground truth labels for the training videos and iteratively refine them <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19]</ref>. In <ref type="bibr" target="#b34">[35]</ref>, a framewise loss function is combined with the Viterbi algorithm to generate the target labels. While these approaches have been very successful, they suffer from a slow inference time as they iterate over all the training transcripts and select the one with the highest score. Souri et al. <ref type="bibr" target="#b36">[37]</ref> addressed this issue by predicting the transcript besides the frame-wise scores at inference time. While these approaches rely on a cheap transcript supervision, their performance is much worse than fully supervised approaches. In contrast to these approaches, we propose a higher level of supervision in the form of timestamps that can be obtained with comparable effort to the transcript supervision, and yet reduces the gap to the fully supervised approaches. There is another line of research addressing the action segmentation task from setlevel supervision <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref>. These approaches opt for a weaker level of supervision at the cost of performance. In contrast to these approaches, we propose a good compromise between supervision level and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Timestamp Supervision for Recognizing Activities.</head><p>Timestamp supervision has not yet received much attention from the action recognition community. Initial attempts were inspired by the success of point supervision for semantic segmentation <ref type="bibr" target="#b1">[2]</ref>. Mettes et al. <ref type="bibr" target="#b29">[30]</ref> apply multiple instance learning for spatio-temporal action localization using points annotation on a sparse subset of frames. Ch?ron et al. <ref type="bibr" target="#b5">[6]</ref> use discriminative clustering to integrate different types of supervision for the spatio-temporal action localization task. Recently, Moltisanti et al. <ref type="bibr" target="#b30">[31]</ref> proposed a sampling distribution based on a plateau function centered around temporal timestamps annotations to train a fine-grained action classifier. This approach relies on the classifier response to sample frames around the annotated timestamps and uses them for training. The method was tested for classifying trimmed videos and also showed promising results for temporal action localization. Ma et al. <ref type="bibr" target="#b27">[28]</ref> extended the action localization setup by mining action frames and background frames for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal Action Segmentation</head><p>Temporal action segmentation is the task of predicting frame-wise action labels for a given input video. Formally, given a sequence of video frames X = [x 1 , . . . , x T ], where T is the number of frames, the goal is to predict a sequence of frame-wise action labels [a 1 , . . . , a T ]. In contrast to the fully supervised approaches, which assume that the framewise labels are given at training time, we consider a weaker level of supervision in the form of timestamps. In Section 3.1 we introduce the timestamp supervision for the temporal action segmentation task. Then, we describe the proposed framework for learning from timestamp supervision in Section 3.2. Finally, we provide the details of the loss function in Section 3.3. <ref type="figure">Figure 2</ref>. The framework of the proposed approach for training with timestamp supervision. Given the output of the segmentation model and the timestamps annotations, we generate action labels for each frame in the input video by estimating where the action labels change. A loss function is then computed between the predictions and the generated labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Timestamp Supervision</head><p>In a fully supervised setup, the frame-wise labels [a 1 , . . . , a T ] of the training videos are available. On the contrary, for timestamp supervision, only a single frame for each action segment is annotated. Given a training video X with T frames and N action segments, where N &lt;&lt; T , only N frames are annotated with labels A T S = [a t1 , . . . , a t N ], where frame t i belongs to the i-th action segment. To annotate timestamps, one can go fast forward through a video and press a button when an action occurs. This does not take more time than annotating transcripts. Whereas annotating the start and end frames of each action requires going slowly back and forth between the frames. As reported in <ref type="bibr" target="#b27">[28]</ref>, annotators need 6 times longer to annotate the start and end frame compared to annotating a single timestamp. While timestamps are much easier to obtain compared to the full annotation of the video frames, they provide much more information compared to weaker forms of supervision such as transcripts. <ref type="figure">Fig. 1</ref> illustrates the difference between timestamp supervision and full supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Action Segmentation from Timestamp Supervision</head><p>Given an action segmentation model M and a set of training videos with timestamps annotations, the goal is to train the model M to predict action labels for each frame in the input video. If the frame-wise labels are available during the training, as in the fully supervised case, then it is possible to apply a classification loss on the output of the model M for each frame in the input video. However, in timestamp supervision, only a sparse set of frames are annotated. To alleviate this problem, we propose to generate frame-wise labels for the training videos, and use them as a target for the loss function as illustrated in <ref type="figure">Fig. 2</ref>.</p><p>Detecting Action Changes. Given the timestamps annotations A T S = [a t1 , . . . , a t N ] for a video X, we want to generate frame-wise action labels? = [? 1 , . . . ,? T ] for each frame in that video such that? ti = a ti for i ? [1, N ]. As for each action segment there is an annotated frame, finding the frame-wise labels can be reduced to finding the action change between each consecutive annotated timestamps. To this end, we pass the input video X to the segmentation model M, which will be described in Section 4.2, and use the output of the penultimate layer H combined with the timestamps annotations to estimate where the action labels change between the timestamps. To generate the labels, all the frames that lie between an annotated timestamp and an estimated time of action change are assigned with the same action label as the annotated timestamp as illustrated in <ref type="figure">Fig. 3</ref>. To detect the action change between two timestamps t i and t i+1 , we find the time t bi that minimizes the following stamp-to-stamp energy function</p><formula xml:id="formula_0">t bi = arg min tt t=ti d(h t , c i ) + ti+1 t=t+1 d(h t , c i+1 ), s.t. c i = 1 t ? t i + 1t t=ti h t , c i+1 = 1 t i+1 ?t ti+1 t=t+1 h t , t i ?t &lt; t i+1 ,<label>(1)</label></formula><p>where d(., .) is the Euclidean distance, h t is the output of the penultimate layer at time t, c i is the average of the output between the first timestamp t i and the estimatet, and c i+1 is the average of the output between the estimatet and the second timestamp t i+1 . I.e., we find the time t bi that divides the frames between two timestamps into two clusters with the minimum distance between frames and the corresponding cluster center.</p><p>Forward-Backward Action Change Detection. In (1), the stamp-to-stamp energy function considers only the frames between the annotated timestamps to estimate where the actions change. Nonetheless, if we already have an estimate for t bi?1 , then we already know that frames between the estimate t bi?1 and the next timestamp t i will be assigned to action label a ti . This information can be used to estimate the time of action change for the next action segment t bi . The same argument also holds if we start estimating the boundaries in reverse order. I.e., if we already know t bi+1 , then frames between t i+1 and t bi+1 can be used to estimate t bi . We call the former estimate a forward estimate for the i-th action change, whereas the later is called the backward <ref type="figure">Figure 3</ref>. Given the timestamps annotations, we first estimate where the actions change between consecutive timestamps. To generate the frame-wise labels, all the frames that lie between an annotated timestamp and an estimated time of action change are assigned with the same action label as the annotated timestamp. estimate. The final estimate for t bi is the average of these two estimates. Formally</p><formula xml:id="formula_1">t bi = t bi,F W + t bi,BW 2 s.t. t bi,F W = arg min tt t=t b i?1 d(h t , c i ) + ti+1 t=t+1 d(h t , c i+1 ), t bi,BW = arg min tt t=ti d(h t , c i ) + t b i+1 t=t+1 d(h t , c i+1 ).</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>Recent fully supervised approaches for action segmentation use a combination of a classification loss and a smoothing loss <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b40">41]</ref>. Besides these losses, we further introduce a novel confidence loss for the timestamp supervision. In the following, we describe in detail each loss function.</p><p>Classification Loss. We use a cross entropy loss between the predicted action probabilities and the corresponding generated target label</p><formula xml:id="formula_2">L cls = 1 T t ?log(? t,? ),<label>(3)</label></formula><p>where? t,? is the predicted probability for the target label? at time t.</p><p>Smoothing Loss. As the classification loss treats each frame independently, it might result in an undesired oversegmentation effect. To encourage a smooth transition between frames and reduce over-segmentation errors, we use the truncated mean squared error <ref type="bibr" target="#b0">[1]</ref> as a smoothing loss where T is the video length, C is the number of action classes, and? t,a is the probability of action a at time t. Confidence Loss. Our approach relies on the model output to detect action changes. Nonetheless, as some frames are more informative than others, the model confidence might alternate between high and low values within the same action segment. Such behavior might result in ignoring regions with low confidence within the segments. To alleviate this problem, we apply the following loss</p><formula xml:id="formula_3">L T ?M SE = 1 T C t,a? 2 t,a ,<label>(4)</label></formula><formula xml:id="formula_4">? t,a = ? t,a : ? t,a ? ? ? : otherwise ,<label>(5)</label></formula><formula xml:id="formula_5">? t,a = | log? t,a ? log? t?1,a |,<label>(6)</label></formula><formula xml:id="formula_6">L conf = 1 T at i ?A T S ? ? ti+1 t=ti?1 ? at i ,t ? ? ,<label>(7)</label></formula><formula xml:id="formula_7">? at i ,t = max(0, log? t,at i ? log? t?1,at i ) if t ? t i max(0, log? t?1,at i ? log? t,at i ) if t &lt; t i ,<label>(8)</label></formula><p>where? t,at i is the probability of action a ti at time t, and T = 2(t N ? t 1 ) is the number of frames that contributed to the loss. For the first and last timestamps, we set t 0 = t 1 and t N +1 = t N . This loss penalizes an increase in confidence as we deviate from the annotations as illustrated in <ref type="figure" target="#fig_0">Fig. 4</ref>.</p><p>Enforcing monotonicity on the model confidence has two effects as shown in <ref type="figure">Fig. 6</ref>. First, it encourages the model to predict higher probabilities for low confident regions that are surrounded by regions with high confidence. Second, it suppresses outlier frames with high confidence that are far from the timestamps and not supported by high confident regions.</p><p>The final loss function to train the segmentation model is the sum of these three losses</p><formula xml:id="formula_8">L total = L cls + ?L T ?M SE + ?L conf ,<label>(9)</label></formula><p>where ? and ? are hyper-parameters to balance the contribution of each loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>Datasets. We evaluate our approach on four datasets: 50Salads <ref type="bibr" target="#b37">[38]</ref>, Breakfast <ref type="bibr" target="#b15">[16]</ref>, BEOID <ref type="bibr" target="#b6">[7]</ref>, and Georgia Tech Egocentric Activities (GTEA) <ref type="bibr" target="#b9">[10]</ref>.</p><p>The 50Salads dataset contains 50 videos with roughly 0.6M frames, where the frames are annotated with 17 action classes. The videos show actors preparing different kind of salads. We use five-fold cross validation for evaluation and report the average.</p><p>The Breakfast dataset contains 1712 videos with roughly 3.6M frames, where the frames are annotated with 48 action classes. All actions are related to breakfast preparation activities. We use the standard four splits for evaluation and report the average.</p><p>The BEOID dataset contains 58 videos, where the frames are annotated with 34 actions classes. For evaluation, we use the same training-testing split as in <ref type="bibr" target="#b27">[28]</ref>.</p><p>The GTEA dataset contains 28 videos with roughly 32K frames, where the frames are annotated with 11 action classes.</p><p>For evaluation, we report the average of four splits.</p><p>To generate the timestamps annotations, we randomly select one frame from each action segment in the training videos. We further evaluate our approach using human and noisy annotations in Section 4.7. Additional settings are evaluated in the supplementary material.</p><p>Metrics. We use the standard metrics for fully supervised action segmentation and report frame-wise accuracy (Acc), segmental edit distance (Edit) and segmental F1 scores at overlapping thresholds 10%, 25% and 50%.</p><p>Baselines. We implement two baselines: a Naive and a Uniform baseline. The Naive baseline computes the loss at the annotated timestamps only and does not generate framewise labels. Whereas the Uniform baseline generates the frame-wise labels by assuming that action labels change at the center frame between consecutive timestamps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use a multi-stage temporal convolutional network <ref type="bibr" target="#b0">[1]</ref> as a segmentation model M. Following <ref type="bibr" target="#b38">[39]</ref>, we use two parallel stages for the first stage with kernel size 5 and 3 and pass the sum of the outputs to next stages. We train our model for 50 epochs with Adam optimizer. To minimize the impact of initialization, only the annotated timestamps are used for the classification loss in the first 30 epochs, and the generated labels are used afterwards. The learning rate is set to 0.0005 and the batch size is 8. For the loss function, we set ? = 4, ? = 0.15 as in <ref type="bibr" target="#b0">[1]</ref> and set ? = 0.075. As input for our model, we use the same I3D <ref type="bibr" target="#b3">[4]</ref> features that were used in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the Baselines</head><p>In this section, we compare the proposed approach for action segmentation from timestamp supervision with the naive and uniform baselines. The results on the three datasets are shown in <ref type="table" target="#tab_0">Table 1</ref>. Our approach outperforms these baselines with a large margin in all the evaluation metrics. While the naive baseline achieves a good frame-wise accuracy, it suffers from a severe over-segmentation prob- lem as indicated by the low F1 and Edit scores. This is because it only uses the sparse timestamps annotations for training, which leaves a lot of ambiguity for frames without annotations. Using the uniform baseline reduces the oversegmentation by uniformly assigning a label for each frame. However, this results in inferior frame-wise accuracy as the uniform assignment generates many wrong labels. On the contrary, our approach utilizes the model predictions to generate much better target labels, which is reflected in the performance as illustrated in <ref type="figure" target="#fig_1">Fig. 5</ref>. We also compare the performance of our approach to the fully supervised setup in <ref type="table" target="#tab_0">Table 1</ref>. Our approach achieves comparable performance to the fully supervised case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Impact of the Loss Function</head><p>The loss function to train our model consists of three losses: a classification loss, a smoothing loss, and a confidence loss. <ref type="table">Table 2</ref> shows the impact of each loss on both the 50Salads and the Breakfast dataset. While either of the smoothing loss and the confidence loss gives an additional boost in performance, the best performance is achieved when both of the losses are combined with the  <ref type="table">Table 3</ref>. Impact of ? on the 50Salads dataset.</p><p>into account. On the contrary, the confidence loss forces the predicted probabilities to monotonically decrease as the distance to the timestamps increases. This encourages the model to have a high confidence for all frames within an action segment, and yet it suppresses outlier frames that are far from the annotations and not supported by regions with high confidence as illustrated in <ref type="figure">Fig. 6</ref>.</p><p>To balance the contribution of the different losses, we set the weight of the smoothing loss to 0.15 as in <ref type="bibr" target="#b0">[1]</ref>, and the weight of the confidence loss ? = 0.075. In <ref type="table">Table 3</ref>, we study the impact of ? on the performance on the 50Salads dataset. As shown in the table, good results are achieved for ? between 0.05 and 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Impact of the Energy Function for Action Change Detection</head><p>Our approach generates target labels by estimating where the action labels change using the forward-backward estimate as in <ref type="bibr" target="#b1">(2)</ref>. To analyze the impact of this estimate, we train another model that directly uses the stamp-tostamp estimate (Stamp-to-Stamp (Features)) as in <ref type="bibr" target="#b0">(1)</ref>. As shown in <ref type="table">Table 4</ref>, our approach performs better. We also tried another variant of the stamp-to-stamp energy function that maximizes the average probabilities of the action segments (Stamp-to-Stamp (Prob.)) instead of minimizing the distances to cluster centers. However, the performance is worse than the proposed energy function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Impact of the Segmentation Model M</head><p>In all experiments, we used a multi-stage temporal convolutional architecture based on <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b38">[39]</ref>. In this section we study the impact of the segmentation model on the performance. To this end, we apply the proposed training scheme on the original MS-TCN <ref type="bibr" target="#b0">[1]</ref> and the recently introduced MS-TCN++ <ref type="bibr" target="#b26">[27]</ref>. As shown in <ref type="table">Table 5</ref>, our approach (a) (b) <ref type="figure">Figure 6</ref>. Impact of the confidence loss. Forcing monotonicity encourages the model to have a high confidence for all frames within an action segment (a). It also suppresses outlier frames with high confidence (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1@{10, 25, 50}</head><p>Edit Acc  is agnostic to the segmentation model and performs well with all these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Comparison with the State-of-the-Art</head><p>In this section, we compare our approach with recent state-of-the-art approaches for timestamp supervision. To the best of our knowledge, timestamp supervision has not been studied for the temporal action segmentation task. We, therefore, compare with similar methods in the context of action recognition <ref type="bibr" target="#b30">[31]</ref> and action localization <ref type="bibr" target="#b27">[28]</ref>.</p><p>Since the approach of <ref type="bibr" target="#b30">[31]</ref> assumes the testing videos are trimmed and does not work for long untrimmed videos, we replaced their backbone network with our segmentation model for a fair comparison. To this end, we initialized the plateau functions around the timestamps annotations of the training videos and iteratively update their parameters based on the segmentation model output as in <ref type="bibr" target="#b30">[31]</ref>. Results for the 50Salads, Breakfast, and GTEA datasets are shown in  <ref type="table">Table 8</ref>. Comparison with different levels of supervision on the GTEA dataset.</p><p>Tables 6-8, respectively. Our approach outperforms <ref type="bibr" target="#b30">[31]</ref> on all datasets with a large margin of up to 13.5% frame-wise accuracy and 11.8% for the F1 score with 50% overlapping threshold on the GTEA dataset. We also compare our approach for timestamp supervision with other levels of supervision for the temporal action segmentation task. As shown in Tables 6-8, timestamp supervision outperforms weaker levels of supervision in the form of transcripts or sets with a large margin. Our approach provides a good compromise between annotation effort and performance, and further reduces the gap to fully supervised approaches.</p><p>Timestamp supervision has recently been studied for action localization in <ref type="bibr" target="#b27">[28]</ref>. In their approach, they use the model confidence to sample foreground action frames and background frames for training. To compare with <ref type="bibr" target="#b27">[28]</ref>, we use the same setup and the provided human annotations to train our model and report mean average precision at different overlapping thresholds. <ref type="table" target="#tab_4">Table 9</ref> shows the results on the GTEA and BEOID <ref type="bibr" target="#b6">[7]</ref> datasets. Our approach outperforms  Ma et al. <ref type="bibr" target="#b27">[28]</ref> with a large margin of 5.4% average mAP on GTEA and 4.3% on the BEOID dataset. In contrast to <ref type="bibr" target="#b27">[28]</ref> where only the frames with high confidence are used for training, our approach detects action changes and generates a target label for each frame in the training videos. Finally, we compare our approach with the semisupervised setup on the Breakfast dataset proposed in Kuehne et al. <ref type="bibr" target="#b18">[19]</ref>. In this setup, the training videos are annotated with the transcript of actions and a fraction of the frames as well. Compared to the timestamp supervision, this setup provides annotations for much more frames. Since the timestamps are randomly sampled from the video, there are sometimes multiple timestamps for one action and not all actions are annotated as reported in the supplementary material. As shown in <ref type="table">Table C</ref>, our approach outperforms <ref type="bibr" target="#b18">[19]</ref> with a large margin. While the approach of <ref type="bibr" target="#b18">[19]</ref> relies on an expensive Viterbi decoding during inference, our approach directly predicts the frame-wise labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed an approach to train a temporal action segmentation model using only timestamps annotations. Our approach combines the model predictions with the timestamps annotations for estimating where the action labels change. We further introduced a confidence loss that enforces monotonicity on the model confidence. The loss encourages high confidence values for all frames within an action segment and suppresses outlier frames. Results on four datasets show that models trained with timestamp supervision achieve comparable performance to the fully supervised setup. The proposed approach is model agnostic and can be applied to any segmentation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We evaluate our model on additional settings for generating the timestamps annotations. We further analyze the impact of noise on the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Frame Selection for the Timestamp Annotations</head><p>In the paper, we randomly select one frame from each action segment in the training videos. <ref type="table">Table A</ref> shows results for two additional settings. While using the center frame of each segment achieves comparable results to a random frame, the performance drops when the start frame is used. Humans, however, would not annotate the start frame since it is more ambiguous (see <ref type="figure" target="#fig_0">Fig. 4</ref> in <ref type="bibr" target="#b27">[28]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Human Annotations vs Generated Annotations</head><p>In <ref type="table">Table B</ref>, we directly compare the human annotations from <ref type="bibr" target="#b27">[28]</ref> with simulated annotations on the GTEA dataset. The results show that the performance with random sampling is very close to real annotations.  <ref type="table">Table B</ref>. Human vs. generated annotations on the GTEA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Impact of Noise</head><p>In <ref type="table" target="#tab_0">Table 10</ref> in the paper, the timestamps are randomly sampled from the videos. Thus, there are sometimes multiple timestamps for one action and not all actions are annotated. <ref type="table">Table C</ref> shows the percentage of action segments with 0, 1, or &gt;1 timestamps (TS). As shown in the table, our approach is robust to annotation errors.  <ref type="table">Table C</ref>. Percentage of actions with 0, 1, or &gt;1 timestamps (TS) using the protocol of <ref type="table" target="#tab_0">Table 10</ref>. The last row is the protocol of <ref type="table" target="#tab_3">Table 7</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>The confidence loss penalizes increases in the model confidence for label at i as we move away from the annotated timestamp ti.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results on (a) 50Salads, (b) Breakfast, and (c) GTEA datasets. As the naive baseline only trains on the sparse annotations, it suffers from an over-segmentation problem. While the uniform baseline reduces this problem by uniformly assigning labels to the frames, the durations of the predicted segments are not accurate and the predictions tend towards a uniform segmentation of the videos. On the contrary, our approach generates better predictions by utilizing the model output to detect where the action labels change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with the baselines on the three datasets. L cls + ?L T ?M SE + ?L conf</figDesc><table><row><cell></cell><cell cols="3">F1@{10, 25, 50}</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>50Salads</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Naive</cell><cell>47.9</cell><cell>43.3</cell><cell cols="2">34.0 37.2 69.6</cell></row><row><cell>Uniform</cell><cell>62.9</cell><cell>58.2</cell><cell cols="2">42.3 60.4 63.4</cell></row><row><cell>Ours</cell><cell>73.9</cell><cell>70.9</cell><cell cols="2">60.1 66.8 75.6</cell></row><row><cell>Full Supervision</cell><cell>70.8</cell><cell>67.7</cell><cell cols="2">58.6 63.8 77.8</cell></row><row><cell>Breakfast</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Naive</cell><cell>34.1</cell><cell>29.1</cell><cell cols="2">20.1 37.4 56.8</cell></row><row><cell>Uniform</cell><cell>66.2</cell><cell>56.3</cell><cell cols="2">36.4 68.1 51.0</cell></row><row><cell>Ours</cell><cell>70.5</cell><cell>63.6</cell><cell cols="2">47.4 69.9 64.1</cell></row><row><cell>Full Supervision</cell><cell>69.9</cell><cell>64.2</cell><cell cols="2">51.5 69.4 68.0</cell></row><row><cell>GTEA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Naive</cell><cell>59.7</cell><cell>55.3</cell><cell cols="2">39.6 51.1 56.5</cell></row><row><cell>Uniform</cell><cell>78.9</cell><cell>72.5</cell><cell cols="2">50.9 73.1 56.5</cell></row><row><cell>Ours</cell><cell>78.9</cell><cell>73.0</cell><cell cols="2">55.4 72.3 66.4</cell></row><row><cell>Full Supervision</cell><cell>85.1</cell><cell>82.7</cell><cell cols="2">69.6 79.6 76.1</cell></row><row><cell cols="5">classification loss with a frame-wise accuracy improvement</cell></row><row><cell cols="5">of 2.8% and 3.9% on 50Salads and the Breakfast dataset</cell></row><row><cell cols="5">respectively, and roughly 10% on the F1 score at 50% over-</cell></row><row><cell>lapping threshold.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">While the smoothing loss forces a smooth transition be-</cell></row><row><cell cols="5">tween consecutive frames, it does not take the annotations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Impact of the energy function for action change detection on the 50Salads and Breakfast datasets. Impact of the segmentation model M on the 50Salads and GTEA datasets.</figDesc><table><row><cell>Dataset</cell><cell>Seg. Model M</cell><cell>F1@{10, 25, 50}</cell><cell>Edit Acc</cell></row><row><cell cols="2">50Salads MS-TCN [1]</cell><cell cols="2">71.7 68.7 57.0 64.0 74.7</cell></row><row><cell></cell><cell cols="3">MS-TCN++ [27] 75.0 71.1 55.8 67.2 72.9</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">73.9 70.9 60.1 66.8 75.6</cell></row><row><cell>GTEA</cell><cell>MS-TCN [1]</cell><cell cols="2">79.8 73.3 47.7 76.3 59.3</cell></row><row><cell></cell><cell cols="3">MS-TCN++ [27] 78.3 72.2 49.1 74.5 62.2</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">78.9 73.0 55.4 72.3 66.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>Timestamps Seg. Model M + plateau<ref type="bibr" target="#b30">[31]</ref> 71.2 68.2 56.1 62.6 73.<ref type="bibr" target="#b8">9</ref> Ours 73.9 70.9 60.1 66.8 75.<ref type="bibr" target="#b5">6</ref> Comparison with different levels of supervision on the Breakfast dataset.</figDesc><table><row><cell cols="2">Supervision Method</cell><cell cols="3">F1@{10, 25, 50}</cell><cell cols="2">Edit Acc</cell></row><row><cell>Full</cell><cell>MS-TCN [1]</cell><cell cols="5">76.3 74.0 64.5 67.9 80.7</cell></row><row><cell></cell><cell>MS-TCN++ [27]</cell><cell cols="5">80.7 78.5 70.1 74.3 83.7</cell></row><row><cell></cell><cell>BCN [41]</cell><cell cols="5">82.3 81.3 74.0 74.3 84.4</cell></row><row><cell></cell><cell>ASRF [15]</cell><cell cols="5">84.9 83.5 77.3 79.3 84.5</cell></row><row><cell cols="2">Transcripts CDFL [25]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.7</cell></row><row><cell></cell><cell>NN-Viterbi [35]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.4</cell></row><row><cell></cell><cell>HMM-RNN [33]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.5</cell></row><row><cell cols="7">Table 6. Comparison with different levels of supervision on the</cell></row><row><cell cols="2">50Salads dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Supervision Method</cell><cell cols="3">F1@{10, 25, 50}</cell><cell cols="2">Edit Acc</cell></row><row><cell>Full</cell><cell>MS-TCN [1]</cell><cell cols="5">52.6 48.1 37.9 61.7 66.3</cell></row><row><cell></cell><cell>MS-TCN++ [27]</cell><cell cols="5">64.1 58.6 45.9 65.6 67.6</cell></row><row><cell></cell><cell>BCN [41]</cell><cell cols="5">68.7 65.5 55.0 66.2 70.4</cell></row><row><cell></cell><cell>ASRF [15]</cell><cell cols="5">74.3 68.9 56.1 72.4 67.6</cell></row><row><cell cols="7">Timestamps Seg. Model M + plateau [31] 65.5 59.1 43.2 65.9 63.5</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">70.5 63.6 47.4 69.9 64.1</cell></row><row><cell cols="2">Transcripts CDFL [25]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.2</cell></row><row><cell></cell><cell>MuCon [37]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.1</cell></row><row><cell></cell><cell>D 3 TW [5]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.7</cell></row><row><cell></cell><cell>NN-Viterbi [35]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.0</cell></row><row><cell></cell><cell>TCFPN [8]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.4</cell></row><row><cell></cell><cell>HMM-RNN [33]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>33.3</cell></row><row><cell></cell><cell>ECTC [13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.7</cell></row><row><cell>Sets</cell><cell>SCT [11]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>30.4</cell></row><row><cell></cell><cell>SCV [26]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>30.2</cell></row><row><cell></cell><cell>Action Sets [34]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>23.3</cell></row><row><cell cols="2">Supervision Method</cell><cell cols="3">F1@{10, 25, 50}</cell><cell cols="2">Edit Acc</cell></row><row><cell>Full</cell><cell>MS-TCN [1]</cell><cell cols="5">85.8 83.4 69.8 79.0 76.3</cell></row><row><cell></cell><cell>MS-TCN++ [27]</cell><cell cols="5">88.8 85.7 76.0 83.5 80.1</cell></row><row><cell></cell><cell>BCN [41]</cell><cell cols="5">88.5 87.1 77.3 84.4 79.8</cell></row><row><cell></cell><cell>ASRF [15]</cell><cell cols="5">89.4 87.8 79.8 83.7 77.3</cell></row><row><cell cols="7">Timestamps Seg. Model M + plateau [31] 74.8 68.0 43.6 72.3 52.9</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">78.9 73.0 55.4 72.3 66.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 9 .</head><label>9</label><figDesc>Comparison with SF-Net<ref type="bibr" target="#b27">[28]</ref> for action localization with timestamp supervision on the GTEA and BEOID datasets.</figDesc><table><row><cell>mAP@IoU</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>Avg</cell></row><row><cell>GTEA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SF-Net [28]</cell><cell>58.0</cell><cell>37.9</cell><cell>19.3</cell><cell>11.9</cell><cell>31.0</cell></row><row><cell>Ours</cell><cell>60.2</cell><cell>44.7</cell><cell>28.8</cell><cell>12.2</cell><cell>36.4</cell></row><row><cell>BEOID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SF-Net [28]</cell><cell>62.9</cell><cell>40.6</cell><cell>16.7</cell><cell>3.5</cell><cell>30.1</cell></row><row><cell>Ours</cell><cell>71.5</cell><cell>40.3</cell><cell>20.3</cell><cell>5.5</cell><cell>34.4</cell></row><row><cell>Fraction</cell><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>Acc</cell></row><row><cell>0.1</cell><cell cols="3">HMM-RNN [19]</cell><cell></cell><cell>60.9</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell>68.4</cell></row><row><cell>0.01</cell><cell cols="3">HMM-RNN [19]</cell><cell></cell><cell>58.8</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell>67.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 10 .</head><label>10</label><figDesc>Comparison with Kuehne et al. [19] on the Breakfast dataset with semi-supervised setup.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>).Table A. Using start, center, or random frame as timestamp for each action on the Breakfast dataset.</figDesc><table><row><cell>Timestamps</cell><cell cols="3">F1@{10, 25, 50}</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>Start frame</cell><cell>65.5</cell><cell>52.2</cell><cell cols="2">28.0 70.4 51.2</cell></row><row><cell>Center frame</cell><cell>70.8</cell><cell>63.5</cell><cell cols="2">45.4 71.3 61.3</cell></row><row><cell>Random</cell><cell>70.5</cell><cell>63.6</cell><cell cols="2">47.4 69.9 64.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code for our model and the timestamps annotations are publicly available at https://github.com/ZheLi2020/ TimestampActionSeg.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: The work has been funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -GA 1927/4-2 (FOR 2535 Anticipating Human Behavior) and the ERC Starting Grant ARCA (677650).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MS-TCN: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3575" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="628" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">D 3 TW: Discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3546" to="3555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A flexible model for training action localization with varying levels of supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilhem</forename><surname>Ch?ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="942" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">You-do, I-learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teesid</forename><surname>Leelasawassuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osian</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Calway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1491" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SCT: Set constrained temporal transformer for set supervised action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving action segmentation via graph-based temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14024" to="14034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Alleviating over-segmentation errors by detecting action boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchi</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seito</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimitsu</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2322" to="2331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An end-toend generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Hybrid RNN-HMM approach for weakly supervised temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="765" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal CNNs for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11320" to="11327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly-supervised temporal action localization by uncertainty modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6742" to="6751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised energy-based learning for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6243" to="6251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Set-constrained viterbi for set-supervised action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10820" to="10829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MS-TCN++: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><forename type="middle">Abu</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SF-Net: Single-frame supervision for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxin</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourab</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning motion in feature space: Locally-consistent deformable convolution networks for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Khoi-Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Mac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><forename type="middle">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="437" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition from single timestamp supervision in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9915" to="9924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with RNN based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action sets: Weakly supervised action segmentation without ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5987" to="5996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">NeuralNetwork-Viterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahsan</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7386" to="7395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fast weakly supervised action segmentation using mutual consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Souri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianpiero</forename><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03116</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Event detection in coarsely annotated sports videos via parallel multi-receptive field 1D convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnaz</forename><surname>Fani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">F</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2641" to="2648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Boundary-aware cascade networks for temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziteng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
