<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Keypoint-based Global Association Network for Lane Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinchao</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Keypoint-based Global Association Network for Lane Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lane detection is a challenging task that requires predicting complex topology shapes of lane lines and distinguishing different types of lanes simultaneously. Earlier works follow a top-down roadmap to regress predefined anchors into various shapes of lane lines, which lacks enough flexibility to fit complex shapes of lanes due to the fixed anchor shapes. Lately, some works propose to formulate lane detection as a keypoint estimation problem to describe the shapes of lane lines more flexibly and gradually group adjacent keypoints belonging to the same lane line in a pointby-point manner, which is inefficient and time-consuming during postprocessing. In this paper, we propose a Global Association Network (GANet) to formulate the lane detection problem from a new perspective, where each keypoint is directly regressed to the starting point of the lane line instead of point-by-point extension. Concretely, the association of keypoints to their belonged lane line is conducted by predicting their offsets to the corresponding starting points of lanes globally without dependence on each other, which could be done in parallel to greatly improve efficiency. In addition, we further propose a Lane-aware Feature Aggregator (LFA), which adaptively captures the local correlations between adjacent keypoints to supplement local information to the global association. Extensive experiments on two popular lane detection benchmarks show that our method outperforms previous methods with F1 score of 79.63% on CULane and 97.71% on Tusimple dataset with high FPS. The code will be released at https://github.com/Wolfwjs/GANet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Autonomous driving <ref type="bibr" target="#b9">[10]</ref> has drawn remarkable attention of researchers from both academia and industry. In  <ref type="figure">Figure 1</ref>. (a) Anchor-based methods, which regress the predefined anchors into the shape of lanes. (b) Keypoint-based methods, which predict offsets between keypoint to its neighbourhood to group them one-by-one. (c) Illustration of our GANet, which directly regresses each keypoint to its belonged lane by predicting offset between each keypoint and the starting point of its corresponding lane line. (d) Illustration of our LFA module, which correlates each keypoint with its adjacent points for local information supplement.</p><p>order to ensure the safety of the car during driving, the autonomous system needs to keep the car moving along the lane lines on the road, requiring accurate perception of the lane lines. Thus, lane detection plays an important role in the autonomous driving system, especially in Advanced Driver Assistance System (ADAS). Given a front-viewed image taken by a camera mounted on the vehicle, lane detection aims to produce the accurate shape of each lane line on the road. Due to the slender shapes of lane lines and the need for instance-level discrimination, it is crucial to formulate lane detection task appropriately. Inspired by the anchor-based object detection methods <ref type="bibr" target="#b21">[22]</ref>, some works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref> follow a top-down design as illustrated in <ref type="figure">Figure 1a</ref>. Similar to object detection, a group of straight lines with various orientations are defined as anchors. Points on anchors are regressed to lane lines by predicting the offsets between anchor points and lane points. Afterward, Non-Maximum Suppression (NMS) is applied to select lane lines with the highest confidence. Although this kind of method is efficient in lane discrimination, it is inflexible because of the predefined anchor shapes. The strong shape prior limits the ability of describing various lane shapes, resulting in sub-optimal performances of these methods.</p><p>To describe complex shapes of lane lines flexibly, Qu et al. <ref type="bibr" target="#b20">[21]</ref> propose to formulate lane detection as a keypoint estimation and association problem, which takes a bottom-up design as illustrated in <ref type="figure">Figure 1b</ref>. Concretely, lanes are represented with a group of ordered key points evenly sampled in a sparse manner. Each key point is associated with its neighbours by estimating the spatial offsets between them. In this way, key points belonging to the same lane are integrated into a continuous curve iteratively. Though keypointbased methods are flexible on the shape of lane lines, it is inefficient and time-consuming to associate only one keypoint to its belonged lane line at each step. Besides, the point-by-point extension of keypoints is easy to cause error accumulation due to the lack of global view. Once a particular keypoint is wrongly associated, estimation of the rest part of the lane line will fail.</p><p>To overcome the above limitations, we formulate the lane detection problem from a new keypoint-based perspective where each keypoint is directly regressed to its belonged lane, based on which a novel pipeline named Global Association Network (GANet) is proposed. As illustrated in <ref type="figure">Figure 1c</ref>, each lane line is represented uniquely with its starting point, which is easy to determine without ambiguity. To associate a keypoint properly, we estimate the offset from the keypoint to its corresponding starting point. Keypoints whose approximated starting points fall into the same neighborhood area will be assigned to the same lane line instance, thus separating keypoints into different groups. Different from previous keypoint-based method <ref type="bibr" target="#b20">[21]</ref>, our assignment of keypoints to their belonged lanes is independent of each other and makes the parallel implementation feasible, which greatly improves the efficiency of postprocessing. Besides, the keypoint association is more robust to the accumulated single-point errors since each keypoint owns a global view.</p><p>Although keypoints belonging to the same lane line are integrated during post-processing, it is important to ensure the correlations between adjacent points in order to obtain a continuous curve. To this end, we develop a local information aggregation module named Lane-aware Feature Aggregator (LFA) to enhance the correlations between adjacent keypoints. To adapt to the slender and long shapes of lanes, we modify the sampling positions of the standard 2D deformable convolution <ref type="bibr" target="#b2">[3]</ref> by predicting offsets to adjacent points to sample within a local area on the lane each time. In this way, features of each keypoint are aggregated with other adjacent points, thus acquiring more representative features. We further add an auxiliary loss to facilitate estimating the offset predicted on each key point. Our LFA module complements the global association process to enable both local and global views, which is essential for dense labeling tasks like lane detection.</p><p>Our contributions are summarized as follows:</p><p>? We propose a novel Global Association Network (GANet) to formulate lane detection from a new keypoint-based perspective which directly regress each keypoint to its belonged lane. To the best of our knowledge, we are the first to regress keypoints in a global manner, which is more efficient than local regression.</p><p>? We develop a local information aggregation module named as Lane-aware Feature Aggregator (LFA) to enhance correlations among adjacent keypoints to supplement local information.</p><p>? Our proposed GANet achieves state-of-the-art performances on two popular benchmarks of lane detection with faster speed, which shows a superior performance-efficiency trade-off and great potential of our global association formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Lane Detection Methods</head><p>Lane detection aims at obtaining the accurate shape of lane lines as well as distinguishing between them. According to the way of lane modeling, current deep-learningbased methods can be roughly divided into several categories. We will elaborate on these methods separately in this section.</p><p>Segmentation-based methods. Segmentation-based methods model lane line detection as a per-pixel classification problem, with each pixel classified as either lane area or background <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. To distinguish different lane lines, SCNN <ref type="bibr" target="#b17">[18]</ref> treats different lane lines as different categories and thus lane detection is transformed into a multi-class segmentation task. A slice-by-slice CNN structure is also proposed to enable message passing across rows and columns. In order to meet the real-time requirement in practice, ENet-SAD [6] applies a self-attention distillation mechanism for contextual aggregation so as to allow the use of a lightweight backbone. LaneNet <ref type="bibr" target="#b15">[16]</ref> adopts a different way of lane representation by casting lane detection as an instance segmentation problem. A binary segmentation branch and an embedding branch are included to disentangle the segmented results into lane instances. Different from LaneNet, our method use offsets instead of embedding features to cluster each lane lines, which is more efficient and time-saving.</p><p>Detection-based methods. This kind of method usually follows a top-down manner to predict lane lines. Among them, anchor-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref> design line-like anchors and regress the offsets between sampled points and predefined anchor points. Non-Maximum Suppression (NMS) is then applied to select lane lines with the highest confidence. LineCNN <ref type="bibr" target="#b9">[10]</ref> uses straight rays emitted from the image boundaries with certain orientations as a group of anchors. Curve-NAS <ref type="bibr" target="#b27">[28]</ref> defines anchors as vertical lines and further adopts neural architecture search (NAS) to search for better backbones. LaneATT <ref type="bibr" target="#b24">[25]</ref> proposes an anchor-based pooling method and an attention mechanism to aggregate more global information. Another kind of methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref> formulates lane detection as a row-wise classification problem. For each row, the model predicts the locations that possibly contain lane lines.</p><p>Keypoint-based methods. Inspired by human pose estimation, some works treat lane detection as a keypoint estimation and association problem. PINet <ref type="bibr" target="#b8">[9]</ref> uses a stacked hourglass network <ref type="bibr" target="#b16">[17]</ref> to predict keypoint positions and feature embedding. Different lane instances are clustered based on the similarity between feature embeddings. FOLOLane <ref type="bibr" target="#b20">[21]</ref> produces a pixel-wise heatmap with the same resolution as input to obtain points on lanes. A local geometry construction manner is also developed to associate keypoints belonging to the same lane instance. Our GANet adopts a more efficient postprocessing approach, which needs neither feature embeddings nor local association to cluster or reconstruct the whole lane. Each keypoint finds its corresponding lane by adding its coordinate with the offset to the lane line start points in a parallel manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deformable Modeling</head><p>Traditional CNNs are inherently limited to model irregular structures due to the fixed grid-like sampling ranges of convolution operations. To overcome this limitation, Dai et al. <ref type="bibr" target="#b2">[3]</ref> proposes deformable convolution to adaptively aggregate information within local areas. Compared with standard convolutions, 2D offsets obtained by an extra convolution are added at each spatial location during sampling to enable free-form deformation of the sampling grid. Through the learned offsets, the receptive field and the sampling location of convolutions are adaptively adjusted according to the random scale and shape of objects. The spirit of deformable modeling has been applied in many tasks such as object detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>, object tracking <ref type="bibr" target="#b32">[33]</ref> and video comprehension <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>. RepPoints <ref type="bibr" target="#b29">[30]</ref> models an object as a set of points and predicts the offsets of these points to the object center with deformable convolutions. This deformable object representation provides accurate geometric localization for object detection as well as adaptive semantic feature extraction. Ying et al. <ref type="bibr" target="#b30">[31]</ref> proposes deformable 3D convolution to explore spatio-temporal in-formation and realize adaptive motion comprehension for video super-resolution. Different from these methods, our LFA module adapts to the long structure of lane lines and restricts the range of feature aggregation to adjacent points on each lane with lane-aware deformable convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The overall architecture of our proposed Global Association Network (GANet) is illustrated in <ref type="figure">Figure 2</ref>. Given a front-viewed image as input, a CNN backbone together with an FPN <ref type="bibr" target="#b11">[12]</ref> neck is adopted to extract multi-level visual representations of the input images. For better feature learning, a self-attention layer <ref type="bibr" target="#b26">[27]</ref> is further inserted between the backbone and the neck to obtain rich context information. In the decoder, a keypoint head and an offset head are exploited to generate confidence map and offset map respectively. Both heads are composed of fully convolutional layers. We further devise a Lane-aware Feature Aggregator module before keypoint head to enhance the local correlations between adjacent keypoints, which facilitates to produce continuous lane lines. For each lane instance, we first obtain its starting point as cluster centroid by selecting points with value less than 1 over the offset map. Afterward, keypoints belonging to the same lane are clustered around the sampled starting point with the combination of the confidence map and offset map to construct the complete lane line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Global Keypoint Association</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Keypoint Estimation</head><p>Given an input image I ? R H?W ?3 , the goal of our GANet is to predict a collection of lanes L = {l 1 , l 2 , ..., l N }, where N is the total number of lanes, with each lane line being denoted with K sampled keypoints as:</p><formula xml:id="formula_0">l i = {p 1 i , p 2 i , ..., p K i } N i=1 ,<label>(1)</label></formula><p>where p j i = (x j i , y j i ) denotes the coordinate of the j-th keypoint on the i-th lane. To estimate all the keypoints, we develop a keypoint head to produce a confidence map</p><formula xml:id="formula_1">Y ? R H r ? W r ,</formula><p>where r is the output stride. The confidence map represents the probability of each location being a keypoint on the lane. As shown in <ref type="figure">Figure 2</ref>(a), the brighter location indicates a higher probability.</p><p>During the training phase, we sample K keypoints on each lane line as ground truth keypoints and then splat them all onto a confidence map</p><formula xml:id="formula_2">Y ? R H r ? W r using a non- normalized Gaussian kernel Y yx = exp(? (x?x) 2 +(y??) 2 2? 2 ),</formula><p>wherex and? denote the coordinate of each keypoint and the standard deviation ? depends on the scale of input. If there is overlap between two Gaussian maps, we take the element-wise maximum between them.  <ref type="figure">Figure 2</ref>. The overall architecture of GANet. Given a front-viewed image as input, a CNN backbone followed by a Self-Attention layer (SA) and an FPN neck are used to extract multi-scale visual features. In the decoder, a keypoint head and an offset head are used to generate confidence map and offset map respectively, which are then combined to cluster keypoints into several groups, with each group indicating a lane line instance. Our LFA module is applied before the keypoint head to better capture local context over lane lines for keypoint estimation.</p><p>We adopt penalty-reduced focal loss <ref type="bibr" target="#b12">[13]</ref> to deal with the imbalance between keypoint regions and non-keypoint regions as follows: Due to the output stride r, the point (x j i , y j i ) of the input image is mapped to the location (? x j i r ?, ? y j i r ?), which can cause performance degradation. To address this quantization error, we additionally predict a compensation map? yx and apply L1 loss to keypoint locations only:</p><formula xml:id="formula_3">Lpoint = ?1 H ? ? W ? yx (1 ??yx) ? log(?yx)) Yyx = 1 (1 ? Yyx) ?? ? yx log(1 ??yx)) otherwise,<label>(2)</label></formula><formula xml:id="formula_4">L quant = 1 H ? ? W ? yx ? yx ? ? yx ,<label>(3)</label></formula><p>where ? yx = (</p><formula xml:id="formula_5">x j i r ? ? x j i r ?, y j i r ? ? y j i r ?)</formula><p>denotes the ground truth of quantization compensation map. This part is not shown in <ref type="figure">Figure 2</ref> for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Starting Point Regression</head><p>To distinguish different lane lines, we propose to use the starting point to represent each lane instance uniquely due to its stability and largest margins between each other. Instead of regressing the absolute coordinate (sx i , sy i ) of the starting point directly, we regress the offset from each keypoint to it, which can be defined as:</p><formula xml:id="formula_6">(?x j i , ?y j i ) = (sx i , sy i ) ? (x j i , y j i ),<label>(4)</label></formula><p>Thus, we can generate the ground truth offset map O yx with the shape of H r ? W r ? C. In particular, the subscript yx denotes the value on location (x j i , y j i ) which is equal to (?x j i , ?y j i ) while other locations have zero values. C = 2 contains the x-direction and y-direction offsets respectively.</p><p>In order to estimate the offset map? yx , we introduce an offset head as shown in <ref type="figure">Figure 2</ref>. Similarly, L1 loss is used to constrain the offset map as follows:</p><formula xml:id="formula_7">L of f set = 1 H ? ? W ? yx ? yx ? O yx ,<label>(5)</label></formula><p>The supervision applies only on keypoint locations and the rest locations are ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Lane Construction</head><p>The pipeline of lane construction is presented in <ref type="figure" target="#fig_1">Figure 3</ref>, which includes obtaining the locations of all the possible lane points and then grouping them into different lane instances. We first apply a 1 ? 3 max pooling layer on the keypoint confidence map? to select points of maximum responses within a horizontal local region as valid keypoints, which is shown in <ref type="figure" target="#fig_1">Figure 3</ref>(a). Then, we group them to describe each lane as an ordered list of keypoints as follows:</p><formula xml:id="formula_8">l = {(sx, sy), (x 2 , y 2 ), (x 3 , y 3 ), ..., (x K , y K )},<label>(6)</label></formula><p>where (sx, sy) denotes the starting point of the lane and (x j , y j ), j ? [2, K] are the subsequent keypoints.</p><p>To obtain the starting point of each lane, we select keypoints whose values are less than 1 on the offset map as the candidate starting points. Since there might be multiple keypoints matching the above criteria within the same local region, the geometric center point of the region is chosen to ensure uniqueness. By this means, instances of all the lanes are preliminarily determined with their starting points.</p><p>Afterward, we associate the rest keypoints to their belonged lanes according to the estimated offsets between keypoints and corresponding starting points, which is shown in <ref type="figure" target="#fig_1">Figure 3</ref> (sx ? , sy ? ) = (x, y) + (?x, ?y),</p><p>where (x, y) is the coordinate of the observed keypoint and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Lane-aware Feature Aggregator</head><p>Traditional 2D convolutions sample features within a fixed grid-like region, which is not suitable for handling the slender shapes of lane lines. Inspired by Dai et al. <ref type="bibr" target="#b2">[3]</ref>, we propose a Lane-aware Feature Aggregator (LFA) module to adaptively gather information from adjacent points on the lanes, so as to enhance the local feature representation of each keypoint. The illustration of our LFA module is shown in <ref type="figure">Figure 4</ref>. Take a specific keypoint as an example, we first use a convolution layer to predict the offset between it and its surrounded M keypoints on the same lane as follows:</p><formula xml:id="formula_10">?P i = ?(F(p i )),<label>(8)</label></formula><p>where p i denotes the coordinate of the i-th keypoint, F(p i ) denotes the feature representation of the i-th keypoint and ?P i = {?p m i |m = 1, ..., M } ? R 2M denotes the predicted offsets. Afterwards, features of adjacent points are Local offset map Offsets <ref type="figure">Figure 4</ref>. Illustration of LFA module. The red dot denotes the observed keypoint. We first predict offsets between the red dot and its adjacent keypoints (in blue) and then gather features of these keypoints to enhance the context of the red keypoint.</p><p>integrated with a deformable convolution to aggregate context of the i-th keypoint as:</p><formula xml:id="formula_11">F(p i ) = M m=1 w m ? F(p i + ?p m i ),<label>(9)</label></formula><p>where w m , m = 1, ..., M is the weight of the convolution and (?) means multiplication.</p><p>To enhance the ability of LFA for learning the local shapes of lane lines, we further introduce an auxiliary loss to supervise the offsets ?P i . We denote the ground truth of offsets between the i-th keypoint and the keypoints on the corresponding lane line as ?G i = {?g k i |k = 1, ..., K}, which is calculated with ?g k i = g k i ? p i , where g k i is the ground-truth coordinate of the k-th keypoint on the same lane line with the i-th keypoint.</p><p>As is shown in <ref type="figure" target="#fig_4">Figure 5</ref>, a matching need to be established between ?p i and ?g i . We search for an assignment ? with the lowest matching cost:</p><formula xml:id="formula_12">? = arg min ? M m L match (?p m i , ?g i ?(m) ),<label>(10)</label></formula><p>where L match = L 2 (?p m i , ?g i ?(m) ). Following prior works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>, the Hungarian algorithm is adopted to efficiently compute the optimal assignment. SmoothL1 loss is then applied to supervise the prediction of adjacent keypoints:</p><formula xml:id="formula_13">L aux = 1 KN M KN i=1 M m=1 SmoothL1(?p m i , ?g i? (m) ),<label>(11)</label></formula><p>where K denotes the number of keypoints on each lane line, N denotes the number of lane lines and M denotes the number of sampled adjacent keypoints.</p><p>The total loss function is the combination of different losses with corresponding coefficients:</p><formula xml:id="formula_14">L total = ? point L point + ? quant L quant +? of f set L of f set + ? aux L aux .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the experimental setting of our method. The next subsection discusses the results for each dataset. Ablation experiments for each module is presented in the last subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets and Evaluation Metrics</head><p>We conduct experiments on two popular lane detection benchmarks including CULane <ref type="bibr" target="#b17">[18]</ref> and TuSimple <ref type="bibr" target="#b25">[26]</ref>.</p><p>CULane: CULane dataset contains 88, 880 training images and 34, 680 testing images, including both urban and highway scenes. The test images are classified as 9 different scenarios. F1 measure is the only metric for evaluation, which is based on IoU. A predicted lane whose IoU is greater than 0.5 is judged as true positive (TP), otherwise false positive (FP) or false negative (FN). F1 measure is defined as the harmonic average of precision and recall.</p><p>TuSimple: TuSimple is a real highway dataset which consists of 3, 626 images for training and 2, 782 images for testing. The main evaluation metric of TuSimple dataset is accuracy, which is formulated as follows:</p><formula xml:id="formula_15">accuracy = clip C clip clip S clip</formula><p>where C clip is the number of points correctly predicted by the model and S clip is the total number of points in the clip (or image). A predicted point is considered correct only if it is within 20 pixels to the ground truth point. The predicted lane with accuracy greater than 85% is considered as a true positive. We also report the F1 score in the following experiments. <ref type="table">GANet-S  resnet-18  3  8  GANet-M  resnet-34  3  8  GANet-L  resnet-101  4  4   Table 1</ref>. Details of different versions of GANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model version Backbone FPN layers Output downscale</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Implementation Details</head><p>We choose ResNet-18, ResNet-34 and ResNet-101 <ref type="bibr" target="#b4">[5]</ref> as the backbones to form three different versions of GANet, which are referred as GANet-S, GANet-M and GANet-L. The detail of each version is shown in <ref type="table">Table 1</ref>. We first resize the input images to 800 ? 320 during the training and testing phases. The number of sampled points in LFA is set as M = 9. The loss weights are set as ? point = 1.0, ? quant = 1.0, ? of f set = 0.5, ? aux = 1.0. The hyperparameters ? and ? in Equation 2 are set as 2 and 4 respectively. For optimization, we used Adam optimizer and poly learning rate decay with an initial learning rate of 0.001. We train 300 and 40 epochs for Tusimple and CULane respectively with a batchsize of 32 per GPU. Data augmentation is applied to the training phase, including random scaling, cropping, horizontal flipping, random rotation, and color jittering. In the test phase, we set the threshold of keypoints as 0.4 and ? dis for keypoint association as 4. Training and testing are both performed on Tesla-V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Results on CULane</head><p>The results on CULane test set are shown in <ref type="table">Table 2</ref>. Our GANet-L achieves the state-of-the-art result on CULane dataset with 79.63% F1 score and 63 FPS, which exceeds models of similar size, like LaneATT-ResNet122, with large margins on both performance and speed. Compared with another keypoint-based method, FOLOLane-ERF <ref type="bibr" target="#b20">[21]</ref>, our GANet-S achieves a comparable performance of 78.79% F1 score but runs 3.8 times faster, which shows a superior trade-off between performance and efficiency and demonstrate the speed advantage of our global association formulation. Furthermore, our methods achieve the highest F1 score in six scenarios, especially in Curve scenario. Our GANet-L achieves 77.37% in this scenario and outperforms previous state-of-the-art method, ERF-E2E <ref type="bibr" target="#b31">[32]</ref>, with more than 5%, indicating the superiority of our method in describing complex lane line shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results on TuSimple</head><p>The comparison results on TuSimple test set are shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Ablation Study</head><p>To explore the properties of our proposed LFA module, we conduct an ablation study on the CULane dataset. All the following experiments are based on the small version of GANet. Results are shown in <ref type="table">Table 4</ref>. The first row shows the baseline method without our LFA module. In the second row, the LFA module is integrated into GANet without auxiliary loss. The last row shows the result of our whole GANet. From the first two rows we can observe that LFA module without auxiliary loss is effective for lane line detection, which is due to flexible integration of context. Comparing the last two rows, we can also find that the auxiliary loss is vital to the LFA module, which can guide LFA to focus on the key information on the lane line. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative results</head><p>We visualize the qualitative results w/wo LFA in <ref type="figure" target="#fig_5">Figure 6</ref>. The 2-nd and 4-th columns are the visualization of confidence map without and with LFA correspondingly. As shown in the results from the first row, the LFA module makes correct prediction even with vehicle occlusion due to the fact that predicted lane points enhance each other. From the results in second and third rows, it can also be concluded that the LFA module is able to suppress background noise which may be introduced by global attention.</p><p>To intuitively investigate the properties of the LFA module, we visualize the predicted feature aggregation points in <ref type="figure" target="#fig_6">Figure 7</ref>. The first row shows a common straight lane case. With the addition of auxiliary losses, the LFA module can predict the aggregation points around the lane line. Meanwhile, the predicted aggregation points are irregular without the auxiliary loss. The last two rows show the aggregation points in the curved lane case. It is demonstrated that the LFA module is robust in its understanding of the local structures of the lane lines. This property contributes to the enhancement of lane line features and the suppression of background noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>In this paper, we propose a Global Association Network (GANet) to formulate the lane detection problem from a new perspective, where each keypoint is directly regressed to the starting point of the lane line instead of point-bypoint extension. The association of keypoints to their belonged lane line is conducted by predicting their offsets to the corresponding starting points of lanes globally, which greatly improves the effectiveness. We further propose a Lane-aware Feature Aggregator (LFA) to adaptively capture the local correlations between adjacent keypoints to supplement local information. Experimental results show our GANet outperforms previous methods with higher speed.</p><p>Limitation. The limitation of our method is that the off- sets to the starting point may become difficult to regress when the output stride is set to 1 due to the large absolute value of the offsets. In the future, we hope to address this problem by regressing the offsets with multiple levels to alleviate the regression difficulty.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Equal contribution ? Corresponding author (tzzhang@ustc.edu.cn)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of lane construction. (a) Valid keypoints are selected from the confidence map. (x, y) is taken as an example. (b) Starting point (sx, sy) (blue dot) is sampled first. The rest keypoints point to the starting point with the predicted offset (?x, ?y) and estimate the coordinate of the starting points as (sx ? , sy ? ) = (x, y) + (?x, ?y) (hollow dots). (c) Keypoints that point to the neighbourhood of starting point (sx, sy) are grouped as a whole lane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b). Each keypoint estimates the coordinate of the lane line starting point as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(?x, ?y) = O yx denotes the corresponding offset obtained in Section 3.1.2. The keypoint (x, y) is associated to the ith lane only if the distance between (sx ? , sy ? ) and (sx, sy) is less than a predefined threshold ? dis . As shown in Figure 3(c), keypoints that point to the neighborhood of the same starting point are grouped to produce a whole lane. The above procedures are done by matrix operations to ensure parallel keypoints association.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of the matching between predict points and their ground truth. The red dot is the observed keypoint. The blue dots are the predicted locations of adjacent keypoints. The green dots are the ground-truth locations of adjacent keypoints on the lane line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>ImageFigure 6 .</head><label>6</label><figDesc>Confidence map w/o LFA Prediction w/o LFA Confidence map w/ LFA Prediction w/ LFA Groundtruth Visualization results of GANet w/wo LFA. The first column is the input image. The second and third columns are the predicted point confidence map and lane lines without LFA. The fourth and fifth columns are the predicted point confidence map and lane lines with LFA. The last column is the ground-truth lane lines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Visualization results of LFA w/wo auxiliary loss. The red point is the observation point. The green points are the predicted aggregation points. The light blue points are the groundtruth points on the lane line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where ? and ? are hyper-parameters of focal loss and H ? ? W ? denotes H</figDesc><table /><note>r ? W r . The subscript yx represents obtaining the value at coordinate (x, y).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 2 .</head><label>32</label><figDesc>Our GANet-S outperforms all other methods and achieves the highest F1 score of 97.71% with high FPS. Comparison with state-of-the-art methods on CULane test set. The evaluation metric is F1 score with IoU threshold=0.5. For cross scenario, only FP are shown.</figDesc><table><row><cell>It is worth noting GANet-S exceeds UFast-ResNet34 and</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal deformable convolutional encoder-decoder networks for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">El-gan: Embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N?ra</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection CNNS by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards lightweight lane detection by optimizing spatial embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokwoo</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungha</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Azam</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Key Points Estimation and Point Instance Segmentation Approach for Lane Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongmin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younkwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoaib</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzeen</forename><surname>Munir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moongu</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Witold</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Line-cnn: Endto-end traffic line detection with line proposal unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Line-cnn: Endto-end traffic line detection with line proposal unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Condlanenet: A top-to-down lane detection framework based on conditional convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Endto-end lane shape prediction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards End-to-End Lane Detection: An Instance Segmentation Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fastdraw: Addressing the long tail of lane detection by adapting a sequential prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ultra fast structureaware deep lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Focus on local: Detecting lane marker from bottom up via key point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Polylanenet: Lane estimation via deep polynomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto F De</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Keep your eyes on the lane: Real-time attention-guided lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">F De</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Tusimple lane detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tusimple</surname></persName>
		</author>
		<ptr target="https://github.com/TuSimple/tusimple-benchmark" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Curvelane-nas: Unifying lanesensitive architecture search and adaptive point blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning deformable kernels for image and video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06903</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deformable 3d convolution for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end lane marker detection via row-wise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">Seok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesoo</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrack</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghoon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duck Hoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deformable siamese attention networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuechen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
