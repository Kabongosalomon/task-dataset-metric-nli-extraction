<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-Efficient Instance Generation from Instance Discrimination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong ? ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong ? ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong ? ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong ? ByteDance Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Data-Efficient Instance Generation from Instance Discrimination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GANs) have significantly advanced image synthesis, however, the synthesis quality drops significantly given a limited amount of training data. To improve the data efficiency of GAN training, prior work typically employs data augmentation to mitigate the overfitting of the discriminator yet still learn the discriminator with a bi-classification (i.e., real vs. fake) task. In this work, we propose a data-efficient Instance Generation (InsGen) method based on instance discrimination. Concretely, besides differentiating the real domain from the fake domain, the discriminator is required to distinguish every individual image, no matter it comes from the training set or from the generator. In this way, the discriminator can benefit from the infinite synthesized samples for training, alleviating the overfitting problem caused by insufficient training data. A noise perturbation strategy is further introduced to improve its discriminative power. Meanwhile, the learned instance discrimination capability from the discriminator is in turn exploited to encourage the generator for diverse generation. Extensive experiments demonstrate the effectiveness of our method on a variety of datasets and training settings. Noticeably, on the setting of 2K training images from the FFHQ dataset, we outperform the state-of-the-art approach with 23.5% FID improvement. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The real-fake domain classification task could be too simple for the discriminator to gain sufficient discriminative power as an adaptive loss to train the generator, especially when the size of training set is small. In this work, we propose to improve the data efficiency in GAN training by assigning a more challenging task to the discriminator, which is to distinguish every image instance as an independent category. In this way, the discriminator is forced to improve its discriminative capability to accomplish the instance discrimination task. Notably, besides distinguishing real samples, we also demand the discriminator to differentiate fake samples synthesized by the generator. Thus the discriminator can be considered to train with infinite data, preventing it from memorizing the training samples. When distinguishing synthesized data, we design a noise perturbation strategy to increase the difficulty of the task and hence make the discriminator more capable. Meanwhile, we also alter the training objectives from the generator side. Concretely, besides making the generator to fool the discriminator, we expect all the samples produced by the generator to be well identified as different instances with our instance-induced discriminator. This matches the goal of diverse generation, which requires every synthesis to be as different as possible. We evaluate our method on a range of datasets and it achieves appealing generation result in terms of image quality, diversity, and data efficiency. Experiments show that our method significantly improves the baselines and outperform previous data-augmentation methods. To be specific, our method improves the FID from 15.60 to 11.92, 7.29 to 4.90, and 3.88 to 3.31 with 2K, 10K, and 70K training images from FFHQ <ref type="bibr" target="#b22">[23]</ref> respectively. We can even learn a large-scale GAN with only 100 in-the-wild images to produce satisfying synthesis.</p><p>Our main contributions are summarized as follows: 1) We propose a data-efficient instance generation (InsGen) method which incorporates instance discrimination as an auxiliary task in GAN training. 2) The synthesized data is used as infinite samples for improving the discriminative power of the discriminator, which in turn substantially improves the synthesis quality and diversity of the generator. 3) Under various data-regime settings, our method consistently surpasses existing alternatives by a substantial margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Data Augmentation in GANs. Data augmentation makes the maximum use of available data to alleviate the overfitting of deep models that have millions of parameters. It plays an essential role in training discriminative models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref>. Some recent work explores how data augmentation can help the training of GANs <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b23">24]</ref>. Zhao et al. <ref type="bibr" target="#b50">[51]</ref> conduct empirical studies on the effects of different types of augmentations for GAN training. Tran et al. <ref type="bibr" target="#b38">[39]</ref> make a theoretical analysis of several data augmentations. Zhao et al. <ref type="bibr" target="#b48">[49]</ref> propose a differentiable augmentation method such that the augmenting operations can be applied to both real and synthesized data. Similarly, Karras et al. <ref type="bibr" target="#b23">[24]</ref> design augmentations that do not leak and introduce a probability-based adaptive strategy to stabilize the training process. Different from prior work, we focus on introducing the unsupervised representation learning which also requires augmentations into GAN training. Our work shows that the recent instance discrimination task <ref type="bibr" target="#b39">[40]</ref> can be used as an auxiliary task for the discriminator to learn more discriminative representations with limited training data, which in turn substantially improves the synthesis quality of the generator.</p><p>Self-supervised Learning in GANs. The rationale behind self-supervised learning is to set up various pretext tasks with supervisory-free labels <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>. Similar idea is recently introduced in GAN training as an auxiliary loss to improve the synthesis performance. For example, Chen et al. <ref type="bibr" target="#b5">[6]</ref> assign the rotation prediction task to the discriminator to prevent it from catastrophic forgetting, and Tran et al. <ref type="bibr" target="#b37">[38]</ref> propose a multi-class minimax game to encourage the generator to produce diverse samples. Among all self-supervised learning approaches, contrastive learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3]</ref>, which aims at distinguishing instances, shows great potential in large-scale representation learning. Many attempts have been made to improve generative models by drawing lessons from contrastive learning, like the consistency regularization for GANs <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50]</ref>, the patch-level contrastive learning for image-to-image translation <ref type="bibr" target="#b32">[33]</ref>, and the latent-augmented contrastive loss for conditional image synthesis <ref type="bibr" target="#b28">[29]</ref>. Akin to supervised contrastive loss <ref type="bibr" target="#b26">[27]</ref>, some concurrent work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43]</ref> reformulates the conventional bi-classification task (i.e., real domain vs. fake domain) with contrastive loss. By contrast, we assign the discriminator a simple auxiliary task, which is to recognize every image instance, no matter it is real or synthesized by the generator. Such instance discrimination task helps sustain the discriminative power of the discriminator under a low-data regime, which in turn improves the synthesis performance substantially. <ref type="figure">Figure 1</ref>: Illustration of the InsGen method. Besides the bi-classification task to differentiate real and fake domains, the discriminator is assigned an auxiliary task, which aims at maximally distinguishing each image instance as illustrated on the right. C denotes the training objective for such instance discrimination task. (a) The discriminator is asked to recognize not only every real sample x i but also every synthesized sample G(z i ) by a frozen generator. (b) With the instance-induced discriminator, the generator is encouraged to make all synthesis recognizable from each other, leading to more diverse generation.</p><formula xml:id="formula_0">D ? ! + ! " + ! # ? ! + ! " G $ % ? ( $ ) ( % ) ? $ % ? (a) Train Discriminator with Instance Discrimination (b) Train Generator with Instance Discrimination $ % ? Instance Discrimination ( ! ) ( " ) ( # ) ( $ ) ( % ) ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we introduce the proposed InsGen method. Recall that our method is built based on GAN, which is commonly formulated as a two-player game between a generator and a discriminator. They compete with each other in that the generator tries to produce as realistic data as possible while the discriminator works on recognizing synthesized data from real data. Besides the conventional bi-classification task (i.e., differentiating real and fake domains), we also require the discriminator to distinguish every image instance. With such a challenging task, the discriminator can mitigate the overfitting problem even with limited training data. We will briefly introduce the image synthesis and instance discrimination mechanisms in Sec. 3.1, followed by our improved training pipeline in Sec. 3.2 and the practical usage of InsGen on the state-of-the-art StyleGAN2-ADA model <ref type="bibr" target="#b23">[24]</ref> in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Synthesizing Images with GANs. GAN is a popular paradigm for image generation <ref type="bibr" target="#b15">[16]</ref>. It typically consists of two networks: a generator G(?) that learns to map a latent variable z to a photo-realistic image, and a discriminator D(?) that aims at separating real images x from synthesized ones G(z). These two networks compete with each other and are jointly optimized with</p><formula xml:id="formula_1">L D = ?E x?X [log(D(x))] ? E z?Z [log(1 ? D(G(z)))],<label>(1)</label></formula><formula xml:id="formula_2">L G = ?E z?Z [log(D(G(z)))],<label>(2)</label></formula><p>where Z and X denote the pre-defined latent distribution and real data distribution respectively. After the training converges, the synthesized images should be as realistic as real ones to fool the discriminator. From this perspective, the synthesis quality highly depends on the discriminative power of the discriminator. Prior literature <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref> has affirmed that GANs will suffer from the insufficient training of the discriminator and proposed to apply a series of data augmentations T (?) to alleviate the overfitting problem. But they do not change the learning objectives of GAN and observe drastic performance drop given limited training data.</p><p>Distinguishing Images with Contrastive Learning. It is well-known that image classification tasks usually benefit from more discriminative representations <ref type="bibr" target="#b11">[12]</ref>. Unlike supervised training algorithms that optimize the model parameters based on annotated data, contrastive learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3]</ref> is able to extract representative features from images in an unsupervised manner. As shown in <ref type="figure">Fig. 1a</ref>, the rationale behind is to "label" every sample as an individual class, i.e., instance discrimination.</p><p>Concretely, given an image x, two random "views" (e.g., through different augmentations) are created as the query x q and the key x k+ . This query-key pair is regarded as the positive pair while all "views" from other images, {x ki } N i=1 , are treated as negative pairs with respect to the query. Here, N is the total number of images in addition to the query image. Contrastive learning aims at maximizing the agreement across augmentations (i.e., x q and x k+ ) and make the query as much dissimilar to a number of negative samples as possible. Accordingly, we can design a pretext task of (N + 1)-way classification and learn the model with the contrastive loss C, i.e., InfoNCE loss <ref type="bibr" target="#b31">[32]</ref> v</p><formula xml:id="formula_3">q = F (x q ), v k+ = F (x k+ ), v ki = F (x ki ), i = 1 . . . N,<label>(3)</label></formula><formula xml:id="formula_4">C F (?),?(?) (x q , x k+ , {x ki } N i=1 ) = ? log exp(?(v q ) T ?(v k+ )/? ) N i=0 exp(?(v q ) T ?(v ki )/? ) ,<label>(4)</label></formula><p>where F (?) is the backbone network to extract the representation v from a given image x, and ?(?) is the head network (e.g., usually implemented with several fully-connected layers) to project the extracted feature onto a unit sphere. ? stands for the temperature, which is a hyper-parameter. Recall that the primitive goal of the discriminator in GANs can also be viewed as a bi-classification task, which is to recognize real and fake domains. In this work, we demonstrate that introducing the instance discrimination task can help enhance the discriminative power of the discriminator and in turn improve the synthesis quality of the generator significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generating Diverse Instances from Distinguishing Instances</head><p>In this part, we will introduce how instance discrimination <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b16">17]</ref> is incorporated into the GAN training for data-efficient and diverse image generation. There are four essential components of the InsGen method: 1) distinguishing real images, 2) distinguishing fake images that can be sampled infinitely, 3) a noise perturbation strategy, and 4) a loop-back mechanism to encourage the generator for the diverse generation.</p><p>Distinguishing Real Images. As discussed above, the synthesis quality of GAN models not only depends on the training scheme <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref> and the architecture design of the generator <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>, but more importantly it relies on the discriminative capability of the discriminator. That is because the discriminator is the one that sees how real data looks like and further guides the training of generator accordingly, while the generator doesn't have the direct access to the real data. To make the maximum use of the limited training data and avoid the discriminator from memorizing the entire dataset, we assign it with a more challenging task beyond domain classification, which is to recognize every independent instance from the dataset, as shown in <ref type="figure">Fig. 1a</ref>. For this purpose, we introduce a new task head ? r (?) beyond the original domain classification head ? domain (?) on top of its backbone d(?) 2 and train the discriminator with an extra training objective</p><formula xml:id="formula_5">C r D = C d(?),? r (?) (T q (x q ), T k+ (x q ), {T ki (x ki )} N i=1 ).<label>(5)</label></formula><p>Here, x q , {x ki } N i=1 are all sampled from the real data distribution X and transformed with various differentiable augmentations T (?).</p><p>Distinguishing Fake Images. However, the amount of training data could be extremely few (like thousands or even hundreds) in practice. In such a case, the improvement of the discriminator gained by differentiating real instances will be also limited. On the other hand, we notice that the number of synthesized samples can be sufficiently large due to the sampling mechanism of GANs. Ideally, different latent codes z ? Z should lead to different synthesis G(z). Hence, we propose to also ask the discriminator to recognize every individual fake images, as shown in <ref type="figure">Fig. 1a</ref>. Similarly, we introduce another task head ? f (?) into the discriminator. It is worth mentioning that we use separate task heads (i.e., ? r (?) and ? f (?)) for real and fake data. That is because even though the synthesized images can be with high-quality, they still lie in a different distribution from the real ones, especially when the generator starts training from scratch. Meanwhile, the task of discriminating a real instance from a fake instance can be achieved by the native domain classification head ? domain (?).</p><p>Noise Perturbation. Prior work has observed the continuity of the latent space <ref type="bibr" target="#b35">[36]</ref> such that images synthesized from the latent codes within a neighbourhood are very close to each other. Accordingly, they are more suitable to be treated as positive pairs than negative pairs. From this perspective, we introduce a noise perturbation strategy into fake image discrimination. The objective becomes</p><formula xml:id="formula_6">x q = T q (G(z q )), x k+ = T k+ (G(z q + )), x ki = T ki (G(z ki )),<label>(6)</label></formula><formula xml:id="formula_7">C f D = C d(?),? f (?) (x q , x k+ , {x ki } N i=1 ).<label>(7)</label></formula><p>Concretely, given a query image x q , the key image x k+ is created with T k+ (G(z q + )) instead of T k+ (G(z q )). Here, stands for the perturbation term, which is sampled from a Gaussian distribution whose variance is sufficiently smaller than that of Z, and T q (?) and T k+ (?) denote two different augmentations. Such design aims to enforce the discriminator invariant to the small perturbation, which makes the instance discrimination task more challenging.</p><p>Toward Diverse Generation. Besides utilizing the instance discrimination task to improve the discriminative power of the discriminator, we further design a loop-back mechanism to in turn use the learned instance discrimination to guide the generator. Recall that image diversity, in addition to image quality, is also an important metric to evaluate generative models. Diverse generation, which requires all generated samples to be distinguishable from each other, exactly matches our goal of instance discrimination. In other words, given a discriminator with the ability to distinguish different instances, we would like all the samples produced by the generator to be recognized as different ones. This idea is illustrated in <ref type="figure">Fig. 1b</ref>. By comparing <ref type="figure">Fig. 1a</ref> and <ref type="figure">Fig. 1b</ref>, we can see that the generator shares the same target as the discriminator yet is trained separately. Hence, the same objective function is added into the generator loss</p><formula xml:id="formula_8">x k+ = T k+ (G(z q )),<label>(8)</label></formula><formula xml:id="formula_9">C f G = C d(?),? f (?) (x q , x k+ , {x ki } N i=1 ),<label>(9)</label></formula><p>where the only difference is that noise perturbation is not applied during the training of the generator.</p><p>Complete Objective Function. To summarize, with the purposes of both image synthesis and instance discrimination, the discriminator and the generator in InsGen are optimized with</p><formula xml:id="formula_10">L D = L D + ? r D C r D + ? f D C f D ,<label>(10)</label></formula><formula xml:id="formula_11">L G = L G + ? G C f G ,<label>(11)</label></formula><p>where ? G , ? r D , and ? f D denote the weights for different terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>On top of the adversarial training pipeline in GANs, our InsGen method only inserts an extra loss output on the discriminator network for instance discrimination. Therefore, it can be easily implemented on any GAN framework. In this part, we take the state-of-the-art GAN model, StyleGAN2-ADA <ref type="bibr" target="#b23">[24]</ref>, as an example to demonstrate how InsGen is implemented in practice.</p><p>Generative Model. StyleGAN2-ADA <ref type="bibr" target="#b23">[24]</ref> adopts the architecture of StyleGAN2 <ref type="bibr" target="#b24">[25]</ref> and proposes the adaptive discriminator augmentation strategy for training with limited data. In particular, it designs a differentiable augmentation pipeline, consisting of 18 transformations, as well as an adaptive hyper-parameter to control the strength of these augmentations. For a fair comparison, in this work, we exactly reuse the network structure, the augmentation pipeline, the adaptive strategy of the augmenting strength, and other hyper-parameters like batch size and learning rate.</p><p>Instance Discrimination. We reuse the backbone of the discriminator to perform instance discrimination, so that the extra computing load is extremely small and the training efficiency is barely affected. We treat the last fully-connected layer in the StyleGAN2-ADA discriminator as the domain-classification head ? domain (?), while all remaining layers serve as the backbone network d(?). The real instance discrimination head ? r (?) and the fake head ? f (?) are both implemented with 2 fully-connected layers, followed by 2 normalization. Strictly following MoCo-v2 <ref type="bibr" target="#b7">[8]</ref>, an extra queue is employed for each task head to store the sample features to save computational cost. The number of samples in L r D and L f D is thus equal to the queue size, which usually contains around 5% data of the whole set. We also introduce the momentum encoder D , whose parameters are updated with moving average scheme: ? D ? ?? D + (1 ? ?)? D . Here, ? = 0.999 follows the same setting in MoCo-v2 <ref type="bibr" target="#b7">[8]</ref>. The temperature ? in Eq. (4) is set as 2. <ref type="table">Table 1</ref>: Performance on FFHQ. FID (lower is better) is reported as the evaluation metric. "2K", "10K", and "140K" stand for the number of samples used for training, where "140K" horizontally flips the original FFHQ dataset (with 70K samples) to double the size of data. Results with * are also achieved with horizontally flipped data, which are slightly better than those reported in <ref type="bibr" target="#b23">[24]</ref>. Numbers in blue color indicate our improvements over the baseline <ref type="bibr" target="#b23">[24]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>256?256 Resolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>Datasets. We evaluate our InsGen with a number of other approaches on FFHQ <ref type="bibr" target="#b22">[23]</ref> and AFHQ <ref type="bibr" target="#b8">[9]</ref> datasets. FFHQ contains unique 70,000 high-resolution images (1024?1024), with large variation regarding age, ethnicity, and background. All images of FFHQ are well aligned <ref type="bibr" target="#b25">[26]</ref> and cropped. In order to conduct a fair comparison, we resize images to 256?256. For the experiments of limited data, we follow ADA <ref type="bibr" target="#b23">[24]</ref> to collect a subset of training data by randomly sampling. Moreover, AFHQ consists of around 5000 images per category for dogs, cats, and wild life at 512?512 resolution. Each category is regarded as a dataset and thus we train a different network on each dataset.</p><p>Training. We implement our InsGen on the official implementation of StyleGAN2-ADA. The training regularization is preserved, including path length regularization, lazy regularization, and style mixing regularization. Moreover, all parameters share the same learning rate and the minibatch standard deviation layer is adopted at the end of the discriminator. Exponential moving average of generator weights, non-saturating logistic loss with R 1 regularization, and Adam optimizer <ref type="bibr" target="#b27">[28]</ref> is also adopted. In particular, the coefficient of gradient penalty would be decreased correspondingly, according to the official implementation of ADA <ref type="bibr" target="#b23">[24]</ref>. All the experiments are conducted on a server with 8 GPUs. Mixed-precision training is also used for faster training.</p><p>Evaluation Metric. We use Fr?chet Inception Distance (FID) <ref type="bibr" target="#b18">[19]</ref> as the metric for quantitative comparison metric since FID tends to reflect the human perception of synthesis quality. As mentioned in <ref type="bibr" target="#b18">[19]</ref>, we always calculate the FID between 50,000 fake images and all training images, no matter how much data the training set contains. The official pre-trained Inception network is used to compute the FID.</p><p>Results on FFHQ. Tab. 1 presents the comparison on FFHQ. Akin to ADA <ref type="bibr" target="#b23">[24]</ref>, we compare against PA-GAN <ref type="bibr" target="#b43">[44]</ref>, zCR <ref type="bibr" target="#b49">[50]</ref> and auxiliary rotation <ref type="bibr" target="#b5">[6]</ref>. Also, StyleGAN2 together with its variants is also introduced as the baseline methods. For instance, less data is usually required when a shallower mapping network is applied. Besides, dropout <ref type="bibr" target="#b36">[37]</ref> is also well-studied to be replaced with the augmentations as the regularization. Note that * means the dataset is amplified by 2? via the horizontal flip, which is recommended in the official implementation of ADA <ref type="bibr" target="#b23">[24]</ref>. Such that, "2K" denotes 2,000 unique images and the dataset is enlarged to 4,000 via the flip operation, leading to a better baseline.  All images on FFHQ are synthesized with truncation following <ref type="bibr" target="#b23">[24]</ref> while those on AFHQ are not.</p><p>Although ADA <ref type="bibr" target="#b23">[24]</ref> has already improved the performance significantly under various low-data regimes, our InsGen continues to improve the low-data image generation by a clear margin, establishing a new state-of-the-art synthesis quality with limited training images. To be specific, our method improves the FID from 15.60 to 11.92, 7.29 to 4.90, and 3.88 to 3.31 with 2K, 10K and 70K training images from FFHQ <ref type="bibr" target="#b22">[23]</ref> respectively. <ref type="figure" target="#fig_0">Fig. 2</ref> presents several generated examples under various data regimes. All images on FFHQ are generated with truncation. It is also worth noting that our approach further improves the synthesis quality when the full dataset is given, even outperforming previous best one, i.e., zCR <ref type="bibr" target="#b49">[50]</ref>. Namely, the data can be further exploited when it is not the bottleneck for training.</p><p>Results on AFHQ. We also evaluate our approach on AFHQ dataset <ref type="bibr" target="#b8">[9]</ref> which is divided into cat, dog and wild life, with the number of 5153, 4739 and 4738 images respectively. Therefore, three models are trained on them individually. Note that all models on AFHQ are trained on 512?512 images while the generated samples are resized to present. We involve StyleGAN2 <ref type="bibr" target="#b24">[25]</ref>, ContraD <ref type="bibr" target="#b19">[20]</ref> and ADA <ref type="bibr" target="#b23">[24]</ref> as the baseline approaches, compared to our InsGen. Quantitative and qualitative results are shown in Tab. 2 and <ref type="figure" target="#fig_0">Fig. 2</ref> respectively.</p><p>The synthesis quality on those datasets is substantially improved by our method, which also outperforms previous data-augmentation methods. To be specific, our method improves the FID from 3.55 to 2.60, 7.40 to 5.44, and 3.05 to 1.77 on cat, dog and wild life images respectively.</p><p>In particular, ContraD <ref type="bibr" target="#b19">[20]</ref> introduced stronger augmentations to train a better discriminator via <ref type="table">Table 3</ref>: Ablation Study. FID (lower is better) is reported as the evaluation metric. Here, vanilla C f D means that the noise perturbation is not applied in the fake instance discrimination. contrastive learning. One term in this method shares the similar motivation that real images could result in powerful representations. In terms of the use of synthesized samples, ContraD turned to focus on the binary classification (i.e., real vs. fake) with some specific designs like the stop-gradient operation. Differently, our method leverages the generated images as a kind of data complement to produce a stronger representation and guide the learning of the generator. Accordingly, InsGen achieves the new state-of-the-art performances on AFHQ <ref type="bibr" target="#b8">[9]</ref>.</p><formula xml:id="formula_12">C r D vanilla C f D C f D C f G 2K 10K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>In order to investigate the importance of each component in our InsGen, we conduct an ablation study on FFHQ <ref type="bibr" target="#b22">[23]</ref> with the image resolution of 256?256. FID serves as the main metric for the comparison, and the results on 2K, 10K and 70K unique images are reported. During training each unique image go through random flip operation to obtain a stronger baseline. Tab. 3 presents the collection of various experiments in the ablation study. We choose the ADA <ref type="bibr" target="#b23">[24]</ref> as the baseline.</p><p>How important is the instance discrimination? After performing the real image discrimination, the synthesis quality is improved, with the FID consistently decreased by -1.45, -1.31 and -0.20 in Tab. 3, no matter how many unique images the training set includes. To some extent, the discriminator would benefit from the powerful representations derived from the challenging pretext task. Accordingly, the generator is required to produce more photo-realistic images in order to confuse the discriminator.</p><p>When adding instance discrimination with fake images, performances could be further boosted. For instance, FID obtains an improvement of -0.69 and -0.30 with 2K and 10K images respectively. In particular, the gains rise as the number of real images goes down, verifying one of our motivations that the fake samples can be also regarded as data source for unsupervised representation learning.</p><p>How important is the noise perturbation? In Sec. 3.2, a noise perturbation strategy is proposed as a type of latent space augmentation for fake image discrimination. In particular, this latent space augmentation, i.e., the small movement in the latent space always leads to an obvious but semantically consistent change of the original image, which could not easily be implemented by some geometric and color transformations. Meanwhile, the discriminator is required to be invariant to such noise perturbation due to the goal of instance discrimination. Accordingly, the fake images are made best use of to result in stronger representations for the discrimination. As shown in Tab. 3, such strategy further brings consistent gains of -1.27, -0.38 and -0.18 on 2K, 10K and 70K datasets respectively.</p><p>How important is the supervision signal for the generator? The last row of Tab. 3 shows the performances with the gradients which are back-propagated to the generator. Even if we have already obtained quite strong results, such a supervision signal on the generator could also introduce improvements under various data regimes.</p><p>The goal of instance discrimination is to distinguish every individual image according to its appearance cues <ref type="bibr" target="#b39">[40]</ref>. Assuming this pretext task is well-performed on a fixed dataset, the semantic representation would be derived from this learning process. However, when distinguishing fake images, the fake dataset actually varies dynamically. Namely, we could accomplish this pretext task from the perspective of data, if the engine of this dynamical fake dataset, i.e., the generator could produce as many different images as possible. In general, this pretext task is exploited to encourage the diverse generation directly on the generator.</p><p>How important is the number of negative samples? We follow the MoCo-v2 <ref type="bibr" target="#b7">[8]</ref> to store multiple features in a queue, in order to reduce the computational complexity. Empirically, the length of the feature queue tends to be the 5% number of the dataset. Therefore, it is 200 when we have 2K unique  <ref type="figure">Figure 3</ref>: Effect of the number of synthesized and real images used for instance discrimination. FID (lower is better) in log-scale is reported as the evaluation metric. We can see the consistent performance gain along with the increasing number of instances for discrimination.</p><p>Ours-Real ADA-Real ADA-Fake Ours-Fake  <ref type="figure">Figure 4</ref>: Training progress on FFHQ-2K. Larger value means that the image is more realistic under the view of the discriminator. Our discriminator can better and more stably differentiate real and fake data compared to ADA <ref type="bibr" target="#b23">[24]</ref>.</p><p>images and enlarge them via the flip operation. However, there is no any reference number for the synthesized data. Accordingly, we collect as the same amount of fake data as that of the real.</p><p>As mentioned in Sec. 3.2, there could be much more synthesized samples than the real samples. Namely, we could leverage infinite samples for the synthesized instance discrimination. Therefore, we investigate the effect of the different number of synthesized samples i.e., the length of the feature queue, shown in <ref type="figure">Fig. 3a</ref>. Obviously, FID gradually decreases with the increasing number of synthesized samples, suggesting that involving more fake images is of great benefit to the synthesis, especially with the limited training data.</p><p>Whether the discriminative ability of the discriminator is really enhanced. As is mentioned in our work, it is challenging to gain sufficient discriminative power for the discriminator to train the generator when the size of training set is small. However, introducing instance discrimination is able to improve its discriminative capability, achieving new state-of-the-art synthesis quality. In order to investigate whether the discriminative ability is improved, we plot the logits (derived from the discriminator) of any input image during the training in <ref type="figure">Fig. 4</ref>. To be specific, the logit denotes how much the input image is identified as the real. And the number of training images are 2000.</p><p>Obviously, our method produces higher real and lower fake scores throughout the whole training progress, compared to the baseline approach ADA <ref type="bibr" target="#b23">[24]</ref>. It indicates that the discriminator of our method performs the domain bi-classification (i.e., real vs. fake) better than that of baseline, showing stronger discriminative ability. It also verifies our motivation that a challenging pretext task which is to distinguish every individual image could indeed enhance the discriminator. Besides, the training progress is much more stable when equipped with our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Towards the Limit of Data-efficiency</head><p>Although we have obtained the new state-of-the-art synthesis performances under the standard settings, we also wonder how much data-efficiency our InsGen could achieve. Therefore, the number of real data in the training set is further reduced to 1000, 500, 250 and 100. In order to conduct the apple-to-apple comparison, we remain to train the same model of StyleGAN2 without decreasing its generative capacity by using fewer channels or shallower mapping networks since such designs require less data. Meanwhile, the generated resolution remains 256?256 and the datasets are amplified via the horizontal flip operation as well.  <ref type="figure">Figure 5</ref>: Results with different number of training images. The number of training images and the corresponding FID are reported above the synthesis samples. All images are generated with truncation following <ref type="bibr" target="#b23">[24]</ref>.</p><p>The quantitative and qualitative results are shown in <ref type="figure">Fig. 3b</ref> and <ref type="figure">Fig. 5</ref> respectively. Obviously, FID significantly increases with the decreasing number of training images from 70K to 100. Nevertheless, our InsGen trained with only 100 unique images remains to outperform many approaches like PA-GAN in <ref type="figure">Fig. 3b</ref> with 2K images. Besides, with 500 training samples, our method is able to obtain the competitive performance to those using 10k images. Namely, our InsGen could improve the data-efficiency by more than 20?. Qualitative results suggest that our approach still produces meaningful images without incurring the model collapse no matter how many training images exist in the data collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we develop a novel data-efficient Instance Generation (InsGen) method for training GANs with limited data. With the instance discrimination as an auxiliary task, our method makes the best use of both real and fake images to train the discriminator. In turn the discriminator is exploited to train the generator to synthesize as many diverse images as possible. Experiments under different data regimes show that InsGen brings a substantial improvement over the baseline in terms of both image quality and image diversity, and outperforms previous data augmentation algorithms by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Generated images under various data regimes. The number of training images and the corresponding FID are reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance on AFHQ. FID (lower is better) is reported as the evaluation metric. Numbers in blue color indicate our improvements over the baseline<ref type="bibr" target="#b23">[24]</ref>.</figDesc><table><row><cell>512?512 Resolution</cell><cell>Cat</cell><cell>Dog</cell><cell>Wild life</cell></row><row><cell>StyleGAN2 [23]</cell><cell>5.13</cell><cell>19.4</cell><cell>3.48</cell></row><row><cell>ContraD [20]</cell><cell>3.82</cell><cell>7.16</cell><cell>2.54</cell></row><row><cell>ADA [24]</cell><cell>3.55</cell><cell>7.40</cell><cell>3.05</cell></row><row><cell>InsGen (Ours)</cell><cell>2.60 (?0.95)</cell><cell>5.44 (?1.96)</cell><cell>1.77 (?1.28)</cell></row><row><cell>140 , FID 3.31</cell><cell>10 , FID 4.90</cell><cell></cell><cell>2 , FID 11.92</cell></row><row><cell>FFHQ-256</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cat (5153), FID 2.60</cell><cell cols="2">Dog (4739), FID 5.44</cell><cell>Wild life (4738), FID 1.77</cell></row><row><cell>AFHQ-512</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The conventional discriminator is a composition of d(?) and ? domain (?) to perform real/fake classification, i.e., D(?) = ? domain (?) ? d(?).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<title level="m">Towards principled methods for training generative adversarial networks. Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog., 2020. 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh., 2020</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog., 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn., 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training gans with stronger augmentations via contrastive discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent., 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contragan: Contrastive learning for conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Divco: Diverse conditional image synthesis via contrastive generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-supervised gan: Analysis and improvement with multi-class minimax game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On data augmentation for gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generative hierarchical features from synthesizing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Instance localization for self-supervised detection pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16748</idno>
		<title level="m">Dual contrastive loss and attention for gans</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pa-gan: Improving gan training by progressive augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improved consistency regularization for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Assoc</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02595</idno>
		<title level="m">Image augmentations for gan training</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
