<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Motion In-betweening</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07">July 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F?lix</forename><forename type="middle">G</forename><surname>Harvey</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polytechnique</forename><surname>Montreal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mila</forename><surname>Canada</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ubisoft</forename><surname>Canada</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename><forename type="middle">Mike</forename><surname>Montreal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ubisoft</forename><surname>Yurick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename><surname>Montreal</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Yurick</surname></persName>
							<email>mike.yurick@ubisoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">DEREK NOWROUZEZAHRAI</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<region>Mila</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CHRISTOPHER PAL</orgName>
								<orgName type="institution" key="instit2">CIFAR AI Chair</orgName>
								<address>
									<addrLine>Polytechnique Montreal</addrLine>
									<region>Mila, AI</region>
									<country>Canada, Canada, Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>2500 Chemin de la Poly-techique, 6666 St-Urbain Street, #200, Montreal, Canada, Ubisoft Montreal, #2000, Montreal</addrLine>
									<postCode>H3T 1J4, H2S 3H1, 5505, H2T 1S6</postCode>
									<settlement>Montreal, Boul Saint-Laurent</settlement>
									<region>QC, Mila, QC, QC</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Ubisoft Montreal</orgName>
								<address>
									<addrLine>#2000, Montreal</addrLine>
									<postCode>5505, H2T 1S6</postCode>
									<settlement>Boul Saint-Laurent</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Derek Nowrouzezahrai</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<addrLine>3480 University St, 6666 St-Urbain Street, #200, Montreal</addrLine>
									<postCode>H3A 0E9, H2S 3H1</postCode>
									<settlement>Montreal</settlement>
									<region>QC, Mila, QC</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<addrLine>661 University Ave., Suite 505, Polytechnique Montreal, 2500 Chemin de la Poly-techique, 6666 St-Urbain Street, #200, Montreal, Canada, Element AI, 6650 St-Urbain Street, #500, Montreal</addrLine>
									<postCode>M5G 1M1, H3T 1J4, H2S 3H1, H2S 3G9</postCode>
									<settlement>Toronto, Montreal</settlement>
									<region>ON, QC, Mila, QC, QC</region>
									<country>Canada, Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Motion In-betweening</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Trans. Graph. 39, 4, Article</title>
						<imprint>
							<biblScope unit="volume">60</biblScope>
							<date type="published" when="2020-07">July 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3386569.3392480</idno>
					<note>Authors&apos; addresses: F?lix G. Harvey, Polytechnique Montreal, 0730-0301/2020/7-ART60 $15.00 ACM Reference Format: F?lix G. Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. 2020. Robust Motion In-betweening.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ? Computing methodologies ? Motion capture; Neural networks Additional Key Words and Phrases: animation</term>
					<term>locomotion</term>
					<term>transition gener- ation</term>
					<term>in-betweening</term>
					<term>deep learning</term>
					<term>LSTM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. Transitions automatically generated by our system between target keyframes (in blue). For clarity, only one in four generated frames is shown. Our tool allows for generating transitions of variable lengths and for sampling different variations of motion given fixed keyframes.</p><p>In this work we present a novel, robust transition generation technique that can serve as a new tool for 3D animators, based on adversarial recurrent neural networks. The system synthesizes high-quality motions that use temporally-sparse keyframes as animation constraints. This is reminiscent of the job of in-betweening in traditional animation pipelines, in which an animator draws motion frames between provided keyframes. We first show that a state-of-the-art motion prediction model cannot be easily converted into a robust transition generator when only adding conditioning information about future keyframes. To solve this problem, we then propose two novel additive embedding modifiers that are applied at each timestep to latent representations encoded inside the network's architecture. One modifier is a time-to-arrival embedding that allows variations of the transition length with a single model. The other is a scheduled target noise vector that allows the system to be robust to target distortions and to sample different transitions given fixed keyframes. To qualitatively evaluate our method, we present a custom MotionBuilder plugin that uses our trained model to perform in-betweening in production scenarios. To quantitatively evaluate performance on transitions and generalizations to longer time horizons, we present well-defined in-betweening benchmarks on a subset of the widely used Human3.6M dataset and on LaFAN1, a novel high quality motion capture dataset that is more appropriate for transition generation. We are releasing this new dataset along with this work, with accompanying code for reproducing our baseline results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human motion is inherently complex and stochastic for long-term horizons. This is why Motion Capture (MOCAP) technologies still often surpass generative modeling or traditional animation techniques for 3D characters with many degrees of freedom. However, in modern video games, the number of motion clips needed to properly animate a complex character with rich behaviors is often very large and manually authoring animation sequences with keyframes or using a MOCAP pipeline are highly time-consuming processes. Some methods to improve curve fitting between keyframes <ref type="bibr" target="#b8">[Ciccone et al. 2019]</ref> or to accelerate the MOCAP workflow <ref type="bibr" target="#b21">[Holden 2018]</ref> have been proposed to improve these processes. On another front, many auto-regressive deep learning methods that leverage high quality MOCAP for motion prediction have recently been proposed <ref type="bibr" target="#b2">[Barsoum et al. 2018;</ref><ref type="bibr">Chiu et al. 2019;</ref><ref type="bibr" target="#b12">Fragkiadaki et al. 2015;</ref><ref type="bibr" target="#b15">Gopalakrishnan et al. 2019;</ref><ref type="bibr" target="#b25">Jain et al. 2016;</ref><ref type="bibr" target="#b37">Martinez et al. 2017;</ref><ref type="bibr" target="#b41">Pavllo et al. 2019]</ref>. Inspired by these achievements, we build in this work a transition generation tool that leverages the power of Recurrent Neural Networks (RNN) as powerful motion predictors to go beyond keyframe interpolation techniques, which have limited expressiveness and applicability.</p><p>We start by building a state-of-the-art motion predictor based on several recent advances on modeling human motion with <ref type="bibr">RNNs [Chiu et al. 2019;</ref><ref type="bibr" target="#b12">Fragkiadaki et al. 2015;</ref><ref type="bibr" target="#b41">Pavllo et al. 2019</ref>]. Using a recently proposed target-conditioning strategy <ref type="bibr" target="#b18">[Harvey and Pal 2018]</ref>, we convert this unconstrained predictor into a transition generator, and expose the limitations of such a conditioning strategy. These limitations include poor handling of transitions of different lengths for a single model, and the inherent determinism of the architectures. The goal of this work is to tackle such problems in order to present a new architecture that is usable in a production environment.</p><p>To do so, we propose two different additive modifiers applied to some of the latent representations encoded by the network. The first one is a time-to-arrival embedding applied on the hidden representation of all inputs. This temporal embedding is similar to the positional encoding used in transformer networks <ref type="bibr" target="#b52">[Vaswani et al. 2017]</ref> in natural language modeling, but serves here a different role. In our case, these embeddings evolve backwards in time from the target frame in order to allow the recurrent layer to have a continuous, dense representation of the number of timesteps remaining before the target keyframe must be reached. This proves to be essential to remove artifacts such as gaps or stalling at the end of transitions. The second embedding modifier is an additive scheduled target noise vector that forces the recurrent layer to receive distorted target embeddings at the beginning of long transitions. The scheduled scaling reduces the norm of the noise during the synthesis in order to reach the correct keyframe. This forces the generator to be robust to noisy target embeddings. We show that it can also be used to enforce stochasticity in the generated transitions more efficiently than another noise-based method. We then further increase the quality of the generated transitions by operating in the Generative Adversarial Network (GAN) framework with two simple discriminators applied on different timescales.</p><p>This results in a temporally-aware, stochastic, adversarial architecture able to generate missing motions of variable length between sparse keyframes of animation. The network takes 10 frames of past context and a single target keyframe as inputs and produces a smooth motion that leads to the target, on time. It allows for cyclic and acyclic motions alike and can therefore help generate high-quality animations from sparser keyframes than what is usually allowed by curve-fitting techniques. Our model can fill gaps of an arbitrary number of frames under a soft upper-bound and we show that the particular form of temporal awareness we use is key to achieve this without needing any smoothing post-process. The resulting system allows us to perform robust, automatic inbetweening, or can be used to stitch different pieces of existing motions when blending is impossible or yields poor quality motion.</p><p>Our system is tested in production scenarios by integrating a trained network in a custom plugin for Autodesk's MotionBuilder, a popular animation software, where it is used to greatly accelerate prototyping and authoring new animations. In order to also quantitatively assess the performance of different methods on the transition generation task, we present the LaFAN1 dataset, a novel collection of high quality MOCAP sequences that is well-suited for transition generation. We define in-betweening benchmarks on this new dataset as well as on a subset of Human3.6M, commonly used in the motion prediction literature. Our procedure stays close to the common evaluation scheme used in many prediction papers and defined by <ref type="bibr" target="#b25">Jain et al. [2016]</ref>, but differs on some important aspects. First, we provide error metrics that take into consideration the global root transformation of the skeleton, which provides a better assessment of the absolute motion of the character in the world. This is mandatory in order to produce and evaluate valid transitions. Second, we train and evaluate the models in an action-agnostic fashion and report average errors on a large evaluation set, as opposed to the commonly used 8 sequences per action. We further report generalization results for transitions that are longer than those seen during training. Finally, we also report the Normalized Power Spectrum Similarity (NPSS) measure for all evaluations, as suggested by <ref type="bibr" target="#b15">Gopalakrishnan et al. [2019]</ref> which reportedly correlates better with human perception of quality.</p><p>Our main contributions can thus be summarized as follow:</p><p>? Latent additive modifiers to convert state-of-the-art motion predictors into robust transition generators: -A time-to-arrival embedding allowing robustness to varying transition lengths, -A scheduled target-noise vector allowing variations in generated transitions, ? New in-betweening benchmarks that take into account global displacements and generalization to longer sequences, ? LaFAN1, a novel high quality motion dataset well-suited for motion prediction that we make publicly available with accompanying code for reproducing our baseline results 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Motion Control</head><p>We refer to motion control here as scenarios in which temporallydense external signals, usually user-defined, are used to drive the generation of an animation. Even if the main application of the present work is not focused on online control, many works on motion control stay relevant to this research. Motion graphs <ref type="bibr" target="#b0">[Arikan and Forsyth 2002;</ref><ref type="bibr" target="#b3">Beaudoin et al. 2008;</ref><ref type="bibr" target="#b26">Kovar et al. 2008;</ref><ref type="bibr" target="#b27">Lee et al. 2002]</ref> allow one to produce motions by traversing nodes and edges that map to character states or motions segments from a dataset. <ref type="bibr" target="#b46">Safonova and Hodgins [Safonova and Hodgins 2007]</ref> combine an interpolated motion graph to an anytime * search algorithm in order produce transitions that respect some constraints. Motion matching <ref type="bibr" target="#b5">[B?ttner and Clavet 2015]</ref> is another search driven motion control technique, where the current character pose and trajectory are matched to segments of animation in a large dataset. <ref type="bibr">Chai &amp; Hodgins, and Tautges et al. [2005;</ref><ref type="bibr" target="#b49">2011]</ref> rely on learning local PCA models on pose candidates from a motion dataset given low-dimensional control signals and previously synthesized poses in order to generate the next motion frame. All these techniques require a motion database to be loaded in memory or in the latter cases to perform searches and learning at run-time, limiting their scalability compared to generative models. Many machine learning techniques can mitigate these requirements. Important work has used the Maximum A Posteriori (MAP) framework where a motion prior is used to regularize constraint(s)related objectives to generate motion. <ref type="bibr" target="#b7">[Chai and Hodgins 2007]</ref> use a statistical dynamics model as a motion prior and user constraints, such as keyframes, to generate motion. <ref type="bibr" target="#b39">Min et al. [2009]</ref> use deformable motion models and optimize the deformable parameters at run-time given the MAP framework. Other statistical models, such as Gaussian Processes <ref type="bibr" target="#b38">[Min and Chai 2012]</ref> and Gaussian Process Latent Variable Models <ref type="bibr" target="#b16">[Grochow et al. 2004;</ref><ref type="bibr" target="#b32">Levine et al. 2012;</ref><ref type="bibr" target="#b53">Wang et al. 2008;</ref><ref type="bibr" target="#b55">Ye and Liu 2010]</ref> have been applied to the constrained motion control task, but are often limited by heavy run-time computations and memory requirements that still scale with the size of the motion database. As a result, these are often applied to separate types of motions and combined together with some post-process, limiting the expressiveness of the systems.</p><p>Deep neural networks can circumvent these limitations by allowing huge, heterogeneous datasets to be used for training, while having a fixed computation budget at run-time. <ref type="bibr" target="#b23">Holden et al. [2016;</ref> use feed-forward convolutional neural networks to build a constrained animation synthesis framework that uses root trajectory or end-effectors' positions as control signals. Online control from a gamepad has also been tackled with phase-aware <ref type="bibr" target="#b22">[Holden et al. 2017]</ref>, mode-aware <ref type="bibr" target="#b57">[Zhang et al. 2018</ref>] and action-aware <ref type="bibr" target="#b47">[Starke et al. 2019</ref>] neural networks that can automatically choose a mixture of network weights at run-time to disambiguate possible motions. Recurrent Neural Networks (RNNs) on the other hand keep an internal memory state at each timestep that allows them to perform naturally such disambiguation, and are very well suited for modeling time series. <ref type="bibr" target="#b29">Lee et al. [2018]</ref> train an RNN for interactive control using multiple control signals. These approaches <ref type="bibr" target="#b22">[Holden et al. 2017</ref><ref type="bibr" target="#b23">[Holden et al. , 2016</ref><ref type="bibr" target="#b29">Lee et al. 2018;</ref><ref type="bibr" target="#b57">Zhang et al. 2018</ref>] rely on spatially or temporally dense signals to constrain the motion and thus reduce ambiguity. In our system, a character might have to precisely reach a temporally distant keyframe without any dense spatial or temporal information provided by the user during the transition. The spatial ambiguity is mostly alleviated by the RNN's memory and the target-conditioning, while the timing ambiguity is resolved in our case by time-to-arrival embeddings added to the RNN inputs. Remaining ambiguity can be alleviated with generative adversarial training <ref type="bibr" target="#b14">[Goodfellow et al. 2014]</ref>, in which the motion generator learns to fool an additional discriminator network that tries to differentiate generated sequences from real sequences. <ref type="bibr" target="#b2">Barsoum et al. [2018]</ref> and <ref type="bibr" target="#b17">Gui et al. [2018]</ref> both design new loss functions for human motion prediction, while also using adversarial losses using different types of discriminators. These losses help reduce artifacts that may be produced by generators that average different modes of the plausible motions' distribution.</p><p>Motion control has also been addressed with Reinforcement Learning (RL) approaches, in which the problem is framed as a Markov Decision Process where actions can correspond to actual motion clips <ref type="bibr" target="#b28">[Lee and Lee 2006;</ref><ref type="bibr" target="#b51">Treuille et al. 2007]</ref> or character states <ref type="bibr" target="#b30">[Lee et al. 2010</ref>], but again requiring the motion dataset to be loaded in memory at run-time. Physically-based control gets rid of this limitation by having the output of the system operate on a physically-driven character. <ref type="bibr" target="#b10">Coros et al. [2009]</ref> employ fitted value iteration with actions corresponding to optimized Proportional-Derivative (PD) controllers proposed by <ref type="bibr" target="#b56">Yin et al. [2007]</ref>. These RL methods operate on value functions that have discrete domains, which do not represent the continuous nature of motion and impose run-time estimations through interpolation.</p><p>Deep RL methods, which use neural networks as powerful continuous function approximators have recently started being used to address these limitations. Peng et al.</p><p>[2017] apply a hierarchical actor-critic algorithm that outputs desired joint angles for PDcontrollers. Their approach is applied on a simplified skeleton and does not express human-like quality of movement despite their style constraints. Imitation-learning based RL approaches <ref type="bibr" target="#b1">[Baram et al. 2016;</ref><ref type="bibr" target="#b20">Ho and Ermon 2016]</ref> try to address this with adversarial learning, while others tackle the problem by penalizing distance of a generated state from a reference state <ref type="bibr" target="#b4">[Bergamin et al. 2019;</ref><ref type="bibr" target="#b42">Peng et al. 2018]</ref>. Actions as animation clips, or control fragments <ref type="bibr" target="#b34">[Liu and Hodgins 2017]</ref> can also be used in a deep-RL framework with Q-learning to drive physically-based characters. These methods show impressive results for characters having physical interactions with the world, while still being limited to specific skills or short cyclic motions. We operate in our case in the kinematics domain and train on significantly more heterogeneous motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motion Prediction</head><p>We limit here the definition of motion prediction to generating unconstrained motion continuation given single or multiple frames of animation as context. This task implies learning a powerful motion dynamics model which is useful for transition generation. Neural networks have shown over the years to excel in such representation learning. Early work from Taylor et al. <ref type="bibr">[2007]</ref> using Conditional Restricted Boltzmann Machines showed promising results on motion generation by sampling at each timestep the next frame of motion conditioned on the current hidden state and previous frames. More recently, many RNN-based approaches have been proposed for motion prediction from a past-context of several frames, motivated by the representational power of RNNs for temporal dynamics. <ref type="bibr">Fragkiadki et al. [2015]</ref> propose to separate spatial encoding and decoding from the temporal dependencies modeling with the Encoder-Recurrent-Decoder (ERD) networks, while <ref type="bibr" target="#b25">Jain et al. [2016]</ref> apply structural RNNs to model human motion sequences represented as spatio-temporal graphs. Other recent approaches <ref type="bibr">[Chiu et al. 2019;</ref><ref type="bibr" target="#b15">Gopalakrishnan et al. 2019;</ref><ref type="bibr" target="#b35">Liu et al. 2019;</ref><ref type="bibr" target="#b37">Martinez et al. 2017;</ref><ref type="bibr" target="#b41">Pavllo et al. 2019;</ref><ref type="bibr" target="#b48">Tang et al. 2018]</ref> investigate new architectures and loss functions to further improve short-term and long-term prediction of human motion. Others <ref type="bibr" target="#b13">[Ghosh et al. 2017;</ref>] investigate ways to prevent divergence or collapsing to the average pose for long-term predictions with RNNs. In this work, we start by building a powerful motion predictor based on the state-of-the-art recurrent architecture for long-term prediction proposed by Chiu et al. <ref type="bibr">[2019]</ref>. We combine this architecture with the feed-forward encoders of Harvey et al. <ref type="bibr">[2018]</ref> applied to different parts of the input to allow our embedding modifiers to be applied on distinct parts of the inputs. In our case, we operate on joint-local quaternions for all bones, except for the root, for which we use quaternions and translations local to the last seed frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transition generation</head><p>We define transition generation as a type of control with temporally sparse spatial constraints, i.e. where large gaps of motion must be filled without explicit conditioning during the missing frames such as trajectory or contact information. This is related to keyframe or motion interpolation (e.g. <ref type="bibr" target="#b8">[Ciccone et al. 2019]</ref>), but our work extends interpolation in that the system allows for generating whole cycles of motion, which cannot be done by most key-based interpolation techniques, such a spline fitting. Pioneering approaches <ref type="bibr" target="#b9">[Cohen et al. 1996;</ref><ref type="bibr" target="#b54">Witkin and Kass 1988]</ref> on transition generation and interpolation used spacetime constraints and inverse kinematics to produce physically-plausible motion between keyframes. Work with probabilistic models of human motion have also been used for filling gaps of animation. These include the MAP optimizers of <ref type="bibr" target="#b7">Chai et al. [2007]</ref> and <ref type="bibr" target="#b39">Min et al. [2009]</ref>, the Gaussian process dynamical models from Wang et al. <ref type="bibr">[2008]</ref> and Markov models with dynamic auto-regressive forests from Lehrmann et al. <ref type="bibr">[2014]</ref>. All of these present specific models for given action and actors. This can make combinations of actions look scripted and sequential. The scalability and expressiveness of deep neural networks has been applied to keyframe animation by <ref type="bibr" target="#b57">Zhang et al. [2018]</ref>, who use an RNN conditioned on key-frames to produce jumping motions for a simple 2D model. <ref type="bibr" target="#b18">Harvey et al. [2018]</ref> present Recurrent Transition Networks (RTN) that operate on a more complex human character, but work on fixed-lengths transitions with positional data only and are deterministic. We use the core architecture of the RTN as we make use of the separately encoded inputs to apply our latent modifiers. <ref type="bibr" target="#b19">Hernandez et al. [2019]</ref> recently applied convolutional adversarial networks to pose the problem of prediction or transition generation as an in-painting one, given the success of convolutional generative adversarial networks on such tasks. They also propose frequency-based losses to assess motion quality, but do not provide a detailed evaluation for the task of in-betweening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS 3.1 Data formatting</head><p>We use a humanoid skeleton that has = 28 joints when using the Human3.6M dataset and = 22 in the case of the LaFAN1 dataset. We use a local quaternion vector q of * 4 dimensions as our main data representation along with a 3-dimensional global root velocity vector r at each timestep . We also extract from the data, based on toes and feet velocities, contact information as a binary vector c of 4 dimensions that we use when working with the LaFAN1 dataset. The offset vectors o and o contain respectively the global root position's offset and local-quaternions' offsets from the target keyframe at time . Even though the quaternion offset could be expressed as a valid, normalized quaternion, we found that using simpler element-wise linear differences simplifies learning and yields better performance. When computing our positional loss, we reformat the predicted state into a global positions vector p +1 using q +1 , r +1 and the stored, constant local bone translations b by performing Forward Kinematics (FK). The resulting vector p +1 has * 3 dimensions. We also retrieve through FK the global quaternions vector g +1 , which we use for quantitatively evaluating transitions.</p><p>The discriminator use as input sequences of 3-dimensional vectors of global root velocities r, concatenated with x and x, the rootrelative positions and velocities of all other bones respectively. The vectors x and x both have ( ? 1) * 3 dimensions.</p><p>To simplify the learning process, we rotate each input sequence seen by the network around the axis (up) so that the root of the skeleton points towards the + axis on the last frame of past context. Each transition thus starts with the same global horizontal facing. We refer to rotations and positions relative to this frame as global in the rest of this work. When using the network inside a content creation software, we store the applied rotation in order to rotate back the generated motion to fit the context. Note however that this has no effect on the public dataset Human3.6M since root transformations are set to the identity on the first frame of any sequences, regardless of the actual world orientation. We also augment the data by mirroring the sequences over the + axes with a probability of 0.5 during training.  <ref type="bibr" target="#b41">[Pavllo et al. 2019]</ref>. It is also augmented with our latent space modifiers z tta and z target . Finally it also uses different losses, such as an adversarial loss for improved realism of the generated motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transition Generator</head><p>As seen in <ref type="figure" target="#fig_0">Figure 2</ref>, the generator has three different encoders that take the different data vectors described above as inputs; the character state encoder, the offset encoder, and the target encoder. The encoders are all fully-connected Feed-Forward Networks (FFN) with a hidden layer of 512 units and an output layer of 256 units. All layers use the Piecewise Linear Activation function (PLU) [Nicolae 2018], which performed slightly better than Rectified Linear Units (ReLU) in our experiments. The time-to-arrival embedding z has 256 dimensions and is added to the latent input representations. Offset and target embeddings h offset and h target are then concatenated and added to the 512-dimensional target-noise vector z target . Next, the three augmented embeddings are concatenated and fed as input to a recurrent Long-Short-Term-Memory (LSTM) layer. The embedding from the recurrent layer, h LSTM is then fed to the decoder, another FFN with two PLU hidden layers of 512 and 256 units respectively and a linear output layer. The resulting output is separated into local-quaternion and root velocities^ q +1 and^ r +1 to retrieve the next character state. When working with the LaFAN1 dataset, the decoder has four extra output dimensions that go through a sigmoid non-linearity to retrieve contact prediction? c +1 . The estimated quaternionsq +1 are normalized as valid unit quaternions and used along with the new root positionr +1 and the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Time-to-arrival embeddings</head><p>We present here our method to allow robustness to variable lengths of in-betweening. In order to achieve this, simply adding conditioning information about the target keyframe is insufficient since the recurrent layer must be aware of the number of frames left until the target must be reached. This is essential to produce a smooth transition without teleportation or stalling. Transformer networks <ref type="bibr" target="#b52">[Vaswani et al. 2017</ref>] are attention-based models that are increasingly used in natural language processing due to their state-of-theart modeling capacity. They are sequence-to-sequence models that do not use recurrent layers but require positional encodings that modify a word embedding to represent its location in a sentence. Our problem is also a sequence-to-sequence task where we translate a sequence of seed frames to a transition sequence, with additional conditioning on the target keyframe. Although our generator does use a recurrent layer, it needs time-to-arrival awareness in order to gracefully handle transitions of variable lengths. To this end, we use the mathematical formulation of positional encodings, that we base in our case on the time-to-arrival to the target:</p><formula xml:id="formula_0">z ,2 = basis 2 / (1) z ,2 +1 = basis 2 / (2)</formula><p>where tta is the number of timesteps until arrival and the second subscript of the vector z tta,_ represents the dimension index. The value is the dimensionality of the input embeddings, and ? [0, ..., /2]. The basis component influences the rate of change in frequencies along the embedding dimensions. It is set to 10,000 as in most transformer implementations.</p><p>Time-to-arrival embeddings thus provide continuous codes that will shift input representations in the latent space smoothly and uniquely for each transition step due to the phase and frequency shifts of the sinusoidal waves on each dimension. Such embedding is thus bounded, smooth and dense, three characteristics beneficial for learning. Its additive nature makes it harder for a neural network to ignore, as can be the case with concatenation methods. This follows the successful trend in computer vision <ref type="bibr" target="#b11">[Dumoulin et al. 2017;</ref><ref type="bibr" target="#b44">Perez et al. 2018</ref>] of conditioning through transformations of the latent space instead of conditioning with input concatenation. In these cases, the conditioning signals are significantly more complex and the affine transformations need to be learned, whereas <ref type="bibr" target="#b52">Vaswani et al. [2017]</ref> report similar performance when using this sine-based formulation as when using learned embeddings.</p><p>It is said that positional encodings can generalize to longer sequences in the natural language domain. However, since z tta evolves backwards in time to retrieve a time-to-arrival representation, generalizing to longer sequences becomes a more difficult challenge. Indeed, in the cases of Transformers (without temporal reversal), the first embeddings of the sequence are always the same and smoothly evolve towards new ones when generalizing to longer sequences. In our case, longer sequences change drastically the initial embedding seen and may thus generate unstable hidden states inside the recurrent layer before the transition begins. This can hurt performance on the first frames of transitions when extending the time-horizon after training. To alleviate this problem, we define a maximum duration in which we allow z tta to vary, and fix it past this maximum duration. Precisely, the maximum duration max (z tta ) is set to (trans) + ? 5, where max (trans) is the maximum transition length seen during training and past is the number of seed frames given before the transition. This means that when dealing with transitions of length max (trans), the model sees a constant z tta for 5 frames before it starts to vary. This allows the network to handle a constant z tta and to keep the benefits of this augmentation even when generalizing to longer transitions. Visual representations of z tta and the effects max (z tta ) are shown in Appendix A.2.</p><p>We explored simpler approaches to induce temporal awareness, such as concatenating a time-to-arrival dimension either to the inputs of the state encoder, or to the LSTM layer's inputs. This dimension is a single scalar increasing from 0 to 1 during the transition. Its period of increase is set to max (z tta ). Results comparing these methods with a temporally unaware network, and our use of z tta can be visualized in <ref type="figure">Figure 3</ref>. <ref type="figure">Fig. 3</ref>. Reducing the L2Q loss with z tta . We compare simple interpolation with our temporally unaware model (TG-Q) on the walking subset of Human 3.6M. We further test two strategies based on adding a single dimension either to the character state (TG-Q + input scalar) or the LSTM inputs (TG-Q + LSTM scalar). Finally, our use of time-to-arrival embeddings (TG-Q + z tta ) yields the best results, mostly noticeable at the end of transitions, where the generated motion is smoother than interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Scheduled target noise</head><p>Another contribution of this work is to improve robustness to keyframe modifications and to enforce diversity in the generated transitions given a fixed context. To do so we propose a scheduled target-distortion strategy. We first concatenate the encoded embeddings h offset and h target of the current offset vector and the target keyframe respectively. We then add to the resulting vector the target noise vector z target , sampled once per sequence from a spherical, zero-centered Gaussian distribution N (0, * target ).</p><p>The standard deviation target is an hyper-parameter controlling the level of accepted distortion. In order to produce smooth transitions to the target, we then define a target noise multiplier target , responsible for scaling down z target as the number of remaining timesteps goes down. We define a period of noise-free generation (5 frames) where target = 0 and a period of linear decrease of the target-noise (25 frames) to produce our noise-scale schedule. Beyond 30 frames before the target, the target-noise is therefore constant and target = 1.</p><formula xml:id="formula_1">target = ? ? ? ? ? ? ? ? ? 1 if tta ? 30 tta?5 25 if 5 ? tta &lt; 30 0 if tta &lt; 5<label>(3)</label></formula><p>Since this modifier is additive, it also corrupts time-to-arrival information, effectively distorting the timing information. This allows to modify the pace of the generated motion. Our target noise schedule is intuitively similar to an agent receiving a distorted view of its long-term target, with this goal becoming clearer as the agent advances towards it. This additive embedding modifier outperformed in our experiments another common approach in terms of diversity of the transitions while keeping the motion plausible. Indeed a widespread approach to conditional GANs is to use a noise vector z concat as additional input to the conditioned generator in order to enable stochasticity and potentially disambiguate the possible outcomes from the condition (e.g. avoid mode collapsing). However in highly constrained cases like ours, the condition is often informative enough to obtain good performance, especially at the beginning of the training. This leads to the generator learning to ignore the additional noise, as observed in our tests (see <ref type="figure">Figure 4</ref>). We thus force the transition generator to be stochastic by using z target to distort its view of the target and current offsets. <ref type="figure">Fig. 4</ref>. Increasing variability with z target . We compare z concat (left) against z target (right) midway in a 100-frames transition re-sampled 10 times. The generator successfully learns to ignore z concat while z target is imposed and leads to noticeable variations with controllable scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Motion Discriminators</head><p>A common problem with reconstruction-based losses and RNNs is the blurriness of the results, which is translated into collapse to the average motion and foot slides when predicting motion. The target keyframe conditioning can slightly alleviate these problems, but additional improvement comes from our use of adversarial losses, given by two discriminator networks. We use two variants of a relatively simple feed-forward architecture for our discriminators, or critics, C 1 and C 2 . Each discriminator has 3 fully-connected layers, with the last one being a 1D linear output layer. C 1 is a long-term critic that looks at sliding windows of 10 consecutive frames of motion and C 2 is the short-term critic and looks at windows of instant motion over 2 frames. Both critics have 512 and 256 units in their first and second hidden layers respectively. The hidden layers use ReLU activations. We average the discriminator scores over time in order to produce a single scalar loss. A visual summary of our sliding critics is presented in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Losses</head><p>In order to make the training stable and to obtain the most realistic results, we use multiple loss functions as complementary soft constraints that the neural network learns to respect.</p><p>3.6.1 Reconstruction Losses. All of our reconstruction losses for a predicted sequence^given its ground-truth are computed with the L1 norm:</p><formula xml:id="formula_2">1 ?1 ?? =0 ?q ? q ? 1 (4) root = 1 ?1 ?? =0 ?r ? r ? 1 (5) pos = 1 ?1 ?? =0 ?p ? p ? 1 (6) contacts = 1 ?1 ?? =0 ?? ? c ? 1 (7)</formula><p>where is the sequence length. The two main losses that we use are the local-quaternion loss quat and the root-position loss root . The former is computed over all joints' local rotations, including the root-node, which in this case also determines global orientation. The latter is responsible for the learning of the global root displacement. As an additional reconstruction loss, we use a positional-loss pos that is computed on the global position of each joints retrieved through FK. In theory, the use of pos isn't necessary to achieve a perfect reconstruction of the character state when using quat and root , but as noted by <ref type="bibr" target="#b41">Pavllo et al. [2019]</ref>, using global positions helps to implicitly weight the orientation of the bone's hierarchy for better results. As we will show in Section 4.2, adding this loss indeed improves results on both quaternion and translation reconstructions. Finally, in order to allow for runtime Inverse-Kinematics correction (IK) of the legs inside an animation software, we also use a contact prediction loss contacts , between predicted contacts? and true contacts c . We use the contact predictions at runtime to indicate when to perform IK on each leg. This loss is used only for models trained on the LaFAN1 dataset and that are deployed in our MotionBuilder plugin.</p><p>3.6.2 Adversarial Losses. We use the Least Square GAN (LSGAN) formulation <ref type="bibr" target="#b36">[Mao et al. 2017]</ref>. As our discriminators operate on sliding windows of motion, we average their losses over time. Our LSGAN losses are defined as follows:</p><formula xml:id="formula_3">gen = 1 2 E X p ,X f ? [( (X p , ( p , X f ), X f ) ? 1) 2 ], (8) disc = 1 2 E X p ,X trans ,X f ? [( (X p , X trans , X f ) ? 1) 2 ] + 1 2 E X p ,X f ? [( (X p , ( p , X f ), X f )) 2 ],<label>(9)</label></formula><p>where X p , X f , and X trans represent the past context, target state, and transition respectively, in the discriminator input format described in Section 3.1. is the transition generator network. Both discriminators use the same loss, with different input sequence lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Training</head><p>3.7.1 Progressive growing of transitions. In order to accelerate training, we adopt a curriculum learning strategy with respect to the transition lengths. Each training starts at the first epoch with min = max = 5, where min and?m ax are the minimal and current maximal transition lengths. During training, we increase?m ax until it reaches the true maximum transition length max . The increase rate is set by number of epochs ep?max by which we wish to have reached?m ax = max . For each minibatch, we sample uniformly the current transition length between min and?m ax , making the network train with variable length transitions, while beginning the training with simple tasks only. In our experiments, this leads to similar results as using any teacher forcing strategy, while accelerating the beginning of training due to the shorter batches. Empirically, it also outperformed gradient clipping. At evaluation time, the transition length is fixed to the desired length.</p><p>3.7.2 Sliding critics. In practice, our discriminators are implemented as 1D temporal convolutions, with strides of 1, without padding, and with receptive fields of 1 in the last 2 layers, yielding parallel feed-forward networks for each motion window in the sequence. 3.7.3 Hyperparameters. In all of our experiments, we use minibatches of 32 sequences of variable lengths as explained above. We use the AMSgrad optimizer <ref type="bibr" target="#b45">[Reddi et al. 2018</ref>] with a learning rate of 0.001 and adjusted parameters ( 1 = 0.5, 2 = 0.9) for increased stability. We scale all of our losses to be approximately equal on the LaFAN1 dataset for an untrained network before tuning them with custom weights. In all of our experiments, these relative weights (when applicable) are of 1.0 for quat and root , 0.5 for pos , and 0.1 for gen and contacts . The target noise's standard deviation target is 0.5. In experiments on Human3.6M, we set ep?max to 5 while it is set to 3 on the larger LaFAN1 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS 4.1 Motion prediction</head><p>Based on recent advances in motion prediction, we first build a motion prediction network that yields state-of-the-art results. We evaluate our model on the popular motion prediction benchmark that uses the Human 3.6M dataset. We follow the evaluation protocol defined by <ref type="bibr" target="#b25">Jain et al. [2016]</ref> that we base on the code from <ref type="bibr" target="#b37">Martinez et al. [2017]</ref>. We train the networks for 40,500 iterations before evaluation. We use the core architecture of Harvey et al. <ref type="bibr">[2018]</ref> since their separate encoders allow us to apply our embedding modifiers. In the case of unconstrained prediction however, this is more similar to the Encoder-Recurrent-Decoder (ERD) networks from <ref type="bibr" target="#b12">Fragkiadaki et al. [2015]</ref>. We also apply the velocity-based input representation of <ref type="bibr">Chiu et al. [2019]</ref> which seems to be a key component to improve performance. This is empirically shown in our experiments for motion prediction, but as we will see in Section 4.2, it doesn't hold for transition generation, where the character state as an input is more informative than velocities to produce correct transitions, evaluated on global angles and positions. Another difference lies in our data representation, which is based on quaternions instead of exponential maps. We call our architecture for motion prediction ERD-Quaternion Velocity network (ERD-QV). This model is therefore similar to the one depicted in <ref type="figure" target="#fig_0">Figure 2</ref>, with quaternions velocities q as only inputs of the state encoder instead of q and r , and without the two other encoders and their inputs. No embedding modifier and no FK are used in this case, and the only loss used is the L1 norm on joint-local quaternions. In this evaluation, the root transform is ignored, to be consistent with previous works.</p><p>In <ref type="table">Table 1</ref>, we compare this model with the TP-RNN which obtains to our knowledge state-of-the-art results for Euler angle differences. We also compare with two variants of the VGRU architecture proposed by <ref type="bibr" target="#b15">Gopalakrishnan et al. [2019]</ref>, who propose a novel Normalized Power Spectrum Similarity (NPSS) metric for motion <ref type="table">Table 1</ref>. Unconstrained motion prediction results on Human 3.6M. The VGRU-d/rl models are from <ref type="bibr" target="#b15">[Gopalakrishnan et al. 2019</ref>]. The TP-RNN is from <ref type="bibr">[Chiu et al. 2019</ref>] and has to our knowledge the best published results on motion prediction for this benchmark. Our model, ERD-QV is competitive with the state-of-the-art on angular errors and improves performance with respect to the recently proposed NPSS metric on all actions.  prediction that is more correlated to human assessment of quality for motion. Note that in most cases, we improve upon the TP-RNN for angular errors and perform better than the VGRU-d proposed by <ref type="bibr" target="#b15">Gopalakrishnan et al. [2019]</ref> on their proposed metric. This allows us to confirm the performance of our chosen architecture as the basis of our transition generation model.</p><formula xml:id="formula_4">- - VGRU-d - - - - - -0.1170 - - - - - -0.1210 - - - - - -0.0840 - - - - - -0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Walking in-betweens on Human 3.6M</head><p>Given our highly performing prediction architecture, we now build upon it to produce a transition generator (TG). We start off by first adding conditioning information about the future target and current offset to the target, and then sequentially add our proposed contributions to show their quantitative benefits on a novel transition benchmark. Even though the Human 3.6M dataset is one of the most used in motion prediction research, most of the actions it contains are ill-suited for long-term prediction or transitions (e.g. smoking, discussion, phoning, ...) as they consists of sporadic, random short movements that are impossible to predict beyond some short time horizons. We thus choose to use a subset of the Human 3.6M dataset consisting only of the three walk-related actions (walking, walkingdog, walkingtogether) as they are more interesting to test for transitions over 0.5 seconds long. Like previous studies, we work with a 25Hz sampling rate and thus subsample the original 50Hz data. The walking data subset has 55,710 frames in total and we keep Subject 5 as the test subject. The test set is composed of windows of motion regularly sampled every 10 frames in the sequences of Subject 5. Our test set thus contains 1419 windows with lengths that depend on the evaluation length. In order to evaluate robustness to variable lengths of transitions, we train the models on transitions of random lengths ranging from 0.2 to 2 seconds (5 to 50 frames) and evaluate on lengths going up to 4 seconds (100 frames) to also test generalization to longer time horizons. We train the models for 55,000 iterations, and report average L2 distances of global quaternions (L2Q) and global positions (L2P):</p><formula xml:id="formula_5">2 = 1 |D| 1 ?? ? D ?1 ?? =0 ? ? g 2 (10) 2 = 1 |D| 1 ?? ? D ?1 ?? =0 p ? p 2<label>(11)</label></formula><p>where is a transition sequence of the test set D, and is the transition length. Note that we compute L2P on normalized global positions using statistics from the training set. Precisely, we extract the global positions' statistics on windows of 70 frames 2 offset by 10 frames in which the motion has been centered around the origin on the horizontal plane. We center the motion by subtracting the mean of the root's XZ positions on all joints' XZ positions. We report the L2P metric as it is arguably a better metric than any angular loss for assessing visual quality of transitions with global displacements. However, it is not complete in that bone orientations might be wrong even with the right positions. We also report NPSS scores, which are based on angular frequency comparisons with the ground truth.</p><p>Our results are shown in <ref type="table" target="#tab_1">Table 2</ref>. Our first baseline consists of a naive interpolation strategy in which we linearly interpolate the root position and spherically interpolate the quaternions between the keyframes defining the transition. On the very short-term, this is an efficient strategy as motion becomes almost linear in sufficiently small timescales. We then compare transition generators that receive quaternion velocities as input (TG-QV) with one receiving normal quaternions (TG-Q) as depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. Both approaches have similar performance on short transitions, while TG-QV shows worst results on longer transitions. This can be expected with such a model that isn't given a clear representation of the character state at each frame. We thus choose TG-Q as our main baseline onto which we sequentially add our proposed modifications. We first add the global positional loss (+ pos ) as an additional training signal, which improves performance on most metrics and lengths. We then add the unconstrained time-to-arrival embedding modifier (+z tta ) and observe our most significant improvement. These effects on 50frames translations are summarized in <ref type="figure">Figure 3</ref>. Next, we evaluate the effects of our scheduled target embedding modifier z target . Note that it is turned off for quantitative evaluation. The effects are minor for transitions of 5 and 10 frames, but z target is shown to generally improve performances for longer transitions. We argue that these improvements come from the fact that this target noise probably helps generalizing to new sequences as it improves the model's robustness to new or noisy conditioning information. Finally, we obtain our complete model (TG complete ) by adding our adversarial loss gen , which interestingly not only improves the visual results of the generated motions, but also most of the quantitative scores. Qualitatively, enabling the target noise allows the model to produce variations of the same transitions, and it is trivial to control the level of variation by controlling target . We compare our approach to a simpler variant that also aims at inducing stochasticity in the generated transition. In this variant, we aim at potentially disambiguating the missing target information such as velocities by concatenating a random noise vector z concat to the target keyframe input q . This is similar to a strategy used in conditional GANs to avoid mode collapse given the condition. <ref type="figure">Figure 4</ref> and the accompanying video show typical results obtained with our technique against this more classical technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scaling up with the LaFAN1 dataset</head><p>Given our model selection based on the Human 3.6M walking benchmark discussed above, we further test our complete model on a novel, high quality motion dataset containing a wide range of actions, often with significant global displacements interesting for in-betweening compared to the Human3.6M dataset. This dataset contains 496,672 motion frames sampled at 30Hz and captured in a production-grade MOCAP studio. It contains actions performed by 5 subjects, with Subject 5 used as the test set. Similarly to the procedure used for the Human3.6M walking subset, our test set is made of regularly-sampled motion windows. Given the larger size of this dataset we sample our test windows from Subject 5 at every 40 frames, and thus retrieve 2232 windows for evaluation. The training statistics for normalization are computed on windows of 50 frames offset by 20 frames. Once again our starting baseline is a normal interpolation. We make public this new dataset along with accompanying code that allows one to extract the same training set and statistics as in this work, to extract the same test set, and to evaluate naive baselines (zero-velocity and interpolation) on this test set for our in-betweening benchmark. We hope this will facilitate future research and comparisons on the task of transition generation. We train our models on this dataset for 350,000 iterations on Subjects 1 to 4. We then go on to compare a reconstruction-based, futureconditioned Transition Generator (TG rec ) using quat , root , pos and contacts with our augmented adversarial Transition Generator (TG complete ) that adds our proposed embedding modifiers z tta , z tta and our adversarial loss gen . Results are presented in <ref type="table" target="#tab_2">Table 3</ref>. Our contributions improve performance on all quantitative measurements. On this larger dataset with more complex movements, our proposed in-betweeners surpass interpolation even on the very short transitions, as opposed to what was observed on the Human3.6M walking subset. This motivates the use of our system even on short time-horizons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Practical use inside an animation software</head><p>In order to also qualitatively test our models, we deploy networks trained on LaFAN1 in a custom plugin inside Autodesk's Motion-Builder, a widely used animation authoring and editing software. This enables the use of our model on user-defined keyframes or the generation of transitions between existing clips of animation. <ref type="figure" target="#fig_3">Figure  5</ref> shows an example scene with an incomplete sequence alongside our user interface for the plugin. The Source Character is the one from which keyframes are extracted while the generated frames are applied onto the Target Character's skeleton. In this setup it is trivial to re-sample different transitions while controlling the level of target noise through the Variation parameter. A variation of 0 makes the model deterministic. Changing the temporal or spatial location of the target keyframes and producing new animations is also trivial. Such examples of variations can be seen in <ref type="figure">Figure 6</ref>. The user can decide to apply IK guided by the network's contact predictions through the Enable IK checkbox. An example of the workflow and rendered results can be seen in the accompanying video.</p><p>The plugin with the loaded neural network takes 170MB of memory. <ref type="table" target="#tab_3">Table 4</ref> shows a summary of average speed performances of different in-betweening cases. This shows that an animator can use our tool to generate transition candidates almost for free when compared to manually authoring such transitions or finding similar motions in a motion database. On the left is a scene where the last seed frame and target keyframe are visible. On the right is our user interface for the plugin that allows, among other things, to specify the level of scheduled target noise for the next generation through the variation parameter, and to use the network's contact predictions to apply IK. On the bottom is the timeline where the gap of missing motion is visible. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION 5.1 Additive modifiers</head><p>We found our time-to-arrival and scheduled target noise additive modifiers to be very effective for robustness to time variations and for enabling sampling capabilities. We explored relatively simpler concatenation-based methods that showed worse performances. We hypothesize that concatenating time-to-arrival or noise dimensions is often less efficient because the neural network can learn to ignore those extra dimensions which are not crucial in the beginning of the training. Additive embedding modifiers however impose a shift in latent space and thus are harder to bypass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablated datasets</head><p>In order to gain some insights on the importance of the training set content, we trained two additional models with ablated versions of the LaFAN1 dataset. For the first one, we removed all dance training sequences (approximately 10% of the data). For the second one, we kept only those sequences, yielding a much smaller dataset (21.5 minutes). Results showed that keeping only the dance sequences yielded similar results as the bigger ablated dataset, but that the full dataset is necessary to generate transitions that stay in a dancing <ref type="bibr">Fig. 6</ref>. Three types of variations of a crouch-to-run transition. A single frame per generated transition is shown, taken at the same timestep. Semitransparent poses are the start and end keyframes. A: Temporal variations are obtained by changing the temporal location of the second keyframe. This relies on our time-to-arrival embeddings (z tta ). B: Spatial variations can be obtained by simply moving the target keyframe in space. C: Motion variation are obtained by re-sampling the same transition with our scheduled target noise (z target ) enabled. These results can also be seen in the accompanying video.</p><p>style. This indicates that large amounts of generic data can be as useful as much fewer specialized sequences for a task, but that combining both is key. An example is shown in the accompanying video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dropping the Triangular-Prism</head><p>When building our motion predictor ERD-QV, we based our input represention on velocities, as suggested with the TP-RNN architecture proposed by <ref type="bibr">[Chiu et al. 2019</ref>]. However, we did not witness any gains when using their proposed Triangular-Prism RNN (TP-RNN) architecture. Although unclear why, it might be due to the added depth of the network by adding in our case a feed-forward encoder, making the triangular prism architecure unnecessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Incompleteness of pos</head><p>Although we use FK for our positional loss pos as suggested by <ref type="bibr" target="#b41">Pavllo et al. [2019]</ref>, this loss isn't sufficient to produce fully defined character configurations. Indeed using only this loss may lead to the correct positions of the joints but offers no guarantee for the bone orientations, and led in our experiments to noticeable artifacts especially at the ends of the kinematic chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Recurrent cell types</head><p>Some recent works on motion prediction prefer Gated Recurrent Units (GRU) over LSTMs for their lower parameter count, but our empirical performance comparisons favored LSTMs over GRUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS AND FUTURE WORK</head><p>A more informative way of representing the current offset to the target o would be to include positional-offsets in the representation. For this to be informative however, it would need to rely on character-local or global positions, which require FK. Although it is possible to perform FK inside the network at every step of generation, the backward pass during training becomes prohibitively slow justifying our use of root offset and rotational offsets only.</p><p>As with many data-driven approaches, our method struggles to generate transitions for which conditions are unrealistic, or outside the range covered by the training set.</p><p>Our scheduled target noise allows us to modify to some extent the manner in which a character reaches its target, reminiscent of changing the style of the motion, but doesn't allow yet to have control over those variations. Style control given a fixed context would be very interesting but is out of scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work we first showed that state-of-the-art motion predictors cannot be converted into robust transition generators by simply adding conditioning information about the target keyframe. We proposed a time-to-arrival embedding modifier to allow robustness to transition lengths, and a scheduled target noise modifier to allow robustness to target keyframe variations and to enable sampling capabilities in the system. We showed how such a system allows animators to quickly generate quality motion between sparse keyframes inside an animation software. We also presented LaFAN1, a new high quality dataset well suited for transition generation benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 Sliding critics <ref type="bibr">Fig. 7</ref>. Visual summary of the two timescales critics. Blue frames are the given contexts and green frames correspond to the transition. First and last critic positions are shown without transparency. At the beginning and end of transitions, the critics are conditional in that they include groundtruth context in their input sequences. Scalar scores at each timestep are averaged to get the final score.</p><p>A.2 Time-to-arrival embedding visualization shows the effect of using max (z tta ), which in practice improves performances when generalizing to longer transitions as it prevents initializing the LSTM hidden state with novel embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>presents a visual depiction of our recurrent generator for a single timestep. It uses the same input separation used by the RTN network [Harvey and Pal 2018], but operates on angular data and uses FK in order to retrieve global positions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the TG complete architecture for in-betweening. Computations for a single timestep are shown. Visual concatenation of input boxes or arrows represents vector concatenation. Green boxes are the jointly trained neural networks. Dashed boxes represent our two proposed embedding modifiers. The "quat norm" and "FK" red boxes represent the quaternion normalization and Forward Kinematics operations respectively. The ? sign represents element-wise addition and is the sigmoid nonlinearity. Outputs are linked to associated losses with dashed lines. constant bone offsets b to perform FK and retrieve the new global positionsp +1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(ours) 0.20 0.34 0.56 0.64 0.72 0.79 0.0767 0.18 0.33 0.53 0.63 0.78 1.17 0.0763 0.23 0.47 0.96 0.99 0.98 1.58 0.0537 0.23 0.59 0.86 0.93 1.30 1.75 0.1201</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Generating animations inside MotionBuilder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Visual depiction of time-to-arrival embeddings. Sub-figure (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Transition generation benchmark on Human 3.6M. Models were trained with transition lengths of maximum 50 frames, but are evaluated beyond this horizon, up to 100 frames (4 seconds).</figDesc><table><row><cell>L2Q</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Improving in-betweening on the LaFAN1 dataset. Models were trained with transition lengths of maximum 30 frames (1 second), and are evaluated on 5, 15, 30, and 45 frames.</figDesc><table><row><cell>L2Q</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Speed performance summary of our MotionBuilder plugin. The model inference also includes the IK postprocess. The last column indicates the time taken to produce a string of 10 transitions of 30 frames. Everything is run on a Intel Xeon CPU E5-1650 @ 3.20GHz.</figDesc><table><row><cell>Transition time (s)</cell><cell cols="2">0.50 1.00 2.00 10 x 1.00</cell></row><row><cell cols="2">Keyframe extraction (s) 0.01 0.01 0.01</cell><cell>0.01</cell></row><row><cell>Model inference (s)</cell><cell>0.30 0.31 0.31</cell><cell>0.40</cell></row><row><cell cols="2">Applying keyframes (s) 0.72 1.05 1.65</cell><cell>6.79</cell></row><row><cell>Total (s)</cell><cell>1.03 1.37 1.97</cell><cell>7.20</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ubisoftinc/Ubisoft-LaForge-Animation-Dataset ACM Trans. Graph., Vol. 39, No. 4, Article 60. Publication date: July 2020.Robust Motion In-betweening ? 60:3</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 39, No. 4, Article 60. Publication date: July 2020.Robust Motion In-betweening ? 60:5</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 39, No. 4, Article 60. Publication date: July 2020. 60:6 ? Harvey et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We train with transitions of maximum lengths of 50 frames, plus 10 seed frames and 10 frames of future context to visually assess the motion continuation, from which the first frame is the target keyframe. This yields windows of 70 frames in total. ACM Trans. Graph., Vol. 39, No. 4, Article 60. Publication date: July 2020.Robust Motion In-betweening ? 60:9</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Ubisoft Montreal, the Natural Sciences and Engineering Research Council of Canada and Mitacs for their support. We also thank Daniel Holden, Julien Roy, Paul Barde, Marc-Andr? Carbonneau and Olivier Pomarez for their support and valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive motion generation from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="483" to="490" />
			<date type="published" when="2002" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02179</idno>
		<title level="m">Model-based Adversarial Imitation Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">HP-GAN: Probabilistic 3D human motion prediction via GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1418" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Motionmotif graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Beaudoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stelian</forename><surname>Coros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Michiel Van De Panne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. Eurographics Association</title>
		<meeting>the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. Eurographics Association</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DReCon: data-driven responsive control of physics-based characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bergamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Clavet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Richard</forename><surname>Forbes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Motion Matching -The Road to Next Gen Animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>B?ttner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Clavet</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=z_wpgHFSWss&amp;t=658s" />
	</analytic>
	<monogr>
		<title level="m">Proc. of Nucl.ai 2015</title>
		<meeting>of Nucl.ai 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Performance animation from lowdimensional control signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="686" to="696" />
			<date type="published" when="2005" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Constraint-based motion optimization using a statistical dynamic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<editor>8. Hsu-kuang Chiu, Ehsan Adeli, Borui Wang, De-An Huang, and Juan Carlos Niebles</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1423" to="1432" />
		</imprint>
	</monogr>
	<note>Action-agnostic human pose forecasting</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tangent-space Optimization for Interactive Animation Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>?ztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Sumner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3306346.3322938</idno>
		<ptr target="https://doi.org/10.1145/3306346.3322938" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient Generation of Motion Transitions Using Spacetime Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Guenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobby</forename><surname>Bodenheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Rose</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/efficient-generation-of-motion-transitions-using-spacetime-constraints/" />
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96 (siggraph 96 ed.)</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust task-based control policies for physics-based characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stelian</forename><surname>Coros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Beaudoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Learned Representation For Artistic Style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1610.07629" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02827</idno>
		<title level="m">Emre Aksan, and Otmar Hilliges. 2017. Learning Human Motion Models for Long-term Predictions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A neural temporal model for human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Mali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12116" to="12125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stylebased inverse kinematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Grochow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on graphics (TOG)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial geometry-aware human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Liang-Yan Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="786" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent transition networks for character locomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>F?lix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human Motion Prediction via Spatio-Temporal Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust solving of optical motion capture data by denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">165</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Phase-functioned neural networks for character control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deep learning framework for character motion synthesis and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning motion manifolds with convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joyce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2015 Technical Briefs</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structural-RNN: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Motion graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2008 classes</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interactive control of avatars animated with human motion data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jehee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Reitsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">S</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="491" to="500" />
			<date type="published" when="2002" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Precomputing avatar behavior from human motion data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jehee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang Hoon Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical Models</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="174" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interactive character animation by learning multi-objective control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jehee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Motion fields for interactive character locomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wampler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilbert</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovan</forename><surname>Popovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2010" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient nonlinear markov models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andreas M Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1314" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Continuous character control with low-dimensional embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Haraux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Popovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjiu</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05363</idno>
		<title level="m">Conditioned LSTM Network for Extended Complex Human Motion Synthesis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to schedule control fragments for physicsbased characters using deep q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards Natural and Accurate Future Motion Prediction of Humans and Animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2891" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Motion graphs++: a compact generative model for semantic motion analysis and synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interactive generation of human animation with deformable motion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">PLU: The Piecewise Linear Unit Activation Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Nicolae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09534</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling Human Motion with Quaternion-Based Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 2018 -to appear)</title>
		<meeting>SIGGRAPH 2018 -to appear)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangkang</forename><surname>Berseth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the Convergence of Adam and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Construction and optimal search of interpolated motion graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Safonova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2007" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural state machine for character-scene interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Long-term human motion prediction by modeling motion context and enhancing motion dynamic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weishi</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02513</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Motion reconstruction using sparse accelerometer data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Tautges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Zinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Helten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meinard</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Eberhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Near-optimal character animation with continuous control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Treuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spacetime Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kass</surname></persName>
		</author>
		<idno type="DOI">10.1145/54852.378507</idno>
		<ptr target="https://doi.org/10.1145/54852.378507" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;88)</title>
		<meeting>the 15th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;88)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Synthesis of responsive motion using a dynamic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="555" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mode-Adaptative Neural Networks for Quadruped Motion Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangkang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Loken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM, 105. He Zhang, Sabastian Starke, Taku Komura, and Jun Saito</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>ACM Transactions on Graphics (TOG)</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Data-driven autocompletion for keyframe animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual International Conference on Motion, Interaction, and Games</title>
		<meeting>the 11th Annual International Conference on Motion, Interaction, and Games</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
