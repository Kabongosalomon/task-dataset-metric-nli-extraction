<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">&quot;Tune It or Don&apos;t Use It: Benchmarking Data-Efficient Image Classification Tune It or Don&apos;t Use It: Benchmarking Data-Efficient Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021">2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Brigato</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Luca Iocchi</roleName><forename type="first">Bj?rn</forename><surname>Barz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">&amp;quot;</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Brigato</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Barz</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Friedrich Schiller University Jena</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Iocchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Friedrich Schiller University Jena</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">&quot;Tune It or Don&apos;t Use It: Benchmarking Data-Efficient Image Classification Tune It or Don&apos;t Use It: Benchmarking Data-Efficient Image Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
						<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops						</meeting>
						<imprint>
							<date type="published" when="2021">2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data-efficient image classification using deep neural networks in settings, where only small amounts of labeled data are available, has been an active research area in the recent past. However, an objective comparison between published methods is difficult, since existing works use different datasets for evaluation and often compare against untuned baselines with default hyper-parameters. We design a benchmark for data-efficient image classification consisting of six diverse datasets spanning various domains (e.g., natural images, medical imagery, satellite data) and data types (RGB, grayscale, multispectral). Using this benchmark, we re-evaluate the standard cross-entropy baseline and eight methods for data-efficient deep learning published between 2017 and 2021 at renowned venues. For a fair and realistic comparison, we carefully tune the hyper-parameters of all methods on each dataset. Surprisingly, we find that tuning learning rate, weight decay, and batch size on a separate validation split results in a highly competitive baseline, which outperforms all but one specialized method and performs competitively to the remaining one.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many recent advances in computer vision and machine learning in general have been achieved by large-scale pretraining on massive datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref>. As the amount of data grows, the importance of methodological advances vanishes. With the number of training samples approaching infinity, a simple k-nearest neighbor classifier provides optimal performance <ref type="bibr" target="#b28">[29]</ref>. The true hallmark of intelligence is, therefore, the ability of learning generalizable concepts from limited amounts of data.</p><p>The research area of deep learning from small data or data-efficient deep learning has been receiving increasing <ref type="bibr">*</ref>   <ref type="figure">Figure 1</ref>: Classification accuracy obtained with standard cross-entropy on ciFAIR-10 with 1% of the training data for different combinations of learning rate and weight decay. Gray configurations led to divergence. interest in the past couple of years <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. However, an objective comparison of proposed methods is difficult due to the lack of a common benchmark. Even if two works use the same dataset for evaluation, their random sub-samples of this dataset for simulating a small-data scenario will be different and not directly comparable.</p><p>Fortunately, there recently have been activities to establish common benchmarks and organize challenges to foster direct competition between proposed methods <ref type="bibr" target="#b5">[6]</ref>. Still, they are often limited to a single dataset, e.g., ImageNet <ref type="bibr" target="#b23">[24]</ref>, which comprises a different type of data than usually encountered in a small-data scenario.</p><p>Moreover, most existing works compare their proposed method against insufficiently tuned baselines <ref type="bibr" target="#b0">[1]</ref> or baselines trained with default hyper-parameters <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b13">14]</ref>, which makes it easy to outperform them. However, careful tuning of hyper-parameters, as one would do in practice, is crucial and can have a considerable impact on the final performance <ref type="bibr" target="#b3">[4]</ref>, as illustrated in <ref type="figure">Fig. 1</ref>. Here, we evaluated the performance of several combinations of learning rate and weight decay for a standard cross-entropy clas-sifier with a Wide ResNet architecture <ref type="bibr" target="#b33">[34]</ref> trained on as few as 1% of the CIFAR-10 training data <ref type="bibr" target="#b14">[15]</ref> and evaluated on the ciFAIR-10 test set <ref type="bibr" target="#b1">[2]</ref> (see Section 4 for details on the training procedure). Typical default hyperparameters such as a learning rate of 0.1 and weight decay of 1 ? 10 ?4 as used by <ref type="bibr" target="#b5">[6]</ref> would achieve 46% accuracy in this scenario, which is entire 12 percentage points below the optimal performance of 58%. Even works that do perform hyper-parameter tuning often only optimize the learning rate and keep the weight decay fixed to some default from [1 ? 10 ?5 , 1 ? 10 ?4 ]. Such a procedure results in similarly suboptimal performance on this small training dataset, which apparently requires much stronger regularization. We can furthermore observe that the best performing hyper-parameter combinations are close to an area of the search space that results in divergence of the training procedure. This makes hyper-parameter optimization a particularly delicate endeavor.</p><p>In this work, we establish a direct, objective, and informative comparison by re-evaluating the state of the art in data-efficient image classification. To this end, we introduce a comprehensive benchmark consisting of six datasets from a variety of domains: natural images of everyday objects, fine-grained classification, medical imagery, satellite images, and handwritten documents. Two datasets consist of non-RGB data, where the common large-scale pretraining and fine-tuning procedure is not straightforward, emphasizing the need for methods that can learn from limited amounts of data from scratch. To facilitate evaluating novel methods, we share the dataset splits of our benchmark under https://github.com/cvjena/deic.</p><p>Using this benchmark, we re-evaluate eight selected state-of-the-art methods for data-efficient image classification. The hyper-parameters of all methods are carefully optimized for each dataset individually on a validation split, while the final performance is evaluated on a separate test split. Surprisingly and somewhat disillusioning, we find that thorough hyper-parameter optimization results in a strong baseline, which outperforms seven of the eight specialized methods published in the recent literature.</p><p>In the following, we first introduce the datasets constituting our benchmark in Section 2. Then, we briefly describe the methods selected for the comparison in Section 3. Our experimental setup and training procedure are detailed in Section 4 and the results are presented in Section 5. Section 6 summarizes the conclusions from our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Datasets</head><p>Most works on deep learning from small datasets use custom sub-sampled versions of popular standard image classification benchmarks such as ImageNet <ref type="bibr" target="#b23">[24]</ref> or CIFAR <ref type="bibr" target="#b14">[15]</ref>. This limited variety bears the risk of overfitting research progress to individual datasets and the domain cov-ered by them, in this case, photographs of natural scenes and everyday objects. In particular, this is not the domain typically dealt with in a small-data scenario, where specialized data that is difficult to obtain or annotate is in the focus. Additionally, very recent work showed that high performance on ImageNet does not necessarily correlate to high performance on other vision datasets <ref type="bibr" target="#b29">[30]</ref>.</p><p>Therefore, we compile a diverse benchmark consisting of six datasets from a variety of domains and with different data types and numbers of classes. We sub-sampled all datasets to fit the small-data regime, with the exception of CUB <ref type="bibr" target="#b32">[33]</ref>, which was already small enough. By default, we aimed for 50 training images per class. This full trainval split is only used for the final training and furthermore split into a training ( 60%) and a validation set ( 40%) for hyper-parameter optimization. For testing the final models trained on the trainval split, we used official standard test datasets where they existed. Only for two datasets, namely EuroSAT <ref type="bibr" target="#b11">[12]</ref> and ISIC 2018 <ref type="bibr" target="#b6">[7]</ref>, we had to create own test splits. A summary of the dataset statistics is given in Table 1. In the following, we briefly describe each individual dataset used for our benchmark. Example images from all datasets are shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>ImageNet-1k <ref type="bibr" target="#b23">[24]</ref> has been the standard benchmark for image classification for almost a decade now and also served as a basis for challenge datasets for data-efficient image classification <ref type="bibr" target="#b5">[6]</ref>. It comprises images from 1,000 classes of everyday objects and natural scenes collected from the web using image search engines. Due to the large number of classes, a sub-sample of 50 images per class still results in a rather large training dataset compared with the rest of our benchmark. ciFAIR-10 [2] is a variant of the popular CIFAR-10 dataset <ref type="bibr" target="#b14">[15]</ref>, which comprises low-resolution images of size 32 ? 32 from 10 different classes of everyday objects. To a large part, its popularity stems from the fact that the low image resolution allows for fast training of neural networks and hence rapid experimentation. However, the test dataset of CIFAR-10 contains about 3.3% duplicates from the training set <ref type="bibr" target="#b1">[2]</ref>, which can potentially bias the evaluation. The ciFAIR-10 dataset <ref type="bibr" target="#b1">[2]</ref> provides a variant of the test set, where these duplicates have been replaced with new images from the same domain.</p><p>Caltech-UCSD Birds-200-2011 (CUB) <ref type="bibr" target="#b32">[33]</ref> is a finegrained dataset of 200 bird species. Annotating this kind of images typically requires a domain expert and is hence costly. Therefore, the dataset is rather small and only comprises 30 training images per class. Pre-training on related large-scale datasets is hence the de-facto standard for CUB <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>  research on data-efficient methods closing the gap between training from scratch and pre-training.</p><p>EuroSAT <ref type="bibr" target="#b11">[12]</ref> is a multispectral image dataset based on Sentinel-2 satellite images of size 64x64 covering 13 spectral bands. Each image is annotated with one of ten land cover classes. This dataset does not only exhibit a substantial domain shift compared to standard pre-training datasets such as ImageNet but also a different number of input channels. This scenario renders the standard pre-training and fine-tuning procedure impossible. Nevertheless, Helber et al. <ref type="bibr" target="#b11">[12]</ref> adhere to this procedure by fine-tuning a CNN pre-trained on RGB images using different combinations of three out of the 13 channels of Eu-roSAT. Unsurprisingly, they find that the combination of the R, G, and B channel provides the best performance in this setting. This limitation to three channels due to pre-training is a waste of data and potential. In our experiments on a smaller subset of EuroSAT, we found that using all 13 channels increases the classification accuracy by 9.5% compared to the three RGB channels when training from scratch.</p><p>ISIC 2018 <ref type="bibr" target="#b6">[7]</ref> is a medical dataset consisting of dermoscopic skin lesion images, annotated with one of seven possible skin disease types. Since medical data usually requires costly expert annotations, this domain is important to be covered by a benchmark on data-efficient deep learning. Due to the small number of classes, we increase the number of images per class to 80 for this dataset, so that the size of the training set is more similar to our other datasets.</p><p>CLaMM <ref type="bibr" target="#b25">[26]</ref> is a dataset for Classification of Latin Medieval Manuscripts. It was originally used in the ICFHR 2016 Competition for Script Classification, where the task was to classify grayscale images of Latin scripts from handwritten books dated 500 C.E. to 1600 C.E. into one of twelve script style classes such as Humanistic Cursive, Praegothica etc. This domain is quite different from that of typical pre-training datasets such as ImageNet and one can barely expect any useful knowledge to be extracted from ImageNet about medieval documents. In addition, the standard pre-training and fine-tuning procedure would require the grayscale images to be converted to RGB for being passed through the pre-trained network, which incurs a waste of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we present the methods whose performance has been re-evaluated on our benchmark using the original code, where available. We selected approaches for which the authors performed experiments on sub-sampled versions of standard computer vision datasets to prove their effectiveness for learning from small datasets.</p><p>Cross-entropy loss is the widely used standard loss function for classification. We use it as a baseline with standard network architectures and optimization algorithms.</p><p>Deep hybrid networks (DHN) represent one of the first attempts to incorporate pre-defined geometric priors via a hybrid approach of combining pre-defined and learned representations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. According to the authors, decreasing the number of parameters to learn could make deep networks more data-efficient, especially in settings where the scarcity of data would not allow the learning of low-level feature extractors. Deep hybrid networks first perform a scattering transform on the input image generating feature maps and then apply standard convolutional blocks. The spatial scale of the scattering transform is controlled by the parameter J ? N.</p><p>Orthogonal low-rank embedding (OL?) is a geometric loss for deep networks that was proposed in <ref type="bibr" target="#b15">[16]</ref> to reduce intra-class variance and enforce inter-class margins. This method collapses deep features into a learned linear subspace, or union of them, and inter-class subspaces are pushed to be as orthogonal as possible. The contribution of the low-rank embedding to the overall loss is weighted by the hyper-parameter ? ole .</p><p>Grad-2 penalty is a regularization strategy tested in the context of improving generalization on small datasets in <ref type="bibr" target="#b2">[3]</ref>. The 2 (squared) gradient norm is computed with respect to the input samples and used as a penalty in the loss weighted by parameter ? grad . Among many regularization approaches evaluated in <ref type="bibr" target="#b2">[3]</ref>, we have chosen the grad-2 penalty because it was among the best performing methods in the experiments with ResNet and sub-sampled versions of CIFAR-10. Since the grad-2 penalty is proposed as an alternative to weight decay, we disable weight decay for this method. Moreover, differently from the original implementation, we enabled the use of batch normalization since, without this component, we obtained extremely low results in preliminary experiments.</p><p>Cosine loss was proposed in <ref type="bibr" target="#b0">[1]</ref> to decrease overfitting in problems with scarce data. Thanks to an 2 normalization of the learned feature space, the cosine loss is invariant against scaling of the network output and solely focuses on the directions of feature vectors instead of their magnitude. In contrast to the softmax function used with the cross-entropy loss, the cosine loss does not push the activations of the true class towards infinity, which is commonly considered as a cause of overfitting <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b10">11]</ref>. Moreover, a further increase of performance was obtained by combining the cosine with the cross-entropy loss after an additional layer on top of the embeddings learned with the cosine loss.</p><p>Harmonic networks (HN) use a set of preset filters based on windowed cosine transform at several frequencies which are combined by learnable weights <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Similar to hybrid networks, the idea of the harmonic block is to have a useful geometric prior that can help to avoid overfitting. Harmonic networks use Discrete Cosine Transform filters which have excellent energy compaction properties and are widely used for image compression.</p><p>Full convolution (F-Conv) was proposed in <ref type="bibr" target="#b12">[13]</ref> to improve translation invariance of convolutional filters. Standard CNNs exploit image boundary effects and learn filters that can exploit the absolute spatial locations of objects in images. In contrast, full convolution applies each value in the filter on all values in the image. According to <ref type="bibr" target="#b12">[13]</ref>, improving translation invariance strengthens the visual inductive prior of convolution, leading to increased data efficiency in the small-data setting.</p><p>Dual Selective kernel networks have been proposed and designed in <ref type="bibr" target="#b26">[27]</ref> to be more data-efficient. The standard residual block is modified, keeping the skip connection, with two forward branches that use 1 ? 1 convolutions, selective kernels <ref type="bibr" target="#b17">[18]</ref> and an anti-aliasing module. To further regularize training, only one of the two branches is randomly selected in the forward and backward passes, while at inference, the two paths are weighted equally.</p><p>Besides the specialized network architecture, the original work uses a combination of three custom loss functions <ref type="bibr" target="#b26">[27]</ref>. Despite best efforts, we were unable to derive the correct implementation from the ambiguous description of these loss functions in the paper. Therefore, we only use the DSK network architecture with cross-entropy loss.</p><p>T-vMF Similarity is a generalization of the cosine similarity that was recently presented in <ref type="bibr" target="#b13">[14]</ref> to make modern CNNs more robust to some realistic learning situations such as class imbalance, few training samples, and noisy labels. As the name suggests, this similarity is mainly based on the von Mises-Fisher distribution of directional statistics and built on top of the heavy-tailed student-t distribution. The combination of these two ingredients provides high compactness in high-similarity regions and low similarity in heavy-tailed ones. The degree of compactness/dispersion of the similarity is controlled by the parameter ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>In this section, we give an overview of the experimental pipeline that we followed for a fair evaluation of the aforementioned methods on the six datasets that constitute our benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation metrics</head><p>In our benchmark, we evaluate each method on each dataset with the widely used balanced classification accuracy. This metric is defined as the average per-class accuracy, i.e., the average of the diagonal in the row-normalized confusion matrix. We turned our attention toward this metric since some datasets in our benchmark do not have balanced test sets. In any case, for balanced test sets, the balanced accuracy equals the standard classification accuracy.</p><p>Since our benchmark contains multiple datasets it is hard to directly make a comparison between two methods without computing an overall ranking. Therefore, for each method, we also compute the average balanced accuracy across all datasets to provide a simple and intuitive way to rank methods. Additionally, in this manner, future methods will be easily comparable with those already evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data pre-processing and augmentation</head><p>All input images were normalized by subtracting the channel-wise mean and dividing by the standard deviation computed on the trainval split. We applied standard data augmentation policies with slightly varying configurations, adapted to the specific characteristics of each dataset and problem domain. Note that none of the currently reevaluated methods in our benchmark had as original contribution a specialized data augmentation technique. Nothing prevents the use of an augmentation-based method from partaking in the benchmark.</p><p>For datasets with a small, fixed image resolution, i.e., ciFAIR-10 and EuroSAT, we perform random shifting by 12.5% of the image size and horizontal flipping in 50% of the cases. For all other datasets, we apply scale augmentation using the RandomResizedCrop transform from Py-Torch 1 as follows: A crop with a random aspect ratio drawn from [ 3 4 , <ref type="bibr">4 3</ref> ] and an area between A min and 100% of the original image area is extracted from the image and then resized to 224?224 pixels. The minimum fraction A min of the area was determined based on preliminary experiments to ensure that a sufficient part of the image remains visible. It therefore varies depending on the dataset: We use A min = 10% for ImageNet, A min = 20% for CLaMM and A min = 40% for CUB and ISIC 2018.</p><p>For ISIC 2018 and EuroSAT, we furthermore perform random vertical flipping in addition to horizontal flipping, since these datasets are completely rotation-invariant and vertical reflection augments the training sets without drifting them away from the test distributions. On CLaMM, in contrast, we do not perform any flipping, since handwritten scripts are not invariant even against horizontal flipping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Architecture and optimizer</head><p>To perform a fair comparison, we use the same backbone CNN architecture for all methods. More precisely, for ciFAIR-10, we employ a Wide Residual Network (WRN) <ref type="bibr" target="#b33">[34]</ref>, precisely WRN-16-8, which is widely used in the existing literature for data-efficient classification on CI-FAR. For all other cases, the popular and well-established ResNet-50 (RN50) architecture <ref type="bibr" target="#b9">[10]</ref> is used. Note that we made changes to the architecture when that was an original contribution of the paper, but all those changes were applied to the selected base architecture. Due to the high popularity of residual networks, the majority of the selected approaches were originally tested with a RN/WRN backbone. This fact allowed us to perform a straightforward porting of the network setup, when necessary.</p><p>We furthermore employ a common optimizer and training schedule across all methods and datasets to avoid any kind of optimization bias. We use standard stochastic gradient descent (SGD) with a momentum of 0.9 and weight decay and a cosine annealing learning rate schedule <ref type="bibr" target="#b19">[20]</ref>, which reduces the learning rate smoothly during the training process. The initial learning rate and the weight decay factor are optimized for each method and dataset individually together with any method-specific hyper-parameters as detailed in the next subsection. The total number of training epochs for each dataset was chosen according to preliminary experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Hyper-parameter optimization</head><p>As we have discussed in Section 1 and shown in <ref type="figure">Fig. 1</ref>, the choice of hyper-parameters has a substantial effect on the classification performance that should in no case be underestimated. Careful hyper-parameter optimization (HPO) <ref type="bibr" target="#b3">[4]</ref> is therefore not only crucial for applying deep learning techniques in practice but also for a fair comparison between different methods, so that each can obtain its optimal performance. Comparing against an untuned baseline with default hyper-parameters is as good as no comparison at all.</p><p>For our benchmark, we hence first tune the hyperparameters of each method on each individual dataset using a training and a validation split, which are disjoint from the test set used for final performance evaluation (see Section 2). For any method, we tune the initial learning rate and weight decay, sampled from a log-uniform space, as well as the batch size, chosen from a pre-defined set. Details about the search space are provided in <ref type="table" target="#tab_3">Table 2</ref>. In addition to these general hyper-parameters, any method-specific hyper-parameters are tuned as well simultaneously, considering the boundaries used in the original paper, if applicable, or lower and upper bounds estimated by ourselves.</p><p>For selecting hyper-parameters to be tested and scheduling experiments, we employ Asynchronous HyperBand with Successive Halving (ASHA) <ref type="bibr" target="#b16">[17]</ref> as implemented in the Ray library 2 . This search algorithm exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems. Trials are evaluated and stopped based on their accuracy on the validation split.</p><p>Two main parameters need to be configured for the ASHA algorithm: the number of trials and the grace period. The former controls the number of hyper-parameter configurations tried in total while the latter the minimum time after which a trial can be stopped. Since the number of trials corresponds to the time budget available for HPO, we choose larger values for smaller datasets, where training is 2 https://docs.ray.io/en/master/tune/ faster. The grace period, on the other hand, should be large enough to allow for a sufficient number of training iterations before comparing trials. Therefore, we choose larger grace periods for smaller datasets, where a single epoch comprises fewer training iterations. The exact values for each dataset as well as the total number of training epochs can be found in <ref type="table" target="#tab_3">Table 2</ref>. These values were determined based on preliminary experiments with the cross-entropy baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Final training and evaluation</head><p>After having completed HPO using the procedure described above, we train the classifier with the determined configuration on the combined training and validation split and evaluate the balanced classification accuracy on the test split. To account for the effect of random initialization, this training is repeated ten times and we report the balanced average accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In the following, we first present the results of reevaluating the eight methods described in Section 3 and the baseline on our benchmark introduced in Section 2, after carefully tuning all methods on each dataset. Then, we compare the performance obtained by our re-implementations, including the baseline, with other values published in the literature. <ref type="table">Table 3</ref> presents the average balanced classification accuracy over 10 runs with different random initializations for all methods and datasets. We performed Welch's ttest to assess the significance of the advantage of the best method for each dataset in comparison to all others. Most results are significantly worse on a level of 5% than the best method on the respective dataset, with only two exceptions: T-vMF Similarity on ciFAIR-10 and Harmonic Networks on CLaMM perform similar to the best method on these  <ref type="table">Table 3</ref>: Average balanced classification accuracy in % over 10 runs for each task and across all tasks. The best value per dataset is highlighted in bold font. Numbers in italic font indicate that the result is not significantly worse than the best one on a significance level of 5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data-efficient image classification benchmark</head><p>datasets, which is the baseline in both cases. This leads us to the main surprising finding of this benchmark: When tuned carefully, the standard cross-entropy baseline is very competitive with the published methods specialized for deep learning from small datasets. On ciFAIR-10 and CLaMM, it actually is the best performing method. It obtains the second rank on CUB and ISIC 2018, the third rank on ImageNet, and the fourth on EuroSAT. The baseline scores an average balanced accuracy across all datasets of 67.90%, which beats all other methods except Harmonic Networks by a large margin (the next best average accuracy is only 64.92%).</p><p>Harmonic Networks are the overall champion of our benchmark, with an average balanced accuracy of 68.70%. On the four datasets where they outperform the baseline, however, they only surpass it by 1%-5%.</p><p>Overall, the finding that the vast majority of recent methods for data-efficient image classification does not even achieve the same performance as the baseline is sobering. We attribute this to the fact that the importance of hyperparameter optimization is immensely underestimated, resulting in misleading comparisons of novel approaches with weak and underperforming baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Published baselines are underperforming</head><p>We show further evidence of why tuning the hyperparameters and not neglecting the baseline in scenarios with small datasets is fundamental to perform a fair comparison between different methods.</p><p>We analyzed the original results reported for the methods considered in our study and selected those that shared a similar setup. Note that due to the lack of a standard bench-mark and the common practice of randomly sub-sampling large datasets, we are unable to conduct a fair comparison with the same dataset split, training procedure, etc. Still, our benchmark shares the base dataset and network architecture with the selected cases. Therefore, we believe that this analysis is suitable for supporting our point regarding the common practice of comparing tuned proposed methods with underperforming baselines.</p><p>The results of this analysis are shown in <ref type="table" target="#tab_6">Table 4</ref>. Deep Hybrid Networks and Harmonic Networks were originally tested with a WRN-16-8 on CIFAR-10 while Full Convolution employed RN50 on ImageNet. In both cases, training sets were comprised of 50 images per class. Our baseline clearly outperforms the original baselines by large margins ( <ref type="table" target="#tab_6">Table 4</ref>, left part). More precisely, our models surpass the reported ones by 12, 6, and 18 percentage points on the CIFAR and ImageNet setups. Recall also that the ciFAIR-10 test set is slightly harder than the CIFAR-10 one due to the removal of duplicates <ref type="bibr" target="#b1">[2]</ref>.</p><p>On the contrary, for the case of the proposed methods ( <ref type="table" target="#tab_6">Table 4</ref>, right part), the difference between ours and original results is sharply less evident. Our DHN and HN slightly underperform the original ones by a 0.5 and 2 percentage points, respectively. However, this was expected due to the higher difficulty of ciFAIR-10. On ImageNet instead, our F-Conv model outperforms the original one by 5 percentage points, confirming once again that careful HPO can further boost the performance.</p><p>From this analysis it seems clear that proposed methods are usually tuned to obtain an optimal or near-optimal result while baselines are trained with default hyper-parameters that have been found useful for large datasets but do not   necessarily generalize to smaller ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Optimal hyper-parameters</head><p>For reproducibility, but also to gain further insights into hyper-parameter optimization for small datasets, we show the best hyper-parameter combinations found during our search for the cross-entropy baseline in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>We can observe that small batch sizes seem to be beneficial, despite the use of batch normalization. While the learning rate exhibits a rather small range of values from 0.7 ? 10 ?3 to 7.4 ? 10 ?3 across datasets and spans only one order of magnitude, weight decay varies within a range of two orders of magnitude from 4.1 ? 10 ?4 to 1.8 ? 10 ?2 .</p><p>Furthermore, learning rate and weight decay appear to be negatively correlated. Higher learning rates are usually accompanied by smaller weight decay factors. The same correlation can be observed in <ref type="figure">Fig. 1</ref>. A quantitative analysis over the hyper-parameters of all methods used in our study instead of only the baseline yields a correlation of r = ?.28, p = .02. After taking the logarithm of learning rate and weight decay, the correlation is strengthened to r = ?.58, p &lt; .01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we laid the foundation for fair and appropriate comparisons among modern data-efficient image classifiers. The motivations that brought us to our work are mainly two-fold: the lack of a common evaluation benchmark with fixed datasets, architectures, and training pipelines; and the experimental evidence of weak assessments of baselines due to a lack of careful tuning.</p><p>The re-evaluation of eight selected state-of-the-art methods guided us to the surprising and sobering conclusion that the standard cross-entropy loss ranks second in our benchmark only behind Harmonic Networks and, competes with or outperforms the remaining methods.</p><p>With these results in mind, we conclude that the importance of hyper-parameter optimization is immensely undervalued and should be taken into account in future studies to elude misleading comparisons of new approaches with weak and underperforming baselines. The publication of our benchmark heads towards this direction and is considered by ourselves an important contribution for the community of data-efficient image classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Example images from the datasets included in our benchmark. For EuroSAT, we only show the RGB bands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Hyper-parameters found with ASHA<ref type="bibr" target="#b16">[17]</ref> for the cross-entropy baseline. BS = batch size, LR = learning rate, WD = weight decay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>brigato@diag.uniroma1.it ? bjoern.barz@uni-jena.de 12% 29.67% 29.98% 28.32% 28.89% 28.71% 29.72% 30.07% 37.46% 36.84% 38.52% 36.81% 36.16% 37.92% 37.34% 39.02% 41.89% 40.33% 41.38% 40.95% 42.07% 40.59% 42.91% 45.00% 45.74% 43.33% 43.17% 43.70% 44.09% 45.56% 49.09% 55.63% 43.73% 46.53% 43.19% 44.04% 44.15% 48.90% 57.30% 46.11% 46.33% 45.00% 45.47% 46.37% 51.02% 58.17% 52.74% 46.21% 43.15% 45.55% 45.59% 53.64% 53.00% 42.87% 41.96% 46.17% 48.00% 41.14%</figDesc><table><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.027</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0072</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Learning Rate</cell><cell>0.00052 0.0019</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00014</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3.7e-05</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1e-05</cell><cell>30.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1e-05</cell><cell>3.7e-05 0.00014 0.00052 0.0019</cell><cell>0.0072</cell><cell>0.027</cell><cell>0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Weight Decay</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>, which makes it particularly interesting for</cell></row></table><note>Datasets constituting our benchmark. Except for CUB, we use sub-samples to simulate a small-data scenario.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Summary of hyper-parameters searched/used with ASHA<ref type="bibr" target="#b16">[17]</ref>. Method specific hyper-parameters were included in the search space but not included in this table due to space limitations. An epoch number in parentheses means that a higher number of epochs was used for the final training than for the hyper-parameter optimization.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Summary of ours/published results of the cross-entropy baseline (left) and other methods (right) on similar setups.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://pytorch.org/vision/stable/transforms. html#torchvision.transforms.RandomResizedCrop</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning on small datasets without pre-training using cosine loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Barz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do we train on test data? purging CIFAR of near-duplicates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Barz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A kernel perspective for regularizing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pielok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Coors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janek</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Ullmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne-Laure</forename><surname>Boulesteix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05847</idno>
		<title level="m">Hyperparameter optimization: Foundations, algorithms, best practices and open challenges</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A close look at deep learning with small data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Brigato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Iocchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">VIPriors 1: Visual inductive priors for data-efficient deep learning challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert-Jan</forename><surname>Bruintjes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attila</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><forename type="middle">Baptista</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osman</forename><surname>Semih Kayhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03768</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (ISIC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadi</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Marchetti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03368</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4109" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On translation invariance in CNNs: Convolutional layers can exploit absolute spatial location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan C Van</forename><surname>Osman Semih Kayhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">T-vMF similarity for regularizing intraclass feature distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takumi</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">OL?: Orthogonal low-rank embedding-a plug and play geometric loss for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Mus?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ben-Tzur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<title level="m">Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. Conference of Machine Learning and Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>2017. 5</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scaling the scattering transform: Deep hybrid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scattering networks for hybrid representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Belilovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>TPAMI)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The whole is more than its parts? from explicit to implicit pose normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Clustering of medieval scripts through computer image analysis: towards an evaluation protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Stutzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Medievalist</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A visual inductive priors framework for data-efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Tuggener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilo</forename><surname>Stadelmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09108</idno>
		<title level="m">Is it enough to optimize cnn architectures on imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Harmonic networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Ulicny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rozenn</forename><surname>Krylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahyot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Harmonic networks with limited training samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Ulicny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rozenn</forename><surname>Krylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahyot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<editor>Edwin R. Hancock Richard C. Wilson and William A. P. Smith</editor>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016-09" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
