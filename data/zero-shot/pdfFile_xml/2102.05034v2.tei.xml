<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
							<email>bfatemi@cs.ubc.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
							<email>layla.elasri@borealisai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><forename type="middle">Mehran</forename><surname>Kazemi</surname></persName>
							<email>mehrankazemi@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Borealis</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) work well when the graph structure is provided. However, this structure may not always be available in real-world applications. One solution to this problem is to infer a task-specific latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the taskspecific supervision may be insufficient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A comprehensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-specific graph structure on established benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph representation learning has grown rapidly and found applications in domains where a natural graph of the data points is available <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. Graph neural networks (GNNs) <ref type="bibr" target="#b44">[45]</ref> have been a key component to the success of the research in this area. Specifically, GNNs have shown promising results for semi-supervised classification when the available graph structure exhibits a high degree of homophily (i.e. connected nodes often belong to the same class) <ref type="bibr" target="#b62">[63]</ref>.</p><p>We study the applicability of GNNs to (semi-supervised) classification problems where a graph structure is not readily available. The existing approaches for this problem either fix a similarity graph between the nodes or learn the GNN parameters and a graph structure simultaneously (see Related Work). In both cases, one main goal is to construct or learn a graph structure with a high degree of homophily with respect to the labels to aid the GNN classification. The latter approach is sometimes called latent graph learning and often results in higher predictive performance compared to the former approach (see, e.g., <ref type="bibr" target="#b12">[13]</ref>).</p><p>We identify a supervision starvation problem in latent graph learning approaches in which the edges between pairs of nodes that are far from labeled nodes receive insufficient supervision; this results in learning poor structures away from labeled nodes and hence poor generalization. We propose a solution for this problem by adopting a multi-task learning framework in which we supplement the classification task with a self-supervised task. The self-supervised task is based on the hypothesis that a graph structure that is suitable for predicting the node features is also suitable for predicting the node labels. It works by masking some input features (or adding noise to them) and training a separate GNN aiming at updating the adjacency matrix in such a way that it can recover the masked (or noisy) features. The task is generic and can be combined with several existing latent graph learning approaches.</p><p>We develop a latent graph learning model, dubbed SLAPS, that adopts the proposed self-supervised task. We provide a comprehensive experimental study on nine datasets (thirteen variations) of various sizes and from various domains and perform thorough analyses to show the merit of SLAPS.</p><p>Our main contributions include: 1) identifying a supervision starvation problem for latent graph learning, 2) proposing a solution for the identified problem through self-supervision, 3) developing SLAPS, a latent graph learning model that adopts the self-supervised solution, 4) providing comprehensive experimental results showing SLAPS substantially outperforms existing latent graph learning baselines from various categories on various benchmarks, and 5) providing an implementation for latent graph learning that scales to graphs with hundreds of thousands of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Existing methods that relate to this work can be grouped into the following categories. We discuss selected work from each category and refer the reader to <ref type="bibr" target="#b66">[67]</ref> for a full survey.</p><p>Similarity graph: One approach for inferring a graph structure is to select a similarity metric and set the edge weight between two nodes to be their similarity <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b2">3]</ref>. To obtain a sparse structure, one may create a kNN similarity graph, only connect pairs of nodes whose similarity surpasses some predefined threshold, or do sampling. As an example, in <ref type="bibr" target="#b14">[15]</ref> a (fixed) kNN graph using the cosine similarity of the node features is created. In <ref type="bibr" target="#b52">[53]</ref>, this idea is extended by creating a fresh graph in each layer of the GNN based on the node embedding similarities in that layer. Instead of choosing a single similarity metric, in <ref type="bibr" target="#b15">[16]</ref> several (potentially weak) measures of similarity are fused. The quality of the predictions of these methods depends heavily on the choice of the similarity metric(s).</p><p>Fully connected graph: Another approach is to start with a fully connected graph and assign edge weights using the available meta-data or employ the GNN variants that provide weights for each edge via an attention mechanism <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b58">59]</ref>. This approach has been used in computer vision [e.g., <ref type="bibr" target="#b47">48]</ref>, natural language processing [e.g., <ref type="bibr" target="#b61">62]</ref>, and few-shot learning [e.g., <ref type="bibr" target="#b13">14]</ref>. The complexity of this approach grows rapidly making it applicable only to small-sized graphs. Zhang et al. <ref type="bibr" target="#b59">[60]</ref> propose to define local neighborhoods for each node and only assume that these local neighborhoods are fully connected. Their approach relies on an initial graph structure to define the local neighborhoods.</p><p>Latent graph learning: Instead of a similarity graph based on the initial features, one may use a graph generator with learnable parameters. In <ref type="bibr" target="#b33">[34]</ref>, a fully connected graph is created based on a bilinear similarity function with learnable parameters. In <ref type="bibr" target="#b12">[13]</ref>, a Bernoulli distribution is learned for each possible edge and graph structures are created through sampling from these distributions. In <ref type="bibr" target="#b54">[55]</ref>, the input structure is updated to increase homophily based on the labels and model predictions. In <ref type="bibr" target="#b5">[6]</ref>, an iterative approach is proposed that iterates over projecting the nodes to a latent space and constructing an adjacency matrix from the latent representations multiple times. A common approach in this category is to learn a projection of the nodes to a latent space where node similarities correspond to edge weights or edge probabilities. In <ref type="bibr" target="#b53">[54]</ref>, the nodes are projected to a latent space by learning weights for each of the input features. In <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref>, a multi-layer perceptron is used for projection. In <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b60">61]</ref>, a GNN is used for projection; it uses the node features and an initial graph structure. In <ref type="bibr" target="#b26">[27]</ref>, different graph structures are created in different layers by using separate GNN projectors, where the input to the GNN projector in a layer is the projected values and the generated graph structure from the previous layer. In our experiments, we compare with several approaches from this category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Leveraging domain knowledge:</head><p>In some applications, one may leverage domain knowledge to guide the model toward learning specific structures. For example, in <ref type="bibr" target="#b24">[25]</ref>, abstract syntax trees and regular languages are leveraged in learning graph structures of Python programs that aid reasoning for downstream tasks. In <ref type="bibr" target="#b23">[24]</ref>, the structure learning is guided for robustness to adversarial attacks through the domain knowledge that clean adjacency matrices are often sparse and low-rank and exhibit feature smoothness along the connected nodes. Other examples in this category include <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43]</ref>. In our paper, we experiment with general-purpose datasets without access to domain knowledge.</p><p>Proposed method: Our model falls within the latent graph learning category. We supplement the training with a self-supervised objective to increase the amount of supervision in learning a structure. Our self-supervised task is inspired by, and similar to, the pre-training strategies for GNNs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b63">64]</ref> (specifically, we adopt the multi-task learning framework of You et al. <ref type="bibr" target="#b56">[57]</ref>), but it differs from this line of work as we use self-supervision for learning a graph structure whereas the above methods use it to learn better (and, in some cases, transferable) GNN parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background and notation</head><p>We use lowercase letters to denote scalars, bold lowercase letters to denote vectors and bold uppercase letters to denote matrices. I represents an identity matrix. For a vector v, we represent its i th element as v i . For a matrix M , we represent the i th row as M i and the element at the i th row and j th column as M ij . For an attributed graph, we use n, m and f to represent the number of nodes, edges, and features respectively, and denote the graph as G = {V, A, X} where V = {v 1 , . . . , v n } is a set of nodes, A ? R n?n is an adjacency matrix with A ij indicating the weight of the edge from v i to v j (A ij = 0 implies no edge), and X ? R n?f is a matrix whose rows correspond to node features.</p><p>Graph convolutional networks (GCNs) <ref type="bibr" target="#b28">[29]</ref> are a powerful variant of GNNs. For a graph G = {V, A, X} with a degree matrix D, layer l of the GCN architecture can be defined as H (l) = ?(?H (l?1) W (l) ) where? represents a normalized adjacency matrix, H (l?1) ? R n?d l?1 represents the node representations in layer l-1 (H (0) = X), W (l) ? R d l?1 ?d l is a weight matrix, ? is an activation function such as ReLU <ref type="bibr" target="#b37">[38]</ref>, and H (l) ? R n?d l is the updated node embeddings. For undirected graphs where the adjacency is symmetric,? = D ? 1 2 (A + I)D ? 1 2 corresponds to a row-and-column normalized adjacency with self-loops, and for directed graphs where the adjacency is not necessarily symmetric,? = D ?1 (A + I) corresponds to a row normalized adjacency matrix with self-loops. Here, D is a (diagonal) degree matrix for (A + I) defined as D ii = 1 + j A ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed method: SLAPS</head><p>SLAPS consists of four components: 1) generator, 2) adjacency processor, 3) classifier, and 4) self-supervision. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates these components. In the next three subsections, we explain the first three components. Then, we point out a supervision starvation problem for a model based only on these components. Then we describe the self-supervision component as a solution to the supervision starvation problem and the full SLAPS model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generator</head><p>The generator is a function G : R n?f ? R n?n with parameters ? G which takes the node features X ? R n?f as input and produces a matrix? ? R n?n as output. We consider the following two generators and leave experimenting with more sophisticated graph generators (e.g., <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35]</ref>) and models with tractable adjacency computations (e.g., <ref type="bibr" target="#b6">[7]</ref>) as future work.</p><p>Full parameterization (FP): For this generator, ? G ? R n?n and the generator function is defined as? = G F P (X; ? G ) = ? G . That is, the generator ignores the input node features and directly optimizes the adjacency matrix. FP is similar to the generator in <ref type="bibr" target="#b12">[13]</ref> except that they treat each element of? as the parameter of a Bernoulli distribution and sample graph structures from these distributions. FP is simple and flexible for learning any adjacency matrix but adds n 2 parameters which limits scalability and makes the model susceptible to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP-kNN:</head><p>Here, ? G corresponds to the weights of a multi-layer perceptron (MLP) and? = G MLP (X; ? G ) = kNN(MLP(X)), where MLP : R n?f ? R n?f is an MLP that produces a matrix with updated node representations X ; kNN : R n?f ? R n?n produces a sparse matrix. The implementation details for the kNN operation is provided in the supplementary material.</p><p>Initialization and variants of MLP-kNN: Let A kN N represent an adjacency matrix created by applying a kNN function on the initial node features. One smart initialization for ? G is to initialize it in a way that the generator initially generates A kN N (i.e.? = A kN N before training starts). This can be trivially done for the FP generator by initializing ? G to A kN N . For MLP-kNN, we consider two variants. In one, hereafter referred to simply as MLP, we keep the input dimension the same throughout the layers. In the other, hereafter referred to as MLP-D, we consider MLPs with diagonal weight matrices (i.e., except the main diagonal, all other parameters in the weight matrices are zero). For both variants, we initialize the weight matrices in ? G with the identity matrix to ensure that the output of the MLP is initially the same as its input and the kNN graph created on these outputs is equivalent to A kN N (Alternatively, one may use other MLP variants but pre-train the weights to output A kN N before the main training starts.). MLP-D can be thought of as assigning different weights to different features and then computing node similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adjacency processor</head><p>The output? of the generator may have both positive and negative values, may be non-symmetric and non-normalized. We let A = 1 2 D ? 1 2 (P(?) + P(?) T )D ? 1 2 . Here P is a function with a non-negative range applied element-wise on its input -see supplementary material for details. The sub-expression 1 2 (P(?) + P(?) T ) makes the resulting matrix P(?) symmetric. To understand the reason for taking the mean of P(?) and P(?) T , assume? is generated by G MLP . If v j is among the k most similar nodes to v i and vice versa, then the strength of the connection between v i and v j will remain the same. However, if, say, v j is among the k most similar nodes to v i but v i is not among the top k for v j , then taking the average of the similarities reduces the strength of the connection between v i and v j . Finally, once we have a symmetric adjacency with non-negative values, we normalize 1 2 (P(?) + P(?) T ) by computing its degree matrix D and multiplying it from left and right to D ? 1 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classifier</head><p>The classifier is a function GNN C : R n?f ? R n?n ? R n?|C| with parameters ? GNN C . It takes the node features X and the generated adjacency A as input and provides for each node the logits for each class. C corresponds to the classes and |C| corresponds to the number of classes. We use a twolayer GCN for which ? GNN C = {W <ref type="bibr" target="#b0">(1)</ref> , W <ref type="bibr" target="#b1">(2)</ref> } and define our classifier as GNN C (A, X; ? GNN C ) = AReLU(AXW (1) )W <ref type="bibr" target="#b1">(2)</ref> but other GNN variants can be used as well (recall that A is normalized). The training loss L C for the classification task is computed by taking the softmax of the logits to produce a probability distribution for each node and then computing the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Using only the first three components leads to supervision starvation</head><p>One may create a model using only the three components described so far corresponding to the top part of <ref type="figure" target="#fig_0">Figure 1</ref>. As we will explain here, however, this model may suffer severely from supervision starvation. The same problem also applies to many existing approaches for latent graph learning, as they can be formulated as a combination of variants of these three components.</p><p>Consider a scenario during training where two unlabeled nodes v i and v j are not directly connected to any labeled nodes according to the generated structure. Then, since a two-layer GCN makes predictions for the nodes based on their two-hop neighbors, the classification loss (i.e. L C ) is not affected by the edge between v i and v j and this edge receives no supervision 2 . <ref type="figure" target="#fig_1">Figure 2</ref> provides an example of such a scenario. Let us call the edges that do not affect the loss function L C (and consequently do not receive supervision) as starved edges. These edges are problematic because although they may not affect the training loss, the predictions at the test time depend on these edges and if their values are learned without enough supervision, the model may make poor predictions at the test time. A natural question concerning the extent of the problem caused by such edges is the proportion of starved edges. The following theorem formally establishes the extent of the problem for Erd?s-R?nyi graphs <ref type="bibr" target="#b10">[11]</ref>; in the supplementary, we extend this result to the Barab?si-Albert model <ref type="bibr" target="#b0">[1]</ref> and scale-free networks <ref type="bibr" target="#b1">[2]</ref>. An Erd?s-R?nyi graph with n nodes and m edges is a graph chosen uniformly at random from the collection of all graphs which have n nodes and m edges.</p><p>Theorem 1 Let G(n, m) be an Erd?s-R?nyi graph with n nodes and m edges. Assume we have labels for q nodes selected uniformly at random. The probability of an edge being a starved edge with a two-layer GCN is equal to</p><formula xml:id="formula_0">(1 ? q n )(1 ? q n?1 ) 2q i=1 (1 ? m?1 ( n 2 )?i ).</formula><p>We defer the proof to the supplementary material. To put the numbers from the theorem in perspective, let us consider three established benchmarks for semi-supervised node classification namely Cora, Citeseer, and Pubmed (the statistics for these datasets can be found in the Appendix). For an Erd?s-R?nyi graph with similar statistics as the Cora dataset (n = 2708, m = 5429, q = 140), the probability of an edge being a starved edge is 59.4% according to the above theorem. For Citeseer and Pubmed, this number is 75.7% and 96.7% respectively. While Theorem 1 is stated for Erd?s-R?nyi graphs, the identified problem also applies to natural graphs. For the original structures of Cora, Citeseer, and Pubmed, for example, 48.8%, 65.2%, and 91.6% of the edges are starved edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Self-supervision</head><p>One possible solution to the supervision starvation problem is to define a prior graph structure and regularize the learned structure toward it. This leads the starved edges toward the prior structure as opposed to neglecting them. The choice of the prior is important as it determines the inductive bias incorporated into the model. We define a prior structure based on the following hypothesis:</p><p>Hypothesis 1 A graph structure that is suitable for predicting the node features is also suitable for predicting the node labels.</p><p>We first explain why the above hypothesis is reasonable for an extreme case that is easy to understand and then extend the explanation to the general case. Consider an extreme scenario where one of the node features is the same as the node labels. A graph structure that is suitable for predicting this feature exhibits homophily for it. Because of the equivalence between this feature and the labels, the graph structure also exhibits homophily for the labels, so it is also suitable for predicting the labels.</p><p>In the general (non-extreme) case, there may not be a single feature that is equivalent to the labels but a subset of the features may be highly predictive of the labels. A graph structure that is suitable for predicting this subset exhibits homophily for the features in the subset. Because this subset is highly predictive of the labels, the structure also exhibits a high degree of homophily for the labels, so it is also suitable for predicting the node labels.</p><p>Next, we explain how to design a suitable graph structure for predicting the features and how to regularize toward it. One could design such a structure manually (e.g., by handcrafting a graph that connects nodes based on the collective homophily between their individual features) and then penalize the difference between this prior graph and the learned graph. Alternatively, in this paper, we take a learning-based approach based on self-supervision where we not only use the learned graph structure for the classification task, but also for denoising the node features. The self-supervised task encourages the model to learn a structure that is suitable for predicting the node features. We describe this approach below and provide comparisons to the manual approach in the supplementary material.</p><p>Our self-supervised task is based on denoising autoencoders <ref type="bibr" target="#b50">[51]</ref>. Let GNN DAE : R n?f ? R n?n ? R n?f be a GNN with parameters ? GNN DAE that takes node features and a generated adjacency as input and provides updated node features with the same dimension as output. We train GNN DAE such that it receives a noisy versionX of the features X as input and produces the denoised features X as output. Let idx represent the indices corresponding to the elements of X to which we have added noise, and X idx represent the values at these indices. During training, we minimize:</p><formula xml:id="formula_1">L DAE = L(X idx , GNN DAE (X, A; ? GNN DAE ) idx )<label>(1)</label></formula><p>where A is the generated adjacency matrix and L is a loss function. For datasets where the features consist of binary vectors, idx consists of r percent of the indices of X whose values are 1 and r? percent of the indices whose values are 0, both selected uniformly at random in each epoch. Both r and ? (corresponding to the negative ratio) are hyperparameters. In this case, we add noise by setting the 1s in the selected mask to 0s and L is the binary cross-entropy loss. For datasets where the input features are continuous numbers, idx consists of r percent of the indices of X selected uniformly at random in each epoch. We add noise by either replacing the values at idx with 0 or by adding independent Gaussian noises to each of the features. In this case, L is the mean-squared error loss.</p><p>Note that the self-supervised task in equation 1 is generic and can be added to different GNNs as well as latent graph learning models. It can be also combined with other techniques in the literature that encourage learning more homophilous structures or increase the amount of supervision. In our experiments, we test the combination of our self-supervised task with two such techniques namely self-training <ref type="bibr" target="#b32">[33]</ref> and AdaEdge <ref type="bibr" target="#b4">[5]</ref>. Self-training helps the model "see" more labeled nodes and AdaEdge helps iteratively create graph structure with higher degrees of homophily. We refer the reader to the supplementary material for descriptions of self-training and AdaEdge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">SLAPS</head><p>Our final model is trained to minimize L = L C + ?L DAE where L C is the classification loss, L DAE is the denoising autoencoder loss (see Equation 1), and ? is a hyperparameter controlling the relative importance of the two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we report our key results. More empirical comparisons, experimental analyses, and ablation studies are presented in the supplementary material.</p><p>Baselines: We compare our proposal to several baselines with different properties. The first baseline is a multi-layer perceptron (MLP) which does not take the graph structure into account. We also compare against MLP-GAM* <ref type="bibr" target="#b46">[47]</ref> which learns a fully connected graph structure and uses this structure to supplement the loss function of the MLP toward predicting similar labels for neighboring nodes. Our third baseline is label propagation (LP) <ref type="bibr" target="#b64">[65]</ref>, a well-known model for semi-supervised learning. Similar to <ref type="bibr" target="#b12">[13]</ref>, we also consider a baseline named kNN-GCN where we create a kNN graph based on the node feature similarities and feed this graph to a GCN; the graph structure remains fixed in this approach. We also compare with prominent existing latent graph learning models including LDS <ref type="bibr" target="#b12">[13]</ref>, GRCN <ref type="bibr" target="#b57">[58]</ref>, DGCNN <ref type="bibr" target="#b52">[53]</ref>, and IDGL <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b5">[6]</ref>, another variant named IDGL-ANCH is also proposed that reduces time complexity through anchor-based approximation <ref type="bibr" target="#b36">[37]</ref>. We compare against the base IDGL model because it does not sacrifice accuracy for time complexity, and because anchor-based approximation is model-agnostic and could be combined with other models too. We  feed a kNN graph to the models requiring an initial graph structure. We also explore how adding self-training and AdaEdge impact the performance of kNN-GCN as well as SLAPS.</p><formula xml:id="formula_2">? 0.1 MLP-GAM* 70.7 ? 70.3 ? ? ? 71.9 ? ? LP 37.6 ? 0.0 23.2 ? 0.0 36.2 ? 0.0 29.1 ? 0.0 41.3 ? 0.0 OOM kNN-GCN 66.5 ? 0.4 ? 68.3 ? 1.3 ? 72.5 ? 0.5 71.8 ? 0.8 70.4 ? 0.4 49.1 ? 0.3 LDS ? ? 71.5 ? 0.8 ? 71.5 ? 1.1 ? OOM OOM GRCN 67.4 ? 0.</formula><p>Datasets: We use three established benchmarks in the GNN literature namely Cora, Citeseer, and Pubmed <ref type="bibr" target="#b45">[46]</ref> as well as the ogbn-arxiv dataset <ref type="bibr" target="#b16">[17]</ref> that is orders of magnitude larger than the other three datasets and is more challenging due to the more realistic split of the data into train, validation, and test sets. For these datasets, we only feed the node features to the models and not their original graph structure. Following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6]</ref>, we also experiment with several classification (non-graph) datasets available in scikit-learn <ref type="bibr" target="#b40">[41]</ref> including Wine, Cancer, Digits, and 20News. Furthermore, following <ref type="bibr" target="#b20">[21]</ref>, we also provide results on MNIST <ref type="bibr" target="#b30">[31]</ref>. The dataset statistics can be found in the supplementary. For Cora and Citeseer, the LDS model uses the train data for learning the parameters of the classification GCN, half of the validation for learning the parameters of the adjacency matrix (in their bi-level optimization setup, these are considered as hyperparameters), and the other half of the validation set for early stopping and tuning the other hyperparameters. Besides experimenting with the original setups of these two datasets, we also consider a setup that is closer to that of LDS: we use the train set and half of the validation set for training and the other half of validation for early stopping and hyperparameter tuning. We name the modified versions Cora390 and Citeseer370 respectively where the number proceeding the dataset name shows the number of labels from which gradients are computed. We follow a similar procedure for the scikit-learn datasets.</p><p>Implementation: We defer the implementation details and the best hyperparameter settings for our model on all the datasets to the supplementary material. Code and data is available at https://github.com/BorealisAI/SLAPS-GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparative results</head><p>The results of SLAPS and the baselines on our benchmarks are reported in <ref type="table" target="#tab_0">Tables 1 and 2</ref>. We start by analyzing the results in <ref type="table" target="#tab_0">Table 1</ref> first. Starting with the baselines, we see that learning a fully connected graph in MLP-GAM* makes it outperform MLP. kNN-GCN significantly outperforms MLP on Cora and Citeseer but underperforms on Pubmed and ogbn-arxiv. Furthermore, both self-training and AdaEdge improve the performance of kNN-GCN. This shows the importance of the similarity metric and the graph structure that is fed into GCN; a low-quality structure can harm model performance. LDS outperforms MLP but the fully parameterized adjacency matrix of LDS results in memory issues for Pubmed and ogbn-arxiv. As for GRCN, it was shown in the original paper that GRCN can revise a good initial adjacency matrix and provide a substantial boost in performance. However, as evidenced by the results, if the initial graph structure is somewhat poor, GRCN's performance becomes on par with kNN-GCN. IDGL is the best performing baseline. In addition to the aforementioned baselines, we also experimented with GCN, GAT, and Transformer (encoder only) architectures applied on fully connected graphs. GCN always learned to predict the majority class. This is because after one fully connected GCN layer, all nodes will have the same embedding and become indistinguishable. GAT also showed similar behavior. We believe this is because the attention weights are (almost) random at the beginning (due to random initialization of the model parameters) resulting in nodes becoming indistinguishable and GAT cannot escape from that state. The skip connections of Transformer helped avoid the problem observed for GCN and GAT and we were able to achieve better results (? 40% accuracy on Cora). However, we observed severe overfitting (even with very small models and with high dropout probabilities).</p><p>SLAPS consistently outperforms the baselines in some cases by large margins. Among the generators, the winner is dataset-dependent with MLP-D mostly outperforming MLP on datasets with many features and MLP outperforming on datasets with small numbers of features. Using the software that was publicly released by the authors, the baselines that learn a graph structure fail on ogbn-arxiv 3 ; our implementation, on the other hand, scales to such large graphs. Adding self-training helps further improve the results of SLAPS. Adding AdaEdge, however, does not seem effective, probably because the graph structure learned by SLAPS already exhibits a high degree of homophily (see Section 5.4).</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, we only compared SLAPS with the best performing baselines from <ref type="table" target="#tab_0">Table 1</ref> (kNN-GCN, LDS and IDGL). We also included an MLP baseline for comparison. On three out of four datasets, SLAPS outperforms the LDS and IDGL baselines. For the Digits dataset, interestingly kNN-GCN outperforms the learning-based models. This could be because the initial kNN structure for this dataset is already a good structure. Among the datasets on which we can train SLAPS with the FP generator, 20news has the largest number of nodes (9,607 nodes). On this dataset, we observed that an FP generator suffers from overfitting and produces weaker results compared to other generators due to its large number of parameters.</p><p>Jiang et al. <ref type="bibr" target="#b21">[22]</ref> show that learning a latent graph structure of the input examples can help with Here, we conduct an experiment to measure the performance of SLAPS on these variants of the MNIST dataset. We compare against GLCN <ref type="bibr" target="#b21">[22]</ref> as well as the baselines in the GLCN paper including manifold regularization <ref type="bibr" target="#b2">[3]</ref>, label propagation, deep walk <ref type="bibr" target="#b41">[42]</ref>, graph convolutional networks (GCN), and graph attention networks (GAT).</p><p>The results are reported in <ref type="table" target="#tab_3">Table 3</ref>. From the results, it can be viewed that SLAPS outperforms GLCN and all the other baselines on the 3 variants. Compared to GLCN, on the three variants SLAPS reduces the error by 7%, 5%, and 2% respectively, showing that SLAPS can be more effective when the labeled set is small and providing more empirical evidence for Theorem 1.  Learning a structure only using self-supervision: To provide more insight into the value provided by the self-supervision task and the generalizability of the adjacency learned through this task, we conduct experiments with a variant of SLAPS named SLAP S 2s that is trained in two stages. We first train the GNN DAE model by minimizing L DAE described in in Equation 1. Recall that L DAE depends on the parameters ? G of the generator and the parameters ? GNN DAE of the denoising autoencoder. After every t epochs of training, we fix the adjacency matrix, train a classifier with the fixed adjacency matrix, and measure classification accuracy on the validation set. We select the epoch that produces the adjacency providing the best validation accuracy for the classifier. Note that in SLAP S 2s , the adjacency matrix only receives gradients from the self-supervised task in Equation 1. <ref type="figure" target="#fig_3">Figure 3</ref> shows the performance of SLAPS and SLAPS 2s on Cora and compares them with kNN-GCN. Although SLAPS 2s does not use the node labels in learning an adjacency matrix, it outperforms kNN-GCN (8.4% improvement when using an FP generator). With an FP generator, SLAPS 2s even achieves competitive performance with SLAPS; this is mainly because FP does not leverage the supervision provided by GCN C toward learning generalizable patterns that can be used for nodes other than those in the training set. These results corroborate the effectiveness of the self-supervision task for learning an adjacency matrix. Besides, the results show that learning the adjacency using both self-supervision and the task-specific node labels results in higher predictive accuracy.  The value of ?: <ref type="figure" target="#fig_5">Figure 4</ref> shows the performance of SLAPS 4 on Cora and Citeseer with different values of ?. When ? = 0, corresponding to removing self-supervision, the model performance is somewhat poor. As soon as ? becomes positive, both models see a large boost in performance showing that self-supervision is crucial to the high performance of SLAPS. Increasing ? further provides larger boosts until it becomes so large that the self-supervision loss dominates the classification loss and the performance deteriorates. Note that with ? = 0, SLAPS with the MLP generator becomes a variant of the model proposed in <ref type="bibr" target="#b8">[9]</ref>, but with a different similarity function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The effectiveness of self-supervision</head><p>Is self-supervision actually solving the supervision starvation problem? <ref type="figure" target="#fig_5">In Fig 4,</ref> we showed that self-supervision is key to the high performance of SLAPS. Here, we examine if this is because self-supervision indeed addresses the supervision starvation problem. For this purpose, we compared SLAPS with and without self-supervision on two groups of test nodes on Cora: 1) those that are not connected to any labeled nodes after training, and 2) those that are connected to at least one labeled node after training. The nodes in group one have a high chance of having starved edges. We observed that adding self-supervision provides 38.0% improvement for the first group and only 8.9% improvement for the latter. Since self-supervision mainly helps with nodes in group 1, this provides evidence that self-supervision is an effective solution to the supervision starvation problem.</p><p>The effect of the training set size: According to Theorem 1, a smaller q (corresponding to the training set size) results in more starved edges in each epoch. To explore the effect of self-supervision as a function of q, we compared SLAPS with and without supervision on Cora and Citeseer while reducing the number of labeled nodes per class from 20 to 5. We used the FP generator for this experiment. With 5 labeled nodes per class, adding self-supervision provides 16.7% and 22.0% improvements on Cora and Citeseer respectively, which is substantially higher than the corresponding numbers when using 20 labeled nodes per class (10.0% and 7.0% respectively). This provides empirical evidence for Theorem 1. Note that the results on Cora390 and Citeseer 370 datasets provide evidence that the self-supervised task is effective even when the label rate is high. The performance of GNNs highly depends on the quality of the input graph structure and deteriorates when the graph structure is noisy [see 68, <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>. Here, we verify whether selfsupervision is also helpful when a noisy structure is provided as input. Toward this goal, we experiment with Cora and Citeseer and provide noisy versions of the input graph as input. The provided noisy graph structure is used only for initialization; it is then further optimized by SLAPS. We perturb the graph structure by replacing ? percent of the edges in the original structure (selected uniformly at random) with random edges. <ref type="figure" target="#fig_6">Figure 5</ref> shows the performance of SLAPS with and without self-supervision (? = 0 corresponds to no supervision). We also report the results of vanilla GCN on these perturbed graphs for comparison. It can be viewed that self-supervision consistently provides a boost in performance especially for higher values of ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments with noisy graphs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analyses of the learned adjacency</head><p>Noisy graphs: Following the experiment in Section 5.3, we compared the learned and original structures by measuring the number of random edges added during perturbation but removed by the model and the number of edges removed during the perturbation but recovered by the model. For Cora, SLAPS removed 76.2% and 70.4% of the noisy edges and recovered 58.3% and 44.5% of the removed edges for ? = 25% and ? = 50% respectively  Homophily: As explained earlier, a properly learned graph for semi-supervised classification with GNNs exhibits high homophily. To verify the quality of the learned adjacency with respect to homophily, for every pair of nodes in the test set, we compute the odds of the two nodes sharing the same label as a function of the normalized weight of the edge connecting them. <ref type="figure" target="#fig_8">Figure 6</ref> represents the odds for different weight intervals (recall that A is row and column normalized). For both Cora and Citeseer, nodes' connected with higher edge weights are more likely to share the same label compared to nodes with lower or zero edge weights. Specifically, when A ij ? 0.1, v i and v j are almost 2.5 and 2.0 times more likely to share the same label on Cora and Citeseer respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed SLAPS: a model for learning the parameters of a graph neural network and a graph structure of the nodes connectivities simultaneously from data. We identified a supervision starvation problem that emerges for graph structure learning, especially when training data is scarce. We proposed a solution to the supervision starvation problem by supplementing the training objective with a well-motivated self-supervised task. We showed the effectiveness of our model through a comprehensive set of experiments and analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Funding Transparency Statement</head><p>This work was fully funded by Borealis AI.</p><p>[68] Daniel Z?gner, Amir Akbarnejad, and Stephan G?nnemann. Adversarial attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pages 2847-2856, 2018.</p><p>A More Experiments and Analyses Importance of k in kNN: <ref type="figure" target="#fig_9">Figure 7</ref> shows the performance of SLAPS on Cora for three graph generators as a function of k in kNN. For all three cases, the value of k plays a major role in model performance. The FP generator is the least sensitive because, in FP, k only affects the initialization of the adjacency matrix but then the model can change the number of neighbors of each node. For MLP and MLP-D, however, the number of neighbors of each node remains close to k (but not necessarily equal as the adjacency processor can add or remove some edges) and the two generators become more sensitive to k. For larger values of k, the extra flexibility of the MLP generator enables removing some of the unwanted edges through the function P or reducing the weights of the unwanted edges resulting in MLP being less sensitive to large values of k compared to MLP-D.</p><p>Increasing the number of layers: In the main text, we described how some edges may receive no supervision during latent graph learning. We pointed out that while increasing the number of layers of the GCN may alleviate the problem to some extent, deeper GCNs typically provide inferior results due to issues such as oversmoothing [see, e.g., <ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>. We empirically tested deeper GCNs for latent graph learning to see if simply using more layers can obviate the need for the proposed self-supervision. Specifically, we tested SLAPS without self-supervision (i.e. ? = 0) with 2, 4, and 6 layers on Cora. We also added residual connections that have been shown to help train deeper GCNs <ref type="bibr" target="#b31">[32]</ref>. The accuracies for 2, 4, and 6-layer models are 66.2%, 67.1%, and 55.8% respectively. It can be viewed that increasing the number of layers from 2 to 4 provides an improvement. This might be because the benefit provided by a 4-layer model in terms of alleviating the starved edge problem outweighs the increase in oversmoothing. However, when the number of layers increases to 6, the oversmoothing problem outweighs and the performance drops significantly. Further increasing the number of layers resulted in even lower accuracies.</p><p>Symmetrization: In the adjacency processor, we used the following equation:  which symmetrized the adjacency matrix by taking the average of P(?) and P(?) T . Here we also consider two other choices: 1) max(P(?), P(?) T ), and 2) not symmetrizing the adjacency (i.e. using P(?)). <ref type="figure" target="#fig_11">Figure 8</ref> compares these three choices on Cora and Citeseer with an MLP generator (other generators produced similar results). On both datasets, symmetrizing the adjacency provides a performance boost. Compared to mean symmetrization, max symmetrization performs slightly worse. This may be because max symmetrization does not distinguish between the case where both v i and v j are among the k most similar nodes of each other and the case where only one of them is among the k most similar nodes of the other.</p><formula xml:id="formula_3">A = D ? 1 2 P(?) + P(?) T 2 D ? 1 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cora</head><p>Fixing a prior graph manually instead of using selfsupervision: In the main text, we validated Hypothesis 1 by adding a self-supervised task to encourage learning a graph structure that is appropriate for predicting the node features, and showing in our experiments how this additional task helps improve the results. Here, we provide more evidence for the validity of Hypothesis 1 by showing that we can obtain good results even when regularizing the learned graph structure toward a manually fixed structure that is appropriate for predicting the node features.  Toward this goal, we experimented with Cora and Citeseer and created a cosine similarity graph as our prior graph A prior where the edge weights correspond to the cosine similarity of the nodes. We sparsified A prior by connecting each node only to the k most similar nodes. Then, we added a term ?||A ? A prior || F to the loss function where ? is a hyperparameter, A is the learned graph structure (i.e. the output of the graph generator), and ||.|| F shows the Frobenius norm. Note that A prior exhibits homophily with respect to the node features because the node features in Cora and Citeseer are binary, so two nodes that share the same values for more features have a higher similarity and are more likely to be connected.</p><p>The results can be viewed in <ref type="figure" target="#fig_13">Figure 9</ref>. According to the results, we can see that regularizing toward a manually designed A prior also provides good results but falls short of SLAPS with self-supervision. The superiority of the self-supervised approach compared to the manual design could be due to two reasons.</p><p>? Some of the node features may be redundant (e.g., they may be derived from other features) or highly correlated. These features can negatively affect the similarity computations for the prior graph in A prior . As an example, consider three nodes with seven binary features [0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0] and [1, 1, 1, 1, 1, 1, 1] respectively and assume the last two features for each node are always equivalent and are computed based on a logical and of the 4th and 5th features 5 . Without these two features, the first node is more similar to the second than the third node, but when considering these derived features, it becomes more similar to the third node. This change in node similarities affects the construction of A prior which can deteriorate the overall performance of the model. The version of SLAPS with the self-supervised task, on the other hand, is not affected by this problem as much because the model can learn to predict the derived node features based on other features and without heavily relying on the graph structure.</p><p>? While many graph structures may be appropriate for predicting the node features, in the manual approach we only regularize toward one particular such structure. Using the selfsupervised task, however, SLAPS can learn any of those structures; ideally, it learns the one that is more suited for the downstream task due to the extra supervision coming from the downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>We implemented our model in PyTorch <ref type="bibr" target="#b39">[40]</ref>, used deep graph library (DGL) <ref type="bibr" target="#b51">[52]</ref> for the sparse operations, and used Adam <ref type="bibr" target="#b27">[28]</ref> as the optimizer. We performed early stopping and hyperparameter tuning based on the accuracy on the validation set for all datasets except Wine and Cancer. For these two datasets, the validation accuracy reached 100 percent with many hyperparameter settings, making it difficult to select the best set of hyperparameters. Instead, we used the validation cross-entropy loss for these two datasets.</p><p>We fixed the maximum number of epochs to 2000. We use two-layer GCNs for both GNN C and GNN DAE as well as for baselines and two-layer MLPs throughout the paper (for experiments on ogbn-arxiv, although the original paper uses models with three layers and with batch normalization after each layer, to be consistent with our other experiments we used two layers and removed the normalization). We used two learning rates, one for GCN C as lr C and one for the other parameters of the models as lr DAE . We tuned the two learning rates from the set {0.01, 0.001}. We added dropout layers with dropout probabilities of 0.5 after the first layer of the GNNs. We also added dropout to the adjacency matrix for both GNN C and GNN DAE as dropout C dropout DAE respectively and tuned the values from the set {0.25, 0.5}. We set the hidden dimension of GNN C to 32 for all datasets except for ogbn-arxiv for which we set it to 256. We used cosine similarity for building the kNN graphs and tuned the value of k from the set {10, 15, 20, 30}. We tuned ? (? controls the relative importance of the two losses) from the set {0.1, 1, 10, 100, 500}. We tuned r and ? from the sets {1, 5, 10} and {1, 5} respectively. The best set of hyperparameters for each dataset chosen on the validation set is in table 4. The code of our experiments will be available upon acceptance of the paper.</p><p>For GRCN <ref type="bibr" target="#b57">[58]</ref>, DGCNN <ref type="bibr" target="#b52">[53]</ref>, and IDGL <ref type="bibr" target="#b5">[6]</ref>, we used the code released by the authors and tuned the hyperparameters as suggested in the original papers. The results of LDS <ref type="bibr" target="#b12">[13]</ref> are directly taken from the original paper. For LP <ref type="bibr" target="#b65">[66]</ref>, we used scikit-learn python package <ref type="bibr" target="#b40">[41]</ref>.</p><p>All the results for our model and the baselines are averaged over 10 runs. We report the mean and standard deviation. We ran all the experiments on a single GPU (NVIDIA GeForce GTX 1080 Ti).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-training and AdaEdge:</head><p>We combined SLAPS (and kNN-GCN) with two techniques from the literature namely self-training and AdaEdge. For completeness sake, we provide a brief description of these approaches and refer the reader to the original papers for detailed descriptions.</p><p>For self-training, we first trained a model using the existing labels in the training set. Then we used this model to make predictions for the unlabeled nodes that were not in the train, validation, or test sets. We considered the label predictions for the top ? most confident unlabeled nodes as ground truth labels and added them to the training labels. Finally, we trained a model from scratch on the expanded set of labels. Here, ? is a hyperparameter. We tuned its value from the set {50, 100, 200, 300, 400, 500}.</p><p>For AdaEdge, in the case of kNN-GCN, we first trained a kNN-GCN model. Then we changed the structure of the graph from the kNN graph to a new graph by following these steps: 1) add edges between nodes with the same class predictions if both prediction confidences surpass a threshold, 2) remove edge between nodes with different class predictions if both prediction confidences surpass a threshold. Then, we trained a GCN model on the new structure and repeated the aforementioned steps to generate a new structure. We did this iteratively until generating a new structure did not provide a boost in performance on the validation set. For SLAPS, we followed a similar approach except that the initial model was a SLAPS model instead of a kNN-GCN model.</p><p>kNN Implementation: For our MLP generator, we used a kNN operation to sparsify the generated graph. Here, we explain how we implemented the kNN operation to avoid blocking the gradient flow. Let M ? R n?n with M ij = 1 if v j is among the top k similar nodes to v i and 0 otherwise, and let S ? R n?n with S ij = Sim(X i , X j ) for some differentiable similarity function Sim (we used cosine). Then? = kNN(X ) = M S where represents the Hadamard (element-wise) product. With this formulation, in the forward phase of the network, one can first compute the matrix M using an off-the-shelf k-nearest neighbors algorithm and then compute the similarities in S only for pairs of nodes where M ij = 1. In our experiments, we compute exact k-nearest neighbors; one can approximate it using locality-sensitive hashing approaches for larger graphs (see, e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref>).</p><p>In the backward phase of our model, we compute the gradients only with respect to those elements in S whose corresponding value in M is 1 (i.e. those elements S ij such that M ij = 1); the gradient with respect to the other elements is 0. Since S is computed based on X , the gradients flow to the elements in X (and consequently to the weights of the MLP) through S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjacency processor:</head><p>We used a function P in our adjacency processor to make the values of the? positive. In our experiments, when using an MLP generator, we let P be the ReLU function applied element-wise on the elements of?. When using the fully-parameterized (FP) generator, applying ReLU results in a gradient flow problem as any edge whose corresponding value in? becomes less than or equal to zero stops receiving gradient updates. For this reason, for FP we apply the ELU <ref type="bibr" target="#b7">[8]</ref> function to the elements of? and then add a value of 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Dataset statistics</head><p>The statistics of the datasets used in the experiments can be found in <ref type="table" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Supervision starvation in Erd?s-R?nyi and scale-free networks</head><p>We start by defining some new notation that helps simplify the proofs and analysis in this section. We let l v be a random variable indicating that v is a labeled node, with l v indicating that its negation, c v,u be a random variable indicating that v is connected to u with an edge, with c v,u indicating its negation, and cl v be random variable indicating that v is connected to at least one labeled node with cl v indicating its negation (i.e. it indicates that v is connected to no labeled nodes).</p><p>Theorem 2 Let G(n, m) be an Erd?s-R?nyi graph with n nodes and m edges. Assume we have labels for q nodes selected uniformly at random. The probability of an edge being a starved edge with a two-layer GCN is equal to</p><formula xml:id="formula_4">(1 ? q n )(1 ? q n?1 ) 2q i=1 (1 ? m?1 ( n 2 )?i ).</formula><p>Proof 1 To compute the probability of an edge being a starved edge, we first compute the probability of the two nodes of the edge being unlabeled themselves and then the probability of the two nodes not being connected to any labeled nodes. Let v and u represent two nodes connected by an edge.</p><p>With n nodes and q labels, the probability of a node being labeled is q n . Therefore, P r(l v ) = (1 ? q n ) and P r(l u | l v ) = (1 ? q n?1 ). Therefore, P r(l v ? l u ) = (1 ? q n )(1 ? q n?1 ). Since there is an edge between v and v, there are m ? 1 edges remaining. Also, there are n 2 ? 1 pairs of nodes that can potentially have an edge between them. Therefore, the probability of v being disconnected from the first labeled node is 1 ? m?1 ( n 2 )?1 .</p><p>If v is disconnected from the first labeled node, there are still m ? 1 edges remaining and there are now n 2 ? 2 pairs of nodes that can potentially have an edge between them. So the probability of v being disconnected from the second node given that it is disconnected from the first labeled node is 1 ? m?1 ( n 2 )?2</p><p>. With similar reasoning, we can see that the probability of v being disconnected from the i-th labeled node given that it is disconnected from the first i ? 1 labeled nodes is 1 ? m?1 ( n 2 )?i . We can follow similar reasoning for u. The probability of u being disconnected from the first labeled node given that v is disconnected from all q labeled nodes is 1 ? m?1 ( n 2 )?q?1 . That is because there are still m ? 1 edges remaining and n 2 ? q ? 1 pairs of nodes that can potentially be connected with an edge. We can also see that the probability of u being disconnected from the i-th labeled node given that it is disconnected from the first i ? 1 labeled nodes and that v is disconnected from all q labeled nodes is 1 ? m?1 ( n 2 )?q?i .</p><p>As the probability of the two nodes being unlabeled and not being connected to any labeled nodes in the graph are independent, their joint probability is the multiplication of their probabilities computed above and it is equal to</p><formula xml:id="formula_5">(1 ? q n )(1 ? q n?1 ) 2q i=1 (1 ? m?1 ( n 2 )?i ).</formula><p>Barab?si-Albert and scale-free networks: We also extend the above result for Erd?s-R?nyi graphs to the Barab?si-Albert <ref type="bibr" target="#b1">[2]</ref> model. Since Barab?si-Albert graph generation results in scale-free networks with a scale parameter ? = ?3, we present results for the general case of scale-free networks as it makes the analysis simpler and more general. In what follows, we compute the probability of an edge being a starved edge in a scale-free network. Let G be a scale-free network with n nodes, q labels (selected uniformly at random), and scale parameter ?. Then, if we select a random edge between two nodes v and u, the probability of the edge between them being a starved edge is: P r(l v ) * P r(l u |l v ) * P r(cl v |c v,u , l v , l u ) * P r(cl u |c v,u , l v , l u , cl v ).</p><p>Each of these terms can be computed as follows ( a b represents the number of combinations of selecting b items from a set with a items):</p><p>? P r(l v ) = (1 ? q n ) ? P r(l u |l v ) = (1 ? q n?1 )</p><p>? P r(cl v |c v,u , l u , l v ) = n?1 k=1 k ? ( n?q?2 k?1 ) ( n?2 k?1 ) n?1 k=1 k ? For a large enough network, P r(cl u |c v,u , l v , l u , cl v ) can be approximated as P r(cl u |c v,u , l v , l u ) and it can be computed similarly as the previous case.</p><p>With the derivation above, for a scale-free network with n = 2708 and q = 140 (corresponding to the stats from Cora), the probability of an edge being a starved edge for ? = ?3 is 0.87 and for ? = ?2 is 0.76 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Why not compare the learned graph structures with the original ones?</head><p>A comparison between the learned graph structures using SLAPS (or other baselines) and the original graph structure of the datasets we used may not be sensible. We explain this using an example. Before getting into the example, we remind the reader that the goal of structure learning for semi-supervised classification with graph neural networks is to learn a structure with a high degree of homophily. Following <ref type="bibr" target="#b62">[63]</ref>, we define the edge homophily ratio as the fraction of edges in the graph that connect nodes that have the same class label. <ref type="figure" target="#fig_0">Figure 10</ref> demonstrates an example where two graph structures for the same set of nodes have the same edge homophily ratio (0.8 for both) but have no edges in common. For our task, it is possible that the original graph structure (e.g., the citation graph in Cora) corresponds to the structure on the left but SLAPS (or any other model) learns the graph on the right, or vice versa. While both these structures may be equally good 6 , they do not share any edges. Therefore, measuring the quality of the learned graph using SLAPS by comparing it to the original graph of the datasets may not be sensible. However, if a noisy version of the initial structure is provided as input for SLAPS, then one may expect that SLAPS recovers a structure similar to the cleaned original graph and this is indeed what we demonstrate in the main text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Limitations</head><p>In this section, we discuss some of the limitations of the proposed model. Firstly, in cases where nodes do not have input features but an initial noisy structure of the nodes is available, our self-supervised task cannot be readily applied. One possible solution is to first run an unsupervised node embedding model such as DeepWalk <ref type="bibr" target="#b41">[42]</ref> to obtain node embeddings, then treat these embeddings as node features and run SLAPS. Secondly, the FP graph generator is not applicable in the inductive setting; this is because FP directly optimizes the adjacency matrix. However, our other two graph generators (MLP and MLP-D) can be applied in the inductive setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of SLAPS. At the top, a generator receives the node features and produces a non-symmetric, non-normalized adjacency having (possibly) both positive and negative values (Section 4.1). The adjacency processor makes the values positive, symmetrizes and normalizes the adjacency (Section 4.2). The resulting adjacency and the node features go into GNN C which predicts the node classes (Section 4.3). At the bottom, some noise is added to the node features. The resulting noisy features and the generated adjacency go into GNN DAE which then denoises the features (Section 4.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Using a two-layer GCN, the predictions made for the labeled nodes are not affected by the dashed (starved) edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>SLAPS vs SLAPS 2s on Cora with different generators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>The performance of SLAPS with MLP graph generator as a function of ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Performance comparison when noisy graphs are provided as input (? indicates the percentage of perturbations).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>The odds of two nodes in the test set sharing the same label as a function of the edge weights learned by SLAPS.while SLAPS with ? = 0 only removed 62.8% and 54.9% of the noisy edges and recovered 51.4% and 35.8% of the removed edges. This provides evidence on self-supervision being helpful for structure learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>The performance of SLAPS on Cora as a function of k in kNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>The performance of SLAPS on Cora and Citeseer with different adjacency symmetrizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>The performance of SLAPS and regularization toward a manually defined prior structure on Cora and Citeseer when using the MLP generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Two example graph structures. Node colors indicates the class labels. Solid lines indicate homophilous edges and dashed lines indicate non-homophilous edges. The two graphs exhibit the same degree of homophily yet there is not overlap between their edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of SLAPS and the baselines on established node classification benchmarks. ? indicates results have been taken from Franceschi et al.<ref type="bibr" target="#b12">[13]</ref>. ? indicates results have been taken from Stretcu et al.<ref type="bibr" target="#b46">[47]</ref>. Bold and underlined values indicate best and second-best mean performances respectively. OOM indicates out of memory. OOT indicates out of time (we allowed 24h for each run). NA indicates not applicable. ? 1.6 ? 56.7 ? 1.7 ? 65.8 ? 0.4 67.1 ? 0.5 71.4 ? 0.0 54.7</figDesc><table><row><cell>Model</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Cora390 Citeseer370</cell><cell>Pubmed</cell><cell>ogbn-arxiv</cell></row><row><cell>MLP</cell><cell>56.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>3 67.3 ? 0.8 71.3 ? 0.9 70.9 ? 0.7 67.3 ? 0.3 OOM DGCNN 56.5 ? 1.2 55.1 ? 1.4 67.3 ? 0.7 66.6 ? 0.8 70.1 ? 1.3 ? 0.3 72.6 ? 0.6 75.1 ? 0.5 73.9 ? 0.4 73.1 ? 0.7 52.9 ? 0.1 SLAPS (MLP) + AdaEdge 72.8 ? 0.7 70.6 ? 1.5 75.2 ? 0.6 72.6 ? 1.4 OOT OOT SLAPS (MLP) + self-training 74.2 ? 0.5 73.1 ? 1.0 75.5 ? 0.7 73.3 ? 0.6 74.3 ? 1.4 NA</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>OOM</cell></row><row><cell>IDGL</cell><cell cols="2">70.9 ? 0.6 68.2 ? 0.6 73.4 ? 0.5 72.7 ? 0.4 72.3 ? 0.4</cell><cell>OOM</cell></row><row><cell>kNN-GCN + AdaEdge</cell><cell>67.7 ? 1.0 68.8 ? 1.0 72.2 ? 0.4 71.8 ? 0.6</cell><cell>OOT</cell><cell>OOT</cell></row><row><cell>kNN-GCN + self-training</cell><cell cols="2">67.3 ? 0.3 69.8 ? 1.0 71.1 ? 0.3 72.4 ? 0.2 72.7 ? 0.1</cell><cell>NA</cell></row><row><cell>SLAPS (FP)</cell><cell>72.4 ? 0.4 70.7 ? 0.4 76.6 ? 0.4 73.1 ? 0.6</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell>SLAPS (MLP)</cell><cell cols="3">72.8 ? 0.8 70.5 ? 1.1 75.3 ? 1.0 73.0 ? 0.9 74.4 ? 0.6 56.6 ? 0.1</cell></row><row><cell>SLAPS (MLP-D)</cell><cell>73.4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on classification datasets. ? indicates results have been taken from Franceschi et al. [13]. Bold and underlined values indicate best and second-best mean performances respectively. ? 0.4 95.4 ? 0.4 46.3 ? 0.3 LDS 97.3 ? 0.4 ? 94.4 ? 1.9 ? 92.5 ? 0.7 ? 46.4 ? 1.6 ?</figDesc><table><row><cell>Model</cell><cell>Wine</cell><cell>Cancer</cell><cell>Digits</cell><cell>20news</cell></row><row><cell>MLP</cell><cell>96.1 ? 1.0</cell><cell>95.3 ? 0.9</cell><cell>81.9 ? 1.0</cell><cell>30.4 ? 0.1</cell></row><row><cell cols="3">kNN-GCN 95.3 IDGL 93.5 ? 0.7 97.0 ? 0.7 94.2 ? 2.3</cell><cell>92.5 ? 1.3</cell><cell>48.5 ? 0.6</cell></row><row><cell>SLAPS (FP)</cell><cell>96.6 ? 0.4</cell><cell>94.6 ? 0.3</cell><cell>94.4 ? 0.7</cell><cell>44.4 ? 0.8</cell></row><row><cell>SLAPS (MLP)</cell><cell>96.3 ? 1.0</cell><cell>96.0 ? 0.8</cell><cell>92.5 ? 0.7</cell><cell>50.4 ? 0.7</cell></row><row><cell>SLAPS (MLP-D)</cell><cell>96.5 ? 0.8</cell><cell>96.6 ? 0.2</cell><cell>94.2 ? 0.1</cell><cell>49.8 ? 0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on the MNIST dataset. Bold values indicate best mean performances. Underlined values indicate second best mean performance. All the results for baseline have been taken from [21]. ManiReg 92.74 ? 0.3 93.96 ? 0.2 94.62 ? 0.2 LP 79.28 ? 0.9 81.91 ? 0.8 83.45 ? 0.5 DeepWalk 94.55 ? 0.3 95.04 ? 0.3 95.34 ? 0.3 GCN 90.59 ? 0.3 90.91 ? 0.2 91.01 ? 0.2 GAT 92.11 ? 0.4 92.64 ? 0.3 92.81 ? 0.3 GLCN 94.28 ? 0.3 95.09 ? 0.2 95.46 ? 0.2 SLAPS 94.66 ? 0.2 95.35 ? 0.1 95.54 ? 0.0</figDesc><table><row><cell>Model</cell><cell>MNIST1000 MNIST2000 MNIST3000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Best set of hyperparameters for different datasets chosen on validation set.</figDesc><table><row><cell>Dataset</cell><cell>Generator</cell><cell>lrC</cell><cell cols="3">lrDAE dropoutc dropoutDAE</cell><cell>k</cell><cell>?</cell><cell>r</cell><cell>?</cell></row><row><cell>Cora</cell><cell>FP</cell><cell>0.001</cell><cell>0.01</cell><cell>0.5</cell><cell>0.25</cell><cell>30</cell><cell>10</cell><cell cols="2">10 5</cell></row><row><cell>Cora</cell><cell>MLP</cell><cell>0.01</cell><cell>0.001</cell><cell>0.25</cell><cell>0.5</cell><cell>20</cell><cell>10</cell><cell cols="2">10 5</cell></row><row><cell>Cora</cell><cell>MLP-D</cell><cell>0.01</cell><cell>0.001</cell><cell>0.25</cell><cell>0.5</cell><cell>15</cell><cell>10</cell><cell cols="2">10 5</cell></row><row><cell>Citeseer</cell><cell>FP</cell><cell>0.01</cell><cell>0.01</cell><cell>0.5</cell><cell>0.5</cell><cell>30</cell><cell>1</cell><cell cols="2">10 1</cell></row><row><cell>Citeseer</cell><cell>MLP</cell><cell>0.01</cell><cell>0.001</cell><cell>0.25</cell><cell>0.5</cell><cell>30</cell><cell>10</cell><cell cols="2">10 5</cell></row><row><cell>Citeseer</cell><cell>MLP-D</cell><cell>0.001</cell><cell>0.01</cell><cell>0.5</cell><cell>0.5</cell><cell>20</cell><cell>10</cell><cell cols="2">10 5</cell></row><row><cell>Cora390</cell><cell>FP</cell><cell>0.01</cell><cell>0.01</cell><cell>0.25</cell><cell>0.5</cell><cell cols="4">20 100 10 5</cell></row><row><cell>Cora390</cell><cell>MLP</cell><cell>0.01</cell><cell>0.001</cell><cell>0.25</cell><cell>0.5</cell><cell>20</cell><cell>10</cell><cell cols="2">10 5</cell></row><row><cell>Cora390</cell><cell>MLP-D</cell><cell>0.001</cell><cell>0.001</cell><cell>0.25</cell><cell>0.5</cell><cell>20</cell><cell>10</cell><cell cols="2">10 5</cell></row><row><cell>Citeseer370</cell><cell>FP</cell><cell>0.01</cell><cell>0.01</cell><cell>0.5</cell><cell>0.5</cell><cell>30</cell><cell>1</cell><cell cols="2">10 1</cell></row><row><cell>Citeseer370</cell><cell>MLP</cell><cell>0.01</cell><cell>0.001</cell><cell>0.25</cell><cell>0.5</cell><cell>30</cell><cell>10</cell><cell cols="2">10 5</cell></row><row><cell>Citeseer370</cell><cell>MLP-D</cell><cell>0.01</cell><cell>0.01</cell><cell>0.25</cell><cell>0.5</cell><cell>20</cell><cell>10</cell><cell cols="2">10 5</cell></row><row><cell>Pubmed</cell><cell>MLP</cell><cell>0.01</cell><cell>0.01</cell><cell>0.5</cell><cell>0.5</cell><cell>15</cell><cell>10</cell><cell cols="2">10 5</cell></row><row><cell>Pubmed</cell><cell>MLP-D</cell><cell>0.01</cell><cell>0.01</cell><cell>0.25</cell><cell>0.25</cell><cell cols="2">15 100</cell><cell>5</cell><cell>5</cell></row><row><cell>ogbn-arxiv</cell><cell>MLP</cell><cell>0.01</cell><cell>0.001</cell><cell>0.25</cell><cell>0.5</cell><cell>15</cell><cell>10</cell><cell>1</cell><cell>5</cell></row><row><cell>ogbn-arxiv</cell><cell>MLP-D</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>0.25</cell><cell>15</cell><cell>10</cell><cell>1</cell><cell>5</cell></row><row><cell>Wine</cell><cell>FP</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>0.5</cell><cell cols="2">20 0.1</cell><cell>5</cell><cell>5</cell></row><row><cell>Wine</cell><cell>MLP</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>0.25</cell><cell cols="2">20 0.1</cell><cell>5</cell><cell>5</cell></row><row><cell>Wine</cell><cell>MLP-D</cell><cell>0.01</cell><cell>0.01</cell><cell>0.25</cell><cell>0.5</cell><cell>10</cell><cell>1</cell><cell>5</cell><cell>5</cell></row><row><cell>Cancer</cell><cell>FP</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>0.25</cell><cell cols="2">20 0.1</cell><cell>5</cell><cell>5</cell></row><row><cell>Cancer</cell><cell>MLP</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>0.5</cell><cell cols="2">20 1.0</cell><cell>5</cell><cell>5</cell></row><row><cell>Cancer</cell><cell>MLP-D</cell><cell>0.01</cell><cell>0.01</cell><cell>0.5</cell><cell>0.5</cell><cell cols="2">20 0.1</cell><cell>5</cell><cell>5</cell></row><row><cell>Digits</cell><cell>FP</cell><cell>0.01</cell><cell>0.001</cell><cell>0.25</cell><cell>0.5</cell><cell cols="2">20 0.1</cell><cell>5</cell><cell>5</cell></row><row><cell>Digits</cell><cell>MLP</cell><cell>0.01</cell><cell>0.001</cell><cell>0.25</cell><cell>0.5</cell><cell>20</cell><cell>10</cell><cell>5</cell><cell>5</cell></row><row><cell>Digits</cell><cell>MLP-D</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>0.25</cell><cell cols="2">15 0.1</cell><cell>5</cell><cell>5</cell></row><row><cell>20news</cell><cell>FP</cell><cell>0.01</cell><cell>0.01</cell><cell>0.5</cell><cell>0.5</cell><cell cols="2">20 500</cell><cell>5</cell><cell>5</cell></row><row><cell>20news</cell><cell>MLP</cell><cell>0.001</cell><cell>0.001</cell><cell>0.25</cell><cell>0.5</cell><cell cols="2">20 500</cell><cell>5</cell><cell>5</cell></row><row><cell>20news</cell><cell>MLP-D</cell><cell>0.01</cell><cell>0.01</cell><cell>0.25</cell><cell>0.25</cell><cell cols="2">20 100</cell><cell>5</cell><cell>5</cell></row><row><cell>MNIST (1000)</cell><cell>MLP</cell><cell>0.01</cell><cell>0.01</cell><cell>0.5</cell><cell>0.5</cell><cell>15</cell><cell>10</cell><cell cols="2">10 5</cell></row><row><cell>MNIST (2000)</cell><cell>MLP-D</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>0.5</cell><cell cols="4">15 100 10 5</cell></row><row><cell>MNIST (3000)</cell><cell>MLP</cell><cell>0.01</cell><cell>0.01</cell><cell>0.5</cell><cell>0.5</cell><cell>15</cell><cell>10</cell><cell>5</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="2">Classes Features</cell><cell>Label rate</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>0.052</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>0.036</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>3</cell><cell>500</cell><cell>0.003</cell></row><row><cell cols="3">ogbn-arxiv 169,343 1,166,243</cell><cell>40</cell><cell>128</cell><cell>0.537</cell></row><row><cell>Wine</cell><cell>178</cell><cell>0</cell><cell>3</cell><cell>13</cell><cell>0.112</cell></row><row><cell>Cancer</cell><cell>569</cell><cell>0</cell><cell>2</cell><cell>30</cell><cell>0.035</cell></row><row><cell>Digits</cell><cell>1,797</cell><cell>0</cell><cell>10</cell><cell>64</cell><cell>0.056</cell></row><row><cell>20news</cell><cell>9,607</cell><cell>0</cell><cell>10</cell><cell>236</cell><cell>0.021</cell></row><row><cell>MNIST</cell><cell>10,000</cell><cell>0</cell><cell>10</cell><cell>784</cell><cell>0.1, 0.2 and 0.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">While using more layers may somewhat alleviate this problem, deeper GCNs typically produce inferior results, e.g., due to oversmoothing<ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref> -see the supplementary material for empirical evidence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We note that IDGL-ANCH also scales to ogbn-arxiv.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The generator used in this experiment is MLP; other generators produced similar results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For the first node in the example, the 4th and 5th features are both 1 so their logical and is also 1 and so the last two features for this node are both 1. The computation for the other two nodes is similar.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We are disregarding the features for simplicity sake.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?ka</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-L?szl?</forename><surname>Barab?si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of modern physics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-L?szl?</forename><surname>Barab?si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?ka</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLRR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<title level="m">Machine learning on graphs: A model and comprehensive taxonomy</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep iterative and adaptive learning for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Latent patient network learning for automatic diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13620</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02371</idno>
		<title level="m">Adversarial attack on graph structured data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Erd?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>R?nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publicationes Mathematicae Debrecen</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">How robust are graph neural networks to structural noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivasankaran</forename><surname>Rajamanickam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10206</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Grale: Designing networks for graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Halcrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Mosoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2523" to="2532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soobeom</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Eun</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Seok</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11678</idno>
		<title level="m">Brain signal classification via learning connectivity structure</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with graph learning-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doudou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11313" to="11320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with graph learning-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doudou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11313" to="11320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<title level="m">Self-supervised learning on graphs: Deep insights and new direction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10203</idno>
		<title level="m">Graph structure learning for robust graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning graph structure with a finite-state automaton layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarlow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Representation learning for dynamic graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishab</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poupart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Differentiable graph module (dgm) for graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04999</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>ATT Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03226</idno>
		<title level="m">Adaptive graph convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient graph generation with graph recurrent attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13556" to="13566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large graph construction for scalable semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning representations of irregular particle-detector geometry with distance-weighted graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Shah Rukh Qasim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Kieseler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Iiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal C</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Nonlinear dimensionality reduction by locally linear embedding. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph agreement models for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otilia</forename><surname>Stretcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnamurthy</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanouil</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8713" to="8723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mixture-kernel graph attention network for situation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10363" to="10372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A global geometric framework for nonlinear dimensionality reduction. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Joshua B Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<title level="m">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A quest for structure: Jointly learning the graph structure and semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Topology optimization based graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zesheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4054" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphrnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08773</idno>
		<title level="m">Generating realistic graphs with deep auto-regressive models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">When does self-supervision help graph convolutional networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09136</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graph-revised convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<title level="m">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06830</idno>
		<title level="m">Data augmentation for graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Graph neural networks with generated parameters for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00756</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Tat-seng Chua, and Maosong Sun</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Self-supervised training of graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qikui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingkun</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02380</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
		<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03036</idno>
		<title level="m">Deep graph structure learning for robust representations: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
