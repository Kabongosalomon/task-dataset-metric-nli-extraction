<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ATST: Audio Representation Learning with Teacher-Student Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
							<email>lixian@westlake.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Li</surname></persName>
							<email>lixiaofei@westlake.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ATST: Audio Representation Learning with Teacher-Student Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Audio pretraining</term>
					<term>Self-supervised learning</term>
					<term>Teacher-student model</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning (SSL) learns knowledge from a large amount of unlabeled data, and then transfers the knowledge to a specific problem with a limited number of labeled data. SSL has achieved promising results in various domains. This work addresses the problem of segment-level general audio SSL, and proposes a new transformer-based teacher-student SSL model, named ATST. A transformer encoder is developed on a recently emerged teacher-student baseline scheme, which largely improves the modeling capability of pre-training. In addition, a new strategy for positive pair creation is designed to fully leverage the capability of transformer. Extensive experiments have been conducted, and the proposed model achieves the new stateof-the-art results on almost all of the downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, learning audio representations with self-supervised learning (SSL) has been widely studied <ref type="bibr" target="#b0">[1]</ref>[2] <ref type="bibr" target="#b2">[3]</ref>[4] <ref type="bibr" target="#b4">[5]</ref>. Among them, the contrastive learning methods <ref type="bibr" target="#b2">[3]</ref>[4] <ref type="bibr" target="#b4">[5]</ref> maximize the classification similarity of two augmented views of the same audio clip (called a positive pair), having shown a great promise for learning good representation. The above idea often confronts the issue of model collapse, e.g. the model can find an easy solution to output a constant value for any inputs. COLA <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b3">[4]</ref> overcome this issue by distinguishing positive audio samples from a batch of negative audio samples. Considering the fact that for audio data, negative samples are possibly similar to positive samples in some scenarios, BYOL-A <ref type="bibr" target="#b4">[5]</ref> proposed to discard the negative samples, and use a teacher-student scheme to overcome the issue of model collapse. The teacher and student networks process two different views of an audio clip, and the student network is trained to predict a representation being identical to the prediction of the teacher model. The teacher network is updated by taking an exponential moving average (EMA) of the student network. Another technical line for audio SSL follows the spirit of Bert <ref type="bibr" target="#b5">[6]</ref>, performing a predictive task for the masked frames, e.g. wav2vec2 <ref type="bibr" target="#b6">[7]</ref> and Hubert <ref type="bibr" target="#b7">[8]</ref>.</p><p>Transformer network has shown powerful abilities in learning long-term dependencies, and has been used for speech SSL in several works, e.g. MockingJay <ref type="bibr" target="#b8">[9]</ref>, wav2vec2 <ref type="bibr" target="#b6">[7]</ref>, Hubert <ref type="bibr" target="#b7">[8]</ref>, Tera <ref type="bibr" target="#b9">[10]</ref>. As for general audio SSL, people usually use convolution neural network (CNN), e.g. in COLA, BYOL-A, etc. To the best of our knowledge, SSAST <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref> are the only two very recent works that use transformer for general audio SSL. They both follow the line of wav2vec2 <ref type="bibr" target="#b6">[7]</ref>.</p><p>According to the grain size of the representation at the pretraining stage, all the above methods can be categorized into two types: segment-level method and frame-level method. The * corresponding author segment-level methods, e.g. COLA <ref type="bibr" target="#b2">[3]</ref> and BYOL-A <ref type="bibr" target="#b4">[5]</ref>, extract a fixed-length segment embedding from an input audio segment. On the other hand, the frame-level methods, e.g. SSAST <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref>, extract an individual embedding for all frames. Learning a segment embedding is suitable for a variety of segment-level audio tasks, e.g. sound event classification, music instrument classification, speaker identification, etc. COLA <ref type="bibr" target="#b2">[3]</ref> and BYOL-A <ref type="bibr" target="#b4">[5]</ref> have been proven very effective for segment-level general audio SSL, and have achieved the state-of-the-art performance. Although SSAST <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref> are pre-trained by a frame-level criterion, the downstream tasks that they have applied to are all segment-level tasks, where average pooling is applied to obtain the segment embedding.</p><p>This work focuses on the problem of segment-level general audio SSL, and proposes a new transformer-based teacherstudent SSL model, named ATST 1 . Main contributions of this work include: i) adopting transformer encoder into the baseline teacher-student scheme of BYOL-A <ref type="bibr" target="#b4">[5]</ref>, which shows a clear superiority over the CNN encoder of BYOL-A, especially for learning the long-term semantic information of speech; ii) proposing a new view creation strategy. BYOL-A uses one short segment to create two views (one positive pair). Instead, we propose to use two different long segments, which is more fit for transformer, as the network needs to learn longer temporal dependencies and to match a more distinct positive pair created by two segments. The length of segments is carefully studied to control the distinction and overlap of the two segments, which is especially important for rationalizing the difficulty of matching positive pairs. Experiments have been conducted using the large-scale Audioset <ref type="bibr" target="#b12">[13]</ref> dataset for pre-training. Downstream tasks cover all the three types of audio signals, namely audio event, speech, and music. Ablation experiments show the effectiveness of each of the proposed modules. The proposed model as a whole achieves the new state-of-the-art results on almost all of the downstream tasks, and surpasses other methods by a large margin on some of the downstream tasks. For example, the accuracy of speaker identification is 72% versus 40.1% without finetuning, and 94.3% versus 80.8% after finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Baseline Teacher-Student Scheme</head><p>In this work, we adopt the teacher-student scheme as our baseline framework, which was first proposed by Bootstrap you own latent (BYOL) <ref type="bibr" target="#b13">[14]</ref> for image pre-training, and adopted by BYOL-A <ref type="bibr" target="#b4">[5]</ref> for audio pre-training. Given one augmented view of an audio clip, the student network is trained to predict a data representation being identical to the teacher network's prediction on one another augmented view of the same audio clip. During training, the teacher network is updated by taking the EMA of student network. Specifically, the student network, de- fined by a set of weights ?, contains an encoder f ? , a projector g ? and a predictor q ? , while the teacher network, defined by a set of weights ?, contains only an encoder f ? and a projector g ? . The encoders extract a representation from the augmented views. It has been shown that the additional predictor in the student network combined with the stop-gradient operation introduced by using EMA teacher network is the key factor that prevents the model from collapsing <ref type="bibr" target="#b14">[15]</ref>. During training, ? is updated by the EMA of ? as:</p><formula xml:id="formula_0">? ? m? + (1 ? m)?, where</formula><p>m is a decay rate. ? is updated as follows. Let (X, X ? ) be a pair of positive views created from an audio clip. X is fed into the teacher network to obtain h = f ? (X) and z = g ? (h).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head><p>? is fed into the student network to obtain h</p><formula xml:id="formula_1">? = f ? (X ? ), z ? = g ? (h ? ) and q ? (z ? )</formula><p>. z and q ? (z ? ) are then L2-norm normalized, and the mean square error between them is calculated as L ? . A symmetric loss L ? ? is also calculated by feeding X to the student network and X ? to the teacher network. During training, ? is updated by minimizing L total ? = L ? + L ? ? . In BYOL-A, encoder is a CNN, projectors and predictors are multi-layer perceptrons (MLPs) that consist in a linear layer (with output dimension of 4096) followed by batch normalization, rectified linear units (RELU), and a final linear layer (with output dimension of 256).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Proposed Method</head><p>Overview of the proposed method is depicted in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>. The major differences between the proposed method and BYOL-A are two folds. The proposed method uses a transformer as encoder to leverage its powerful abilities on modeling long-term dependencies, and uses a new view (positive pair) creation strategy specifically being fit for the transformer encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Creation of Views</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BYOL-A [5]</head><p>randomly crops a single 1-second segment from the input audio and then creates two views by applying different data augments to this single segment. It is considered in BYOL-A <ref type="bibr" target="#b4">[5]</ref> that different segments may be too different to be identified as a positive pair. <ref type="bibr" target="#b3">[4]</ref> uses two segments to create positive views, however, it uses negative views to mitigate the problem caused by using two segments.</p><p>Our view creation strategy is shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>. The time domain input audio clip is first transformed to melspectrogram. We randomly crop two different segments from the mel-spectrogram. Then, two types of data augmentation are applied to each of the segments, including Mixup <ref type="bibr" target="#b4">[5]</ref> and Random Resized Crop (RRC) <ref type="bibr" target="#b4">[5]</ref>, creating two views of the input audio clip, i.e. (X, X ? ). Note that this work does not use negative samples. In order to take full advantage of the transformer's ability in modeling long-term dependencies, the proposed method intends to use longer segments, e.g. 6 seconds in experiments. The proposed method separately creates two views from two different segments for the purpose of increasing the difficulty of identifying the two views as a positive pair, thus leading the model to learn more generalized representations. On the other hand, the two segments cannot be too far away from each other, otherwise the similarity between them is completely lost. This is guaranteed by properly setting the segment length to make the two segments have a certain portion of overlap. Overall, the proposed strategy does not lose the rationality of identifying two segments as a positive pair due to the overlap constraint, and meanwhile increases the task difficulty and thus the model capability by using two segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Transformer Encoder</head><p>The encoding procedure is illustrated in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>. The augmented mel-spectrogram X ? R L?C , where L and C denote frames and channels respectively, is fed into a transformer encoder <ref type="bibr" target="#b15">[16]</ref> to obtain a fixed-length segment embedding h ? R 1?d , where d denotes the dimension of embedding.</p><p>Four consecutive frames of X are first stacked to reduce the temporal resolution and sequence length. The stacked frames are fed to a linear projection layer (with output dimension of d) to obtain a new embedding sequence E ? R L 4 ?d as the input sequence of transformer encoder. Besides this input sequence, we use an extra trainable class token CLS ? R 1?d to represent the entire segment, which is inserted to the beginning of the input sequence. This kind of segment class token is widely used for sentence embedding in neural language processing <ref type="bibr" target="#b5">[6]</ref>, global image embedding <ref type="bibr" target="#b16">[17]</ref>, as well as audio segment embedding <ref type="bibr" target="#b17">[18]</ref>. A trainable absolute lookup table positional embedding P ? R ( L 4 +1)?d is then added to the input sequence. Eventually, we use a standard transformer encoder <ref type="bibr" target="#b15">[16]</ref> to process the input embedding sequence, obtaining an output embedding sequence of O ? R ( L 4 +1)?d . In the output sequence, the segment class token, i.e. O1, aggregates information from the embedding sequence at each block of the transformer, based on the self-attention mechanism. Therefore, O1 is taken as the final segment embedding, and is denoted as h or h ? in the teacherstudent pre-training scheme.</p><p>As for downstream tasks, the pre-trained transformer encoder of teacher network is used as the feature extractor, while the projector is removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We evaluate the performance of our model under the protocol of linear evaluation or finetuning. In linear evaluation, the pretrained encoder is frozen as a feature extractor, on top of which a linear classifier is trained. Whereas in finetuning, the pretrained encoder and linear classifier are finetuned together. Audio is re-sampled to 16 kHz. Audio clips are transformed to the mel-spectrogram domain, with a Hamming window, a window length of 25 ms, a hop size of 10 ms, and 64 frequency bins ranging from 60 Hz to 7800 Hz. The mel-spectrogram feature is min-max normalized, where the minimum and maximum values are calculated globally on the pre-training dataset. We intentionally set the length of two segments (for creating two views) to 6 seconds, which will leads to a segment overlap of at least 1 second, considering that the length of audio clip is 10 seconds. The two randomly sampled segments are augmented by Mixup and RRC with the same configurations used in BYOL-A <ref type="bibr" target="#b4">[5]</ref>.</p><p>We pre-train our models with the ADAMW optimizer <ref type="bibr" target="#b18">[19]</ref>. The learning rate lr is warmed up for 10 epochs, and then annealed to 1e-6 at cosine rate. Following DINO <ref type="bibr" target="#b16">[17]</ref>, the weight decay of transformer is increased from 0.04 to 0.4 at cosine rate. The EMA decay rate m increases from an initial value m0 to 1 at cosine rate. Batch size is set to 1536. The Base model is trained using AS-2M for 200 epochs, with lr being 2e-4, and m0 being 0.9995. The Small model is trained using AS-200K for 300 epochs, with lr being 5e-4, and m0 being 0.99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Downstream Tasks</head><p>Evaluations are carried out on a variety of downstream tasks, which cover all the three types of audio signals, namely audio event, speech and music. Datasets and downstream tasks are described as follows.</p><p>? AS-20K for multi-label sound event classification. We use the balanced subset of Audioset-2M, with 527 audio classes. It contains 20,886 audio clips for training. For test, we use the evaluation set of Audioset, with 18,886 audio clips. ? US8K for single-label audio scene classification. We use the Urbansound8k dataset <ref type="bibr" target="#b19">[20]</ref> to classify audio clips (less than 4 seconds) into 10 classes. It contains 8,732 audio clips and has ten folds for cross-validation. ? SPCV2 for spoken command recognition. We use Speech Command V2 <ref type="bibr" target="#b20">[21]</ref> to recognize 35 spoken commands for one second of audio. It contains 84,843, 9,981 and 11,005 audio clips for training, validation and evaluation, respectively.</p><p>? VOX1 for speaker identification. We use the Voxceleb1 dataset <ref type="bibr" target="#b21">[22]</ref>, with 1,251 speakers. It contains 13,8361, 6,904 and 8,251 for training, validation and evaluation, respectively. ? NSYNTH for music instrument classification. We use the NSYNTH dataset <ref type="bibr" target="#b22">[23]</ref>, to recognize 11 instrument family classes from 4-seconds audio clips.</p><p>The mel-spectrogram feature is computed in the same way as for the pre-training data. For linear evaluation, from the pretrained encoder, segment embedding is obtained by concatenating the class token O1 and the average of the embedding sequence of all blocks. For finetuning, segment embedding is obtained by concatenating the class token and the average of the embedding sequence of the last block. Audio clips that are longer than 12 seconds are centrally cropped with a maximum length of 12 seconds, and then split into 6-second long chunks without overlap. The chunks are independently processed by the pre-trained encoder, and their outputs are averaged to obtain the final segment embedding. Audio clips that are shorter than 6 seconds are directly processed by the pre-trained encoder to obtain the segment embedding.</p><p>For linear evaluation, we train the linear classifier for 100 epochs with the SGD optimizer. The learning rate is annealed to 1e-6 at cosine rate during training. The optimal initial learning rate is searched for each task separately. Batch size is set to 1024. Augmentation is not used.</p><p>For finetuning, we finetune all models with the SGD optimizer. The learning rate lr is warmed up for 5 epochs, and then annealed to 1e-6 at cosine rate. The optimal lr is searched for each task separately. Batch size is set to 512. We trained SPCV2 and VOX1 for 50 epochs, and AS-20K for 200 epochs. For SPCV2 and AS-20K, we use Mixup <ref type="bibr" target="#b23">[24]</ref> and RRC for data augmentation. As for supervised downstream tasks, Mixup mixes both audio clips and labels. For VOX1, data augmentation is not applied.</p><p>Classification accuracy (Acc) is taken as the performance metric for single-label tasks, including audio scene classification, spoken command recognition, speaker identification and music instrument classification, and mean average precision (mAP) for the task of multi-label sound event classification. For US8K, we conduct 10-fold cross-validation, and report the average accuracy of the 10 folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablation study</head><p>We separately evaluate the effectiveness of transformer encoder and the proposed view creation strategy. Ablation experiments are conducted using the Small model with the linear evaluation protocol, due to their low computational complexities. <ref type="table" target="#tab_0">Table 1</ref> shows the results. The result of BYOL-A is also given, which uses a CNN encoder and a single 1-second segment. Our models use a single or two segments, with a length of 1 second or Normalized Acc/mAP SPCV2 VOXCELEB1 NSYNTH AS-20K US8K Avg <ref type="figure">Figure 2</ref>: Normalized Acc/mAP as a function of segment length, Acc/mAP of each task is normalized into the range of (0,1). "Avg" denotes averaging over normalized score of all tasks.</p><p>6 seconds. For a fair comparison, when the segment length is set to 1 second, we split audio clips into 1-second chunks for downstream tasks.</p><p>Transformer Encoder: With the same view creation strategy, i.e. creating two views from a 1-second segment, our model (line 2 in <ref type="table" target="#tab_0">Table 1</ref>) outperforms BYOL-A, especially for the two speech tasks ( SPCV2 and VOX1). Speech involves more longterm semantic information, and transformer is more suitable than CNN for learning these long-term dependencies.</p><p>View Creation Strategy: As shown in <ref type="table" target="#tab_0">Table 1</ref>, when the segment length is set to 1 second, using one single segment is better than using two segments. This phenomenon is consistent with the claim made in BYOL-A <ref type="bibr" target="#b4">[5]</ref> that the two segments may be too different to be identified as a positive pair. However, two views created from a single segment may share too much semantic content, thus leading our model to find an easy solution. When the segment length is increased to 6 seconds, the performance measures of AS-20K, VOX1 and US8K are systematically increased, no matter whether using one or two segments. This is partially due to the capability of learning longterm dependencies of the transformer encoder. In addition, for the 6-seconds case, using two segments exhibits superior performance over using one segment. The possible reasons are: the two segments can be rationally identified as a positive pair as they share a small portion of overlap, and meanwhile they are different enough to increases the task difficulty and thus leads the model to learn a more generalized representation. <ref type="figure">Fig. 2</ref> shows the normalized performance of each task as a function of segment length, where two segments are used. We can see that the performance measures increase along with the increasing of segment length until 6 seconds. This further verifies our new findings: i) when transformer encoder is used, increasing the segment length helps to learn more information; ii) when two segments are used, the segment length should be set to make the segments share a proper amount of overlap, and have a proper difficulty for matching them as a positive pair. <ref type="table" target="#tab_1">Table 2</ref> shows the linear evaluation results. For fair comparison, we compare with other methods that also use Audioset for pretraining, including TRILL <ref type="bibr" target="#b24">[25]</ref>, COLA <ref type="bibr" target="#b2">[3]</ref> and BYOL-A <ref type="bibr" target="#b4">[5]</ref>. The performance scores are directly quoted from their original papers. It can be seen that our Small model outperforms other methods on all tasks, even though it only uses 1/10 data of Audioset-2M, while BYOL-A and COLA use the full Audioset-2M dataset. In particular, on the speaker identification task, our Small model obtains an accuracy of 61.9%, compared to the 40.1% accuracy of BYOL-A. By increasing the network size  and the amount of training data, the performance can be further systematically increased by our Base model. The superiority of the proposed model comes from the use of transformer encoder and the proposed view creation strategy, which both are critical modules for the success of contrastive learning based pretraining technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Linear Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Fine-tuning Results</head><p>To evaluate to what extent our models can further achieve, finetuning experiments are conducted on the tasks of multi-label audio event classification (AS-20K), Spoken command recognition (SPCV2) and speaker identification (VOX1). We compare with those methods that also report the finetuning results, including COLA <ref type="bibr" target="#b2">[3]</ref>, SSAST <ref type="bibr" target="#b11">[12]</ref> and Conformer <ref type="bibr" target="#b11">[12]</ref>. <ref type="table" target="#tab_2">Table 3</ref> shows the results. Compared to the best results achieved by other methods, our Base model performs better on AS-20K (0.374 versus 0.310) and VOX1 (94.3% versus 80.8%) by a large margin, and perform comparably on SPCV2. Remarkably, our Small model performs even better than SSAST and Conformer on AS-20K and VOX1, by using a much smaller network (22 M versus about 88 M). It is worth mentioning that both SSAST and Conformer use a transformer encoder as the proposed model, and use a wav2vec2-style pre-training scheme. Better results achieved by the proposed model may indicate that the teacher-student scheme is superior to the wav2vec2-style scheme for (segment-level) general audio pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>In this work, we propose a general audio pre-training method with a transformer-based teacher-student scheme, named ATST. A new view creation strategy is also proposed to fully leverage the capability of transformer. We evaluate the learned representation on diverse downstream tasks. Experiments show that the proposed view creation strategy is able to improve pre-training by properly increasing the difficulty of positive pair matching.</p><p>Overall, the proposed model achieves the new state-of-the-art results on almost all of the tasks. We hope our work can facilitate the progress of general audio representation learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Overview of the proposed method. "Aug" denotes augmentation (b) Transformer encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Linear evaluation results of Small model w.r.t. different view creation strategies. "Average" is taken over the last four tasks.</figDesc><table><row><cell>Method</cell><cell>Segments</cell><cell>length of segment (s)</cell><cell>AS-20K mAP</cell><cell>SPCV2 Acc (%)</cell><cell>VOX1 Acc (%)</cell><cell>NSYNTH Acc (%)</cell><cell>US8K Acc (%)</cell><cell>Average Acc (%)</cell></row><row><cell>BYOL-A [5]</cell><cell>single</cell><cell>1</cell><cell>-</cell><cell>92.2</cell><cell>40.1</cell><cell>74.1</cell><cell>79.1</cell><cell>71.4</cell></row><row><cell>Small (Ours)</cell><cell>single two single two</cell><cell>1 1 6 6</cell><cell>0.210 0.191 0.257 0.279</cell><cell>94.3 91.3 94.0 93.6</cell><cell>52.3 50.0 57.3 61.9</cell><cell>73.8 74.3 73.8 75.3</cell><cell>79.3 76.6 80.9 82.0</cell><cell>74.9 73.1 76.5 78.2</cell></row><row><cell cols="3">3.1. Implementation Details of Pre-training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">We use Audioset [13] for pre-training. The full Audioset (AS-2M) contains 200 million audio clips with a length of 10 sec-onds captured from Youtube Videos. Using the full Audioset, a Base model is trained, which contains 12 blocks, and 12 heads for each block. The dimension and inner dimension are 768 and 3072 respectively. Besides, using a subset with 200 thou-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>sand audio clips (AS-200K) randomly sampled from AS-2M, we also trained a Small model, which contains 12 blocks, and 6 heads for each block. The dimension and inner dimension are 384 and 1536 respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Linear evaluation results.</figDesc><table><row><cell>Method</cell><cell>AS-20K mAP</cell><cell>SPCV2 Acc (%)</cell><cell>VOX1 Acc (%)</cell><cell>NSYNTH Acc (%)</cell><cell>US8K Acc (%)</cell></row><row><cell>TRILL [25] COLA [3] BYOL-A [5]</cell><cell>---</cell><cell>-62.4 92.2</cell><cell>17.9 29.9 40.1</cell><cell>-63.4 74.1</cell><cell>--79.1</cell></row><row><cell cols="2">Small (ours) 0.279 Base (ours) 0.338</cell><cell>93.6 95.1</cell><cell>61.9 72.0</cell><cell>75.3 75.6</cell><cell>82.0 84.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Finetuning results.</figDesc><table><row><cell>Method</cell><cell># Params</cell><cell>AS-20K mAP</cell><cell>SPCV2 Acc (%)</cell><cell>VOX1 Acc (%)</cell></row><row><cell>COLA [3] Small-SSAST [11] SSAST-PATCH [11] SSAST-FRAME [11] Conformer [12]</cell><cell>23M 89M 89M 88M</cell><cell>-0.308 0.310 0.292 0.276</cell><cell>95.5 97.7 98.0 98.1 -</cell><cell>37.7 60.9 64.2 80.8 -</cell></row><row><cell>Small (ours) Base (ours)</cell><cell>22M 86M</cell><cell>0.315 0.374</cell><cell>97.6 98.0</cell><cell>88.3 94.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">code: https://github.com/Audio-WestlakeU/audio ssl</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pre-Training Audio Representations With Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gfeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Chaumont Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="600" to="604" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10915</idno>
		<title level="m">Contrastive Learning of General-Purpose Audio Representations</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Contrastive Learning of Sound Event Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="371" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Niizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06695</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07447</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="6419" to="6423" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2351" to="2366" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><forename type="middle">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09784</idno>
		<title level="m">SSAST: Self-Supervised Audio Spectrogram Transformer</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Conformer-Based Self-Supervised Learning for Non-Speech Audio Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07313</idno>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised Learning</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring Simple Siamese Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
	</analytic>
	<monogr>
		<title level="j">Attention Is All You Need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Emerging Properties in Self-Supervised Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01778</idno>
		<title level="m">AST: Audio Spectrogram Transformer</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural audio synthesis of musical notes with wavenet autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1068" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards learning a universal non-semantic representation of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D C</forename><surname>Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Haviv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12764</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
