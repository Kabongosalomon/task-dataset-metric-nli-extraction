<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DUAL-TASKS SIAMESE TRANSFORMER FRAMEWORK FOR BUILDING DAMAGE ASSESSMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongruixuan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">United Nations Satellite Centre (UNOSAT)</orgName>
								<orgName type="institution">Institute for Training and Research (UNITAR)</orgName>
								<address>
									<country>United Nations</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Nemni</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">United Nations Satellite Centre (UNOSAT)</orgName>
								<orgName type="institution">Institute for Training and Research (UNITAR)</orgName>
								<address>
									<country>United Nations</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofia</forename><surname>Vallecorsa</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">European Organization for Nuclear Research (CERN)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Bromley</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">United Nations Satellite Centre (UNOSAT)</orgName>
								<orgName type="institution">Institute for Training and Research (UNITAR)</orgName>
								<address>
									<country>United Nations</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DUAL-TASKS SIAMESE TRANSFORMER FRAMEWORK FOR BUILDING DAMAGE ASSESSMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Transformer</term>
					<term>building damage assess- ment</term>
					<term>multi-task learning</term>
					<term>deep learning</term>
					<term>multitemporal im- ages</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate and fine-grained information about the extent of damage to buildings is essential for humanitarian relief and disaster response. However, as the most commonly used architecture in remote sensing interpretation tasks, Convolutional Neural Networks (CNNs) have limited ability to model the non-local relationship between pixels. Recently, Transformer architecture first proposed for modeling longrange dependency in natural language processing has shown promising results in computer vision tasks. Considering the frontier advances of Transformer architecture in the computer vision field, in this paper, we present a Transformerbased damage assessment architecture (DamFormer). In DamFormer, a siamese Transformer encoder is first constructed to extract non-local and representative deep features from input multitemporal image-pairs. Then, a multitemporal fusion module is designed to fuse information for downstream tasks. Finally, a lightweight dual-tasks decoder aggregates multi-level features for final prediction. To the best of our knowledge, it is the first time that such a deep Transformerbased network is proposed for multitemporal remote sensing interpretation tasks. The experimental results on the large-scale damage assessment dataset xBD demonstrate the potential of the Transformer-based architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Disasters such as hurricanes, earthquakes, floods, volcanic eruptions, and wildfires cause significant damage and economic losses every year. Timely and accurate building dam-age assessment is critical for humanitarian assistance, disaster response, and post-disaster reconstruction. Multitemporal very high-resolution satellite imageries are often used in the dual-task of identifying the extent of buildings and the severity of the damage.</p><p>Deep learning, especially Convolutional Neural Networks (CNNs), has shown great results in computer vision and remote sensing applications including building damage assessment <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. However, CNN lacks a global understanding of the images and the ability to model the non-local relationship between pixels. To capture long-range dependency for further performance improvement, some advanced modules, such as dilated convolution and self-attention mechanism were introduced <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Nonetheless, in addition to a significant computational cost, these modules were often designed at the end of CNN-backbone, which implies that the relationship in lowlevel information was not captured.</p><p>Transformers architectures <ref type="bibr" target="#b5">[6]</ref> have shown great performances in natural language processing (NLP) and their ability to model long-range dependencies had motivated researchers to explore its adaptation to computer vision tasks. In fact, the Vision Transformer (ViT) <ref type="bibr" target="#b6">[7]</ref> and its variants <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> outperformed the state-of-the-art CNN in many computer vision tasks when trained on sufficient data. However, the Transformer-based architecture has not been investigated in large-scale multitemporal remote sensing interpretation tasks. More importantly, compared to computer vision tasks, multitemporal remote sensing interpretation tasks are subject to different complex challenges, such as larger object scale differences, and diverse image patterns caused by different sensor types and atmospheric conditions.</p><p>In this paper, we explore the potential of Transformerbased backbones applying an end-to-end dual-tasks siamese Transformer architecture named DamFormer for the task of building damage assessment. We believe that this work can provide a different perspective to multitemporal remote sensing interpretation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head><p>The building damage assessment task is two-fold: building localization and building per-pixel damage segmentation, namely damage classification. In this work, we present an end-to-end Transformer-based architecture DamFormer for both tasks. The overall proposed network architecture DamFormer is presented in <ref type="figure" target="#fig_0">Fig. 1</ref>, which is constructed by two parts: a siamese Transformer encoder and a lightweight dual-tasks decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Siamese Transformer Encoder</head><p>Transformer backbones have shown their potential in feature extraction for semantic segmentation. Nevertheless, many of the current models are based on the ViT backbone, whose numerous network parameters inevitably introduce high computation overhead, which is paramount in the case of processing large-scale multitemporal remote sensing data. Therefore, in our work, we construct our siamese Transformer encoder based on the cutting-edge Transformer architecture SegFormer <ref type="bibr" target="#b7">[8]</ref>.</p><p>SegFormer consists of two main modules: a hierarchical transformer encoder named Mix Transformer (MiT) that outputs multi-level features and a lightweight All-MLP decoder to aggregate them and predict the semantic segmentation mask. As in ViT, an input image is first divided into patches, however MiT selects a smaller patches of size 4?4 to favor the dense prediction task. To reduce the computational complexity of the standard self-attention mechanism, MiT uses a sequence reduction process to reduce the length of the semantic sequence. Also, considering the positional encoding in Transformer demands the same resolution for input data in training and testing stages, a 3?3 convolutional layer is introduced to replace the positional encoding. Benefiting from it, SegFormer architecture can accept multitemporal images of any size in training and testing stages.</p><p>Specifically, based on the Mix Transformer encoder, our encoder has two streams to extract low-resolution finegrained and high-resolution coarse features from multitemporal very high-resolution imagery where each stream has four Transformer blocks as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. In this work, since pre-and post-disaster images are homogeneous with similar visual patterns, a weight-sharing mechanism is applied for the two streams, making the extracted features comparable (in the same feature space) and reducing network parameters. In addition, the overlap patch merging mechanism <ref type="bibr" target="#b7">[8]</ref> is introduced at each block to shrink the features obtained from the previous block to half the size.</p><p>Finally, we propose a multitemporal adaptive fusion module, consisting of concatenation, convolutional layer, and channel attention mechanism <ref type="bibr" target="#b9">[10]</ref>. The concatenation operation and convolutional layer merge the feature maps of the same size from the two streams, which can facilitate the information interaction between pre-disaster and post-disaster feature maps. The channel attention mechanism can weight the importance of each channel and yield task-specific features that are well-suited for downstream building localization and damage classification tasks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Lightweight Dual-Tasks Decoder</head><p>Keeping a large receptive field to include enough contextual information is key to segmenting remote sensing data. Therefore, sophisticated and computational demanding decoder designs such as multi-scale dilated convolution and self-attention head are often used by the CNN-based architecture. By contrast, our architecture can extract non-local features in each layer benefiting from the Transformer-based encoder. Hence, a simple lightweight decoder is therefore sufficient for downstream tasks.</p><p>Since the building damage assessment includes both building localization and damage classification tasks, our decoder includes two sub-networks with the same structure containing a cross-level fusion module and a classifier. In each decoder, the hierarchical task-specific features are first upsampled to the same size. Then, the feature maps are concatenated and a cross-layer fusion model based on a 1?1 convolutional layer is applied to aggregate information from different layers. The yielded feature map containing multilevel information is used for final prediction by an attached 1?1 convolutional layer. In addition, the multi-level feature map in the localization sub-network is added back to the classification sub-network to contribute to the damage assessment performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Loss Function</head><p>To train the proposed architecture, we utilize a compound loss function to jointly optimize building localization and damage classification tasks. For building detection, we optimize predictions according to the building reference maps with binary cross-entropy loss. To address the problem of skewed class where pixels belonging to building classes (foreground pixels) are far less than background pixels, dice loss is intro-duced to balance the foreground and background pixels. For damage assessment, we optimize predictions according to the damage reference maps with cross-entropy loss. Compared to building detection, the sample imbalance problem is more challenging in damage classification, which is not limited to the foreground and background pixels but also lies in the distribution of pixels with different damage levels. Considering this, we apply Lovasz softmax loss function <ref type="bibr" target="#b10">[11]</ref> to support the optimization step, since it encounters sample imbalance problems by directly optimizing the IoU metric. Finally, our overall loss function can be formed as follows:</p><formula xml:id="formula_0">L overall = L loc + ?L dam<label>(1)</label></formula><p>where ? is a trade-off parameter that controls the importance of damage assessment, we set ? to 1 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset, Metrics, Benchamrk Methods</head><p>To evaluate the effectiveness of our DamFormer architecture, we conduct experiments on the xBD dataset <ref type="bibr" target="#b11">[12]</ref>, which is currently the largest publicly available damage assessment dataset, containing 11'034 multitemporal very-highresolution image-pairs with a size of 1024?1024 and six disaster types: earthquake/tsunami, flood, volcanic eruption, wildfire, and wind. The damage annotations in the xBD dataset are divided into four levels: non-damage, minor damage, major damage, and destroyed. We implemented our network using Pytorch. In the two siamese streams, the specific number of MixFomer units in the four blocks is 3, 4, 6 and 3, keeping the same number of the residual units in ResNet-50 architecture. To train the overall network, we apply AdamW optimizer with an initial learning rate of 6e ?5 and a weight decay of 5e ?3 . Following the widely used metrics suggested in the xView2 Computer Vision for Building Damage Assessment Challenge 1 , an evaluation metric based on F 1 score was utilized, including building localization score F loc 1 , the harmonic mean of subclass-wise damage classification scores F dam 1 and overall score F oa 1 , which can be formed as</p><formula xml:id="formula_1">F oa 1 = 0.3F loc 1 + 0.7F dam 1 .</formula><p>To evaluate the proposed framework we adopted four CNN-based architectures as a comparison methods, i.e., the xView2 baseline method based on UNet + ResNet 2 , the xView2 1st place solution method 3 Siamese-UNet, MaskRCNN-DA <ref type="bibr" target="#b12">[13]</ref>, and ChangeOS <ref type="bibr" target="#b2">[3]</ref>. Because the 1st place solution is based on a multi-model ensemble and ChangeOS applied object-based post-processing to improve the final performance, we used the ResNet-34 based models and the pixelbased ChangeOS respectively for a fair comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2 illustrates some visualization results obtained by</head><p>DamFormer on the xBD dataset, which shows the proposed method can yield accurate and intact segmentation maps reflecting different levels of damages. Furthermore, in Table 1, we report the benchmark comparison on the xView2 holdout split. As is evident, DamFormer outperformed on both building localization and damage classification tasks, which indicates the effectiveness of Transformer architecture in multitemporal remote sensing image processing tasks of building damage assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we preliminary evaluate the potential of Transformer in multitemporal remote sensing data processing tasks. Specifically, we propose a dual-tasks siamese Transformer framework called DamFormer for building damage assessment tasks. DamFormer is made up of a siamese Transformer encoder and a lightweight dual-tasks decoder. Different from the limited receptive field of CNN-based architecture, DamFormer can extract non-local and representative features for building localization and damage assessment tasks. The experimental results on the xBD dataset demonstrate the superiority of our architecture for building damage assessment in comparison with CNN-based architectures. Our future work includes but is not limited to applying DamFormer architecture and its variants to man-made disasters response and further exploring Transfomer architecture in other remote sensing-related tasks, such as change detection, land-cover, and land-use classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of the proposed DamFormer architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Visualization of the damage assessment results obtained by DamFormer from the xBD dataset. (a) Pre-disaster image. (b) Post-disaster image. (c) Reference map. (d) Prediction map. In (c) and (d), background pixels are shown in black, undamaged pixel in white, minor damaged pixels in green, major damaged pixels in yellow, and destroyed pixels in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Transformer Block I Transformer Block II Transformer Block IV Transformer Block III Multitemporal Adaptive Fusion Transformer Block I Transformer Block II Transformer Block IV Transformer Block III Weight-sharedStream T 2 Stream T 1 Siamese Encoder Cross-layer Fusion Dual-tasks Decoder Cross-layer Fusion Localization Classification Add Classifier I Classifier II Building Prediction Damage Prediction Upsample &amp; Concat Upsample &amp; Concat</head><label></label><figDesc></figDesc><table><row><cell>H/4?W/4?C1</cell><cell>H/8?W/8?C2</cell><cell>H/16?W/16?C3</cell><cell>H/32?W/32?C4</cell></row><row><cell>H/4?W/4?C1</cell><cell>H/8?W/8?C2</cell><cell>H/16?W/16?C3</cell><cell>H/32?W/32?C4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>BENCHMARK COMPARISON OF DAMAGE ASSESSMENT RESULTS PRODUCED ON THE XBD DATASET</figDesc><table><row><cell>Method</cell><cell>F oa 1</cell><cell>F loc 1</cell><cell>F dam 1</cell><cell>No</cell><cell cols="2">Damage F1 per class Minor Major Destroyed</cell></row><row><cell cols="3">xView2 Baseline 26.54 80.47</cell><cell>3.42</cell><cell>66.31</cell><cell>14.35</cell><cell>0.94</cell><cell>46.57</cell></row><row><cell>Siamese-UNet</cell><cell cols="4">71.68 85.92 65.58 86.74</cell><cell>50.02</cell><cell>64.43</cell><cell>71.68</cell></row><row><cell>MaskRCNN</cell><cell cols="4">74.10 83.60 70.02 90.60</cell><cell>49.30</cell><cell>72.20</cell><cell>83.70</cell></row><row><cell>ChangeOS</cell><cell cols="4">75.50 85.69 71.14 89.11</cell><cell>53.11</cell><cell>72.44</cell><cell>80.79</cell></row><row><cell>DamFormer</cell><cell cols="4">77.02 86.86 72.81 89.86</cell><cell>56.78</cell><cell>72.56</cell><cell>80.51</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.xview2.org/ 2 https://github.com/DIUx-xView/xView2 baseline 3 https://github.com/DIUx-xView/xView2 first place</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional neural network for rapid flood segmentation in synthetic aperture radar imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nemni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bullock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belabbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bromley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">16</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Change detection in multisource vhr images via deep siamese convolutional multiple-layers recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2848" to="2864" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Building damage assessment for rapid disaster response with a deep object-based semantic change detection framework: From natural disasters to man-made disasters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">265</biblScope>
			<biblScope unit="page">112636</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selfattention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The lov?szsoftmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="4413" to="4421" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hosfelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sajeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gaston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09296</idno>
		<title level="m">xbd: A dataset for assessing building damage from satellite imagery</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Building disaster damage assessment in satellite imagery with multi-temporal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kan?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05525</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
