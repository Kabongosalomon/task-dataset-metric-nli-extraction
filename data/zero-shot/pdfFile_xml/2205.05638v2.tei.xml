<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Liu</surname></persName>
							<email>haokunl@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Tam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Muqeeth</surname></persName>
							<email>muqeeth@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Mohta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tenghao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
							<email>craffel@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA) 3 that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model <ref type="bibr" target="#b0">[1]</ref> called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark [2], attaining super-human performance for the first time and outperforming the state-of-the-art by 6% absolute. All of the code used in our experiments is publicly available. 1 * Equal contribution. 1 https://github.com/r-three/t-few Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained language models have become a cornerstone of natural language processing, thanks to the fact that they can dramatically improve data efficiency on tasks of interest -i.e., using a pre-trained language model for initialization often produces better results with less labeled data. A historically common approach has been to use the pre-trained model's parameters for initialization before performing gradient-based fine-tuning on a downstream task of interest. While fine-tuning has produced many state-of-the-art results <ref type="bibr" target="#b0">[1]</ref>, it results in a model that is specialized for a single task with an entirely new set of parameter values, which can become impractical when fine-tuning a model on many downstream tasks.</p><p>An alternative approach popularized by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> is in-context learning (ICL), which induces a model to perform a downstream task by inputting prompted examples. Few-shot prompting converts a small collection of input-target pairs into (typically) human-understandable instructions and examples <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, along with a single unlabeled example for which a prediction is desired. Notably, ICL requires no gradient-based training and therefore allows a single model to immediately perform a wide variety of tasks. Performing ICL therefore solely relies on the capabilities that a model learned during pre-training. These characteristics have led to a great deal of recent interest in ICL methods <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Susie loves her grandma's banana bread. Susie called her grandma and asked her to send some. Grandma lived very far away. A week passed and grandma surprised Susie by coming to visit. What is a possible continuation for the story?</p><p>Susie was so happy.</p><p>Susie was upset.</p><p>(IA) <ref type="bibr" target="#b2">3</ref> Losses used in T-Few <ref type="figure">Figure 1</ref>: Diagram of (IA) <ref type="bibr" target="#b2">3</ref> and the loss terms used in the T-Few recipe. Left: (IA) <ref type="bibr" target="#b2">3</ref> introduces the learned vectors l k , l v , and l ff which respectively rescale (via element-wise multiplication, visualized as ) the keys and values in attention mechanisms and the inner activations in position-wise feed-forward networks. Right: In addition to a standard cross-entropy loss L LM , we introduce an unlikelihood loss L UL that lowers the probability of incorrect outputs and a length-normalized loss L LN that applies a standard softmax cross-entropy loss to length-normalized log-probabilities of all output choices.</p><p>Despite the practical benefits of ICL, it has several major drawbacks. First, processing all prompted input-target pairs every time the model makes a prediction incurs significant compute costs. Second, ICL typically produces inferior performance compared to fine-tuning <ref type="bibr" target="#b3">[4]</ref>. Finally, the exact formatting of the prompt (including the wording <ref type="bibr" target="#b10">[11]</ref> and ordering of examples <ref type="bibr" target="#b11">[12]</ref>) can have significant and unpredictable impact on the model's performance, far beyond inter-run variation of fine-tuning. Recent work has also demonstrated that ICL can perform well even when provided with incorrect labels, raising questions as to how much learning is taking place at all <ref type="bibr" target="#b8">[9]</ref>.</p><p>An additional paradigm for enabling a model to perform a new task with minimal updates is parameterefficient fine-tuning (PEFT), where a pre-trained model is fine-tuned by only updating a small number of added or selected parameters. Recent methods have matched the performance of fine-tuning the full model while only updating or adding a small fraction (e.g. 0.01%) of the full model's parameters <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Furthermore, certain PEFT methods allow mixed-task batches where different examples in a batch are processed differently <ref type="bibr" target="#b13">[14]</ref>, making both PEFT and ICL viable for multitask models.</p><p>While the benefits of PEFT address some shortcomings of fine-tuning (when compared to ICL), there has been relatively little focus on whether PEFT methods work well when very little labeled data is available. Our primary goal in this paper is to close this gap by proposing a recipe -i.e., a model, a PEFT method, and a fixed set of hyperparameters -that attains strong performance on novel, unseen tasks while only updating a tiny fraction of the model's parameters. Specifically, we base our approach on the T0 model <ref type="bibr" target="#b0">[1]</ref>, a variant of T5 <ref type="bibr" target="#b14">[15]</ref> fine-tuned on a multitask mixture of prompted datasets. To improve performance on classification and multiple-choice tasks, we add unlikelihood <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and length normalization-based <ref type="bibr" target="#b3">[4]</ref> loss terms. In addition, we develop (IA) <ref type="bibr" target="#b2">3</ref> , a PEFT method that multiplies intermediate activations by learned vectors. (IA) 3 attains stronger performance than full-model fine-tuning while updating up to 10,000? fewer parameters. Finally, we demonstrate the benefits of pre-training the (IA) 3 parameters before fine-tuning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Our overall recipe, which we dub "T-Few", performs significantly better than ICL (even against 16? larger models) and outperforms humans for the first time on the real-world few-shot learning benchmark RAFT <ref type="bibr" target="#b1">[2]</ref> while requiring dramatically less compute and allowing for mixed-task batches during inference. To facilitate the use of T-Few on new problems and future research on PEFT, we release our code. <ref type="bibr" target="#b0">1</ref> After providing background on ICL and PEFT in the following section, we discuss the design of T-Few in section 3. In section 4, we present experiments comparing T-Few to strong ICL baselines. Finally, we discuss related work in appendix B and conclude in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we provide am verview of ICL and PEFT with a focus on characterizing the computation, memory, and on-disk storage costs of making a prediction. Real-world costs depend on implementation and hardware, so we report costs in terms of FLOPs for computation and bytes for memory and storage, respectively. Additional related work is discussed in appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-shot in-context learning (ICL)</head><p>ICL <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> aims to induce a model to perform a task by feeding in concatenated and prompted input-target examples (called "shots") along with an unlabeled query example. Taking the cycled letter task from Brown et al. <ref type="bibr" target="#b3">[4]</ref> as an example, a 4-shot input or context would be "Please unscramble the letters into a word, and write that word: asinoc = casino, yfrogg = froggy, plesim = simple, iggestb = biggest, astedro =", for which the desired output would be "roasted". ICL induces an autoregressive language model to perform this task by feeding in the context and sampling from the model. For classification tasks, each label is associated with a string (e.g. "positive" and "negative" for sentiment analysis) and a label is assigned by choosing the label string that the model assigns the highest probability to. For multiple-choice tasks (e.g. choosing between N possible answers to a question), the model's prediction is similarly determined by determining which choice is assigned the highest probability.</p><p>The primary advantage of ICL is that it enables a single model to perform many tasks immediately without fine-tuning. This also enables mixed-task batches, where different examples in a batch of data correspond to different tasks by using different contexts in the input. ICL is also typically performed with only a limited number of labeled examples -called few-shot learning -making it data-efficient.</p><p>Despite these advantages, ICL comes with significant practical drawbacks: First, making a prediction is dramatically more expensive because the model needs to process all of the in-context labeled examples. Specifically, ignoring the quadratic complexity of self-attention operations in Transformer language models (which are typically small compared to the costs of the rest of the model <ref type="bibr" target="#b19">[20]</ref>), processing the k training examples for k-shot ICL increases the computational cost by approximately k + 1 times compared to processing the unlabeled example alone. Memory costs similarly scale approximately linearly with k, though during inference the memory costs are typically dominated by storing the model's parameters. Separately, there is a small amount of on-disk storage required for storing the in-context examples for a given task. For example, storing 32 examples for a task where the prompted input and target for each example is 512 tokens long would require about 66 kilobytes of storage on disk (32 examples ? 512 tokens ? 32 bits).</p><p>Beyond the aforementioned costs, ICL also exhibits unintuitive behavior. Zhao et al. <ref type="bibr" target="#b11">[12]</ref> showed that the ordering of examples in the context heavily influences the model's predictions. Min et al. <ref type="bibr" target="#b8">[9]</ref> showed that ICL can still perform well even if the labels of the in-context examples are swapped (i.e. made incorrect), which raises questions about whether ICL is really "learning" from the labeled examples.</p><p>Various approaches have been proposed to mitigate these issues. One way to decrease computational costs is to cache the key and value vectors for in-context examples. This is possible because decoderonly Transformer language models have a causal masking pattern, so the model's activations for the context do not do not depend on the unlabeled example. In an extreme case, 32-shot ICL with 512 tokens per in-context example would result in over 144 gigabytes of cached key and value vectors for the GPT-3 model (32 examples ? 512 tokens ? 96 layers ? 12288 d model ? 32 bits each for the key and value vectors). Separately, Min et al. <ref type="bibr" target="#b20">[21]</ref> proposed ensemble ICL, where instead of using the output probability from concatenating the k training examples, the output probabilities of the model on each training example (i.e. 1-shot ICL for each of the k examples) are multiplied together. This lowers the non-parameter memory cost by a factor of k/2 but increases the computational cost by a factor of 2. In terms of task performance, Min et al. <ref type="bibr" target="#b20">[21]</ref> find that ensemble ICL outperforms the standard concatenative variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parameter-efficient fine-tuning</head><p>While standard fine-tuning updates all parameters of the pre-trained model, it has been demonstrated that it is possible to instead update or add a relatively small number of parameters. Early methods proposed adding adapters <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>, which are small trainable feed-forward networks inserted between the layers in the fixed pre-trained model. Since then, various sophisticated PEFT methods have been proposed, including methods that choose a sparse subset of parameters to train <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, produce low-rank updates <ref type="bibr" target="#b12">[13]</ref>, perform optimization in a lower-dimensional subspace <ref type="bibr" target="#b26">[27]</ref>, add low-rank adapters using hypercomplex multiplication <ref type="bibr" target="#b27">[28]</ref>, and more. Relatedly, prompt tuning <ref type="bibr" target="#b13">[14]</ref> and prefix tuning <ref type="bibr" target="#b28">[29]</ref> concatenate learned continuous embeddings to the model's input or activations to induce it to perform a task; this can be seen as a PEFT method <ref type="bibr" target="#b29">[30]</ref>. State-of-the-art PEFT methods can match the performance of fine-tuning all of the model's parameters while updating only a tiny fraction (e.g. 0.01%) of the model's parameters.</p><p>PEFT drastically reduces the memory and storage requirements for training and saving the model. In addition, certain PEFT methods straightforwardly allow mixed-task batches -for example, prompt tuning enables a single model to perform many tasks simply by concatenating different prompt embeddings to each example in the batch <ref type="bibr" target="#b13">[14]</ref>. On the other hand, PEFT methods that re-parameterize the model (e.g. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b12">13]</ref>) are costly or onerous for mixed-task batches. Separately, different PEFT methods increase the computation and memory required to perform inference by different amounts. For example, adapters effectively add additional (small) layers to the model, resulting in small but non-negligible increases in computational costs and memory. An additional cost incurred by PEFT is the cost of fine-tuning itself, which must be performed once and is then amortized as the model is used for inference. However, we will show that PEFT can be dramatically more computationally efficient when considering both fine-tuning and inference while achieving better accuracy than ICL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Designing the T-Few Recipe</head><p>Given that PEFT allows a model to be adapted to a new task with relatively small storage requirements and computational cost, we argue that PEFT presents a promising alternative to ICL. Our goal is therefore to develop a recipe that allows a model to attain high accuracy on new tasks with limited labeled examples while allowing mixed-task batches during inference and incurring minimal computational and storage costs. By recipe, we mean a specific model and hyperparameter setting that provides strong performance on any new task without manual tuning or per-task adjustments. In this way, we can ensure that our approach is a realistic option in few-shot settings where limited labeled data is available for evaluation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model and Datasets</head><p>As a first step, we must choose a pre-trained model. Ideally, the model should attain high performance on new tasks after fine-tuning on a limited number of labeled examples. In preliminary experiments applying PEFT methods to different pre-trained models, we attained the best performance with T0 <ref type="bibr" target="#b0">[1]</ref>. T0 is based on T5 <ref type="bibr" target="#b14">[15]</ref>, an encoder-decoder Transformer model <ref type="bibr" target="#b32">[33]</ref> that was pre-trained via a masked language modeling objective <ref type="bibr" target="#b33">[34]</ref> on a large corpus of unlabeled text data. T0 was created by fine-tuning T5 on a multitask mixture of datasets in order to enable zero-shot generalization, i.e. the ability to perform tasks without any additional gradient-based training. Examples in the datasets used to train T0 were prompted by applying the prompt templates from the Public Pool of Prompts (P3 <ref type="bibr" target="#b34">[35]</ref>), which convert each example in each dataset to a prompted text-to-text format where each label corresponds to a different string. For brevity, we omit a detailed description of T0 and T5; interested readers can refer to Sanh et al. <ref type="bibr" target="#b0">[1]</ref> and Raffel et al. <ref type="bibr" target="#b14">[15]</ref>. T0 was released in three billion and eleven billion parameter variants, referred to as "T0-3B" and simply "T0" respectively. In this section (where our goal is to design the T-Few recipe through extensive experimentation), we use T0-3B to reduce computational costs. For all models and experiments, we use Hugging Face Transformers <ref type="bibr" target="#b35">[36]</ref>.</p><p>While T0 was designed for zero-shot generalization, we will demonstrate that it also attains strong performance after fine-tuning with only a few labeled examples. To test T0's generalization, Sanh et al. <ref type="bibr" target="#b0">[1]</ref> chose a set of tasks (and corresponding datasets) to hold out from the multitask training mixture -specifically, sentence completion (COPA <ref type="bibr" target="#b36">[37]</ref>, H-SWAG <ref type="bibr" target="#b37">[38]</ref>, and Story Cloze <ref type="bibr" target="#b38">[39]</ref> datasets), natural language inference (ANLI <ref type="bibr" target="#b39">[40]</ref>, CB <ref type="bibr" target="#b40">[41]</ref>, and RTE <ref type="bibr" target="#b41">[42]</ref>), coreference resolution (WSC <ref type="bibr" target="#b42">[43]</ref> and Winogrande <ref type="bibr" target="#b43">[44]</ref>), and word sense disambiguation (WiC <ref type="bibr" target="#b44">[45]</ref>). Evaluation of generalization capabilities can then be straightforwardly done by measuring performance on these held-out datasets. We also will later test T-Few's abilities in the RAFT benchmark <ref type="bibr" target="#b1">[2]</ref> in section 4.3, a collection of unseen "real-world" few-shot tasks with no validation set and a held-out test set. ANLI, WiC, WSC is licensed under a Creative Commons License. Winogrande is licnsed under an Apache license. COPA is under a BSD-2 Clause license. We could not find the license of RTE and CB but they are part of SuperGLUE which mentions the datasets are allowed for use in research context.</p><p>To ease comparison, we use the same number of few-shot training examples for each dataset as Brown et al. <ref type="bibr" target="#b3">[4]</ref>, which varies from 20 to 70. Unfortunately, the few-shot dataset subsets used by Brown et al. <ref type="bibr" target="#b3">[4]</ref> have not been publicly disclosed. To allow for a more robust comparison, we therefore constructed five few-shot datasets by sampling subsets with different seeds and report the median and interquartile range. We prompt examples from each dataset using the prompt templates from P3 Bach et al. <ref type="bibr" target="#b34">[35]</ref>, using a randomly-sampled prompt template for each example at each step. Unless otherwise stated, we train our model for 1K steps with a batch size of 8 and report performance at the end of training.</p><p>For evaluation, we use "rank classification", where the model's log-probabilities for all possible label strings are ranked and the model's prediction is considered correct if the highest-ranked choice is the correct answer. Rank classification evaluation is compatible with both classification and multiplechoice tasks. Since model performance can vary significantly depending on the prompt template used, we report the median accuracy across all prompt templates from P3 and across few-shot data subsets for each dataset. For all datasets, we report the accuracy on the test set or validation set when the test labels are not public (e.g. SuperGLUE datasets). In the main text, we report median accuracy across the nine datasets mentioned above. Detailed results on each dataset are provided in the appendices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unlikelihood Training and Length Normalization</head><p>Before investigating PEFT methods, we first explore two additional loss terms to improve the performance of few-shot fine-tuning of language models. Language models are normally trained with cross-entropy loss L LM = ? 1 T t log p(y t |x, y &lt;t ) where the model is trained to increase the probability of the correct target sequence y = (y 1 , y 2 , . . . , y T ) given the input sequence x.</p><p>For evaluation, we use rank classification (described in section 3.1) which depends on both the probability that the model assigns to the correct choice as well as the probabilities assigned by the model to the incorrect choices. To account for this during training, we consider adding an unlikelihood loss <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>:</p><formula xml:id="formula_0">L UL = ? N n=1 T (n) t=1 log(1 ? p(? (n) i |x,? (n) &lt;t )) N n=1 T (n)<label>(1)</label></formula><p>which discourages the model from predicting tokens from incorrect target sequences, where? (n) = (? 1 ,? 2 , . . . ,? T (n) ) is the n-th of N incorrect target sequences. We hypothesize that adding L UL will improve results on rank classification because the model will be trained to assign lower probabilities to incorrect choices, thereby improving the chance that the correct choice is ranked highest.</p><p>The possible target sequences for a given training example can have significantly different lengths, especially in multiple-choice tasks. Ranking each choice based on probability can therefore "favor" shorter choices because the model's assigned probability to each token is ? 1. To rectify this, we consider using length normalization when performing rank classification, which divides the model's score on each possible answer choice by the number of tokens in the choice (as used in GPT-3 <ref type="bibr" target="#b3">[4]</ref>). When using length normalization during evaluation, we introduce an additional loss term during training that more closely reflects length-normalized evaluation. First, we compute the length-normalized log probability of a given output sequence ?(x, y) = 1 T T t=1 log p(y t |x, y &lt;t ). Then, we maximize the length-normalized log probability of the correct answer choice by minimizing the softmax cross-entropy loss:</p><formula xml:id="formula_1">L LN = ? log exp(?(x, y)) exp(?(x, y)) + N n=1 exp(?(x,? (n) ))<label>(2)</label></formula><p>When training a model with L LM , L UL , and L LN , we simply sum them. This avoids introducing any hyperparameters that would be problematic to tune in the few-shot setting (where realistically-sized validation sets are tiny by necessity <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>).</p><p>We report the results of fine-tuning all of T0-3B's parameters with and without length normalization on all datasets in appendix C. We find that adding L LN improves the accuracy from 60.7% to 62.71% and including both L UL and L LN provides a further improvement to 63.3%. Since these loss terms improve performance without introducing any additional hyperparameters, we include them in our recipe and use them in all following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter-efficient fine-tuning with (IA) 3</head><p>In order to compare favorably to few-shot ICL, we need a PEFT method that has the following properties: First, it must add or update as few parameters as possible to avoid incurring storage and memory costs. Second, it should achieve strong accuracy after few-shot training on new tasks. Finally, it must allow for mixed-task batches, since that is a capability of ICL. In order to easily enable mixed-task batches, a PEFT method should ideally not modify the model itself. Otherwise, each example in a batch would effectively need to be processed by a different model or computational graph. A more convenient alternative is provided by methods that directly modify the activations of the model since this can be done independently and cheaply to each example in the batch according to which task the example corresponds to. Prompt tuning and prefix tuning methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref> work by concatenating learned vectors to activation or embedding sequences and are therefore examples of activation-modifying PEFT methods that allow for mixed-task batches. However, as we will discuss later, we were unable to attain reasonable accuracy with prompt tuning and found that the more performant PEFT methods did not allow for mixed-task batches. We therefore developed a new PEFT method that meets our desiderata.</p><p>As an alternative, we explored element-wise multiplication (i.e. rescaling) of the model's activations against a learned vector. Specifically, we consider adaptation of the form l x where l ? R d is a learned task-specific vector, represents element-wise multiplication, and x ? R T ?d is a length-T sequence of activations. We use "broadcasting notation" <ref type="bibr" target="#b45">[46]</ref> so that the (i, j) th entry of l x is l j x i,j .</p><p>In preliminary experiments, we found it was not necessary to introduce a learned rescaling vector for each set of activations in the Transformer model. Instead, we found it was sufficient to introduce rescaling vectors on the keys and values in self-attention and encoder-decoder attention mechanisms and on the intermediate activation of the position-wise feed-forward networks. Specifically, using the notation from Vaswani et al. <ref type="bibr" target="#b32">[33]</ref>, we introduce three learned vectors l k ? R d k , l v ? R dv , and l ff ? R d ff , which are introduced into the attention mechanisms as:</p><formula xml:id="formula_2">softmax Q(l k K T ) ? d k (l v V )</formula><p>and in the position-wise feed-forward networks as (l ff ?(W 1 x))W 2 , where ? is the feed-forward network nonlinearity. We introduce a separate set of l k , l v , and l ff vectors in each Transformer layer block. This adds a total of L(d k + d v + d ff ) new parameters for a L-layer-block Transformer encoder and L(2d k + 2d v + d ff ) (with factors of 2 accounting for the presence of both self-attention and encoder-decoder attention) for a L-layer-block decoder. l k , l v , and l ff are all initialized with ones so that the overall function computed by the model does not change when they are added. We call our method (IA) 3 , which stands for "Infused Adapter by Inhibiting and Amplifying Inner Activations".</p><p>(IA) 3 makes mixed-task batches possible because each sequence of activations in the batch can be separately and cheaply multiplied by its associated learned task vector. We also note that, in the event that a model will only be used on a single task, the modifications introduced by (IA) <ref type="bibr" target="#b2">3</ref> can also be applied to weight matrices permanently so that no elementwise multiplication is required and the model's architecture remains unchanged. This possible because element-wise multiplications performed in (IA) 3 always co-occur with a matrix multiplication, and l W x = (l W )x. In this case, our method incurs no additional computational cost compared to the original model.</p><p>To validate (IA) 3 , we compare it to a large variety of existing adaptation methods in our setting of fine-tuning T0-3B on few-shot datasets from held-out tasks. Specifically, we compare with 9 strong PEFT methods: BitFit <ref type="bibr" target="#b46">[47]</ref> which updates only the bias parameters; Adapters <ref type="bibr" target="#b22">[23]</ref> which introduce task-specific layers after the self-attention and position-wise feed-forward networks; Compacter and Compacter++ <ref type="bibr" target="#b27">[28]</ref> which improve upon adapters by using low-rank matrices and hypercomplex multiplication; prompt tuning <ref type="bibr" target="#b13">[14]</ref> which learns task-specific prompt embeddings that are concatenated to the model's input; FISH Mask <ref type="bibr" target="#b25">[26]</ref> which chooses a subset of parameters to update based on their approximate Fisher information; Intrinsic SAID <ref type="bibr" target="#b26">[27]</ref> which performs optimization in a low-dimensional subspace; prefix-tuning <ref type="bibr" target="#b28">[29]</ref> which learns task-specific vectors that are concatenated to the model's activations; and LoRA <ref type="bibr" target="#b12">[13]</ref> which assigns low-rank updates to parameter matrices. Additionally, we include the baselines of full-model fine-tuning and updating only the layer normalization parameters. For certain methods that allow changing the parameter efficiency, we report results for different budgets: 0.2% and 0.02% sparsity for FISH Mask, 10 and 100 learned prompt vectors for prompt tuning, and 20,000-or 500,000-dimensional subspaces for Intrinsic SAID.</p><p>The results are shown in <ref type="figure" target="#fig_1">fig. 2</ref>, with detailed per-dataset results in appendix D. We find that (IA) <ref type="bibr" target="#b2">3</ref> is the only method that attains higher accuracy than the full-model-fine-tuning baseline. While other PEFT methods (e.g. Intrinsic SAID and prompt tuning) update or introduce fewer parameters, (IA) 3 performs considerably better. Our results and setting differ with some past work on the PEFT methods we compare against. Mahabadi et al. <ref type="bibr" target="#b27">[28]</ref> report that Compacter and Compacter++ outperform full-model fine-tuning, including in the few-shot setting. Lester et al. <ref type="bibr" target="#b13">[14]</ref> found that prompt tuning could match full-model fine-tuning, and in subsequent work Wei et al. <ref type="bibr" target="#b47">[48]</ref> found that prompt tuning performed well when applied to a multitask fine-tuned model in the few-shot setting.</p><p>In both cases, we experimented with various hyperparameter choices to try to match past results. We hypothesize the disagreement comes from us using a different model and different datasets. For prompt tuning specifically, we noticed that the validation set performance could fluctuate wildly over the course of training, hinting at possible optimization issues.  Accuracy T-Few T0 T5+LM</p><p>GPT-3 6.7B GPT-3 13B GPT-3 175B </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pre-training (IA) 3</head><p>In recent work, Gu et al. <ref type="bibr" target="#b17">[18]</ref>, Vu et al. <ref type="bibr" target="#b18">[19]</ref> showed that pre-training the prompt embeddings in prompt tuning can improve performance when fine-tuning on downstream few-shot tasks. For pretraining, Gu et al. <ref type="bibr" target="#b17">[18]</ref> use a suite of self-supervised tasks applied to unlabeled text data, and Vu et al. <ref type="bibr" target="#b18">[19]</ref> consider using embeddings from a separate task or multitask mixture. We follow Vu et al. <ref type="bibr" target="#b18">[19]</ref> and simply pre-train the new parameters introduced by (IA) 3 on the same multitask mixture used to train T0. We pre-train for 100,000 steps with a batch size of 16 before fine-tuning the (IA) 3 parameters on each individual downstream dataset. A full comparison of accuracy with and without pre-training (IA) 3 is detailed in appendix E. We find that pre-training improves fine-tuned accuracy from 64.6 to 65.8 and therefore add it to our recipe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Combining the ingredients</head><p>In summary, the T-Few recipe is defined as follows:</p><p>We use the T0 model as a backbone. We add (IA) 3 for downstream task adaptation and use parameters initialized from pre-training (IA) 3 on the same multitask mixture for T0. As an objective, we use the sum of a standard language modeling loss L LM , an unlikelihood loss L UL for incorrect choices, and a length-normalized loss L LN . We train for 1,000 steps with a batch size of 8 sequences using the Adafactor optimizer <ref type="bibr" target="#b48">[49]</ref> with a learning rate of 3e ?3 and a linear decay schedule with a 60-step warmup. We apply prompt templates to downstream datasets during training and inference to convert each example into an instructive text-to-text format. Importantly, we apply this recipe to every downstream dataset in exactly the same way without per-dataset hyperparameter tuning or modifications. This makes the recipe a realistic option for few-shot learning settings where validation sets are tiny by definition <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Outperforming ICL with T-Few</head><p>Having designed and established the T-Few recipe on T0-3B, we now apply it to T0 (with 11 billion parameters) and compare performance to strong few-shot ICL baselines. From this point onwards, we use exactly the same recipe and hyperparameters across all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance on T0 tasks</head><p>First, we evaluate T-Few on the datasets that were held out from T0's training mixture. We compare against zero-shot learning with T0 [1] (since we found few-shot ICL to performed worse than zero-shot for T0, see appendix F); few-shot ICL with T5+LM <ref type="bibr" target="#b13">[14]</ref> (the next-step-prediction language model upon which T0 is based); and few-shot ICL with the 6.7, 13, and 175 billion parameter variants of GPT-3. See appendix F for more details on these baselines. The accuracy on the held-out T0 datasets (described in section 3.1) is shown in table 1 and <ref type="figure" target="#fig_3">fig. 3</ref>, with per-dataset results reported in appendix F. We find that T-Few outperforms all other methods by a substantial margin. Notably, T-Few achieves a 6% higher accuracy than few-shot ICL with GPT-3 175B despite being about 16? smaller and outperforms the smaller GPT-3 variants by an even larger margin. T-Few also attains significantly higher accuracy than both zero-shot learning with T0 and few-shot ICL with T5+LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference FLOPs</head><p>Training FLOPs Disk space Acc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc.</p><p>T-Few 75.8% Human baseline <ref type="bibr" target="#b1">[2]</ref> 73.5% PET <ref type="bibr" target="#b49">[50]</ref> 69.6% SetFit <ref type="bibr" target="#b50">[51]</ref> 66.9% GPT-3 <ref type="bibr" target="#b3">[4]</ref> 62.7% <ref type="table">Table 2</ref>: Top-5 best methods on RAFT as of writing. T-Few is the first method to outperform the human baseline and achieves over 6% higher accuracy than the nextbest method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparing computational costs</head><p>Having established that T-Few significantly outperforms ICL-based models, we now compare the relative costs of each few-shot learning approach. For simplicity, we use the FLOPs-per-token estimates for Transformer-based language models introduced by Kaplan et al. <ref type="bibr" target="#b19">[20]</ref>. Specifically, we estimate that a decoder-only Transformer (e.g. the GPT series) with N parameters uses 2N FLOPs per token for inference and 6N FLOPs per token for training. Encoder-decoder models like T0 and T5 (where the encoder and decoder have the same number of layers and layer sizes) only process each token with either the encoder or decoder (each having roughly half the parameters of the full model), so the FLOPs per token estimates are halved to N and 3N FLOPs per token for inference and training. We note that FLOPs are not a direct measurement of real-world computational cost because latency, power usage, and other costs can vary significantly depending on hardware and other factors <ref type="bibr" target="#b51">[52]</ref>. However, we focus on FLOPs because it is a hardware-independent metric that closely with real-world costs the hardware setup used for running the different methods we consider would likely vary significantly across methods. We summarize the costs in table 1 and discuss them below. For all estimates, we use the median number of shots (41) across the datasets we consider. Rank evaluation and our unlikelihood loss both require processing every possible output choice to attain a prediction for an unlabeled example. The median combined tokenized sequence length for the input and all possible targets is 103 for the datasets we consider. For in-context examples processed for few-shot ICL, only the correct target is required, producing a median sequence length of 98. Assuming that key and value vectors are cached, processing a single example with ICL therefore involves processing 41 ? 98 + 103 tokens. A summary of our cost estimates is provided in table 1.</p><p>Inference cost. Beyond improved accuracy, the primary advantage of avoiding few-shot ICL is dramatically lower inference costs. Processing a single input and all target choices with T-Few requires 11e9 ? 103 = 1.1e12 FLOPs, whereas few-shot ICL with GPT-3 175B requires 2 ? 175e9 ? (41 ? 98 + 103) = 1.4e15 FLOPs -more than 3 orders of magnitude more. Inference costs with ICL using the smaller GPT-3 variants are also dramatically higher than the inference cost of T-Few. As discussed in section 2.1, caching the key and value vectors when the same set of in-context examples is to be reused can reduce the computational cost of ICL. However, this would only result in an approximately 41? reduction, which is not nearly enough to make any of the GPT-3 ICL costs as low as T-Few.</p><p>Training cost.</p><p>Since T-Few is the only method that involves updating parameters, it is the only method that incurs a training cost. Training an eleven billion parameter encoder-decoder model for 1,000 steps with a batch size of 8 length-103 sequences requires approximately 3 ? 11e9 ? 1, 000 ? 8 ? 103 = 2.7e16 FLOPs. While not insignificant, this is only about 20 times larger than the FLOPs required to process a single example with few-shot ICL using GPT-3 175B. In other words, training T-Few costs as much as using GPT-3 175B to process 20 examples with few-shot ICL. We also found that fine-tuning T0 with T-Few on a single dataset only takes about a half an hour on a single NVIDIA A100 GPU. As of writing, this would cost about $2 USD using Microsoft Azure. <ref type="bibr" target="#b1">2</ref> Storage cost. T-Few also incurs the largest storage cost. When stored as single-precision floats, the parameters added by (IA) 3 take up 4.2 MB of space on disk. In contrast, ICL methods only require storing the tokenized in-context examples (typically stored as 32-bit integers), resulting in a smaller 41 ? 98 ? 32 bits = 16 kB disk space requirement. However, we note that 4.2 MB is dwarfed by the on-disk size of the model checkpoints themselves -storing the (IA) 3 adaptation vectors for 10,000 tasks would take about as much space as the T0 checkpoint (41.5 GB).</p><p>Memory usage. During inference, the primary memory cost is incurred by the model's parameters. The only model smaller than T0 (used by T-Few) is GPT-3 6.7B; otherwise, T-Few will incur a lower memory cost during inference. Additional memory costs are incurred when training T-Few due to the need to cache intermediate activations for backpropagation and for the gradient accumulator variables in Adafactor. However, as mentioned above, it is possible to use the T-Few recipe on a single 80GB A100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance on Real-world Few-shot Tasks (RAFT)</head><p>So far, we have evaluated performance on a collection of datasets that were not explicitly designed for benchmarking few-shot learning. To better evaluate T-Few's performance in the real world, we evaluated our approach on the RAFT benchmark <ref type="bibr" target="#b1">[2]</ref>. RAFT consists of 11 "economically valuable" tasks that aim to mirror real-world applications. Importantly, each RAFT datasets has only 50 training examples with no validation set and a (larger) test set with no public labels, so it is impossible to "cheat" by tuning on an unrealistically-large validation set or by peeking at the test set <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref>. We apply T-Few to RAFT by using the standard prompts released alongside the dataset. The accuracy of the current top-5 methods is shown in table 2, with further details provided in appendix H. T-Few attains a state-of-the-art accuracy of 75.8% and outperforms the human baseline (73.5% accuracy) for the first time. The next-best model (from Schick and Sch?tze <ref type="bibr" target="#b49">[50]</ref>) achieves 6% lower accuracy and GPT-3 175B attains only 62.7%. These results validate that T-Few can be readily applied as-is to novel real-world tasks to attain strong performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation experiments</head><p>Given that our T-Few design experiments were on T0-3B, we perform an ablation of some of the ingredients of T-Few on T0. Detailed results are shown in appendix G. While the gains from adding each ingredient does not always significant increase the accuracy on each individual dataset, each ingredient consistently improves the average performance across datasets: Removing pre-training decreases accuracy by 1.6%, removing unlikelihood training and length normalization decreases accuracy by 4.1%, and removing both pre-training and our additional loss terms reduces accuracy by 2.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced T-Few, a parameter-efficient few-shot learning recipe that attains higher accuracy than few-shot ICL at a lower computational cost. T-Few uses (IA) 3 , a new PEFT method that rescales inner activations with learned vectors. Using (IA) 3 produces better performance than fine-tuning the full model while only introducing a tiny amount of additional parameters. T-Few also uses two additional loss terms that encourage the model to output lower probabilities for incorrect choices and account for the length of different answer choices. When applying T-Few as-is (with no taskspecific hyperparameter tuning or other changes) to the RAFT benchmark, we attained super-human performance for the first time and outperformed prior submissions by a large margin. Through detailed characterization of computational costs, we found that T-Few uses over 1,000? fewer FLOPs during inference than few-shot ICL with GPT-3 and only requires 30 minutes to train on a single NVIDIA A100 GPU. Since all of our experiments were on classification tasks, we are interested in applying T-Few to generative tasks like as summarization and question answering in future work. We hope our results provide a new perspective on how best to perform few-shot learning with large language models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Compute resources used</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Related Work</head><p>Currently, prompt tuning is one of the most parameter-efficient methods for large language models <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b52">53]</ref>. Liu et al. <ref type="bibr" target="#b53">[54]</ref> introduce several tricks to improve prompt tuning, An et al. <ref type="bibr" target="#b54">[55]</ref> tune prompts along with input embeddings for boost in performance, and Chen et al. <ref type="bibr" target="#b55">[56]</ref> improve prompt embeddings through continued pre-training. Given optimization difficulties when training prompt embeddings, Diao et al. <ref type="bibr" target="#b56">[57]</ref> recently used black-box optimization to train prompt embeddings without requiring gradients. Several works have analyzed prompt tuning from the perspective of interpretability Khashabi et al. <ref type="bibr" target="#b57">[58]</ref> and its similarity to other PEFT methods He et al. <ref type="bibr" target="#b29">[30]</ref>. Prompt tuning has been applied to various applications for NLP including continual learning <ref type="bibr" target="#b58">[59]</ref>, model robustness <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>, summarization <ref type="bibr" target="#b61">[62]</ref>, machine translation <ref type="bibr" target="#b62">[63]</ref>, co-training <ref type="bibr" target="#b63">[64]</ref>, probing language models <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b64">65]</ref>, inverse prompting <ref type="bibr" target="#b65">[66]</ref> and transfer learning <ref type="bibr" target="#b66">[67]</ref>. He et al. <ref type="bibr" target="#b67">[68]</ref> recently proposed the use of a hypernetwork to predict prompts for new tasks (rather than training the prompt parameters with gradient descent). Prompt tuning and other PEFT methods have also been explored outside of the context of language models (e.g. vision <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b68">69]</ref> and vision-and-language models <ref type="bibr" target="#b25">[26]</ref>).</p><p>Separately, various studies have considered few-shot full-model fine-tuning with discrete prompts <ref type="bibr" target="#b69">[70]</ref>. Recent work has analyzed training with discrete prompts, demonstrating a boost in performance with prompting when training on various numbers of examples <ref type="bibr" target="#b70">[71]</ref>, finding that models perform similarly when trained on good and bad prompts <ref type="bibr" target="#b10">[11]</ref>, and exploring which prompts work well for few-shot and full-shot setting <ref type="bibr" target="#b71">[72]</ref>. There have also been efforts to develop methods that find performant discrete prompts <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74]</ref> and training prompts using methods similar to prompt tuning <ref type="bibr" target="#b74">[75]</ref>.</p><p>There has also been a great deal of work on improving ICL. Chen et al. <ref type="bibr" target="#b4">[5]</ref>, Min et al. <ref type="bibr" target="#b5">[6]</ref> use ICL for meta-learning to perform few-shot learning on new tasks. Lampinen et al. <ref type="bibr" target="#b6">[7]</ref> show ICL can improve when explanations are provided and <ref type="bibr" target="#b7">[8]</ref>  With the advent of large language models with billions of parameters, there has been a great deal of recent interest in PEFT methods. A small amount of recent work has also begun to explore the compatibility of PEFT methods in the few-shot setting. Mahabadi et al. <ref type="bibr" target="#b27">[28]</ref> found that PEFT can outperform standard fine-tuning in the low-resource setting. In concurrent work, Mahabadi et al. <ref type="bibr" target="#b75">[76]</ref> compare PEFT to the use of discrete prompts (e.g. PET <ref type="bibr" target="#b69">[70]</ref>) during few-shot fine-tuning and find that PEFT compares favorably. Also concurrently, Moosavi et al. <ref type="bibr" target="#b76">[77]</ref> propose a framework for introducing adapters whose architecture and design vary from task to task and demonstrate improved results in few-shot settings. Gu et al. <ref type="bibr" target="#b17">[18]</ref> and Vu et al. <ref type="bibr" target="#b18">[19]</ref> both explored how pre-training prompt tuning parameters can improve when limited labeled data is available. For few-shot learning, Triantafillou et al. <ref type="bibr" target="#b77">[78]</ref> explore learning universal and dataset dependent parameters that can be blended for generalization. Requeima et al. <ref type="bibr" target="#b78">[79]</ref> use conditional neural adaptive processes and Li et al. <ref type="bibr" target="#b79">[80]</ref> leverage distillation from multiple feature extractors for learning new classes or domains in few-shot learning. <ref type="table" target="#tab_3">Table 3</ref> shows the full results with unlikelihood training and length normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Full Unlikelihood Training and Length Normalization Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Full PEFT Results</head><p>We compare against the following PEFT methods, using a linear decay with warmup scheduler with a warm-up ratio of 0.06 and the Adafactor optimizer <ref type="bibr" target="#b48">[49]</ref>. We show the full per-dataset result of all  PEFT methods we considered and ablate the losses. <ref type="table" target="#tab_6">Table 4</ref> includes all losses, <ref type="table" target="#tab_7">Table 5</ref> includes L LN , <ref type="table" target="#tab_8">Table 6</ref> includes L UL , and <ref type="table" target="#tab_9">Table 7</ref> does not include either loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Model Fine-tuning</head><p>We train for 300 steps with a learning rate of 3e ?4 .</p><p>BitFit <ref type="bibr" target="#b46">[47]</ref> We train for 300 steps with a learning rate of 3e ?4 .</p><p>LayerNorm We train for 300 steps with a learning rate of 3e ?4 .</p><p>Adapter <ref type="bibr" target="#b22">[23]</ref> We use a reduction factor of 32, ReLU nonlinearity, and residual connections. We train for 500 steps with a learning rate of 3e ?3 .</p><p>Compacter <ref type="bibr" target="#b27">[28]</ref> We train for 500 steps with a learning rate of 3e ?3 and hyper complex division factor of 4 (n = 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compacter++ [28]</head><p>We train for 500 steps with a learning rate of 3e ?3 and hyper complex division factor of 4 (n = 4).</p><p>Prompt tuning <ref type="bibr" target="#b13">[14]</ref> We train for 1000 steps with a learning rate of 3e ?1 and use 10 and 100 prompt embeddings.</p><p>Prefix tuning <ref type="bibr" target="#b28">[29]</ref> We train for 1000 steps with a learning rate of 3e ?3 and adopt the two-layer MLP parameterization in the paper with hidden size 512. We use "Question:" and "Answer:" as initialization text for the prefixes attached to the input and target sequence, respectively.</p><p>FishMask <ref type="bibr" target="#b25">[26]</ref> The Fisher is first computed on the training examples and we keep 0.2% or 0.02% of the parameters. Then, these parameters are trained for 1500 steps with a learning rate of 3e ?4 .</p><p>Intrinsic SAID <ref type="bibr" target="#b26">[27]</ref> We train for 3000 steps with a learning rate of 3e ?2 . Due to large model size, we use Intrinsic SAID to produce rank-1 updates for 2D weights via an outer product of two vectors.</p><p>LoRA <ref type="bibr" target="#b12">[13]</ref> We use a rank of 4 with initialization scale of 0.01 and update all the attention and feedforward module. We train for 1000 steps with a learning rate of 3e ?3 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Full Pre-training Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Full Main Results</head><p>We compare against the following baselines:</p><p>T0. To measure the improvement in performance conferred through parameter-efficient few-shot learning, we compare to zero-shot evaluation using T0 itself. In preliminary experiments, we found that T0 was not able to perform few-shot ICL -performance actually decreased as we increased the number of in-context examples. This is likely because of the zero-shot format used during multitask prompted fine-tuning and corroborates a recent finding by <ref type="bibr" target="#b9">[10]</ref>.</p><p>T5+LM. Since T0 is unable to perform ICL on its own, we also compare to T5+LM, the next-stepprediction language model upon which T0 is based. Specifically, we use the LM-adapted variant of T5.1.1.xxl released by Lester et al. <ref type="bibr" target="#b13">[14]</ref>, which has the same architecture and number of parameters as T0. Due to memory constraints and because of its improved performance, we use ensemble ICL for T5+LM <ref type="bibr" target="#b5">[6]</ref>. Specifically, we perform one-shot ICL using each example in the training set individually and average the predictions for a given query example. For fair comparison with GPT-3 models, we use the EleutherAI evaluation harness <ref type="bibr" target="#b80">[81]</ref>, which was designed to replicate the evaluation setup done by Brown et al. <ref type="bibr" target="#b3">[4]</ref>.</p><p>GPT-3. For a strong ICL baseline, we consider models in the GPT-3 family <ref type="bibr" target="#b3">[4]</ref>. Specifically, we compare to the 6.7, 13, and 175 billion parameter variants of GPT-3. Because these models have not been publicly released, we report numbers directly from Brown et al. <ref type="bibr" target="#b3">[4]</ref>. While GPT-3 is available through the commercial OpenAI API, re-running evaluation through the API would be more than an order of magnitude more expensive than running all of the experiments performed for this paper.        <ref type="table">Table 10</ref>: T-Few ablation results when omitting (IA) 3 pre-training (PT) and/or the L UL and L LN losses. Subscripts are IQR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Full Ablation Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Accuracy of PEFT methods with L UL and L LN when applied to T0-3B. Methods that with variable parameter budgets are represented with larger and smaller markers for more or less parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy of different few-shot learning methods. T-Few uses (IA) 3 for PEFT methods of T0, T0 uses zero-shot learning, and T5+LM and the GPT-3 variants use few-shot ICL. The x-axis corresponds to inference costs; details are provided in section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>use ICL with text retrieved from the web for open-domain question-answering. Meanwhile, Min et al. [9] analyze how ICL works and show that ICL can still perform well when incorrect labels are provided for the in-context examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>All T0-3B models were trained on 48GB A6000s. Training T0-3B with different PEFT methods took about an hour to train, except for Intrinsic SAID and FishMask which each took about two hours to train. Pre-training (IA) 3 took 1 day on 4 A6000s. All T0 models were trained 80GB A100s from DataCrunch 3 and took about half an hour to train each. Pre-training (IA) 3 took about 1 day on 4 A100s.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>66.4 1.0 53.8 1.7 + UL 81.0 3.0 46.1 4.8 93.6 2.5 56.5 2.2 61.5 8.7 56.4 4.1 + LN 86.0 4.0 47.1 22.4 94.0 0.6 56.9 3.8 65.4 3.9 53.9 2.0 + UL + LN 81.0 11.0 46.4 8.8 93.8 2.7 56.5 1.5 65.4 7.7 57.7 3.9</figDesc><table><row><cell></cell><cell>COPA</cell><cell cols="3">H-Swag StoryCloze Winogrande</cell><cell>WSC</cell><cell>WiC</cell></row><row><cell>FT</cell><cell>78.0 2.0</cell><cell>39.2 0.2</cell><cell>91.5 1.0</cell><cell>54.5 0.9</cell></row><row><cell></cell><cell>RTE</cell><cell>CB</cell><cell cols="3">ANLI-R1 ANLI-R2 ANLI-R3</cell></row><row><cell>FT</cell><cell cols="2">75.8 5.4 82.1 5.4</cell><cell>47.8 1.5</cell><cell>40.6 0.8</cell><cell>37.8 1.8</cell></row><row><cell>+ UL</cell><cell cols="2">77.6 1.4 89.3 1.8</cell><cell>47.9 1.9</cell><cell>40.9 1.9</cell><cell>38.8 5.0</cell></row><row><cell>+ LN</cell><cell cols="2">75.8 4.3 89.3 7.1</cell><cell>48.2 0.6</cell><cell>40.9 0.9</cell><cell>38.3 1.6</cell></row><row><cell cols="3">+ UL + LN 79.8 3.6 87.5 5.4</cell><cell>46.6 2.5</cell><cell>41.3 0.9</cell><cell>40.2 5.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Per-dataset results for comparing the effect of including the additional loss terms introduced in section 3.2. Subscripts are IQR.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8</head><label>8</label><figDesc>shows the per-dataset results for of pre-training (IA) 3 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table table 10shows the T-Few ablation results. Systematic Review Inclusion, Tai Safety Research, Terms of Service, Tweet Eval Hate, and Twitter Complaints. We use the T-Few recipe on all datasets without putting the labels into the input string except Banking 77. Since Banking 77 has 77 classes which causes memory issues for unlikelihood training, we turn off unlikelihood training for Banking 77. We also feed in all the labels as part of the input string for Banking 77 since there were some labels never seen during training and clean the labels by replacing "." with ",".Per-dataset results of T-Few and the other top-5 methods on RAFT are shown in table 11.</figDesc><table><row><cell cols="5">H RAFT Experiment Details RAFT consists of 11 tasks: Ade Corpus V2, Banking 77, NeurIps Impact Statement Risks, One Stop COPA H-Swag StoryCloze Winogrande Full Model Fine-tuning 3B 81.0 11.0 46.4 8.8 93.8 2.7 56.5 1.5 BitFit (with LayerNorm) 1.3M 75.0 2.0 29.5 3.6 88.6 0.7 49.6 1.3 LayerNorm 250K 76.0 2.0 29.6 3.4 88.7 0.9 49.4 1.4 Adapter 12.9M 84.0 3.0 41.9 3.8 91.7 3.7 54.7 3.6 Compacter 807K 84.0 5.0 46.4 2.5 93.5 2.2 55.5 2.9 Compacter++ 540K 86.0 3.0 46.3 3.0 93.5 1.2 55.1 1.1 Prompt tuning (10) 41K 67.0 5.0 29.9 0.6 84.2 0.8 51.9 1.6 Prompt tuning (100) 409K 60.0 19.0 26.8 0.6 74.0 3.4 51.1 0.8 Prefix tuning 576K 71.0 8.0 42.1 4.0 90.2 3.1 52.0 1.3 FishMask (0.2%) 6M 82.0 5.0 44.1 4.2 94.2 1.8 54.5 2.1 FishMask (0.02%) 600K 84.0 6.0 38.2 3.6 93.6 0.7 53.9 2.8 Intrinsic SAID 500K 77.0 4.0 36.7 4.5 89.3 2.3 52.7 2.1 Intrinsic SAID 20K 76.0 4.0 38.3 6.4 89.7 2.7 50.9 1.0 LoRA 9.1M 88.0 5.0 47.1 3.2 93.6 2.1 56.8 3.3 (IA) 3 540K 87.0 3.0 49.4 4.6 94.7 2.7 59.8 0.6 # of Param WSC WiC RTE CB Full Model Fine-tuning 3B 65.4 7.7 57.7 3.9 79.8 3.6 87.5 5.4 BitFit (with LayerNorm) 1.3M 61.5 11.5 51.7 2.2 72.2 1.1 57.1 1.8 LayerNorm 250K 63.5 12.5 52.2 1.6 71.8 0.4 57.1 1.8 English, Overruling, # of Param Adapter 12.9M 65.4 1.0 55.5 2.7 76.2 3.6 87.5 3.6</cell></row><row><cell>Compacter (n = 4)</cell><cell>807K</cell><cell cols="3">64.4 6.7 55.2 3.8 75.8 6.1 82.1 3.6</cell></row><row><cell>Compacter++ (n = 4)</cell><cell>540K</cell><cell cols="3">65.4 3.9 54.1 2.2 76.9 0.4 82.1 3.6</cell></row><row><cell>Prompt tuning (10)</cell><cell>41K</cell><cell cols="3">54.8 10.6 51.6 2.0 52.7 5.4 66.1 1.8</cell></row><row><cell>Prompt tuning (100)</cell><cell>409K</cell><cell cols="3">60.6 4.8 50.0 1.1 48.0 2.9 53.6 17.9</cell></row><row><cell>Prefix tuning</cell><cell>576K</cell><cell cols="3">56.7 3.3 54.2 3.3 68.6 3.3 84.0 1.8</cell></row><row><cell>FishMask (0.2%)</cell><cell>6M</cell><cell cols="3">63.5 4.8 52.5 3.3 76.9 4.7 83.9 3.6</cell></row><row><cell>FishMask (0.02%)</cell><cell>600K</cell><cell cols="3">61.5 1.0 53.5 1.3 75.5 5.4 76.8 3.6</cell></row><row><cell>SAID</cell><cell>500K</cell><cell cols="3">61.5 8.7 55.0 2.7 69.0 7.6 80.4 0.0</cell></row><row><cell>SAID</cell><cell>20K</cell><cell cols="3">55.8 6.7 55.3 0.5 66.1 5.4 83.9 1.8</cell></row><row><cell>LoRA</cell><cell>9.1M</cell><cell cols="3">60.6 5.8 55.2 5.0 78.3 7.6 85.7 1.8</cell></row><row><cell>(IA) 3</cell><cell>540K</cell><cell cols="3">68.3 6.7 56.0 4.6 78.0 2.5 87.5 1.8</cell></row><row><cell></cell><cell cols="4"># of Param ANLI-R1 ANLI-R2 ANLI-R3</cell></row><row><cell>Full Model Fine-tuning</cell><cell>3B</cell><cell>46.6 2.5</cell><cell>41.3 0.9</cell><cell>40.2 5.3</cell></row><row><cell>BitFit (with LayerNorm)</cell><cell>1.3M</cell><cell>36.5 0.8</cell><cell>35.3 2.2</cell><cell>36.6 0.8</cell></row><row><cell>LayerNorm</cell><cell>250K</cell><cell>36.5 0.7</cell><cell>35.1 2.6</cell><cell>36.3 1.0</cell></row><row><cell>Adapter</cell><cell>12.9M</cell><cell>45.1 2.6</cell><cell>40.4 1.2</cell><cell>35.3 1.3</cell></row><row><cell>Compacter</cell><cell>807K</cell><cell>40.8 3.3</cell><cell>37.4 0.2</cell><cell>35.8 3.3</cell></row><row><cell>Compacter++</cell><cell>540K</cell><cell>41.7 0.4</cell><cell>38.3 1.8</cell><cell>36.9 1.5</cell></row><row><cell>Prompt tuning (10)</cell><cell>41K</cell><cell>34.2 1.9</cell><cell>33.5 1.1</cell><cell>33.5 1.3</cell></row><row><cell>Prompt tuning (100)</cell><cell>409K</cell><cell>33.4 1.2</cell><cell>33.8 0.5</cell><cell>33.3 0.8</cell></row><row><cell>Prefix tuning</cell><cell>576K</cell><cell>43.3 4.1</cell><cell>37.5 1.2</cell><cell>36.5 1.5</cell></row><row><cell>FishMask (0.2%)</cell><cell>6M</cell><cell>43.7 0.3</cell><cell>39.7 1.4</cell><cell>37.2 1.1</cell></row><row><cell>FishMask (0.02%)</cell><cell>600K</cell><cell>39.9 0.9</cell><cell>38.1 2.0</cell><cell>36.2 1.8</cell></row><row><cell>SAID</cell><cell>500K</cell><cell>40.4 3.3</cell><cell>35.4 4.1</cell><cell>35.5 1.6</cell></row><row><cell>SAID</cell><cell>20K</cell><cell>41.3 1.3</cell><cell>38.5 1.8</cell><cell>35.8 2.0</cell></row><row><cell>LoRA</cell><cell>9.1M</cell><cell>45.1 2.5</cell><cell>41.0 1.4</cell><cell>39.5 4.8</cell></row><row><cell>(IA) 3</cell><cell>540K</cell><cell>48.6 2.0</cell><cell>40.8 1.5</cell><cell>40.8 2.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Per-dataset accuracies for the PEFT methods we consider when adding L UL and L LN . 4.00 47.1222.44   93.96 0.<ref type="bibr" target="#b58">59</ref> 56.91<ref type="bibr" target="#b2">3</ref>.79 BitFit (with LayerNorm) 1.3M 80.00 6.00 31.33 0.16 92.89 0.27 51.38 0.71 LayerNorm 250K 82.00 2.00 31.25 0.64 92.84 0.48 51.14 0.39 3.85 53.92 2.04 75.81 4.33 89.29 7.14 BitFit (with LayerNorm) 1.3M 63.46 2.88 54.23 3.13 75.45 1.81 67.86 0.00 LayerNorm 250K 60.58 2.88 55.33 1.88 76.17 1.44 67.86 1.79 Adapter 12.9M 63.46 3.85 55.49 3.61 77.26 3.97 80.36 3.57 Compacter (n = 4) 807K 64.42 3.85 53.29 5.49 75.45 2.89 82.14 5.36 Compacter++ (n = 4) 540K 65.38 3.85 54.86 3.45 77.26 5.78 76.79 7.14 Prompt tuning (10) 41K 53.85 4.81 52.04 1.72 55.23 2.53 66.07 3.57 Prompt tuning (100) 409K 50.96 6.73 51.88 1.57 48.38 3.69 62.50 12.50 Prefix tuning 576K 60.58 3.85 68.95 0.72 80.36 12.50 75.00 8.93 FishMask (0.2%) 6M 66.35 2.88 54.23 1.10 75.81 3.61 83.93 7.14 FishMask (0.02%) 600K 60.58 1.92 52.82 1.10 75.09 3.61 76.79 3.57 1M 61.54 1.92 55.02 4.70 74.73 4.69 85.71 1.79 (IA) 3 540K 66.35 3.85 53.76 0.63 76.90 2.89 83.93 0.00 # of Param ANLI-R1 ANLI-R2 ANLI-R3 Avg. Full Model Fine-tuning 3B 48.20 0.60 40.90 0.90 38.25 1.58 63.25 BitFit (with LayerNorm) 1.3M 36.10 1.40 35.60 1.40 35.42 2.00 56.7 LayerNorm 250K 37.30 0.50 37.10 0.70 36.25 1.08 57.07 Adapter 12.9M 42.40 3.20 38.80 0.60 36.50 3.83 60.71 Compacter (n = 4) 807K 42.90 3.90 38.00 0.80 37.33 2.33 61.27 Compacter++ (n = 4) 540K 41.90 0.50 38.50 2.40 36.00 0.58 61.13 Prompt tuning (10) 41K 34.20 1.10 34.20 1.30 34.42 0.83 52.12 Prompt tuning (100) 409K 34.10 1.10 34.20 0.20 34.08 1.25 49.82 Prefix tuning 576K 37.50 3.60 34.17 4.50 34.40 9.71 58.71 FishMask (0.2%) 6M 43.40 0.60 40.00 0.90 36.75 2.83 60.03 FishMask (0.02%) 600K 40.10 0.90 38.00 2.00 35.50 0.75 57.73 1M 46.20 1.70 41.40 0.90 38.42 2.67 62.57 (IA) 3 540K 49.20 2.80 40.30 2.30 40.42 3.17 64.05</figDesc><table><row><cell>Subscripts are IQR.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Per-dataset accuracies for the PEFT methods we consider when adding L LN . Subscripts are IQR. 3.00 46.12 4.82 93.64 2.51 56.51 2.21 BitFit (with LayerNorm) 1.3M 81.00 4.00 35.51 2.34 92.78 0.86 50.91 0.08 LayerNorm 250K 82.00 1.00 34.60 2.31 92.68 0.75 51.78 1.26 Adapter 12.9M 83.00 1.00 42.53 5.35 90.49 3.15 53.67 3.63 8.65 56.43 4.08 77.62 1.44 89.29 1.79 BitFit (with LayerNorm) 1.3M 64.42 3.85 53.61 2.51 76.17 3.61 60.71 1.79 LayerNorm 250K 60.58 8.65 53.92 2.35 75.09 1.81 57.14 3.57 Adapter 12.9M 65.38 6.73 54.39 3.13 79.06 5.42 85.71 3.57 Compacter (n = 4) 807K 65.38 4.81 54.55 3.61 75.45 5.05 82.14 0.00 Compacter++ (n = 4) 540K 64.42 3.85 55.64 3.61 77.62 4.69 80.36 7.14 Prompt tuning (10) 41K 54.81 6.73 52.82 3.29 52.71 1.08 69.64 5.36 Prompt tuning (100) 409K 50.00 3.85 50.16 0.94 52.71 4.33 58.93 12.50 Prefix tuning 576K 55.77 1.92 71.12 6.14 82.14 5.36 83.93 8.93 FishMask (0.2%) 6M 62.50 3.85 53.61 1.41 76.17 2.17 83.93 8.93 FishMask (0.02%) 600K 59.62 1.92 53.61 0.47 74.37 5.05 75.00 1.79 1M 59.62 12.50 55.49 4.86 79.06 1.81 87.50 1.79 (IA) 3 540K 65.38 4.81 56.74 4.39 77.26 2.53 87.50 1.79 # of Param ANLI-R1 ANLI-R2 ANLI-R3 Avg. Full Model Fine-tuning 3B 47.90 1.90 40.90 1.90 38.83 5.00 62.71 BitFit (with LayerNorm) 1.3M 36.40 1.10 34.00 0.70 35.25 2.42 56.43 LayerNorm 250K 37.00 1.90 36.00 2.10 35.58 2.17 56.03 Adapter 12.9M 43.90 1.10 38.60 1.10 36.17 2.17 61.17 Compacter (n = 4) 807K 41.80 1.30 37.60 3.00 37.17 1.92 61.14 Compacter++ (n = 4) 540K 41.70 0.60 38.20 2.50 35.58 0.33 61.41 Prompt tuning (10) 41K 35.00 2.10 33.80 0.60 33.67 2.75 52.36 Prompt tuning (100) 409K 35.70 0.90 33.80 1.50 33.00 2.17 49.85 Prefix tuning 576K 34.60 1.60 36.83 4.67 38.52 3.00 58 FishMask (0.2%) 6M 44.10 1.00 38.70 1.50 38.25 0.83 59.79 FishMask (0.02%) 600K 40.50 2.60 37.00 1.20 35.58 0.75 57.66 1M 45.90 2.20 41.10 1.70 38.83 1.08 62.96 (IA) 3 540K 49.80 2.10 40.30 0.30 40.17 3.33 64.06</figDesc><table><row><cell></cell><cell># of Param</cell><cell>COPA</cell><cell>H-Swag</cell><cell cols="2">StoryCloze Winogrande</cell></row><row><cell cols="4">Full Model Fine-tuning 81.00 Compacter (n = 4) 3B 807K 88.00 3.00 42.95 4.06</cell><cell>92.89 1.87</cell><cell>54.62 1.50</cell></row><row><cell>Compacter++ (n = 4)</cell><cell>540K</cell><cell cols="2">85.00 2.00 48.26 2.95</cell><cell>93.85 1.60</cell><cell>54.85 2.84</cell></row><row><cell>Prompt tuning (10)</cell><cell>41K</cell><cell cols="2">74.00 5.00 29.24 2.48</cell><cell>88.88 1.12</cell><cell>51.38 0.47</cell></row><row><cell>Prompt tuning (100)</cell><cell>409K</cell><cell cols="2">68.00 7.00 28.51 2.43</cell><cell>86.91 4.33</cell><cell>50.59 0.16</cell></row><row><cell>Prefix tuning</cell><cell>576K</cell><cell cols="2">69.00 2.00 29.04 10.83</cell><cell>86.44 2.35</cell><cell>50.63 1.41</cell></row><row><cell>FishMask (0.2%)</cell><cell>6M</cell><cell cols="2">85.00 5.00 27.78 0.51</cell><cell>94.01 1.55</cell><cell>53.67 2.60</cell></row><row><cell>FishMask (0.02%)</cell><cell>600K</cell><cell cols="2">84.00 4.00 27.78 0.51</cell><cell>93.16 1.23</cell><cell>53.59 2.21</cell></row><row><cell>Intrinsic SAID</cell><cell>500K</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Intrinsic SAID</cell><cell>20K</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LoRA</cell><cell>9.1M</cell><cell cols="2">87.00 3.00 46.97 1.98</cell><cell>93.11 2.03</cell><cell>57.93 3.63</cell></row><row><cell>(IA) 3</cell><cell>540K</cell><cell cols="2">86.00 4.00 48.78 4.12</cell><cell>94.01 2.83</cell><cell>58.72 1.34</cell></row><row><cell></cell><cell># of Param</cell><cell>WSC</cell><cell>WiC</cell><cell>RTE</cell><cell>CB</cell></row><row><cell cols="3">Full Model Fine-tuning 61.54 SAID 3B 500K</cell><cell></cell><cell></cell></row><row><cell>SAID</cell><cell>20K</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LoRA 9.SAID 500K</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAID</cell><cell>20K</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LoRA</cell><cell>9.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Per-dataset accuracies for the PEFT methods we consider when adding L UL . Subscripts are IQR. Swag StoryCloze Winogrande Full Model Fine-tuning 3B 78.00 2.00 39.16 0.24 91.45 0.96 54.46 0.87 BitFit (with LayerNorm) 1.3M 77.00 7.00 33.76 0.38 90.49 0.27 51.54 0.16 LayerNorm 250K 77.00 7.00 33.58 0.65 90.43 0.21 51.38 0.32 Adapter 12.9M 76.00 5.00 36.41 2.27 90.59 1.71 52.01 0.47 Compacter (n = 4) 807K 81.00 5.00 37.53 0.67 91.50 0.21 52.57 0.87 Compacter++ (n = 4) 540K 78.00 2.00 37.00 1.02 91.98 0.91 53.12 0.87 Prompt tuning (10) 41K 73.00 4.00 30.09 1.67 88.88 1.12 52.25 0.32 Prompt tuning (100) 409K 66.00 4.00 26.31 4.46 87.44 0.21 51.14 0.55 Prefix tuning 576K 70.00 3.00 27.98 6.62 86.75 2.24 51.07 1.10 FishMask (0.2%) 6M 77.00 3.00 35.45 0.87 90.54 1.07 52.96 0.87 FishMask (0.02%) 600K 74.00 2.00 31.15 1.30 89.52 1.28 52.57 0.47 1M 80.00 5.00 39.14 1.26 92.04 1.07 53.75 0.47 (IA) 3 540K 82.00 1.00 40.59 0.56 92.57 0.48 56.91 2.53 0.96 53.76 1.72 75.81 5.42 82.14 5.36 BitFit (with LayerNorm) 1.3M 61.54 3.85 53.13 1.72 76.53 1.08 64.29 8.93 LayerNorm 250K 61.54 3.85 53.29 1.72 76.17 2.17 62.50 8.93 Adapter 12.9M 65.38 7.69 54.70 1.72 77.26 2.89 83.93 1.79 Compacter (n = 4) 807K 61.54 2.88 55.33 3.61 76.17 2.17 83.93 0.00 Compacter++ (n = 4) 540K 61.54 1.92 54.70 4.23 73.65 1.81 78.57 5.36 Prompt tuning (10) 41K 53.85 7.69 52.51 1.88 57.40 4.33 69.64 10.71 Prompt tuning (100) 409K 56.73 6.73 52.35 0.63 54.15 3.97 53.57 19.64 Prefix tuning 576K 52.88 7.69 52.51 0.31 72.56 11.91 75.00 17.86 FishMask (0.2%) 6M 62.50 4.81 54.23 2.04 77.26 5.42 82.14 1.79 FishMask (0.02%) 600K 58.65 2.88 54.39 1.10 76.17 5.05 75.00 3.57 1M 64.42 12.50 54.86 3.45 77.26 4.33 87.50 3.57 (IA) 3 540K 64.42 3.85 54.23 1.57 77.98 1.81 82.14 5.36 # of Param ANLI-R1 ANLI-R2 ANLI-R3 Avg. Full Model Fine-tuning 3B 47.80 1.50 40.60 0.80 37.75 1.83 60.66 BitFit (with LayerNorm) 1.3M 37.30 1.80 36.10 2.60 35.17 3.67 56.08 LayerNorm 250K 37.50 1.50 36.00 2.80 35.08 3.42 55.86 Adapter 12.9M 40.70 3.70 39.20 1.10 35.83 1.92 59.27 Compacter (n = 4) 807K 41.80 2.70 38.00 0.80 36.00 2.75 59.58 Compacter++ (n = 4) 540K 41.10 1.50 38.90 2.50 36.92 1.42 58.68 Prompt tuning (10) 41K 33.60 0.70 33.80 1.10 34.83 1.00 52.71 Prompt tuning (100) 409K 35.60 1.70 34.50 0.70 34.75 1.42 50.23 Prefix tuning 576K 37.60 2.30 34.10 3.50 35.08 0.67 54.14 FishMask (0.2%) 6M 43.50 0.30 40.30 0.40 36.42 2.25 59.3 FishMask (0.02%) 600K 40.40 2.20 37.50 1.00 36.42 1.08 56.89 1M 44.20 2.60 40.40 1.20 37.58 0.58 61.01 (IA) 3 540K 48.50 0.90 40.20 1.80 39.42 1.67 61.72</figDesc><table><row><cell cols="3"># of Param H-Intrinsic SAID COPA 500K</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Intrinsic SAID</cell><cell>20K</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LoRA</cell><cell>9.# of Param</cell><cell>WSC</cell><cell>WiC</cell><cell>RTE</cell><cell>CB</cell></row><row><cell cols="3">Full Model Fine-tuning 66.35 SAID 3B 500K</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAID</cell><cell>20K</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LoRA 9.SAID 500K</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAID</cell><cell>20K</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LoRA</cell><cell>9.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Per-dataset accuracies for the PEFT methods we consider without L UL or L LN . Subscripts are IQR. COPA H-Swag StoryCloze Winogrande WSC WiC (IA) 3 87.0 3.0 49.4 4.6 94.7 2.7 59.8 0.6 68.3 6.7 56.0 4.6 + PT 89.0 5.0 51.2 4.6 95.1 2.5 62.6 1.1 70.2 8.7 57.2 2.5 RTE CB ANLI-R1 ANLI-R2 ANLI-R3 Acc. (IA) 3 78.0 2.5 87.5 1.8 48.6 2.0 40.8 1.5 40.83 2.3 64.6 + PT 80.9 1.4 87.5 1.8 49.3 1.1 41.1 0.5 39.8 4.8 65.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Per-dataset results when pre-training (PT) (IA) 3 vs. not pre-training (IA) 3 . Subscripts are IQR.</figDesc><table><row><cell></cell><cell cols="5">COPA H-Swag StoryCloze Winogrande</cell><cell>WSC</cell><cell>WiC</cell></row><row><cell>T-Few</cell><cell cols="3">93.0 2.0 67.1 6.0</cell><cell>97.9 0.3</cell><cell>74.3 1.5</cell><cell cols="2">75.0 5.5 62.2 7.8</cell></row><row><cell>T0</cell><cell>90.8</cell><cell>33.7</cell><cell></cell><cell>94.7</cell><cell>60.5</cell><cell>64.4</cell><cell>57.2</cell></row><row><cell>T5+LM</cell><cell>68.0</cell><cell cols="2">60.95</cell><cell>62.8</cell><cell>56.9</cell><cell>63.5</cell><cell>50.0</cell></row><row><cell>GPT-3 (175B)</cell><cell>92.0</cell><cell>79.3</cell><cell></cell><cell>87.7</cell><cell>77.7</cell><cell>75.0</cell><cell>55.3</cell></row><row><cell>GPT-3 (13B)</cell><cell>86.0</cell><cell>71.3</cell><cell></cell><cell>83.0</cell><cell>70.0</cell><cell>75.0</cell><cell>51.1</cell></row><row><cell>GPT-3 (6.7B)</cell><cell>83.0</cell><cell>67.3</cell><cell></cell><cell>81.2</cell><cell>67.4</cell><cell>67.3</cell><cell>53.1</cell></row><row><cell></cell><cell></cell><cell>RTE</cell><cell>CB</cell><cell cols="3">ANLI-R1 ANLI-R2 ANLI-R3</cell></row><row><cell>T-Few</cell><cell cols="3">85.6 2.9 87.5 3.6</cell><cell>59.3 3.6</cell><cell>49.8 2.6</cell><cell>44.8 8.0</cell></row><row><cell>T0</cell><cell></cell><cell>81.2</cell><cell>78.6</cell><cell>44.7</cell><cell>39.4</cell><cell>42.4</cell></row><row><cell>T5 + LM</cell><cell></cell><cell>53.4</cell><cell>32.1</cell><cell>33.3</cell><cell>32.7</cell><cell>34.1</cell></row><row><cell cols="2">GPT-3 (175B)</cell><cell>72.9</cell><cell>82.1</cell><cell>36.8</cell><cell>34.0</cell><cell>40.2</cell></row><row><cell cols="2">GPT-3 (13B)</cell><cell>60.6</cell><cell>66.1</cell><cell>33.3</cell><cell>32.6</cell><cell>34.5</cell></row><row><cell cols="2">GPT-3 (6.7B)</cell><cell>49.5</cell><cell>60.7</cell><cell>33.1</cell><cell>33.1</cell><cell>33.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Comparing T-Few with few-shot ICL methods. All GPT-3 numbers are from Brown et al.<ref type="bibr" target="#b3">[4]</ref> and all T0 numbers are from Sanh et al.<ref type="bibr" target="#b0">[1]</ref>. Subscripts are IQR.2.0 67.1 6.0 97.9 0.3 74.3 1.5 75.0 5.5 62.15 7.8 -PT 92.0 2.0 64.5 6.6 97.8 0.8 72.7 1.0 73.1 6.3 60.8 6.4 -L UL -L LN 91.0 2.0 52.1 2.7 97.4 0.5 71.9 1.1 71.2 1.0 62.2 2.4 -PT -L UL -L LN 94.0 2.3 52.7 4.9 98.0 0.3 74.0 1.1 72.6 4.8 62.6 5.0 UL -L LN 82.0 0.7 82.1 3.6 54.8 0.4 46.1 0.6 40.8 5.2 68.3 -PT -L UL -L LN 84.5 2.9 80.4 3.6 57.1 3.1 47.1 2.4 43.8 5.9 69.7</figDesc><table><row><cell></cell><cell cols="4">COPA H-Swag StoryCloze Winogrande</cell><cell>WSC</cell><cell>WiC</cell></row><row><cell>T-Few</cell><cell>93.0 RTE</cell><cell>CB</cell><cell cols="4">ANLI-R1 ANLI-R2 ANLI-R3 Acc.</cell></row><row><cell>T-Few</cell><cell cols="2">85.6 2.9 87.5 3.6</cell><cell>59.3 3.6</cell><cell>49.8 2.6</cell><cell>44.8 8.0</cell><cell>72.4</cell></row><row><cell>-PT</cell><cell cols="2">84.5 2.8 83.9 5.4</cell><cell>57.9 3.2</cell><cell>48.6 3.0</cell><cell>43.1 5.7</cell><cell>70.8</cell></row><row><cell>-L</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://docs.microsoft.com/en-us/azure/virtual-machines/ndm-a100-v4-series</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://cloud.datacrunch.io/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08207</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Lifland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Maham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jess</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmie</forename><surname>Hine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Ashurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Sedille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Carlier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14076</idno>
		<title level="m">A real-world few-shot text classification benchmark</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners. OpenAI blog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07814</idno>
		<title level="m">Meta-learning via language model in-context tuning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.15943</idno>
		<title level="m">Metaicl: Learning to learn in context</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Can language models learn from explanations in context? ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kyle Lampinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishita</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kory</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Henry</forename><surname>Matthewson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hill</surname></persName>
		</author>
		<idno>abs/2204.02329</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Internetaugmented language models through few-shot prompting for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Stokowiec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Grigorev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05115</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.12837</idno>
		<title level="m">Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Alipoormolabashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjana</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arut</forename><surname>Selvan Dhanasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atharva</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07705</idno>
		<title level="m">Benchmarking generalization via in-context instructions on 1,600+ language tasks</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Do prompt-based models really understand the meaning of their prompts?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01247</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09690</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LoRA: Low-rank adaptation of large language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2106.09685</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1910.10683</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Rakesh R Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11955</idno>
		<title level="m">Shashank Srivastava, and Colin Raffel. Improving and simplifying pattern exploiting training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04319</idno>
		<title level="m">Neural text generation with unlikelihood training</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">PPT: Pre-trained prompt tuning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04332</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">SPoT: Better frozen model adaptation through soft prompt transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07904</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Noisy channel language model prompting for few-shot text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.04106</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple visual domains with residual adapters. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00751</idno>
		<title level="m">Parameter-efficient transfer learning for NLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Simple, scalable adaptation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08478</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Parameter-efficient transfer learning with diff pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07463</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Training neural networks with fixed sparse masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Lin</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09839</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13255</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Compacter: Efficient low-rank hypercomplex adapter layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Rabeeh Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04647</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Prefix-Tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards a unified view of parameter-efficient transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04366</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">True few-shot learning with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11447</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Xin</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abheesht</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taewoon</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>F?vry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01279</idno>
		<title level="m">An integrated development environment and repository for natural language prompts</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Cosmin Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">HellaSwag: Can a machine really finish your sentence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tackling the story ending biases in the story cloze test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Bakhshandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="752" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14599</idno>
		<title level="m">Adversarial NLI: A new benchmark for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The CommitmentBank: Investigating projection in naturally occurring discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Tonhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Sinn und Bedeutung 23</title>
		<meeting>Sinn und Bedeutung 23</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09121</idno>
		<title level="m">WiC: the word-in-context dataset for evaluating context-sensitive meaning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The numpy array: a structure for efficient numerical computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Chris</forename><surname>Colbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gael</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in science &amp; engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10199</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01652</idno>
		<title level="m">Finetuned language models are zero-shot learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">True few-shot learning with prompts-a real-world perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13440</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Sentence transformer fine-tuning (SetFit): Outperforming GPT-3 on fewshot text-classification while being 1600 times smaller</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Wasserblat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">The efficiency misnomer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.12894</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning how to ask: Querying LMs with mixtures of soft prompts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06599</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07602</idno>
	</analytic>
	<monogr>
		<title level="m">Prompt tuning can be comparable to fine-tuning universally across scales and tasks</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Shengnan An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03131</idno>
		<title level="m">Input-Tuning: Adapting unfamiliar inputs to frozen pretrained models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04824</idno>
		<title level="m">AdaPrompt: Adaptive model training for prompt-based NLP</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Black-box prompt learning for pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08531</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Prompt waywardness: The curious case of discretized interpretation of continuous prompts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08348</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Perot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08654</idno>
		<title level="m">Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">On robust prefix-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.10378</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A prompting-based approach for adversarial example generation and robustness enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Song</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.10714</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">PSP: Pre-trained soft prompts for few-shot abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04413</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Using natural language prompts for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.11822</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Co-training improves promptbased learning for large language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00828</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Shepherd pre-trained language models to develop a train of thought: An iterative prompting approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boshi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.08383</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Controllable generation from pre-trained language models via inverse prompting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyang</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10685</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06719</idno>
		<title level="m">On transferability of prompt tuning for natural language understanding</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Huaixiu Steven Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jai</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vamsi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.00759</idno>
		<title level="m">Prompt-based task-conditioning of transformers</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bor-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12119</idno>
		<title level="m">Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Exploiting cloze questions for few shot text classification and natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.08493</idno>
		<title level="m">How many data points is a prompt worth</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.00902</idno>
		<title level="m">Do prompts solve NLP tasks using natural language? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Auto-Prompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Differentiable prompt makes pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13161</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">PERFECT: Prompt-free and efficient few-shot learning with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Rabeeh Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lambert</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yazdani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01172</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Nafise Sadat Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Delfosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01549</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">Adaptable adapters. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Learning a universal template for few-shot dataset generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<idno>arXiv:/2105.07029</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07697</idno>
		<title level="m">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Universal representation learning from multiple domains for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5371628</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5371628.MethodAdeCorpusV2Banking77" />
		<imprint>
			<date type="published" when="2021-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Neurips Impact Statement Risks One Stop English Overruling Semiconductor Org Types Systematic Review Inclusion Tai Safety Research Terms Of Service Tweet Eval Hate Twitter Complaints</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<title level="m">Table 11: Detailed per-dataset results for T-Few and the other top-5 methods on RAFT</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
