<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Weakly Supervised Pre-Training of Visual Perception Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Adcock</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freitas</forename><surname>Reis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bugra</forename><surname>Gedik</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meta</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Revisiting Weakly Supervised Pre-Training of Visual Perception Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/facebookresearch/SWAG</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model pre-training is a cornerstone of modern visual recognition systems. Although fully supervised pre-training on datasets like ImageNet is still the de-facto standard, recent studies suggest that large-scale weakly supervised pretraining can outperform fully supervised approaches. This paper revisits weakly-supervised pre-training of models using hashtag supervision with modern versions of residual networks and the largest-ever dataset of images and corresponding hashtags. We study the performance of the resulting models in various transfer-learning settings including zero-shot transfer. We also compare our models with those obtained via large-scale self-supervised learning. We find our weakly-supervised models to be very competitive across all settings, and find they substantially outperform their self-supervised counterparts. We also include an investigation into whether our models learned potentially troubling associations or stereotypes. Overall, our results provide a compelling argument for the use of weakly supervised learning in the development of visual recognition systems. Our models, Supervised Weakly through hashtAGs (SWAG), are available publicly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most modern visual-recognition systems are based on machine-learning models that are pre-trained to perform a task that is different from the downstream task that the system aims to solve. Such pre-training allows the system to leverage (annotated) image or video datasets that are much larger than the datasets available for the downstream task. Arguably the most popular pre-training task is supervised image classification on datasets such as Im-ageNet and JFT <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b75">76]</ref>, but recent studies have also explored self-supervised <ref type="bibr">[11-14, 27, 29, 31, 51]</ref> and weakly supervised <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b56">57]</ref> tasks for pre-training.</p><p>There are trade-offs between these three types of pretraining. Fully supervised pre-training benefits from a strong semantic learning signal for each training example, but does not scale well because manual labeling of training data is time-consuming. By contrast, self-supervised pretraining receives hardly any semantic information on the training examples, but can be scaled to billions of training examples relatively easily <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>. Weakly-supervised approaches fall somewhere in between: for example, hashtags or other text associated with visual data generally provide a noisy semantic learning signal but can be obtained at large scale with relative ease <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b56">57]</ref>.</p><p>Following the success of prior work <ref type="bibr" target="#b48">[49]</ref>, this paper performs an in-depth study of weakly-supervised pre-training using hashtag supervision. We pre-train modern imagerecognition models on the largest-ever-dataset of images and associated hashtags, and evaluate the resulting models in a range of transfer-learning experiments. Specifically, we transfer our models to a variety of image-classification tasks and evaluate the performance of the resulting models. We also evaluate the models in zero-shot transfer and few-shot transfer settings <ref type="bibr" target="#b56">[57]</ref>: that is, we evaluate the "off-the-shelf performance" of these models without finetuning them on the target tasks. The overall goal of our study is to shed light on the trade-offs between fully supervised, self supervised, and weakly supervised pre-training. Throughout our experiments, we find the weakly-supervised approach to be very competitive: our best models perform on par with the state-of-the-art on a range of visual-perception tasks despite employing a relatively simple training pipeline.</p><p>A potential downside of weakly-supervised pre-training is that models may inherit or amplify harmful associations from the underlying supervisory signal. We perform a series of experiments aimed at assessing the extent to which this happens. Our results do not provide conclusive answers, but they do suggest that the risks involved may not be as large as in language modeling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>. Overall, we believe our study presents a compelling argument for weakly-supervised pretraining of visual-recognition systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This study is part of a large body of work on pre-training models for visual recognition. This body of work can be subdivided into three key groups. Fully supervised pre-training was pioneered by <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b58">59]</ref> and is now the de-facto standard approach to a variety of visual-recognition tasks, including fine-grained image classification <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b61">62]</ref>, object detection <ref type="bibr" target="#b60">[61]</ref>, image segmentation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b69">70]</ref>, image captioning <ref type="bibr" target="#b45">[46]</ref>, visual question answering <ref type="bibr" target="#b39">[40]</ref>, video classification <ref type="bibr" target="#b20">[21]</ref>, etc. The ImageNet-1K dataset <ref type="bibr" target="#b62">[63]</ref> is by far the most commonly used image dataset for pre-training, whereas the Kinetics dataset <ref type="bibr" target="#b38">[39]</ref> is often used for pre-training of video-recognition models. Some recent studies have also used the much larger JFT-300M <ref type="bibr" target="#b19">[20]</ref> and JFT-3B <ref type="bibr" target="#b75">[76]</ref> image datasets, but not much is known publicly about those datasets. The effectiveness of supervised pre-training has been the subject of a number of studies, in particular, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b59">60]</ref> analyze the transfer performance of supervised pre-trained models. Self-supervised pre-training has seen tremendous progress in recent years. Whereas early self-supervised learners such as RotNet <ref type="bibr" target="#b25">[26]</ref> or DeepCluster <ref type="bibr" target="#b9">[10]</ref> substantially lagged their supervised counterparts in vision pre-training, more recent approaches have become quite competitive. These approaches learn to predict clusters <ref type="bibr" target="#b10">[11]</ref>, use contrastive learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b50">51]</ref>, or use student-teacher architectures in which the teacher is an exponentially moving average of the student <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>A key advantage of self-supervised pre-training is that is can easily be scaled to billions of training images: several studies have shown that scaling self-supervised learning can lead to substantial performance improvements <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>. Weakly-supervised pre-training has not received nearly as much attention as the other two pre-training paradigms, but has shown very promising performance nonetheless. Whereas early studies that pre-trained models by predicting words <ref type="bibr" target="#b37">[38]</ref> or n-grams <ref type="bibr" target="#b43">[44]</ref> in image captions were not very competitive because of the limited scale of their training data, recent weakly-supervised pre-training methods are much more competitive on a range of visual-recognition tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>. In particular, ALIGN <ref type="bibr" target="#b36">[37]</ref> and CLIP <ref type="bibr" target="#b56">[57]</ref> pre-train vision-and-language models on large numbers of images and associated captions, and successfully perform zero-shot transfer to new recognition tasks.</p><p>Our study builds on <ref type="bibr" target="#b48">[49]</ref>, which trained convolutional networks on billions of images to predict associated hashtags. Compared to <ref type="bibr" target="#b48">[49]</ref>, our study: <ref type="bibr" target="#b0">(1)</ref> trains larger models with more efficient convolutional and transformer architectures on a much larger dataset, <ref type="bibr" target="#b1">(2)</ref> studies the performance of the resulting models in zero-shot transfer settings in addition to standard transfer-learning experiments, (3) performs comparisons of our models with state-of-the-art selfsupervised learners, and <ref type="bibr" target="#b3">(4)</ref> presents an in-depth study of potential harmful associations that models may adopt from the weak supervision they receive. Despite the conceptual similarities in our approach, our best model achieves an ImageNet-1K validation accuracy that is more than 3% higher than that reported in <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pre-Training using Hashtag Supervision</head><p>Our weakly supervised pre-training methodology is based on hashtag supervision. We train image-recognition models to predict the hashtags that were assigned to an image by the person who posted the image. Hashtag prediction has great potential as a pre-training task because hashtags were assigned to images to make them searchable, i.e., they tend to describe some salient semantic aspects of the image. While hashtag prediction is conceptually similar to image classification, it differs in a few key ways <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b67">68</ref>]:</p><p>1. Hashtag supervision is inherently noisy. Whilst some hashtags describe visual content in the image (e.g., #cat), other hashtags may be unrelated to the visual content (e.g., #repost). Different hashtags may be used to describe the same visual content, or the same hashtag may be used to describe different visual content. Importantly, hashtags generally do not provide a comprehensive annotation of the visual content of an image, that is, there tend to be many false negatives. 2. Hashtag usage follows a Zipfian distribution <ref type="bibr" target="#b49">[50]</ref>; see <ref type="figure" target="#fig_0">Figure 1</ref>. This implies that the learning signal follows a very different distribution than is common in imagerecognition datasets like ImageNet <ref type="bibr" target="#b62">[63]</ref>, which tend to have a class distribution that is more or less uniform. 3. Hashtag supervision is inherently multi-label: a single image generally has multiple hashtags associated with it that all serve as positive classification targets. Our data pre-processing and model pre-training procedures are designed to (partly) address these issues. We describe them in more detail in Section 3.1 and 3.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hashtag Dataset Collection</head><p>We follow <ref type="bibr" target="#b48">[49]</ref> in constructing a dataset of public Instagram photos and associated hashtags. We adopt the following four steps in constructing the pre-training dataset:</p><p>1. Construct a hashtag vocabulary by selecting frequently used hashtags and canonicalizing them. 2. Gather publicly available images that are tagged with at least one of the selected hashtags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Combine the resulting images and associated hashtags</head><p>into labeled examples that can be used for pre-training. 4. Resample the resulting examples to obtain the desired hashtag distribution. Next, we describe each of these steps in detail. Hashtag vocabulary. We select hashtags used more than once in public Instagram posts by US users. Next, we filter Right: Frequency of filtered and canonicalized hashtags occurring with public images by users from all countries. We define the head as the set of canonical hashtags associated with more than 5,000 images; the remaining hashtags form the tail. out and canonicalize the hashtags using WordNet synsets <ref type="bibr" target="#b21">[22]</ref>. More details about this process are in Appendix A. This results in a label set, C, that contains ?27k canonical hashtags that correspond to a set of ?75k raw hashtags, where multiple hashtags can map to a single canonical hashtag (e.g., #dog and #canine). We drop the "canonical" qualifier when it is obvious from the context. As the exact images in the dataset may change with time, the number of canonical hashtags varies between 27k and 28k across experiments. The hashtag selection and canonicalization reduces some of the inherent noise in the supervisory signal. Image collection and labeling. We collect all public Instagram images that have at least one hashtag from our vocabulary. <ref type="bibr" target="#b0">1</ref> The images were subjected to an array of automated filters designed to remove potentially offensive content. While certainly not perfect, this substantially reduces the issues that plague other large image datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b54">55]</ref>. We construct a multi-label dataset using these images by converting all hashtags into their corresponding canonical targets (note that a single image may have multiple hashtags). Hashtags that are not in the vocabulary are discarded. Resampling. We adopt a resampling procedure similar to <ref type="bibr" target="#b48">[49]</ref> to generate our final pre-training examples. The resampling procedure aims to down-weight frequent hashtags whilst up-weighting infrequent hashtags in the pre-training task. We do so by resampling according to the inverse square root of the hashtag frequency. Unlike <ref type="bibr" target="#b48">[49]</ref>, we additionally upsample (with replacement) the long tail of images with at least one infrequent hashtag by ?100?. Herein, we define infrequent hashtags as those that occur with fewer than 5,000 images (see <ref type="figure" target="#fig_0">Figure 1</ref>). The resulting resampled dataset comprises 30% tail images and 70% head images (see Appendix A for more details).</p><p>We note that this means that in a single training epoch, each unique tail image appears multiple times. This implies there is a discrepancy between the number of unique images in an epoch and the number of total samples processed in that epoch. We label our dataset by the number of unique images in the dataset: our IG-3.6B dataset has ?3.6 billion unique images. However, a single training epoch over that dataset processes ?5 billion samples due to our re-sampling procedure. This is different from other datasets we compare with (e.g., JFT-300M) in which the unique number of images equals the total samples processed in an epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-Training Procedure</head><p>In preliminary experiments (Appendix C.1), we studied image-recognition models including ResNeXt <ref type="bibr" target="#b73">[74]</ref>, Reg-NetY <ref type="bibr" target="#b57">[58]</ref>, DenseNet <ref type="bibr" target="#b34">[35]</ref>, EfficientNet <ref type="bibr" target="#b64">[65]</ref>, and ViT <ref type="bibr" target="#b19">[20]</ref> . We found RegNetY and ViT models to be most competitive, and focus on those in the experiments presented here.</p><p>During pre-training, we equip our models with an output linear classifier over |C| ? 27k classes. For ViTs we use an additional linear layer with output dimension equal to the input dimension, similar to <ref type="bibr" target="#b19">[20]</ref>. Following <ref type="bibr" target="#b48">[49]</ref>, we use a softmax activation and train the model to minimize the cross-entropy between the predicted probabilities and the target distribution. Each target entry is either 1 /K or 0 depending on whether the corresponding hashtag is present or not, where K is the number of hashtags for that image.</p><p>All our RegNetY models were trained using stochastic gradient descent (SGD) with Nesterov momentum of 0.9. We employed a half-cosine learning rate schedule <ref type="bibr" target="#b47">[48]</ref> with a base initial value of 0.1 for a batch size of 256 and a final value of 0. We used a weight decay of 10 ?5 , but disabled weight decay in batch-normalization layers: preliminary experiments suggested that batch-normalization weight decay is effective when pre-training on ImageNet-1k, but significantly degrades results on larger datasets such as IG-3.6B.</p><p>Our ViT models were trained using AdamW <ref type="bibr" target="#b46">[47]</ref> with ? 1 = 0.9 and ? 2 = 0.95. We used an initial learning rate of 4 ? 10 ?4 , a batch size of 8,192, and a weight decay of 0.1.</p><p>Following <ref type="bibr" target="#b27">[28]</ref>, we scale the initial learning rate linearly with the batch size when doing distributed training. We "warm up" the learning rate for the first 5% of training updates by linearly increasing the learning rate from 1 /10-th of the initial value to the initial value. Similar to <ref type="bibr" target="#b27">[28]</ref>, we find that performance degrades for batch sizes larger than 8,192 so we did not increase our batch size further.</p><p>We trained our models using mixed-precision training on images that were pre-processed to 224?224 resolution using a standard random-resize crop followed by a random horizontal flip. In preliminary experiments, we also evaluated several other training approaches that provide gains in ImageNet-1k pre-training <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b64">65]</ref>, including exponential moving averages <ref type="bibr" target="#b53">[54]</ref>, mixup <ref type="bibr" target="#b76">[77]</ref>, label smoothing <ref type="bibr" target="#b51">[52]</ref>, AutoAugment <ref type="bibr" target="#b14">[15]</ref>, and stochastic depth <ref type="bibr" target="#b35">[36]</ref>. However, we did not find those approaches to lead to performance improvements; some even deteriorated performance.</p><p>We trained our largest model for 2 epochs of the IG-3.6B dataset (10 billion samples seen) using 128 Nvidia V100 32GB GPUs across 16 nodes. The nodes were connected via Ethernet, with 8 GPUs / node connected via NVLink.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We performed a series of experiments to test the efficacy of our hashtag-based pre-training strategy. We compare our weakly supervised models in transfer-learning experiments with modern supervised (Section 4.2) and selfsupervised models (Section 4.3), and with other weakly supervised models in zero-shot transfer (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>In our experiments, we focus on different types of transfer learning to image-classification tasks. Specifically, we study: (1) transfer learning using linear classifiers, (2) transfer learning using finetuning, (3) zero-shot transfer learning, and (4) few-shot transfer learning. We compare the efficacy of our pre-training strategy with that of fully supervised (4.2) and self-supervised (4.3) pre-training strategies. Datasets. We perform experiments in which we transfer models to ImageNet classification <ref type="bibr" target="#b62">[63]</ref> on ImageNet-1k (1.28M training images, 50,000 validation images, 1,000 classes), and ImageNet-5k (6.57M training images, 250,000 validation images, 5,000 classes) as defined in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b73">74]</ref>. We also perform experiments in which we transfer pre-trained models to other commonly used image-classification benchmarks, including the iNaturalist 2018 <ref type="bibr" target="#b66">[67]</ref>, Places365-Standard <ref type="bibr" target="#b78">[79]</ref>, and Caltech-UCSD Birds-200-2011 (CUB-2011) <ref type="bibr" target="#b68">[69]</ref> datasets. Finetuning. We follow <ref type="bibr" target="#b40">[41]</ref> in finetuning our pre-training models for downstream tasks. We finetune the models using SGD with a batch size of 512 and a half-cosine learning rate schedule <ref type="bibr" target="#b47">[48]</ref>. The initial value was tuned for every each model-task combination separately via grid-search. We did not use weight decay during finetuning. We finetune RegNetY and ViT B/16 models using an image resolution of 384 ? 384, and ViT L/16 and H/14 models with larger 512 ? 512 and 518 ? 518 resolutions respectivelyhigher resolutions help these models significantly. For Ef-ficientNets, we use the pre-training resolution for finetuning. For "large" transfer datasets (defined as datasets with N &gt; 500,000 examples), we finetune for 20,000 parameter updates; for "medium" datasets (20,000 &lt; N ? 500,000 examples), we finetune for 10,000 steps; and for "small" datasets (N ? 20,000 examples), we finetune for 500 steps. We use mixup <ref type="bibr" target="#b76">[77]</ref> with ? = 0.1 during finetuning on all datasets. We used synchronous batch normalization across GPUs, as it improves transfer performance (see appendix).</p><p>For ImageNet-1k finetuning, we additionally compute an exponential moving average (EMA) of the parameters during training with a decay rate of 10 ?4 and use the averaged weights for inference <ref type="bibr" target="#b53">[54]</ref>. We found this improved the top-1 accuracy for our best RegNetY and ViT models by 0.2%. Lastly, we finetuned ViTs for 28 epochs on ImageNet-1k since the longer schedule helped improve performance.</p><p>During evaluation, we resize the smaller side of the image to the final resolution and then take a center crop of the same size (e.g., resize smaller side to 224 then 224 ? 224 center crop). This differs from standard practice <ref type="bibr" target="#b65">[66]</ref> but gives a boost of 0.1% to 0.5% on the ImageNet-1k dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with Supervised Pre-Training</head><p>We compare our weakly supervised RegNetY and ViT models with state-of-the-art supervised EfficientNets <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b72">73]</ref> and ViTs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b75">76]</ref> in transfer-learning experiments on five datasets: (1) ImageNet-1k , (2) ImageNet-5k, (3) iNaturalist, (4) Places365, and (5) CUB-2011. We finetune all models (see 4.1) on the training split of the transfer dataset and measure the classification accuracy of the finetuned models on the validation or test split. <ref type="table">Table 1</ref> presents an overview of the results of these experiments. For each model, the table shows the pre-training dataset used, the image resolution used during pre-training and finetuning, the inference throughput of the model, the number of FLOPs and parameters in the finetuned model, and the test accuracy on the transfer datasets. We do not report results for an approach when its pre-trained model and pre-training dataset are not publicly available. In the table, accuracies that we adopted from the original paper are italicized. For the ImageNet-1k dataset, we report both results reported in the original papers and results we obtained when we reproduced the model. We boldface the best result and underline the second-best result for each dataset. <ref type="table">Table 1</ref> groups models into supervised and weakly supervised. In this grouping, we consider pre-training on JFT datasets to be supervised pre-training but we acknowledge that little is known on how these datasets were collected: <ref type="bibr" target="#b75">[76]</ref> refers to the JFT-3B dataset as "weakly labeled" and "noisy" but also states that semi-automatic annotation was used to collect it. This suggests that JFT datasets were manually curated and annotated, which is why we consider them as supervised. <ref type="bibr" target="#b1">2</ref> The results in <ref type="table">Table 1</ref> show that our weakly-supervised models are very competitive: they achieve the best or second-best accuracy on all five transfer datasets. We note that models pre-trained on IN-1k datasets observe 5% of the CUB test data during pre-training <ref type="bibr" target="#b48">[49]</ref> as a result of which their performance is overestimated. This makes the strong performance of our weakly-supervised models (which do  <ref type="table">Table 1</ref>. Transfer-learning accuracy of models pre-trained on the specified pre-training dataset followed by finetuning and testing on five transfer datasets. Accuracies that were adopted from the original papers are italicized. The best result on each dataset is boldfaced; the second-best result is underlined. Our weakly-supervised pre-trained models achieve the best or second-best performance on all five transfer datasets. ? It is unknown how much manual curation was performed to annotate the JFT datasets. ? IN-1k is used as supervised pre-training data; JFT 300M is used without labels. ? Model was pre-trained on IN-1k training set, which overlaps with the CUB-2011 test set. not see test data during training) particularly noteworthy.</p><p>To provide more insight into the classification accuracy and throughput trade-off, we plot one as a function of the other in <ref type="figure">Figure 2</ref>. Comparing ViT and RegNetY models trained on the same IG-3.6B dataset, we observe that vision transformers obtain the highest classification accuracies. In terms of accuracy-throughput tradeoff, RegNetYs outperform at small to medium model sizes. The RegNetY 128GF model performs quite similarly on accuracy and throughput to the semi-supervised EfficientNet L2 model, but at smaller size scales, RegNetYs provide a better tradeoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Self-Supervised Pre-Training</head><p>Our experiments so far suggest that the ability to scale up weakly-supervised pretraining to billions of images can offset the lower amount of learning signal obtained per training example. This raises the question if we need weak supervision at all, or whether modern self-supervised learners <ref type="bibr">[10-14, 26, 27, 29, 31, 51]</ref> may suffice. Self-supervised learning scales even more easily than weakly-supervised learning, and prior work has demonstrated the potential of self-supervised pre-training at scale <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>We perform transfer-learning experiments on ImageNet-1k that compare our weakly-supervised learner with Sim-CLR v2 <ref type="bibr" target="#b12">[13]</ref>, SEER <ref type="bibr" target="#b26">[27]</ref>, and BEiT <ref type="bibr" target="#b2">[3]</ref>. The comparison with SEER is of particular interest: because it is trained on a similar collection 3 of Instagram images, we can readily <ref type="bibr" target="#b2">3</ref> The data distribution used in <ref type="bibr" target="#b26">[27]</ref> and in our study may not be exactly compare both learning paradigms on the same data distribution. We perform experiments in two transfer-learning settings: (1) a setting in which a linear classifier is attached on top of the pre-trained model and the resulting full model is finetuned and (2) a setting that initializes this linear classifier using the zero-shot transfer approach described in Section 4.4 (without Platt scaling) before finetuning the full model. Following prior work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>, we vary the amount of labeled ImageNet examples used for finetuning to 1%, 10%, and 100% of the original ImageNet-1k training set. We report results using images of size 224?224 pixels.</p><p>The results of our experiments are presented in <ref type="table">Table 2</ref>. Results for SimCLRv2, SEER and BEiT were adopted from <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref>; small differences in experimental setup may exist. Our results show that weakly-supervised learning substantially outperforms current self-supervised learners, particularly in low-shot transfer settings, likely because our weakly-supervised learners receive more learning signal per sample. Moreover, our results show that weaklysupervised learners benefit from zero-shot initialization in low-shot transfer settings. We note that our observations may change if self-supervised learners are scaled further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Zero-Shot Transfer</head><p>Another potential advantage of weakly-supervised models is that they have observed a large variety of training targets during pre-training. This may help them recognize new the same, as we use the data resampling approach described in Section 3.1.  <ref type="figure">Figure 2</ref>. Transfer-learning accuracy as a function of throughput of pre-trained models that were finetuned on five datasets (please refer to <ref type="table">Table 1</ref> for full results). ViTs and EfficientNets achieve the highest top-line accuracies, but RegNetY models perform better in the high-throughput regime.</p><p>visual concepts quickly. We test the ability of our models to learn and recognize new visual concepts rapidly in zero-shot transfer learning setting. <ref type="bibr" target="#b3">4</ref> In this setting, we use the output layer of the pre-trained model directly without any finetuning. We can do this because we trained on 27k hashtags derived from WordNet <ref type="bibr" target="#b21">[22]</ref>, allowing us to define a mapping between hashtags and class labels for datasets, like ImageNet-1k, also built on WordNet. We use the same image resolution as pre-training, viz., 224 ? 224 pixels.</p><p>Platt scaling. In our zero-shot transfer experiments, we consider a transductive learning setting <ref type="bibr" target="#b23">[24]</ref> in which all test examples are available simultaneously at test time. This allows us to train a Platt scaler <ref type="bibr" target="#b52">[53]</ref> on the test data that corrects for differences in the distribution of hashtags (which are Zipfian) and the distribution of classes in the target task <ref type="bibr" target="#b3">4</ref> Some prior work refers to this learning setting as zero-shot learning <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b56">57]</ref>. We find this term confusing because it differs from classical zero-shot learning <ref type="bibr" target="#b42">[43]</ref>. Hence, we adopt the term zero-shot transfer.  <ref type="table">Table 2</ref>. Transfer accuracy of models on the ImageNet-1k dataset as a function of the percentage of ImageNet-1k training examples used for transfer learning. Transfer learning is performed using either standard finetuning, or zero-shot (ZS) transfer initialization followed by finetuning. The best result in each setting is boldfaced; the second-best result is underlined. Accuracies that are adopted from the original paper are emphasized.</p><p>Our weakly supervised pre-trained models outperform models pretrained with modern self-supervised learners, in particular, in the few-shot regime. ? During finetuning, SimCLRv2 accessed 100% of the ImageNet training images but k% of the labels, whereas SEER and our method accessed k% of the training data.</p><p>(which is uniform). The Platt scaler is parameterized by a weight vector w ? R C and bias vector b ? R C , where C is the number of classes. Given a probability vector p ? ? C with ? C the C-simplex, the Platt scaler computes a new output p ? = softmax (diag(w)p + b). The Platt scaler is trained to minimize the cross-entropy loss between the test distribution of p ? and a uniform distribution over the C classes. Note that this does not use the test labels; it only encourages the predictions to be uniform over classes.</p><p>Mapping from hashtags to ImageNet classes. Because the targets in both the ImageNet and IG-3.6B datasets are English nouns, we can construct a many-to-many mapping between Instagram hashtags and ImageNet classes. To do so, we first map both hashtags and ImageNet classes to WordNet synsets, and then map hashtags to ImageNet classes based on their similarity in WordNet <ref type="bibr" target="#b21">[22]</ref>. We use the resulting many-to-many mapping between hashtags and classes to aggregate hashtag-prediction scores over Ima-geNet classes. We experiment with three different aggregation methods and use the method that we found to work best for each model; see appendix for details.</p><p>Results. The results of our zero-transfer results are presented in <ref type="table" target="#tab_3">Table 3</ref>. The table presents top-1 classification accuracies on four ImageNet-like test sets for our models with and without Platt scaling. We compare the performance of our models with that of CLIP <ref type="bibr" target="#b56">[57]</ref> and ALIGN <ref type="bibr" target="#b36">[37]</ref>. These experiments are system-level comparisons in which many factors are different: For example, CLIP was trained on a dataset of 400 million images and captions that appears more curated than ours, it was finetuned at a higher resolution, and it performs zero-shot transfer via prompt engineering <ref type="bibr" target="#b8">[9]</ref> which is known to improve recognition accuracy <ref type="bibr" target="#b56">[57]</ref>. ALIGN uses a different image-recognition model (viz., EfficientNet) and was trained on 1 billion pairs of web images and corresponding alt-texts <ref type="bibr" target="#b36">[37]</ref>. <ref type="table" target="#tab_3">Table 3</ref> presents our results with zero-shot transfer on four ImageNet-like datasets. The results show that our weakly supervised models perform very well out-of-thebox: without ever seeing an ImageNet image, our best model achieves an ImageNet top-1 accuracy of 75.3%. The results also show that Platt scaling is essential to obtain good zero-shot transfer performance with our model, as it corrects for differences in the distribution of hashtags and ImageNet classes. Finally, we find that our ViT models underperform our RegNetY models in the zero-shot transfer setting. This is unsurprising considering that ViTs also underperformed RegNetYs on ImageNet-1k finetuning at an image resolution of 224 ? 224 pixels.</p><p>Comparing our models with CLIP <ref type="bibr" target="#b56">[57]</ref>, we observe that the CLIP ViT L/14 model slightly outperforms our model in zero-shot transfer to the IN-1k dataset; whereas the smaller RN50?64 CLIP model underperforms it. On some datasets, the ALIGN [37] model performs even slightly better. However, the results are not fully consistent: our models do obtain the best performance on the ImageNet-v2 dataset <ref type="bibr" target="#b59">[60]</ref>. Because these experiments perform system-level comparisons, it is difficult to articulate what drives these differences in performance. Nonetheless, our results provide further evidence that weakly-supervised approaches like ours, CLIP, and ALIGN provide a promising path towards the development of open-world visual-recognition models <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Broader Impact</head><p>A potential downside of weakly-supervised training of models on uncurated web data is that they may learn harmful associations that reflect offensive stereotypes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>. Moreover, the models may not work equally well for different user groups; for example, they do not work as well in non-English speaking countries <ref type="bibr" target="#b16">[17]</ref> because we used English hashtags as the basis for training our models. We performed a series of experiments to better understand: (1) the associations our hashtag-prediction models learn with photos of people with varying characteristics, and (2) how well those models perform on photos taken in non-English speaking countries. We summarize the results of those experiments here and refer to the appendix for further details. Analyzing associations in hashtag predictions. We performed experiments analyzing the associations our Reg-NetY 128GF hashtag-prediction models make for photos that contain people with different apparent skin tone, apparent age, apparent gender, and apparent race. The ex-  periments were performed using: (1) a proprietary dataset that contains 178,448 Instagram photos that were annotated using the Fitzpatrick skin tone scale <ref type="bibr" target="#b22">[23]</ref> and (2) the UTK Faces dataset, which provides apparent age, apparent gender, and apparent race labels <ref type="bibr" target="#b77">[78]</ref>.</p><p>We find that the model has learned several associations between hashtags and skin tone; see the appendix for details. For example, #redhead is more commonly predicted for photos of people with a light skin tone, whereas #black is more often predicted for people with a dark skin tone. Similarly, some hashtag predictions correlate with the apparent age of people in photos; see the appendix for details. For example, our models more commonly predict #baby or #kid for photos that contain people who are 1?10 years old, and more commonly predict #elder for the 80 ? 90 years age group. When analyzing our model for gender stereotypes, we found that our model's hashtag predictions associate men with #football and #basketball more frequently. By contrast, our model associates photos containing women more frequently with #makeup and #bikini; see the appendix for details.</p><p>The most troubling associations we observed stem from an analysis of model predictions for photos that contain people with different apparent race. In particular, some of our experiments suggest that our model may associate photos that contain Black people with #mugshot and #prison more frequently; see the appendix. However, it is unclear whether these observations are due to our model making incorrect or biased predictions for photos in the evaluation dataset, or whether they are due to the evaluation dataset containing a problematically biased image distribution. In particular, a more detailed analysis uncovered the presence of a troubling bias in the evaluation dataset (rather than in our model): we found that the UTK Faces dataset <ref type="bibr" target="#b77">[78]</ref> contains a substantial number of mug shots that disproportionally portray Black individuals.</p><p>Overall, our results suggest that while our hashtagprediction models appear to make fewer troubling predictions than language models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, careful analyses and adaptations would be needed before hashtag predictions from our model can be used in real-world scenarios. Motivated by this observation, we do not release the final hashtag-prediction layer of our models as part of this study. Analyzing hashtag prediction fairness. We also analyzed how well our hashtag-prediction models work on photos taken across the world. We repeated the analysis of <ref type="bibr" target="#b16">[17]</ref> on the Dollar Street dataset and performed analyses on a proprietary dataset that contains millions of images with known country of origin. Akin to <ref type="bibr" target="#b16">[17]</ref>, we observe large accuracy differences of our model on Dollar Street photos from different countries. Our analysis on the much larger and more carefully collected proprietary dataset confirms this result but suggests that the effect sizes are much smaller than reported in <ref type="bibr" target="#b16">[17]</ref>; see the appendix for details. Specifically, we find that the range of per-country accuracies is in a relatively tight range of ?5% i.e., our model achieves percountry recognition accuracies between 65% and 70% for all 15 countries in the dataset. Overall, our results suggest more work is needed to train models that perform equally across the world. In future work, we plan to train multilingual hashtag models <ref type="bibr" target="#b63">[64]</ref> as this may lead to models that achieve equal recognition accuracies across countries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this paper, we have presented an in-depth study of fully supervised, self-supervised, and weakly-supervised pre-training for image recognition. Combined with related work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>, our results provide a compelling argument for the use of weakly-supervised pre-training in the development of systems for visual perception. However, our study also uncovers limitations of this line of research.</p><p>In particular, we find it is increasingly difficult to perform systematic, controlled experiments comparing different approaches and techniques. There are a variety of reasons for this, including the use of proprietary data that was collected via opaque processes <ref type="bibr" target="#b4">5</ref> , the diversity of model architectures used, the complexity of training recipes, the het-erogeneity of hardware and software platforms used, the vast compute resources required, and the fact that not all studies publish pre-trained models. Together, this creates an environment in which researchers cannot perform controlled studies that test the effect of one variable, keeping all other variables fixed. Instead, they can only perform system-level comparisons, as we did in this study. Such comparisons provide signal on the potential of various approaches, but they do not produce conclusive results. This problem is exacerbated by the fact that the signal we are measuring is small, as recognition accuracies on commonly used evaluation datasets appear saturated. To create a thriving research community focused on large-scale learning of vision systems, it is imperative that we address these issues.</p><p>A second limitation of this line of research is the strong focus on recognition accuracy and inference speed as the main measures of merit. While recognition accuracy and inference speed are obviously important, they are not the only measures that matter for the quality of a visual-perception system. Other measures include the recognition accuracy experienced by different groups of users and the prevalence of predictions that reinforce harmful stereotypes. We presented an initial study of such measures in Section 5 but this foray is not completely conclusive or sufficient. In particular, we found there are no well-established evaluation datasets and experimental protocols that facilitate the rigorous analyses. To make matters worse, the presence of harmful stereotypes in some commonly used vision datasets (such as the association between Black people and mug shots we found in the UTK Faces dataset <ref type="bibr" target="#b77">[78]</ref>) appears to be unknown. In order to make hashtag-prediction systems like ours ready for real-world deployment, it is essential that we improve the quality of our analyses, and that we address any issues that those analyses may surface.</p><p>To conclude, we emphasize that we remain convinced about the potential of weakly-supervised learning approaches. If we resolve the aforementioned issues, we believe such approaches may improve visual-perception systems in the same way that large-scale language models have improved natural language understanding, machine translation, and speech recognition.</p><p>Hashtag Filtering and Canonicalization. We considered the set of all hashtags H posted by US users more than once in public posts as our candidate set. We design a many-to-one function to map a hashtag to WordNet synsets <ref type="bibr" target="#b21">[22]</ref>, s : H ? 2 S , where S is the set of Word-Net synsets, and 2 S is the power set of S. s is defined as the get synsets() Python function in Listing 1. We filter out hashtags which map to ?, and consider all hashtags which map to the same set of synsets as the same label. For instance, #eggplant and #aubergine map to the same target, whereas #newyork is filtered out. We finally convert the output of f (h), a set of synsets, to a text string, which we refer to as a canonical hashtag. We refer to the set of all canonical hashtags, which is our output vocabulary, by C. Image Sampling. We follow <ref type="bibr" target="#b48">[49]</ref> and down-weight the relative weight of frequent hashtags. For deciding our sampling weights for images, we assign importance factors to each image based on the (canonical) hashtags associated with it. For a hashtag h ? C, its importance factor, I h , is defined as f (h) ?1/2 , where f (h) is the hashtag's frequency. For an image i, with associated hashtags {h j i }, we define the image's importance factor as I i = max I h j i . Next, we partition the hashtags into two sets -the head, which contains hashtags which occur in at least 5000 images, and the tail which contains the remaining infrequent hashtags (see <ref type="figure" target="#fig_0">Figure 1</ref>). An image is considered a tail image iff it contains at  <ref type="table">Table 4</ref>. Dataset ablations. Ablation study on training set collection using ResNeXt-101 32x8d models trained on IG datasets with 100M unique images; we report the linear classifier accuracy on ImageNet-1k. The baseline approach follows the dataset collection approaches in <ref type="bibr" target="#b48">[49]</ref> and reproduces the results in that paper. Partitioning the hashtags and over-sampling the tail (? = 0.7) improves transfer accuracy significantly, but excessively oversampling the tail (? = 0.3) worsens it. Increasing the hashtag vocabulary size improves transfer accuracy.</p><p>least one tail hashtag. We sample images from the set of all images available to us, I, using the probability distribution p i = cI i ?i ? I, where c is a normalization constant. We continue sampling images independently until we reach our target dataset's total samples. For a target number of samples, M , we sample ?M samples from the head and (1??)M samples from the tail using this sampling procedure (we chose ? = 0.7). We note that because the tail is heavily upsampled, the number of unique images in a single epoch is smaller than the total samples M .</p><p>The deviations from <ref type="bibr" target="#b48">[49]</ref> in dataset collection were ablated by pre-training on datasets of 100 million samples and evaluating linear classifier performance on ImageNet-1k, see <ref type="table">Table 4</ref> for details. Per the results in the table, our changes boost transfer performance on ImageNet-1K by 2.1% when pretraining a ResNeXt-101 32x8d on a 100M dataset. This number might change as we increase the size of the dataset or model.</p><p>Deduplication. <ref type="bibr" target="#b48">[49]</ref> performed an extensive deduplication experiment, which suggests that the percentage of images in common evaluation datasets that appears on Instagram is very small (&lt; 0.5%) and, in fact, smaller than the overlap between those evaluation datasets and the ImageNet training set that is commonly used for model pre-training. While our sampling methodologies may differ, based on those observations, we chose not to repeat the deduplication experiments.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Complexity and Speed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model and Hyperparameter Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Effect of Convolutional Model Family</head><p>We performed experiments investigating the performance of four different model families in weaklysupervised pre-training: ResNeXt <ref type="bibr" target="#b73">[74]</ref>, RegNetY <ref type="bibr" target="#b57">[58]</ref>, DenseNet <ref type="bibr" target="#b34">[35]</ref> and EfficientNet <ref type="bibr" target="#b64">[65]</ref>. As recent model families like EfficientNet and RegNetY use squeeze-andexcitation (SE) layers <ref type="bibr" target="#b33">[34]</ref> for improved accuracies <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b57">58]</ref>, we also use these in our implementations of DenseNet and ResNeXt.</p><p>Since we trained our models at scale, our goal was to identify the most efficient model family in terms of the accuracy achieved with a fixed training budget. In line with this goal, instead of finding models with comparable numbers of FLOPs or parameters, which have been shown to correlate poorly with training speed <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b57">58]</ref>, we instead measured image throughput during training. We also include the test time throughput as well since it is a useful inference time constraint to consider.</p><p>To keep the experiments tractable, we used mediumsized models of each model family. <ref type="table" target="#tab_7">Table 6</ref> lists the candidate models for each of the families we used for our comparison; these models were selected to have similar training speeds (in terms of images processed per second). We note that the test throughputs were also similar except for the EfficientNet model which uses a higher resolution than the other models.</p><p>We tested the models in three settings: (1) training and testing on ImageNet-1k, (2) training and testing on ImageNet-5k, and (3) pre-training on IG-1B followed by a linear classifier trained and tested on ImageNet-1k. The results of our experiments are presented in <ref type="table" target="#tab_7">Table 6</ref>. The results show that for a similar training budget, the RegNetY model family outperforms the other model families on all three datasets, while also having a competitive inference throughput. For that reason, we focused on RegNetY models in all subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Effect of Dataset Size</head><p>During pre-training, usually the focus is on the the total number of unique images in the dataset, which we will refer to as the dataset size <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b75">76]</ref>. In our setup, due to the upsampling of the least frequent hashtags, our final dataset is defined by an additional parameter -the dataset's samples, which we define as the total number of imagelabel pairs, counting duplicates. <ref type="table" target="#tab_8">Table 7</ref> shows the effect of a dataset's number of unique images vs the total samples seen during training. For the IG dataset in the smaller test regimes we explored, the total samples seen determined the model performance across a variety of dataset sizes for different model families (convolutional networks, transformers) and model capacities, rather than the number of unique images seen. We hypothesize that this is because in this regime the model has not yet saturated. It does suggest that the total number of samples seen during training is important to consider when comparing large datasets with a small number of epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Effect of Scaling Parameters</head><p>Due to the inherent noise in the learning signal, weakly supervised pre-training requires substantial scale to obtain optimal results. We performed experiments studying the effect on the transfer performance of two key scaling param-   <ref type="formula">(2)</ref> training set scale. To vary the model scale, we train RegNetY models that were independently optimized for use, starting from 16 GFLOPs, up to 128 GFLOPs. We followed <ref type="bibr" target="#b57">[58]</ref> and searched for each of the models instances on ImageNet-1k. To vary the training set scale, we use IG datasets of varying sizes. We train all models for one full epoch, and measure linear classifier performance on ImageNet-1k.</p><p>The results of our experiments are presented in <ref type="figure" target="#fig_3">Figure 3</ref>. We present the transfer accuracy as a function of both the total samples seen and the total training time in GPU-days, for four different models. The results presented in <ref type="figure" target="#fig_3">Figure 3</ref> are largely in line with those of <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b75">76]</ref>. Specifically, transfer accuracy improves for both larger models and for longer training regimes. Akin to <ref type="bibr" target="#b48">[49]</ref>, we find that the larger models benefit from more training samples than their smaller counterparts: the slope of the accuracy curve of RegNetY 128GF is steeper than that of RegNetY 16GF. Thus, for a large enough training budget it makes sense to use a larger model rather than a smaller model trained on more samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training Details</head><p>All our models were trained with Classy Vision <ref type="bibr" target="#b1">[2]</ref>. For the transfer results for other works in <ref type="table">Table 1</ref>, we used the timm library <ref type="bibr" target="#b70">[71]</ref> to get pre-trained checkpoints. In this section we share details about our fine-tuning setup for <ref type="table">Table  1</ref>, viz., the learning rate used <ref type="table" target="#tab_10">(Table 8)</ref>, and the utility of using synchronized batch normalization for convolutional networks <ref type="figure" target="#fig_4">(Figure 4)</ref>.</p><p>Hashtag-to-class mapping in zero-shot experiments. Because both the ImageNet and IG-3.6B datasets have target sets drawn from English nouns, we can construct a many-to-many mapping from Instagram hashtags to Ima-geNet classes. We first map each hashtag to all Word-Net synsets of the hashtag, and then map each ImageNet class to the most similar hashtag(s) based on the maximum path similarity score in WordNet <ref type="bibr" target="#b21">[22]</ref> between any of the the hashtag synsets and the ImageNet class synset. As the hashtags are nouns or compound nouns, they can have multiple meanings: for example, #crane refers to both the bird and the building structure. However, the synset of crane referring to the bird and synset of crane referring to the structure are two distinct ImageNet classes. In this situation, we map both synsets to #crane. Likewise, a synset can represent a concept specified by multiple words and therefore by multiple hashtags, for example, the synset {porcupine, hedgehog} matches both #porcupine and #hedgehog. In this case, we map the synset to all corresponding hashtags.</p><p>To utilize the resulting many-to-many mapping between hashtags and ImageNet classes, we need to aggregate the model (hashtag) predictions into predictions over the Ima-geNet classes. For the RegNetY models, we first map the prediction value for a hashtag to all ImageNet classes that the hashtag maps to. When Platt scaling is used, we sum all the resulting values for an ImageNet class to aggregate  them. When Platt scaling is not used, we instead average the predicted values for a class. For the ViT models, we achieved better results with a different aggregation method: we map 1 /N of the prediction value for a hashtag to all N ImageNet classes that the hashtag maps to, and take the maximum over all the resulting values for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. ImageNet Robustness Experiments</head><p>A potential advantage of weakly supervised pre-training is that the resulting models have observed more training images. This may lead the model to be more robust to variations in the image content. To evaluate the robustness of our models under small variations in visual content, image distribution, or labeling, we performed additional transfer-learning experiments using three ImageNetlike datasets: (1) ReaL ImageNet <ref type="bibr" target="#b6">[7]</ref>, (2) ImageNet v2 <ref type="bibr" target="#b59">[60]</ref>, and (3) ObjectNet <ref type="bibr" target="#b3">[4]</ref>. We fine-tune pre-trained models on the ImageNet-1k dataset and test them directly on the three evaluation datasets. <ref type="table">Table 9</ref> presents the results of this experiment. While the highest accuracies are obtained by large vision transformers (ViT) trained on 3 billion labeled images (JFT 3B), our weakly pre-trained RegNetY and ViT models are very competitive: our largest models are the runner-up on each of the datasets. In terms of differences in robustness, however, the results are inconclusive: validation accuracy on ImageNet-1k appears to be a good predictor for accuracy on the other tests sets across models and training regimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Broader Impact</head><p>This section presents a more detailed account of the experiments presented in the main paper, which aim to understand: (1) how well our models perform on photos taken in non-English speaking countries, and (2) the associations our hashtag-prediction models learn with photos of people with  <ref type="bibr" target="#b19">[20]</ref> JFT 300M 88.6 90.7 --ViT L/16 <ref type="bibr" target="#b19">[20]</ref> JFT 300M 87.8 90.5 --ViT H/14 <ref type="bibr" target="#b19">[20]</ref> IN-21k 85.1 88.7 --ViT L/16 <ref type="bibr" target="#b19">[20]</ref> IN  <ref type="table">Table 9</ref>. Classification accuracy of models pre-trained on the specified pre-training dataset followed by finetuning on ImageNet-1k. Accuracy is measured on four ImageNet-like datasets: (1) ImageNet-1k itself, (2) ReaL ImageNet <ref type="bibr" target="#b6">[7]</ref>, (3) ImageNet v2 <ref type="bibr" target="#b59">[60]</ref>, and (4) ObjectNet <ref type="bibr" target="#b3">[4]</ref>. The best result on each dataset is boldfaced; the second-best result is underlined. Numbers that are adopted from the original paper are italicized. Higher is better. ? It is unknown how much manual curation was performed in the annotation of JFT datasets. ? Pre-training data also includes IN-1k.</p><p>varying characteristics. In this section we share and discuss all the experimental results in more detail. As a reminder, all results presented below are for the hashtag-prediction models; no fine-tuning is employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Analyzing Hashtag Prediction Fairness</head><p>Following prior work <ref type="bibr" target="#b16">[17]</ref>, we analyzed how well the RegNetY 128GF model works on photos taken across the world. We first repeated the analysis of <ref type="bibr" target="#b16">[17]</ref> on the Dollar Street dataset. To this end, we use the hashtag-prediction model in a zero-shot fashion: we manually define a mapping from hashtags to the 112 classes in the Dollar Street, and task the model with predicting only hashtags that are mapped to a class. We measure the accuracy of the model's predictions per country, and display the results on a world map in which colors correspond to accuracies in the left plot in <ref type="figure" target="#fig_5">Figure 5</ref> (red is 40% correct; green is 70%).</p><p>Although the absolute numbers are lower because the image-recognition model operates in zero-shot mode (the average accuracy over all countries is 48.0%), qualitatively, the observations we obtain are in line with prior work <ref type="bibr" target="#b16">[17]</ref>: observed recognition accuracies are higher in the US and Europe than in most other countries.</p><p>Because the Dollar Street dataset may itself have issues, we repeated the analysis on a proprietary dataset that contains millions of images labeled for visual concepts and their country of origin. The resulting world map is shown in the right plot in <ref type="figure" target="#fig_5">Figure 5</ref>. The results suggest that the range of accuracy values is relatively tight (approximately 5%) on this large proprietary dataset.</p><p>Following common practice <ref type="bibr" target="#b74">[75]</ref>, we also measure the percentage of classes for which the ratio between the classrecognition accuracy in country 1 and country 2 is smaller than 0.8. The results of this analysis are shown in the heat map in <ref type="figure">Figure 6</ref>. If an entry in the heat map is yellow, then the model recognizes a substantial percentage (up to 35%) of classes substantially worse in the "row country" than in the "column country".</p><p>The results in the figure suggest that the hashtagprediction model performs better in the US and worse in Egypt and Nigeria. There is also a notable difference between the accuracy map in <ref type="figure" target="#fig_5">Figure 5</ref> and the heat map in <ref type="figure">Figure 6</ref>. The accuracy map suggests that the hashtagprediction model performs worst in Brazil and Japan, whereas the heat map suggests the lowest accuracy is obtained in Egypt and Nigeria. This result may be due to variations between countries in the distribution of per-class accuracy discrepancies and/or due to variations in the concept distribution per class. % of classes with accuracy ratio &lt; 0.8 <ref type="figure">Figure 6</ref>. Percentage of classes for which recognition accuracy is substantially higher in one country (rows) than in another country (columns). We use the 80% rule to assess whether one accuracy is "substantially higher" than the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Analyzing Associations in Hashtag Predictions</head><p>We performed experiments in which we analyze the associations our hashtag-prediction models make for photos of people with different apparent skin tone, different apparent age, different apparent gender, and different apparent race. We present the results of each experiment separately below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparent Skin Tone</head><p>We first evaluated potentially troubling associations in hashtag predictions by apparent skin tone. To this end, we used a proprietary dataset that contains 178,448 Instagram photos that were annotated using the Fitzpatrick skin tone scale <ref type="bibr" target="#b22">[23]</ref>. We ran all these photos through our RegNetY 128GF hashtag-prediction model, asking it to predict the five highest-scoring hashtags for each photo. We maintain per-skin tone statistics on how often a hashtag in the vocabulary is predicted for a photo of an individual with that skin tone. Next, we inspect differences in the hashtag prediction rate between different skin tones. For each skin tone, we identify the hashtags with the largest absolute difference in hashtag prediction rate compared to the average prediction rate for the other five skin tones. We also compute the associated relative difference in hashtag prediction rate. We show the resulting hashtags for skin tone 1 (lightest skin tone) and skin tone 6 (darkest skin tone) in the top row of <ref type="figure">Figure 7</ref>.</p><p>The results in the figure reveal several associations that may not be unexpected: for example, #redhead is more commonly predicted by the model for photos of people with a light skin tone, whereas #black is more often predicted for people with a dark skin tone. The analysis also reveals associations that are more difficult to explain: do people with lighter skin tones wear more #headbands or #bandanas? It is also unclear to what extent the associations we find are learned by the model and to what extent they reflect characteristics of the evaluation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparent Age</head><p>We performed a similar analysis of associations between predicted hashtags and apparent age groups. For this evaluation, we used the UTK Faces dataset <ref type="bibr" target="#b77">[78]</ref>, which provides apparent age labels. People were grouped into age buckets with a range of 10 years (0 ? 10, 10 ? 20, 20 ? 30 years, etc.). We performed the same analysis as before. The second row of <ref type="figure">Figure 7</ref> shows the most common hashtag predictions for two different (apparent) age groups.</p><p>Some associations that the analysis reveals are not unexpected: for example, predicting #baby or #kid for age group 1 ? 10 years or predicting #elder for age group 80 ? 90 years. The results also show that there may be discrepancies in the meaning of words and hashtags: #rip is in the hashtag dictionary because one may have a rip in their shirt but it is commonly used on Instagram as abbreviation for "rest in peace", which is more likely to apply to people of age. Other disparate associations appear unfortunate, such as the association of #spermbank with photos of people aged 0 ? 10 years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparent Gender</head><p>We performed the same analysis on the UTK Faces dataset <ref type="bibr" target="#b77">[78]</ref> by apparent gender. Due to limitations of the evaluation dataset, we restricted our analyses to males and females but did not consider non-binary genders. The results are presented in the third row of <ref type="figure">Figure 7</ref>.</p><p>The results suggest that the model has learned certain gender-specific stereotypes, for example, associating men with #football and #basketball more frequently or associating women more frequently with #makeup and #bikini. The associations revealed by the analysis vary in how problematic they are: for example, men may not be excited that they are more frequently associated with #mugshot -and in some cases, such an association could be harmful. We will return to this example below. Apparent Race For better or worse (see below), the UTK Faces dataset <ref type="bibr" target="#b77">[78]</ref> also contains annotations of apparent race. We repeated the same hashtag prediction analysis for the groups defined in UTK Faces (Indian, Asian, Black, White, Other) as well. We present the results of this analysis in the fourth row of <ref type="figure">Figure 7</ref>.</p><p>The results analysis suggest a variety of disparate associations, some of which are more problematic than others. Likely the most troubling association suggested by the analysis is the association of photos of Black people with #mugshot and #prison. Because of the sensitivity of this type of association, we investigated it more in-depth. First, we performed a visual analysis of the photos for which the hashtag-prediction model predicted #mugshot or #prison among its top-5 predictions. This inspection revealed that a small percentage of the photos in the UTK Faces dataset are, indeed, mug shots. Specifically, some of the images in the dataset appear to have been sourced from http://mugshots.com/. This observation raises an important question: Are the associations our analyses identify due to associations that the model has learned, due to biases in the evaluation data, or both? This question is difficult to answer without collecting additional annotations.</p><p>In this particular case, we decided to re-use the skin tone dataset we used earlier and measure how often #mugshot is predicted for the images in that dataset. While skin tone does not map to race very well, we would expect to observe at least some correlation between #mugshot prediction and skin tone if the model had learned this association. The results were quite the opposite: #mugshot was predicted 7 times (0.0078%) for images with Fitzpatrick skin tone 1 (lightest skin tone) but only once for skin tone 6 (darkest skin tone; 0.0023%). Combined with our visual inspection, this suggests that the problematic association we observed in the analysis on UTK Faces is most likely to be due to problems in the UTK Faces dataset itself than due to problems in the hashtag-prediction model. Having said that, we acknowledge that there are many caveats here, and that our experiments are not fully conclusive.  <ref type="figure">Figure 7</ref>. Differences in hashtag prediction rate for photos from various apparent subgroups. Absolute differences are sorted, and results for 20 hashtags with the largest difference are shown. Relative hashtag prediction differences are shown on top of the bars. From top to bottom: Differences for photos of people with (apparent) Fitzpatrick skin tone 1 and photos of people with other apparent skin tones (left); and between photos with skin tone 6 and other skin tones (right). Differences between photos with (apparent) age group 0-10 and other age groups; and between age group 80-90 and other age groups. Differences between photos of (apparent) women and photos of men; and between photos of men and women. Differences between photos of (apparent) Black people and people of other races; and between photos of White people and other races.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Hashtag distribution of Instagram images. Left: Frequency of all hashtags occurring with public images posted by US users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Listing 1 .</head><label>1</label><figDesc>.morphy(hashtag, wordnet.NOUN)} 19 for i in range(MIN_LEN, len(hashtag) -MIN_LEN + 1): 20 candidate = hashtag[:i] + "_" + hashtag[i:] 21 candidates.add(wordnet.morphy(candidate)) Hashtag-to-synset mapping code in Python.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Scaling model and dataset sizes. ImageNet top-1 linear classifier accuracy for various model sizes as a function of the number of pre-training samples (left) and the training budget (right). As we go larger in model size, the models become more efficient in utilizing a given number of pre-training samples, and additional training samples improve performance. Training time calculated by dividing the total samples with the training speeds from Table 5. eters: (1) model scale and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Effect of sync batch norm while fine-tuning. Ima-geNet top-1 accuracy while fine tuning a RegNetY 128GF continues to increase as we increase the sync batch size. The model was always trained with a mini batch size of 512, while varying the batch sizes for sync batch norm. Results reported without EMA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Recognition accuracy per country of our zero-shot classifier on the Dollar Street dataset (top) and a proprietary dataset (bottom). The accuracy on all images is 48.0% on the Dollar Street dataset and 63.3% on the proprietary dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>U</head><label></label><figDesc>n it e d A r a b E m ir a t e s A r g e n t in a B r a z il C o lo m b ia E g y p t In d o n e s ia In d ia Ja p a n S o u t h K o r e a M e x ic o R u s s ia T h a il a n d U n it e d S t a t e s o f A m e r ic a S o u t h A fr ic a N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ModelPre-training Resolution IN-1k AccuracyClassification accuracy Throughput FLOPs Params Pre. Fine. Report. Reprod. IN-5k iNat. Places CUB (images/sec.)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(B)</cell><cell>(M)</cell></row><row><cell cols="2">Supervised pre-training  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">EfficientNet L2 [73] JFT 300M  ? 475 800</cell><cell>88.4</cell><cell>88.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>108</cell><cell>479.9</cell><cell>480.3</cell></row><row><cell cols="3">EfficientNet L2 [73] JFT 300M  ? 475</cell><cell>-</cell><cell>88.2</cell><cell>88.0</cell><cell>61.8</cell><cell>86.5</cell><cell cols="2">59.4 91.2  ?</cell><cell>293</cell><cell>172.6</cell><cell>480.3</cell></row><row><cell cols="3">EfficientNet B7 [73] JFT 300M  ? 600</cell><cell>-</cell><cell>86.9</cell><cell>86.7</cell><cell>56.7</cell><cell>82.0</cell><cell cols="2">59.2 90.6  ?</cell><cell>652</cell><cell>38.4</cell><cell>66.3</cell></row><row><cell cols="3">EfficientNet B6 [73] JFT 300M  ? 528</cell><cell>-</cell><cell>86.4</cell><cell>86.3</cell><cell>55.4</cell><cell>79.9</cell><cell cols="2">58.8 89.1  ?</cell><cell>849</cell><cell>19.5</cell><cell>43.0</cell></row><row><cell>EfficientNet B8 [72]</cell><cell>IN-1k</cell><cell>672</cell><cell>-</cell><cell>85.5</cell><cell>85.2</cell><cell>54.8</cell><cell>81.3</cell><cell cols="2">58.6 89.3  ?</cell><cell>480</cell><cell>63.7</cell><cell>87.4</cell></row><row><cell>EfficientNet B7 [72]</cell><cell>IN-1k</cell><cell>600</cell><cell>-</cell><cell>85.2</cell><cell>85.0</cell><cell>54.4</cell><cell>80.6</cell><cell cols="2">58.7 88.9  ?</cell><cell>652</cell><cell>38.4</cell><cell>66.3</cell></row><row><cell>EfficientNet B6 [72]</cell><cell>IN-1k</cell><cell>528</cell><cell>-</cell><cell>84.8</cell><cell>84.7</cell><cell>53.6</cell><cell>79.1</cell><cell cols="2">58.5 88.5  ?</cell><cell>849</cell><cell>19.5</cell><cell>43.0</cell></row><row><cell>ViT G/14 [76]</cell><cell>JFT 3B</cell><cell cols="2">224 518</cell><cell>90.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56</cell><cell cols="2">2826.1 1846.3</cell></row><row><cell>ViT L/16 [76]</cell><cell>JFT 3B</cell><cell cols="2">224 384</cell><cell>88.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>567</cell><cell>191.5</cell><cell>304.7</cell></row><row><cell>ViT H/14 [20]</cell><cell>JFT 300M</cell><cell cols="2">224 518</cell><cell>88.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>116</cell><cell>1018.8</cell><cell>633.5</cell></row><row><cell>ViT L/16 [20]</cell><cell>JFT 300M</cell><cell cols="2">224 512</cell><cell>87.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>255</cell><cell>362.9</cell><cell>305.2</cell></row><row><cell>ViT L/16 [20]</cell><cell>IN-21k</cell><cell cols="2">224 384</cell><cell>85.2</cell><cell>85.2</cell><cell>-</cell><cell>81.7</cell><cell cols="2">59.0 91.3  ?</cell><cell>567</cell><cell>191.5</cell><cell>304.7</cell></row><row><cell>ViT B/16 [20]</cell><cell>IN-21k</cell><cell cols="2">224 384</cell><cell>84.0</cell><cell>84.2</cell><cell>-</cell><cell>79.8</cell><cell cols="2">58.2 90.8  ?</cell><cell>1,161</cell><cell>55.6</cell><cell>86.9</cell></row><row><cell>ViT L/32 [20]</cell><cell>IN-21k</cell><cell cols="2">224 384</cell><cell>81.3</cell><cell>81.5</cell><cell>-</cell><cell>74.6</cell><cell cols="2">57.7 88.7  ?</cell><cell>1,439</cell><cell>54.4</cell><cell>306.6</cell></row><row><cell cols="2">Weakly supervised pre-training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT H/14</cell><cell>IG 3.6B</cell><cell cols="2">224 518</cell><cell>88.6</cell><cell></cell><cell>60.9</cell><cell>86.0</cell><cell>60.7</cell><cell>91.7</cell><cell>116</cell><cell>1018.8</cell><cell>633.5</cell></row><row><cell>ViT L/16</cell><cell>IG 3.6B</cell><cell cols="2">224 512</cell><cell>88.1</cell><cell></cell><cell>59.0</cell><cell>84.2</cell><cell>60.7</cell><cell>91.6</cell><cell>255</cell><cell>362.9</cell><cell>305.2</cell></row><row><cell>ViT B/16</cell><cell>IG 3.6B</cell><cell cols="2">224 384</cell><cell>85.3</cell><cell></cell><cell>54.5</cell><cell>79.9</cell><cell>59.1</cell><cell>89.8</cell><cell>1,161</cell><cell>55.6</cell><cell>86.9</cell></row><row><cell>RegNetY 128GF</cell><cell>IG 3.6B</cell><cell cols="2">224 384</cell><cell>88.2</cell><cell></cell><cell>60.9</cell><cell>85.7</cell><cell>60.1</cell><cell>90.8</cell><cell>307</cell><cell>375.2</cell><cell>644.8</cell></row><row><cell>RegNetY 32GF</cell><cell>IG 3.6B</cell><cell cols="2">224 384</cell><cell>86.8</cell><cell></cell><cell>58.5</cell><cell>82.9</cell><cell>59.6</cell><cell>89.5</cell><cell>976</cell><cell>95.1</cell><cell>145.0</cell></row><row><cell>RegNetY 16GF</cell><cell>IG 3.6B</cell><cell cols="2">224 384</cell><cell>86.0</cell><cell></cell><cell>57.2</cell><cell>81.4</cell><cell>59.2</cell><cell>88.3</cell><cell>1,401</cell><cell>47.0</cell><cell>83.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Zero-shot transfer accuracy of models on four datasets with WordNet-based classes: (1) the ImageNet-1k dataset, (2) the ReaL ImageNet [7] dataset, (3) the ImageNet v2 [60] dataset, and (4) the ObjectNet [4] dataset. The best result on each dataset is boldfaced; the second-best result is underlined. Accuracies that are adopted from the original paper are italicized. When using Platt scaling, our weakly-supervised RegNetY models work very well out-of-the-box. They achieve 75.3% zero-shot transfer accu- racy on ImageNet-1k, and outperform CLIP [57] and ALIGN [37] on the ImageNet v2 [60] dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 presents</head><label>5</label><figDesc>the resolution, FLOPs, number of parameters, number of activations, and train and test throughputs of all models used in our study.</figDesc><table><row><cell>Model</cell><cell cols="4">Resolution Flops Params Acts</cell><cell>Train</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell>(B)</cell><cell>(M)</cell><cell>(M)</cell><cell>(images/sec.) (images/sec.)</cell></row><row><cell>EfficientNet L2 EfficientNet L2 EfficientNet B8 EfficientNet B7 EfficientNet B6 ViT G/14 ViT G/14 ViT H/14 ViT H/14 ViT H/14 ViT L/16 ViT L/16 ViT L/16 ViT B/16 ViT B/16 ViT L/32 ViT L/32 RegNetY 128GF RegNetY 128GF RegNetY 32GF RegNetY 32GF RegNetY 16GF RegNetY 16GF</cell><cell>475 800 672 600 528 224 518 224 392 518 224 384 512 224 384 224 384 224 384 224 384 224 384</cell><cell cols="3">172.6 479.9 63.7 38.4 19.5 484.2 2826.1 1846.3 2639.0 480.3 609.9 480.3 1707.4 87.4 442.9 66.3 289.9 43.0 167.4 1844.4 275.4 167.5 632.0 139.4 545.9 632.7 638.0 1018.8 633.5 1523.9 61.7 304.3 63.5 191.5 304.7 270.2 362.9 305.2 656.4 17.6 86.6 23.9 55.6 86.9 101.6 15.4 306.5 13.3 54.4 306.6 43.9 127.7 644.8 71.6 375.2 644.8 210.2 32.6 145.0 30.3 95.1 145.0 88.9 16.0 83.6 23.0 47.0 83.6 67.7</cell><cell>49 19 103 157 246 - ? - ? 246 56 19 701 177 70 2247 549 3176 921 191 69 607 248 989 440</cell><cell>293 108 480 652 849 379 56 960 242 116 2092 567 255 3861 1161 4431 1439 879 307 2824 976 4562 1401</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="6">Res. FLOPs Param. Act. Throughput Classification accuracy</cell></row><row><cell></cell><cell></cell><cell>(B)</cell><cell>(M)</cell><cell cols="3">(M) Train Test IN-1k IN-5k IG-0.7B</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? IN-1k</cell></row><row><cell cols="2">ResNeXt-101 32x4d 224</cell><cell>8.0</cell><cell>49.0</cell><cell>21.3 2,222 5,214 79.1</cell><cell>50.9</cell><cell>80.0</cell></row><row><cell>DenseNet-264</cell><cell>224</cell><cell>5.9</cell><cell>33.4</cell><cell>8.5 1,813 5,116 76.6</cell><cell>47.9</cell><cell>78.4</cell></row><row><cell>EfficientNet B3</cell><cell>300</cell><cell>1.9</cell><cell>12.2</cell><cell>23.8 1,802 2,979 78.5</cell><cell>49.3</cell><cell>77.9</cell></row><row><cell>RegNetY 8GF</cell><cell>224</cell><cell>8.0</cell><cell>39.2</cell><cell>18.0 1,770 4,562 79.8</cell><cell>51.4</cell><cell>80.8</cell></row></table><note>Model complexity and speed. Complexity and speed of models with an ImageNet-1k head at relevant resolutions. We measure train and train and test speed on a single node with 8 V100 32GB GPUs, maximizing the batch size for each model. Although EfficientNets have very few FLOPs, they produce a large amount of activations resulting in much slower train / test speeds. Training speeds measured for convolutional networks using SGD, and for ViTs using AdamW [47].? We were unable to train a ViT G/14 using our setup, even with a batch size of 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Overview of the convolutional models we evaluated for We find that the RegNetY model performs best in all settings. The best result on each dataset is boldfaced; the second-best result is underlined. Higher is better.</figDesc><table><row><cell>our experiments. ResNeXt and DenseNet models were augmented</cell></row><row><cell>with squeeze-and-excitation (SE [34]) layers. We evaluate the</cell></row><row><cell>classification accuracy of the models in three settings: (1) train-</cell></row><row><cell>ing on ImageNet-1k; (2) training on ImageNet-5k; and (3) pre-</cell></row><row><cell>training 1 epoch on 1B examples of IG-0.7B followed by linear</cell></row><row><cell>classifier evaluation on ImageNet-1k.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Effect of dataset size. We compute the linear classifier accuracy of various models on ImageNet-1k to study the effect of unique images. Every data point corresponds to the same number of total samples trained (2 billion), but the dataset size varies.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Dataset</cell><cell cols="2">Epochs IN-1k transfer accuracy</cell></row><row><cell></cell><cell></cell><cell cols="2">Name Samples</cell><cell></cell><cell>RegNetY</cell><cell>ViT</cell></row><row><cell></cell><cell></cell><cell>(size)</cell><cell></cell><cell></cell><cell>8GF 32GF B/16 L/16</cell></row><row><cell></cell><cell></cell><cell>IG-0.2B IG-0.4B IG-0.7B IG-1.4B</cell><cell>250M 500M 1B 2B</cell><cell>8 4 2 1</cell><cell>81.5 83.7 80.5 83.2 81.5 83.8 80.7 83.5 81.4 83.8 80.3 83.5 81.3 83.7 80.5 83.4</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ImageNet top-1 accuracy</cell><cell>79 80 81 82 0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell>RegNetY 128GF RegNetY 64GF RegNetY 32GF</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RegNetY 16GF</cell></row><row><cell></cell><cell>0.25 0.0 0.0</cell><cell cols="3">0.50 Training samples (billions) 1.00 2.00 0.2 0.4 4.00</cell><cell>16</cell><cell>32 Training time (GPU-days) 64 128 256 0.6 0.8</cell><cell>512 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Base learning rate used for the transfer results inTable 1.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We downloaded images from all countries, but excluded images by users from particular countries to comply with applicable regulations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although our system-level evaluations hamper exact comparisons, our results suggest that the weakly supervised IG-3.6B dataset provides the same amount of supervisory signal as the supervised JFT-300M dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We acknowledge that, although the data we use in our experiments is public, it is hard for others to collect that data. However, unlike other studies, we did strive to be comprehensive in describing our data-collection procedure, as we aim to maximize what the reader can learn from our study.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Ishan Misra, Priya Goyal, Benjamin Lefaudeux, Min Xu and Vinayak Tantia for discussions and feedback, and Haowei Lu and Yingxin Kang for help with the data loader implementation. We thank Deepti Ghadiyaram, Anmol Kalia, and Katayoun Zand for their work on the internal datasets with apparent skin tone and country annotations. We thank Phoebe Helander, Adina Williams, Maximilian Nickel and Emily Dinan for helpful feedback on the Broader Impact analysis. Lastly, we thank Brian O'Horo for support with the training infrastructure.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Dataset Details</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Exploring the limits of large scale pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanie</forename><surname>Sedghi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Classy vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Changhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/ClassyVision,2019.14" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Billion-scale pretraining with vision transformers for multi-task visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05887</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Xiaohua Zhai, and Aaron van den Oord. Are we done with ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multimodal datasets: misogyny, pornography, and malignant stereotypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abeba</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vinay Uday Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kahembwe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01963</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><forename type="middle">M</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Dario Amodei</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05520</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Dandelion Mane, Vijay Vasudevan,</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">User conditional hashtag prediction for images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1731" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Does object recognition work for everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Computer Vision for Global Challenges</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and accurate model scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database. Bradford Books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Soleil et peau</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Fitzpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal de M?decine Esth?tique</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning by transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Largescale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12046" to="12055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining of visual features in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01988</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Transfg: A transformer architecture for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie-Neng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07976</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The devil is in the tails: Fine-grained classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01450</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09382</idno>
		<title level="m">Deep networks with stochastic depth</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="67" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Show, ask, attend, and answer: A strong baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Elqursh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03162</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08974</idno>
		<title level="m">Do better imagenet models transfer better</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning visual n-grams from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning visual n-grams from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Oscar: Objectsemantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens Van Der Maaten</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">When does label smoothing help? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Large datasets: A Pyrrhic win for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">U</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birhane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16923,2020.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Large-scale attribute-object compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11373</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Imagenet-21k pretraining for the masses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10972</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Visual grounding in video for unsupervised word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06423</idno>
		<title level="m">Matthijs Douze, and Herv? J?gou. Fixing the train-test resolution discrepancy</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Separating self-expression and visual content in hashtag supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5919" to="5927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno>arXiv 2012.00759</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Fairness constraints: Mechanisms for fair classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Bilal Zafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">Gomez</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AI-STATS</title>
		<meeting>AI-STATS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
