<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Align and Prompt: Video-and-Language Pre-training with Entity Prompts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
							<email>dongxuli1005@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Salesforce Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<email>hongdong.li@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Salesforce Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
							<email>shoi@salesforce.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Salesforce Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Align and Prompt: Video-and-Language Pre-training with Entity Prompts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video-and-language pre-training has shown promising improvements on various downstream tasks. Most previous methods capture cross-modal interactions with a standard transformer-based multimodal encoder, not fully addressing the misalignment between unimodal video and text features. Besides, learning fine-grained visual-language alignment usually requires off-the-shelf object detectors to provide object information, which is bottlenecked by the detector's limited vocabulary and expensive computation cost.</p><p>In this paper, we propose Align and Prompt: a new video-and-language pre-training framework (ALPRO), which operates on sparsely-sampled video frames and achieves more effective cross-modal alignment without explicit object detectors. First, we introduce a video-text contrastive (VTC) loss to align unimodal video-text features at the instance level, which eases the modeling of cross-modal interactions. Then, we propose a novel visually-grounded pre-training task, prompting entity modeling (PEM), which learns fine-grained alignment between visual region and text entity via an entity prompter module in a self-supervised way. Finally, we pretrain the video-and-language transformer models on large webly-source video-text pairs using the proposed VTC and PEM losses as well as two standard losses of masked language modeling (MLM) and video-text matching (VTM). The resulting pre-trained model achieves state-of-the-art performance on both text-video retrieval and videoQA, outperforming prior work by a substantial margin. Implementation and pre-trained models are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video-and-language pre-training aims to jointly learn multimodal representations that transfer effectively to downstream tasks, such as text-video retrieval and videoQAvideo question answering. Compared with images, videos usually contain more redundancy in consecutive frames. This challenges models on both capacity and computation efficiency. Most prior approaches <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr">57]</ref> circumvent the expensive computation overhead by using offline-extracted video features. Since the video feature ex- Above: previous methods (e.g. ActBERT <ref type="bibr">[57]</ref>) rely on object detectors with expensive computation cost and limited object categories, leaving text data unexploited. Below: ALPRO generates soft entity labels with a prompter module, which computes similarities between video crops and textual entity prompts. ALPRO requires no detector while taking advantage of video-text alignment to generate entity labels with a large vocabulary, thus strengthening the cross-modal learning.</p><p>tractors are fixed without finetuning, these approaches are suboptimal when transferring to distinct target domains. In contrast, recent emerging approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">26]</ref> sample frames sparsely from videos, which enable end-to-end pre-training and finetuning of video backbones. In this work, we adopt the sparse video-text pre-training paradigm considering their effectiveness on downstream tasks. Despite their promising performance, current video-text pre-training models have several limitations. <ref type="bibr" target="#b0">(1)</ref> The interaction between video and text features is commonly mod-eled trivially using either dot-product <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b53">52]</ref> or crossmodal transformer encoders <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr">57]</ref>. However, features from individual modalities typically reside in different embedding spaces. Such misalignment makes it less effective to directly model cross-modal interaction. (2) Many visually-grounded pre-training tasks <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b49">48]</ref> do not explicitly model fine-grained regional visual information (e.g. objects), which proves important for downstream tasks emphasizing on visual reasoning (e.g. videoQA). Although there are attempts which employ object detectors <ref type="bibr">[7,</ref><ref type="bibr">57]</ref> to generate pseudo-labels as supervision, they suffer from imprecise detections and a restricted number of object categories. For example, detectors trained on MSCOCO <ref type="bibr" target="#b31">[31]</ref> recognize less than a hundred different categories. <ref type="bibr" target="#b2">(3)</ref> The previous sparse pre-training model <ref type="bibr" target="#b26">[26]</ref> is trained with image-text pairs using an image encoder, which makes it less effective in modeling temporal information.</p><p>In this paper, we tackle these challenges with a new video-and-language pre-training framework: Align and Prompt (ALPRO). Architecture-wise, ALPRO first encodes frames and text independently using a transformer-based video encoder and a text encoder, and then employs a multimodal encoder to capture cross-modal interaction. AL-PRO learns both instance-level video-text alignment and fine-grained region-entity alignment. The instance-level alignment is learned by applying a video-text contrastive loss (VTC) on the unimodal features, which encourages paired video-text instances to have similar representations.</p><p>In order to better capture fine-grained visual information and strengthen region-entity alignment, ALPRO introduces a new visually-grounded pre-training task, called prompting entity modeling, where we ask the video-text model to predict entities appearing in randomly-selected video crops using jointly video and text inputs (see <ref type="figure" target="#fig_0">Figure 1</ref>). To address the unavailability of entity annotations, we design a standalone entity prompter module that generates reliable pseudo-labels. Specifically, the entity prompter consists of two unimodal encoders to extract video and text features, respectively. We first train the entity prompter using only VTC loss and freeze its parameters thereafter. Then during pre-training, we feed video crops and text prompts (e.g. "A video of {Entity}.") to the prompter, where each Entity is from the frequent nouns appearing in the pretraining corpus. We then compute the normalized similarity between the entity prompts and the video crop as the pseudo-label to supervise the pre-training.</p><p>Our key contributions are: (1) We introduce ALPRO, a video-language pre-training method that is the first to learn effective cross-modal representations from sparse video frames and texts. (2) We introduce a video-text contrastive loss to better align instance-level unimodal representations, thus easing the modeling of cross-modal interaction.</p><formula xml:id="formula_0">(3)</formula><p>We propose a novel visually-grounded pre-training task, prompting entity modeling, that enables the model to capture fine-grained region-entity alignment. (4) We demonstrate the effectiveness of ALPRO on both video-text retrieval and videoQA. ALPRO significantly improves over previous state-of-the-art methods, for example, achieving 3.0% and 5.4% absolute lift in recall scores on the finetuning and zero-shot text-video retrieval task on MSRVTT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Dense versus Sparse Video Representation. Consecutive frames in videos usually contain visually similar information. Such redundancy opens up a research question on how to learn effective video-and-language representations without excessive computation overhead. Most prior methods on text-video retrieval <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b54">53,</ref><ref type="bibr" target="#b56">55]</ref> and videoQA <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b27">27]</ref> employ pre-trained visual backbones and extract video features for each frame densely yet offline. However, since visual backbones are usually pretrained on image <ref type="bibr" target="#b23">[23]</ref> and/or video datasets <ref type="bibr" target="#b21">[21]</ref> without access to text, these features are less effective for video-andlanguage tasks. Besides, video feature extractors in these approaches are not finetuned on target task data, preventing features to easily adapt to different domains. In contrast, recent methods ClipBERT <ref type="bibr" target="#b26">[26]</ref> and FiT <ref type="bibr" target="#b2">[3]</ref> demonstrate more effective results by end-to-end finetuning the visual backbone with only a few sparsely sampled frames. However, ClipBERT is pre-trained with image-text data thus is less effective in aggregating information across frames, while FiT is a retrieval-specific architecture that does not naturally generalize to videoQA task. In this regard, our AL-PRO is the first sparse pre-training architecture that tackles both tasks while in the meantime, demonstrating the benefit of pre-training on video-text pairs. Video-and-language Pre-training. Apart from the canonical pre-training tasks, such as masked language modeling (MLM) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr">57]</ref> and video-text matching (VTM) <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b36">35]</ref>, several methods <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b53">52]</ref> apply contrastive learning on offline extracted visual features. Without adapting the visual backbone, their ability to align cross-modal features remain limited. ALPRO jointly learns the unimodal and multimodal encoders, thus mitigating the disconnection in-between. In order to design effective visually-grounded pre-training tasks, VideoBERT <ref type="bibr" target="#b49">[48]</ref> predicts centroids of vector quantizations of video features. Such unsupervised quantization is noisy per se while neglecting textual cues, which curtails its capabilities in learning cross-modal interactions. ActBERT <ref type="bibr">[57]</ref> uses detectors to acquire object information. In addition to their computational inefficiency, detectors pre-trained on images usually have limited categories and compromised detection results on videos. In contrast, our proposed prompting entity modeling task is detector-free. By exploiting the instancelevel video-text alignment, we can generate reliable entity  pseudo-labels with a large vocabulary, leading to more efficient and effective learning of region-entity alignment. Zero-shot Visual Recognition with Prompts. There have been longstanding efforts to exploit text descriptions for learning visual recognition models. These include early efforts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">24]</ref> that use text to learn attributes of images; <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b48">47]</ref> that map images to the pretrained text embedding space, and visual n-gram <ref type="bibr" target="#b28">[28]</ref> that predicts text ngrams given image inputs. More recently, CLIP <ref type="bibr" target="#b45">[44]</ref> instantiates prompt templates with label text of visual categories. It then predicts the category by computing the similarity between each image-prompt pairs. This inspires us to the design of entity prompter. Since the entity prompter is trained with the entire video-text corpus, during pre-training, it can provide additional entity information unavailable in the text description for each individual video-text pair, thus leading to better entity-informed video representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video-Language Pre-training with ALPRO</head><p>In this section, we first introduce important constituent modules of ALPRO in Section 3.1. Then we present the pre-training objectives in Section 3.2, with a particular focus on the proposed video-text contrastive (VTC) loss and the prompting entity modeling (PEM) pre-training task. We introduce pre-training datasets in Section 3.3. Lastly, we describe important implementation details in Section 3.4. <ref type="figure" target="#fig_1">Figure 2</ref> gives an overview of ALPRO 's architecture. In particular, ALPRO consists of two main modules, a videolanguage pre-training model and a prompter. The prompter serves to generate soft entity labels to supervise the pretraining of the video-language model. Both modules contain their own video encoder and text encoder to extract features for video and text inputs, respectively. The pretraining model has an additional multimodal encoder to further capture the interaction between the two modalities. Details for each component are as follows. Visual Encoder. We use a 12-layer TimeSformer 224 <ref type="bibr" target="#b4">[5]</ref> to extract video features, with 224 the height and width of input frames. For N v frames sparsely sampled from each input video, TimeSformer first partitions each frame into K non-overlapping patches, which are flattened and fed to a linear projection layer to produce a sequence of patch tokens. Learnable positional embeddings are also added to the patch tokens. Then the TimeSformer applies self-attention along the temporal and spatial dimensions separately in order, leading to per-frame features? ? R Nv?K?d , with d the feature dimension. A temporal fusion layer (i.e. meanpooling) is applied to? along the temporal dimension to aggregate per-frame features into video features. As the output of visual encoder, we obtain a sequence of visual embeddings: {v cls , v 1 , ..., v K }, with v i ? R d and v cls the embedding of the video [CLS] token. Text Encoder. We use a 6-layer transformer <ref type="bibr" target="#b50">[49]</ref> model to represent text tokens. Given an input text description of N t tokens, the text encoder outputs an embedding sequence {t cls , t 1 , ..., t Nt }, with t i ? R d and t cls the embedding of the text [CLS] token. Similar to video encoder, we also add positional embeddings to the text tokens. Multimodal Encoder. We employ a 6-layer transformer to model the interaction between video and text features from the two unimodal encoders. Since positional embeddings are already injected in each unimodal encoder, we directly concatenate video and text features to feed the multimodal transformer. The outputs are multimodal embeddings {e cls , e 1 , ..., e Nv+Nt }, with e i ? R d . For notational convenience, we drop the multimodal embedding for the video [CLS] token as it is not used in pre-training losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ALPRO Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-training for ALPRO</head><p>We pre-train ALPRO with four objectives, including two canonical ones, i.e. masked language modeling (MLM) and video-text matching (VTM) as in <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr">57]</ref>. In this section, we focus on presenting the new techniques in ALPRO, i.e. the video-text contrastive (VTC) loss and the prompting entity modeling (PEM) loss, while only briefly outlining MLM and VTM in Section 3.2.3, referring interested readers to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b49">48]</ref> for details.</p><p>The motivation of both VTC and PEM is to strengthen cross-modal alignment between video and text. While VTC emphasizes on capturing instance-level alignment for video-text pairs, PEM encourages the model to align local video regions with textual entities. In the following, we introduce these two pre-training objectives in order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Contrastive Video-Text Alignment</head><p>Existing sparse video-language pre-training models use either dot-product <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b53">52]</ref> or rely entirely on a transformer encoder <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr">57]</ref> to model cross-modal interactions. However, since video and text features reside in different embedding spaces, such methods lead to less satisfactory alignment. To this end, we present a video-text contrastive (VTC) loss to align features from the unimodal encoders before sending them into the multimodal encoder. Specifically, given the embeddings of video and text [CLS] tokens, we optimize a similarity function between video V and text T :</p><formula xml:id="formula_1">s(V, T ) = g v (v cls ) ? g t (t cls ),<label>(1)</label></formula><p>such that paired video and text descriptions have higher similarity scores, where g v (?) and g t (?) are linear projections that transform the [CLS] embeddings to a common normalized low-dimensional (e.g. 256-d) space. Following <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b45">44]</ref>, the contrastive loss considers matched pairs as positive and all others pairs that can be formed in a batch as negatives. For each input video-text pair V i , T i , the video-text contrastive loss consists of two symmetric terms, one for video-to-text classification:</p><formula xml:id="formula_2">L v2t = ?log exp(s(V i , T i )/? ) B j=1 exp(s(V i , T j )/? )<label>(2)</label></formula><p>and the other for text-to-video classification:</p><formula xml:id="formula_3">L t2v = ?log exp(s(T i , V i )/? ) B j=1 exp(s(T i , V j )/? ) ,<label>(3)</label></formula><p>where ? is a learnable temperature parameter, and B is the batch size. The video-text contrastive loss is then defined as L vtc = 1 2 (L v2t + L t2v ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Prompting Entity Modeling</head><p>While masked language modeling has demonstrated its effectiveness on learning token-level text representations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">33]</ref>, it remains a challenge to design its visually-grounded counterpart. As a result, the limited capabilities in visual reasoning adversely impact previous work on downstream tasks, especially those requiring region-level visual information such as objects. This is in particular an issue for existing video-language pre-training models <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b49">48]</ref>, which usually retain only coarse-grained spatial information after pooling thus losing fine-grained visual cues.</p><p>One exception is ActBERT <ref type="bibr">[57]</ref> that attempts to use offthe-shelf object detectors to obtain regional features. Apart from its inefficiency, detectors trained with images tend to produce compromised detection results on video inputs. In addition, detectors are usually trained with restricted object categories (e.g. less than a hundred <ref type="bibr" target="#b31">[31]</ref>), given its prohibitive expense to scale up the laborious annotations. We introduce prompting entity modeling (PEM), a new visually-grounded pre-training task that improves the models' capabilities in capturing local regional information and strengthening cross-modal alignment between video regions and textual entities. Specifically, PEM requires a prompter module that generates soft pseudo-labels identifying entities that appear in random video crops. The pretraining model is then asked to predict the entity categories in the video crop, given the pseudo-label as the target.</p><p>The prompter serves to produce pseudo-labels of entity categories given a video crop, without dense annotations other than webly-sourced video-text pairs with possibly noisy alignment. To this end, we are inspired by CLIP <ref type="bibr" target="#b45">[44]</ref> that learns image-text alignment from noisy pairs. Specifically, we first pre-train the prompter, which consists of two unimodal encoders, on video-text pairs with the VTC loss as in Section 3.2.1, and freeze its parameters thereafter.</p><p>The prompter maintains a predetermined list of M text prompts. Each text prompt is an instantiation of a template, e.g. "A video of {ENTITY}.", where ENTITY is a frequent noun in the pre-training corpus, such as dog, grass, sky, etc. After the prompter is pre-trained, it computes the [CLS] embedding for each text prompt as {t 1 cls , t 2 cls , ..., t M cls }. To generate entity labels, given one video input, we first obtain a random video cropV (e.g. the same spatial region across sampled frames) and its [CLS] embeddingv cls from the prompter's video encoder. The prompter then computes an entity pseudo-label qV ? R M for the video crop as the softmax-normalized similarity betweenv cls and all the prompt embeddings {t m cls } M m=1 :</p><formula xml:id="formula_4">qV ,m = exp(s(V , T m )/? ) M m=1 exp(s(V , T m )/? )<label>(4)</label></formula><p>During pre-training of the video-language model, we apply mean pooling on the embeddings from the multimodal encoder that correspond to the spatial location of the video cropV , denoted as eV ? R d . We use a classifier (e.g. MLP) to compute the softmax-normalized entity prediction pV . The prompting entity modeling loss is then defined as the cross-entropy between pV and qV :</p><formula xml:id="formula_5">L pem = ? M m=1 qV ,m ? log pV ,m<label>(5)</label></formula><p>Prompting entity modeling features a diverge range of entities while requiring no extra human annotations, which yields an efficient and scalable solution to generate visuallygrounded regional supervisions for cross-modal learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Overall Pre-training Objectives</head><p>We also employ the widely-adopted masked language modeling (MLM) loss L mlm and video-text matching L vtm considering their effectiveness. The MLM objective utilizes both video and the contextual text to predict the masked text tokens. We randomly mask input tokens with a probability of 15% and replace them with a special token [MASK].</p><p>Video-text matching is a binary classification task which predicts whether a video and a text description are matched with each other. We use the multimodal [CLS] token e cls as the joint representation of the video-text pair, and trains the model with a cross entropy loss. Negative samples are generated from non-parallel video-text pairs from the batch. Following <ref type="bibr" target="#b29">[29]</ref>, we employ contrastive hard negative mining to find more informative in-batch negatives for VTM. The overall pre-training objective of ALPRO is:</p><formula xml:id="formula_6">L = L vtc + L pem + L mlm + L vtm<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pre-training Datasets</head><p>We pre-train our model with the webly-sourced dataset WebVid-2M <ref type="bibr" target="#b2">[3]</ref>, which contains 2.5M video-text pairs. In addition, as suggested by ClipBERT <ref type="bibr" target="#b26">[26]</ref> and FiT <ref type="bibr" target="#b2">[3]</ref>, pretraining with image-pairs can improve spatial representations of videos, we therefore include CC-3M [46] into our pre-training corpus. During pre-training, we duplicate images from CC-3M to make static videos. This in total amounts to 5.5M video-text pairs, which is an order of magnitude less than the commonly-adopted HowTo100M <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr">57]</ref> and of a comparable size to those used in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>We implement ALPRO in PyTorch <ref type="bibr" target="#b43">[42]</ref>. In detail, we initialize both the spatial and temporal attention blocks of TimeSformer by reusing ViT-B/16 weights pre-trained on ImageNet-21k <ref type="bibr" target="#b10">[11]</ref>. Text encoders are initialized using the first 6-layer of the BERT base model <ref type="bibr" target="#b9">[10]</ref>, and the multimodal encoder is initialized using the last 6-layers weights of BERT base . We pre-train ALPRO for 100k iterations, roughly equivalent to 10 epochs, using a batch size of 256 on 16 NVIDIA A100 GPUs. We use AdamW <ref type="bibr" target="#b35">[34]</ref> optimizer with a weight decay of 0.001. The learning rate is first warmed-up to 1e ?4 , then it follows a linear decay schedule. Since videos are usually of different aspect ratios, we first rescale them to 224 ? 224. For each video, we sample 4 frames randomly as inputs to the visual encoder while preserving their orderings in-between. For PEM, we use POS tagger 1 and retain the top 1k most frequent nouns as the entity names. We obtain random video crops occupying 30% ? 50% of the original spatial area as inputs to the prompter. We discard a pseudo-label if the most likely entity has a normalized similarity score smaller than 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the performance of ALPRO on text-video retrieval and video question answering tasks across four commonly-used datasets, introduced in Section 4.1. The purpose of the evaluation is three-fold. First, we demonstrate the effectiveness of major technical contributions (i.e. video-text contrastive loss and prompting entity modeling) in Section 4.2. We then compare the performance of ALPRO with previous methods, including task-specific and pre-training architectures, in Section 4.3 and Section 4.4, on retrieval and question answering tasks, respectively. Finally, we show ablation results on design choices and analyze model behaviors in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Downstream Tasks and Datasets</head><p>Text-Video Retrieval. (i) MSRVTT <ref type="bibr" target="#b54">[53]</ref> contains 10K videos with 200K text captions. We follow the common protocol <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b56">55,</ref><ref type="bibr">57]</ref> and use 7k videos for training and report results on the 1k test split <ref type="bibr" target="#b56">[55]</ref>. (ii) DiDeMo <ref type="bibr" target="#b1">[2]</ref> contains 10k videos from Flickr with 40k text descriptions. We follow <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b36">35]</ref> and evaluate paragraph-to-video retrieval, where sentence descriptions for each video are concatenated together as a single text query. We do not use the ground-truth proposals for temporal localization to ensure fair comparisons with previous work. Video Question Answering. We focus on the task of openended video question answering. (i) MSVD-QA <ref type="bibr" target="#b52">[51]</ref> is built upon videos and text descriptions from MSVD <ref type="bibr" target="#b5">[6]</ref>. Mt. Fuji with chureito pagoda in spring, fujiyoshida, Japan. <ref type="figure">Figure 3</ref>. Examples of the pseudo-labels generated by the prompter (scores in bracket). The highlighted areas are fed to the prompter. Our method generates a diverse range of common entity categories that are not usually covered by object detectors, e.g. towers, summit, yoga. Besides, entity labels do not always appear in the text description, serving as a source of corpus-level supervision. Bottom left: a random crop that does not contain entities. The prompter thereby produces pseudo-labels with low similarities. During pre-training, we discard a pseudo-label if its most likely entity has a score less than 0.2. Right: labels generated for different crops from the same video. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on the Proposed Methods</head><p>We first evaluate the impact of our main technical contributions (i.e. video-text contrastive loss and prompting entity modeling) in <ref type="table" target="#tab_1">Table 1</ref>. Compared with pre-training using only MLM and VTM, both PEM and VTC substantially improve the performance across all the datasets. VTC is in particular useful for the retrieval task. The reason is that the VTC loss explicitly maximizes the instance-level similarity between positive video-text pairs, which is wellaligned with the goal of retrieval. We notice that PEM significantly improves the performance of videoQA, especially on MSVD-QA, due to its ability to learn finer-grained regional features. While enabling both PEM and VTC losses has complementary effects for most datasets, we also observe it leads to slightly worse accuracy on MSVD-QA. Our observation is that MSVD-QA contains more questions requiring region-level knowledge, including object categories (e.g. dough, swords), animal species (e.g. hare, eagle) and scenes (e.g. river, cliff), which can be well modeled using  <ref type="table">Table 2</ref>. Comparisons with existing text-to-video retrieval methods with finetuning and zero-shot setups on MSRVTT. We follow the common partition with 7k training videos. Methods using 9k training videos are greyed out. Both partition protocols share the same 1k testing videos. R@k denotes recall (%) with k retrieval efforts; MdR denotes median ranking for retrieved videos. The pre-training datasets are HowTo100M (HT) <ref type="bibr" target="#b40">[39]</ref>, MS-COCO (COCO) <ref type="bibr" target="#b31">[31]</ref>, Visual Genome (VG) <ref type="bibr" target="#b22">[22]</ref>, WebVid2M (Web2M) <ref type="bibr" target="#b2">[3]</ref> and Conceptual Captions (CC3M) <ref type="bibr">[46]</ref>. PEM, rendering the impact of VTC negligible. In contrast, MSRVTT-QA involves more coarse-grained visual information such as activities. As a result, using both PEM and VTC complements with each other on MSRVTT-QA. Example Pseudo-labels. In <ref type="figure">Figure 3</ref>, we show examples of pseudo-labels generated by the prompter module. Our approach generates a more diverse range of entity categories beyond typical object classes from detection annotations. This is in particular beneficial when downstream tasks require a large vocabulary, such as open-ended videoQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Video-Text Retrieval</head><p>In <ref type="table">Table 2</ref> and <ref type="table">Table 3</ref>, we compare ALPRO with existing methods using finetuning and zero-shot text-to-video retrieval on MSRVTT and DiDeMo datasets, respectively. ALPRO surpasses previous methods by a significant margin while exploiting orders of magnitude less video-text pairs with no human-written texts required. On both datasets, ALPRO obtains more than 6% lift in terms of R10 scores. Note that we do not include comparison with the re- cent paper <ref type="bibr" target="#b37">[36]</ref> which utilizes the powerful encoders from CLIP <ref type="bibr" target="#b45">[44]</ref> (pretrained on 400M image-text pairs) and further post-trains them on HowTo100M <ref type="bibr" target="#b40">[39]</ref>. Since <ref type="bibr" target="#b45">[44]</ref> uses the same objective as VideoClip <ref type="bibr" target="#b53">[52]</ref>, the improvement over VideoClip validates the advantage of ALPRO pre-training.  <ref type="table">Table 4</ref> compares ALPRO with existing methods on open-ended video question answering datasets MSRVTT-QA and MSVD-QA. Most competitors have QA-specific architectures while that of ALPRO is generic for other videolanguage tasks, such as retrieval. We obtain on-par results with VQA-T <ref type="bibr" target="#b55">[54]</ref>, which exploits 69M QA-specific domain data for pre-training. In contrast, ALPRO uses only 5.5M video-text pairs from the web without domain knowledge. ALPRO surpasses other methods by a substantial margin, with 2.6% and 3.3% lift in accuracy. This demonstrates the competitive visual reasoning ability of ALPRO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Video Question Answering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablations and Analysis</head><p>Prompt design and ensembling. Similar to <ref type="bibr" target="#b45">[44]</ref>, we observe that it is important to design and ensemble prompts with multiple templates. Without much engineering effort, we employ a preliminary set of prompt templates, such as "A video of a {ENTITY}", "A footage of one {ENTITY}" for video inputs; "A photo of a {ENTITY}" and "A picture of the {ENTITY}" for image inputs. In total, we design 12 templates for video and image inputs each. We build the ensemble by averaging over the t cls embeddings of prompts instantiated with the same entity. The effect of prompt ensembling is shown in <ref type="table" target="#tab_5">Table 5</ref>. Despite our minimal engineering efforts (we only experimented with a single set of templates), prompt ensembling demonstrates its importance in generating high-quality pseudo-labels. It is our future work to explore more prompt engineering strategies. Effect of number of entities.</p><p>We investigate the effect of the number of entities for PEM in PEM brings consistent improvement with frequent entities. This suggests that the underlying principle of PEM to learn better region-entity alignment plays the essential role in its effectiveness. However, adding more low-frequency entities introduces noises in generating entity pseudo-labels, thus harming the pre-training. Effect of number of frames. In <ref type="table">Table 7</ref>, we show the results on downstream tasks with different numbers of input frames. Generally more frames lead to better performance, while such benefit saturates with more than 8 frames on the retrieval task. By sparsely sampling frames from the video and enabling end-to-end training of the visual backbone, ALPRO learns more effective representations than previous methods that use fixed offline features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes ALPRO , a new video-language pretraining framework that operates on sparsely-sampled video frames. ALPRO introduces video-text contrastive learning to align instance-level unimodal features, and prompting entity modeling for fine-grained region-entity alignment. We verify the efficiency and efficacy of ALPRO on multiple downstream datasets, where ALPRO achieves substantial performance improvement over existing models.</p><p>We believe ALPRO opens up a new direction for visionlanguage research, by exploiting the uptrending technique of prompting to generate semantic pseudo-labels. Here we list two potential ideas that can be further explored to improve ALPRO: (1) better prompt engineering / prompt tuning to improve the quality of entity pseudo-labels; (2) prompt-guided region selection with temporal information taken into consideration, which might improve the current way of random region selection. Last but not least, AL-PRO is not restricted to the video domain, and can be naturally extended to image-text representation learning, or even image representation learning. Limitations and Broader Impacts. While Section 5 discusses the technical aspect and proposes potential improvements, here we highlight the potential negative societal impact. Our pre-training data is collected from the web, which may contain unsuitable videos, harmful texts, and private information, which could leak into the pre-trained models. Additional model analysis is necessary before deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Prompt Templates</head><p>Inspired by <ref type="bibr" target="#b45">[44]</ref>, we ensemble multiple prompts templates to improve the quality of the generated pseudo-labels. We create the ensemble over the embedding space. Namely, we compute the average embedding of all the prompts for each entity. This allows scalable ensembling with more templates with the same computation cost as using a single prompt. During pre-training, the dataloader switches between video and image inputs. And we employ the following prompt templates for videos and images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Finetuning Setups</head><p>In this section, we describe implementation details for fine-tuning the pre-trained model. For all the downstream datasets, we resize video frames to 224 ? 224. During finetuning, we randomly select N v frames from the video, with parameters Nv 2 frames from the first and second half of the video, respectively. We use RandomAugment <ref type="bibr" target="#b8">[9]</ref>. We apply the same augmentation across frames sampled from one video. The default settings for finetuning on each dataset are in <ref type="table" target="#tab_10">Table 9</ref>. During inference, we do not use augmentation and sample uniformly. In all the experiments, we use the same random seed (i.e. 42) to ensure reproducibility.   <ref type="table" target="#tab_1">Table 10</ref>. End-to-end finetuning configurations for MSRVTT-QA and MSVD-QA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Generating supervision for region-entity alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>ALPRO pre-training framework. Left: the video-language pre-training model contains a space-time video encoder, a text encoder, and a multi-modal encoder, all of which are transformer-based. Besides two canonical objectives masked language modeling (MLM) and video-text matching (VTM), we introduce video-text contrastive loss (VTC) to learn instance-level video-text alignment, and prompting entity modeling (PEM) to learn fine-grained region-entity alignment. Right: the prompter which generates soft entity labels as supervision for PEM. The prompter consists of frozen unimodal encoders that are trained with VTC. During pre-training, it produces similarity scores between a randomly-selected video crop and a set of text prompts instantiated with entity names.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Evaluations of the proposed pre-training objectives on four downstream datasets. MLM: masked language modeling loss. VTM: video-text matching loss. PEM: prompting entity modeling loss. VTC: video-text contrastive loss. R@k denotes recall (%) with k retrieval efforts; MdR denotes median ranking for retrieved videos. We use acc. to denote accuracy.</figDesc><table><row><cell>Pre-training tasks</cell><cell cols="2">MSRVTT Retrieval R1? R5? R10? MdR?</cell><cell cols="2">DiDeMo Retrieval R1? R5? R10? MdR?</cell><cell cols="2">MSVD-QA MSRVTT-QA Acc.? Acc.?</cell></row><row><cell>w/o pre-training</cell><cell>16.5 42.8 57.9</cell><cell>7</cell><cell>9.5 29.1 42.5</cell><cell>14</cell><cell>41.5</cell><cell>39.6</cell></row><row><cell>MLM + VTM</cell><cell>28.5 53.0 66.8</cell><cell>5</cell><cell>29.8 57.7 69.7</cell><cell>4</cell><cell>43.3</cell><cell>40.9</cell></row><row><cell>MLM + VTM + PEM</cell><cell>30.3 56.7 67.8</cell><cell>4</cell><cell>31.0 61.8 73.5</cell><cell>3</cell><cell>46.3</cell><cell>41.8</cell></row><row><cell>MLM + VTM + VTC</cell><cell>32.8 59.2 70.3</cell><cell>3</cell><cell>36.8 64.7 77.4</cell><cell>2</cell><cell>45.5</cell><cell>41.9</cell></row><row><cell cols="2">MLM + VTM + PEM + VTC 33.9 60.7 73.2</cell><cell>3</cell><cell>35.9 67.5 78.8</cell><cell>3</cell><cell>45.9</cell><cell>42.1</cell></row><row><cell></cell><cell cols="2">Top 5 pseudo-labels</cell><cell></cell><cell></cell><cell></cell><cell>Top 5 pseudo-labels</cell></row><row><cell></cell><cell>1. Yoga (0.69) 2. Poses (0.11) 3. Workout (0.08) 4. Exercises (0.04) 5. Fitness (0.04)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1. Monastery (0.25) 2. Architecture (0.18) 3. Towers (0.16) 4. Buildings (0.06) 5. Heritage (0.04)</cell></row><row><cell cols="2">Young fit female in sportswear doing stretching lunge exercise indoor.</cell><cell></cell><cell cols="3">Mt. Fuji with chureito pagoda in spring, fujiyoshida, Japan.</cell><cell></cell></row><row><cell></cell><cell>To discard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top 5 pseudo-labels</cell></row><row><cell></cell><cell>1. Hat (0.07)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1. Summit (0.84)</cell></row><row><cell></cell><cell>2. Gun (0.07)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2. Mountain (0.09)</cell></row><row><cell></cell><cell>3. Shoes (0.05)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3. Peaks (0.02)</cell></row><row><cell></cell><cell>4. Poster (0.05)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4. Town (0.01)</cell></row><row><cell></cell><cell>5. Cartoon (0.04)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5. Pollution (0.01)</cell></row></table><note>A pigeon takes off from the roof of the garage.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>On downstream tasks, ALPRO allows end-to-end finetuning of the video backbone with raw video frames as input. During finetuning, we randomly sample N frames per video, where N = 8 for retrieval and N = 16 for QA, with more ablations present in Section 4.4. Temporal position embeddings in TimeSformer are interpolated to accommodate different number of input frames. During inference, we sample frames uniformly to ensure reproducibility. To keep pre-training and finetuning setups consistent, we resize all the videos to 224 ? 224 before feeding them into the model. Although this does not maintain the original aspect ratios, we observe no significant performance drop as our pre-training dataset contains videos of various aspect ratios. For finetuning on retrieval, we reuse the video-text matching head during pre-training and optimize the sum of both VTC and VTM losses. We obtain similarity scores from the output of VTM head during inference. For QA task, we add a simple MLP on the multimodal [CLS] token for classification and optmize the conventional cross-entropy loss between predictions and ground-truth answer labels. Dur-ing inference, predictions are obtained as the answer with the highest probability. All the finetuning experiments are performed on 8 NVIDIA A100 GPUs, taking one to five hours to complete depending on the datasets. More training details can be found in the appendix.</figDesc><table><row><cell>The MSVD-QA dataset has in total 1,970 videos and</cell></row><row><cell>50k question answer pairs, with 2,423 answer candidates.</cell></row><row><cell>(ii) MSRVTT-QA [51] is built upon videos and captions</cell></row><row><cell>from MSRVTT, which contains 10k videos with 243k open-</cell></row><row><cell>ended questions and 1.5k answer candidates.</cell></row><row><cell>Finetuning Setups.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Effect of ALPRO pre-training with and without prompt ensembling (ens.) on MSVD-QA and MSRVTT text-video retrieval with finetuning (FT) and zero-shot (ZS) setups.</figDesc><table><row><cell></cell><cell cols="2">MSRVTT-FT</cell><cell cols="2">MSRVTT-ZS</cell><cell>MSVD-QA</cell></row><row><cell></cell><cell cols="4">R1? R10? MdR? R1? R10? MdR?</cell><cell>Acc.?</cell></row><row><cell cols="2">w/o ens. 32.7 73.1</cell><cell>3</cell><cell>22.6 52.3</cell><cell>9</cell><cell>45.0</cell></row><row><cell cols="2">with ens. 33.9 73.2</cell><cell>3</cell><cell>24.1 55.4</cell><cell>8</cell><cell>45.9</cell></row><row><cell>#ent.</cell><cell cols="4">MSRVTT-FT R1? R10? MdR? R1? R10? MdR? MSRVTT-ZS</cell><cell>MSVD-QA Acc.?</cell></row><row><cell>?</cell><cell>32.8 70.3</cell><cell>3</cell><cell>22.6 53.0</cell><cell>9</cell><cell>45.5</cell></row><row><cell cols="2">500 33.0 71.9</cell><cell>3</cell><cell>22.7 54.1</cell><cell>8</cell><cell>45.6</cell></row><row><cell cols="2">1000 33.9 73.2</cell><cell>3</cell><cell>24.1 55.4</cell><cell>8</cell><cell>45.9</cell></row><row><cell cols="2">2000 34.7 72.4</cell><cell>3</cell><cell>22.4 52.8</cell><cell>9</cell><cell>45.3</cell></row><row><cell cols="6">Table 6. Effect of the number of entities for PEM. We report</cell></row><row><cell cols="6">results on MSVD-QA and MSRVTT text-video retrieval with</cell></row><row><cell cols="6">finetuning (FT) and zero-shot (ZS) setups. The first row cor-</cell></row><row><cell cols="6">responds to the model trained with MLM+VTM+VTC (i.e w/o</cell></row><row><cell>PEM).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Compared with the model pre-trained with MLM+VTM+VTC, adding Effect of the number of frames on MSRVTT text-video retrieval and MSVD-QA. More frames generally lead to better performance with 8-16 frames achieve a good trade-off between metrics and computation expense.</figDesc><table><row><cell>#frms</cell><cell cols="4">MSRVTT-FT R1? R10? MdR? R1? R10? MdR? MSRVTT-ZS</cell><cell>MSVD-QA Acc.?</cell></row><row><cell>2</cell><cell>25.7 63.9</cell><cell>5</cell><cell>17.3 48.9</cell><cell>11</cell><cell>43.8</cell></row><row><cell>4</cell><cell>31.0 69.6</cell><cell>4</cell><cell>21.4 54.4</cell><cell>8</cell><cell>44.5</cell></row><row><cell>8</cell><cell>33.9 73.2</cell><cell>3</cell><cell>24.1 55.4</cell><cell>8</cell><cell>45.4</cell></row><row><cell>16</cell><cell>34.2 72.6</cell><cell>3</cell><cell>24.7 55.0</cell><cell>7</cell><cell>45.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Prompts for video inputsPrompts for image inputsA footage of a {}. A photo of a {}. A footage of the {}. A photo of the {}. A footage of one {}. A photo of one {}. A video of a {}. A picture of a {}. A video of the {}. A picture of the {}. A video of one {}. A picture of one {}. A portrait of a {}. A good photo of a {}. A portrait of the {}. A good photo of the {}. A portrait of one {}. A good photo of one {}. A video footage of a {}. A good picture of a {}. A video footage of the {}. A good picture of the {}. A video footage of one {}.A good picture of one {}.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Prompt templates used for video and image inputs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>End-to-end finetuning configurations for MSRVTT and DiDeMo text-to-video retrieval.</figDesc><table><row><cell>config</cell><cell>MSRVTT</cell><cell>MSVD</cell></row><row><cell>optimizer</cell><cell>AdamW</cell><cell>AdamW</cell></row><row><cell>base learning rate</cell><cell>5e-5</cell><cell>5e-5</cell></row><row><cell>weight decay</cell><cell>1e-3</cell><cell>1e-3</cell></row><row><cell>optimizer momentum</cell><cell>? 1 , ? 2 =0.9, 0.98</cell><cell>? 1 , ? 2 =0.9, 0.98</cell></row><row><cell>learning rate schedule</cell><cell>linear decay</cell><cell>linear decay</cell></row><row><cell>batch size</cell><cell>96</cell><cell>96</cell></row><row><cell>max. text length</cell><cell>40</cell><cell>40</cell></row><row><cell>frame number</cell><cell>16</cell><cell>16</cell></row><row><cell>warmup ratio</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>training epochs</cell><cell>10</cell><cell>15</cell></row><row><cell>augmentation</cell><cell>RandAug(2, 5)</cell><cell>RandAug(2, 5)</cell></row><row><cell>gradient accumulate step</cell><cell>2</cell><cell>2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/explosion/spaCy</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A couple stands on a rocky shore as waves roll in and crash.</head><p>A couple stands on a rocky shore as waves roll in and crash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top 5 pseudo-labels</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Noise estimation using density estimation for self-supervised multimodal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rotman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6644" to="6652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">G?l Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clustering art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kobus</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William B Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UNITER: universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12375</biblScope>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teachtext: Crossmodal generalized distillation for textvideo retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simion-Vlad</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11583" to="11593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Heterogeneous memory enhanced multimodal attention model for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyou</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Motion-appearance co-memory networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Location-aware graph convolutional networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11021" to="11028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Divide and conquer: Question-guided spatiotemporal contextual attention for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11101" to="11108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reasoning with heterogeneous graph alignment for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11109" to="11116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by betweenclass attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical conditional relation networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Thao Minh Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tvqa: Localized, compositional video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1369" to="1379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning visual n-grams from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hero: Hierarchical encoder for video+ language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2046" to="2065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach. 2019. 4</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Univl: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Look before you speak: Visually contextualized utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16877" to="16887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Multimedia</title>
		<meeting>the ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Xiangnan He, and Yueting Zhuang</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Videoclip: Contrastive pre-training for zero-shot video-text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Just ask: Learning to answer questions from millions of narrated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1686" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Top 5 pseudo-labels</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Cat with big orange eyes sitting on dark wood surface near the houseplant</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Top 5 pseudo-labels</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">ferry boat and skyline in downtown core at marina bay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Singapore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Additional Examples of Pseudo-labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Our method generates a diverse range of common entity categories that are not usually covered by object detectors, e.g. towers, summit, yoga. Besides, entity labels do not always appear in the text description</title>
		<imprint/>
	</monogr>
	<note>The highlighted areas are fed to the prompter. serving as a source of corpus-level supervision</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
