<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pacific Graphics 2020 Monocular Human Pose and Shape Reconstruction using Part Differentiable Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020">2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-L</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pacific Graphics 2020 Monocular Human Pose and Shape Reconstruction using Part Differentiable Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">39</biblScope>
							<biblScope unit="issue">7</biblScope>
							<date type="published" when="2020">2020</date>
						</imprint>
					</monogr>
					<note>(Guest Editors)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS Concepts ? Computing methodologies ? Reconstruction;</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Superior human pose and shape reconstruction from monocular images depends on removing the ambiguities caused by occlusions and shape variance. Recent works succeed in regression-based methods which estimate parametric models directly through a deep neural network supervised by 3D ground truth. However, 3D ground truth is neither in abundance nor can efficiently be obtained. In this paper, we introduce body part segmentation as critical supervision. Part segmentation not only indicates the shape of each body part but helps to infer the occlusions among parts as well. To improve the reconstruction with part segmentation, we propose a part-level differentiable renderer that enables part-based models to be supervised by part segmentation in neural networks or optimization loops. We also introduce a general parametric model engaged in the rendering pipeline as an intermediate representation between skeletons and detailed shapes, which consists of primitive geometries for better interpretability. The proposed approach combines parameter regression, body model optimization, and detailed model registration altogether. Experimental results demonstrate that the proposed method achieves balanced evaluation on pose and shape, and outperforms the state-of-the-art approaches on Human3.6M, UP-3D and LSP datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human body reconstruction from a single image is a challenging task due to the ambiguities of flexible poses, occlusions, and various somatotypes. The reconstruction task aims at predicting both human pose and shape parameters. A large variety of applications such as motion capture, virtual or augmented reality, humancomputer interaction, visual effects, and animations rely on accurately recovered body models.</p><p>Recent results have shown that optimizing 3D-2D consistency between a 3D human body model and 2D image cues is beneficial to both fitting and regression-based methods [BKL * 16, LRK * 17, KBJM18]. With the emergence of large annotated datasets [APGS14, LMB * 14] and deep learning architectures [NYD16, CSWS17, SXLW19], we can acquire robust 2D keypoints prediction of the human joints. Previous methods [BKL * 16, LRK * 17] fit the body model primarily with 2D keypoints to achieve consistent 3D results. However, only 2D keypoints are not sufficient for the alignment between the 3D model and images. We argue that body part segmentation is a compelling image cue for ? Corresponding author: ma-lz@cs.sjtu.edu.cn (Lizhuang Ma) occlusion reasoning. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a), the predicted model is consistent with ground truth 2D keypoints, but the pose is still incorrect as two forearms are at wrong places along with depth. In contrast, with the correct part segmentation in <ref type="figure" target="#fig_0">Figure 1</ref>(b), we can align the model as consistent as the real person in the image.</p><p>However, how to utilize part segmentation as supervision in learning-based approaches is still an open question. We commonly generate the part segmentation from a 3D model by a rendering pipeline, which contains a non-differentiable process called rasterization. The modern rasterizer blocks the back-propagation in the training process, and derivatives cannot be propagated from the image level loss to the vertices of the 3D mesh. In order to achieve a differentiable renderer, some impressive efforts have been made to compute derivatives correlated to the perspective loss in recent years, such as OpenDR <ref type="bibr" target="#b23">[LB14]</ref>, Neural Mesh Renderer [OLPM * 18] and Soft Rasterizer <ref type="bibr" target="#b25">[LLCL19]</ref>. We formulate rasterization in a part-individual way, not only making use of the perspective loss on rendered images but also inferring the correlations of parts on the depth dimension (as known as z-axis). The paradigm of part differentiable rendering distinguishes our approach from existing works. As predicting part segmentation from images becomes more reliable <ref type="bibr" target="#b28">[LMSR17]</ref>, the reconstruction results could be more accurate due to fewer ambiguities caused by occlusions. The only difference between them is part segmentation in the white box, which indicates the different pose of crossed arms. Therefore, (b) with correct part segmentation performs the proper reconstruction rather than (a).</p><p>To reconstruct the pose and shape with part segmentation, representation of the human body should be simple, part-based, parametric and looks roughly like a real body. One of the most widely used models in human pose estimation is the skeletal representation that consists of 3D keypoint locations and kinematic relationships. It is a simple yet effective manner but commonly impractical to reason about occlusion, collision and the somatotype. In contrast, 3D pictorial structures <ref type="bibr" target="#b13">[FH05]</ref> composed of primitive geometries like cylinders, spheres and ellipsoids are adopted frequently in traditional works. This kind of model can be modulated by parameters explicitly, but it does not look like a real person. Detailed human models, like SMPL [LMR * 15], lead to a significant success in human shape reconstruction, whose shape prior is learnt from thousands of scanned bodies. Nevertheless, SMPL depends on a template mesh which means we have to retrain the prior when using a new template. Another problem is that the shape parameters of most detailed models are in the latent space, so it is hard to change body parts independently by the shape parameters.</p><p>Our goal is to propose an intermediate representation to combine the advantages of these models. In one hand, the proposed model has a binding on the skeletal representation to benefit pose estimation. In the other hand, the intermediate model, which is composed of primitive geometries, is applicable for shape reconstruction. It could also be embedded in a detailed human mesh so that we can build the mapping function to a detailed model by minimizing the difference between them. In practice, we predict the parameters of the intermediate model from the 2D image to represent a rough shape of the objective body, and then map to any detailed models.</p><p>In this paper, we propose a Part-level Differentiable Renderer called PartDR. To leverage the part segmentation as supervision, we use this module to compute derivatives of the loss, which indicates the difference between the silhouettes of the rendered and real body parts. Then each vertex obtains the gradient by back-ward propagation. Compared with the object-level Neural Mesh Renderer (NMR) <ref type="bibr" target="#b22">[KUH18]</ref>, we design an occlusion-aware loss in the part renderer to identify the occlusions between different parts and keep occlusion consistency with part segmentation. In addition, we adopt a simple geometric human body representation. The proposed body representation EllipBody is composed of several ellipsoids to represent different body parts. Parameters of EllipBody include the length, thickness, and orientation of each body part. We share the pose parameters between EllipBody and SMPL models and use an Iterative Closest Points (ICP) loss and other model constraints to register SMPL on EllipBody.</p><p>Our approach contains three stages to reconstruct a human body model from coarse to fine. At the first stage, we propose an architecture to regress the parameters of EllipBody from images where supervision, including 2D keypoints and part segmentation, are predicted from pretrained networks. Although a deep neural network can take all the pixels as input, this type of one-shot prediction requires a large amount of data to form the definitive imagemodel mapping. To refine the predicted model, we then optimize the model by minimizing the discrepancy between 2D and projected 3D poses. Both regression and optimization processes use PartDR to compute the gradients of the vertices that derived from the residual of rendered and predicted part segmentation. Since the predicted model is well initialized by networks, optimization can be faster and easier to converge. Finally, as an additional stage, we convert the EllipBody to SMPL, transform our representation as an inscribed model of SMPL, which means we do not need to retrain the network with various templates of the detailed model. With the EllipBody and PartDR, we achieve the balanced performance for pose and shape reconstruction on Human 3.6M [IPOS13], UP-3D [LRK * 17] and LSP <ref type="bibr" target="#b18">[JE10]</ref> datasets.</p><p>In summary, our contributions are three-fold.</p><p>? We propose an occlusion-aware part-level differentiable renderer (PartDR) to make use of part segmentation as supervision for training and optimization. ? We propose an intermediate human body representation (Ellip-Body) for human pose and shape reconstruction. It is a lightweight and part-based representation for regression and optimization tasks and can be converted to detailed models. ? Our approach that contains a deep neural network together with an iterative post-optimization achieves the balanced and predominant performance in human pose and shape reconstruction.  <ref type="bibr">]</ref> propose Soft Rasterizer, which exploits softmax function to approximate the rasterization process during rendering. However, it has several parameters to control the rendering quality that increase the number of super parameters when training. However, it has several parameters to control the rendering quality that increase the super parameters when training. We extend the differentiable renderer to part-level and propose an occlusion-aware loss function for part-level rendering. Therefore, we can explore the spatial relations between various parts in a single model that reduce the ambiguities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Representations for Human Bodies</head><p>The success of monocular human pose and shape reconstruction cannot stand without the parametric representations that contain the body priors. Among these representations, the 3D skeleton is a simple and effective form to represent human pose and helps numerous previous works [VCR * 18, PZDD17, SSLW17, ZHS * 17, WCL * 18, XWW18, HXM * 19, XJS19]. In skeleton-based methods, joint positions [MHRL17, SHRB11] and volumetric heatmaps [SXW * 18, PZD18] are often used to predict the 3D joints. However, these methods only focus on the pose estimation while the human shape is usually disregarded. In order to recover the body shape together with the pose, most approaches employ parametric models to represent the human pose and shape simultaneously. These models are often generated from human body scan data and encode both pose and shape parameters. These models contain thousands of vertices and faces and can represent a more detailed human body. As these parametric models use implicit parameters to indicate the human shape, it is hard to adjust body parts independently. Many of the part-based models are derived from the work on 2D pictorial structures (PS) <ref type="bibr" target="#b13">[FH05,</ref><ref type="bibr" target="#b50">ZB15]</ref>. 3D pictorial structures are introduced in 3D pose estimation but do not attempt to represent detailed body shapes [SIHB12,AARS13,BSC13,BAA * 14]. Zuffi and Black [ZB15] propose a part based detailed model Stitched Puppet which defines a "stitching cost" for pulling the limbs apart. Our method becomes an intermediate representation for current models, representing the pose and shape of body structure with explicit parameters, and is able to be converted to any detailed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Human Pose and Shape Reconstruction</head><p>Recent years have seen significant progress on the 3D human pose estimation by using deep neural networks and large scale Mo-Cap datasets. Due to perspective distortion, body shape and camera settings also affect the pose estimation. The problem to reconstruct both human pose and shape is developed with two paradigms -optimization and regression. Optimization-based solutions provide more accurate results but take a long time towards the optimum. Although these methods state the success of both pose and shape, and some exploit silhouettes or part segmentation as the input of neural networks, most of them only use 2D keypoints as supervision. As far as we know, our approach is the first method to utilize part segmentation as supervision, boosting the performance on both optimizationbased and regression-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present an algorithm to reconstruct the pose and shape of the human body from a single image. First we formulate the learning objective function (section 3.1). In section 3.2, we provide the implementation of our part-level differentiable renderer. In section 3.3, we introduce the design of our part-level intermediate model for human bodies. The pipeline is presented in section 3.4. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the part-level differentiable renderer is integrated into an end-to-end network and an optimization loop to make use of part segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Objective</head><p>Suppose a human model is controlled by pose parameters ? and shape parameters ?. Given a generative function</p><formula xml:id="formula_0">H(?, ?) = (M, S). (1)</formula><p>M is the mesh of the body, including the positions of vertices {v i, j | i = 1, ..., K} and the topological structure of faces {f i, j | i = 1, ..., K}. i indicates the index of body part, and j indicates the index of vertex or face in the corresponding part. S is the skeleton of the mesh, consisting of the 3D positions of joints {s i | i = 1, ..., N}. K is the number of body parts, and N is the number of joints.</p><p>Given a 3 ? 4 projection matrix P, rendering function is</p><formula xml:id="formula_1">R(M, S, P) = (A 1 , ..., A K , S 2D ),<label>(2)</label></formula><p>where A k ? R w?h is the part segmentation map of the k-th part. The size of rendered images are w ? h. S 2D are projected joints.</p><p>The loss function is composed of three terms, including reconstruction loss, projection loss and part segmentation loss.</p><formula xml:id="formula_2">L = ? 3D L 3D + ? pro j L pro j + ?segLseg.</formula><p>(3) </p><formula xml:id="formula_3">L 3D = S ?? 2 2 .<label>(4)</label></formula><formula xml:id="formula_4">L pro j = S 2D ?? 2D 1 . (5) Lseg = K ? k=1 w ? i=1 h ? j=1 A k (i, j) ?? k (i, j) 2 2 .<label>(6)</label></formula><p>? 3D ,? pro j and ?seg are weights for each loss. We set ? 3D = 0 for images that only have 2D annotations? 2D and? k (i, j) where (i, j) is the pixel index in part segmentation.</p><p>To achieve refined pose and shape parameters, we employ a two stage framework as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. First we use a deep neural network ? to predict features.</p><formula xml:id="formula_5">?(Image, W) = (?, ?, S 2D , {A k } K k=1 ).<label>(7)</label></formula><p>W are the weights of ? trained by minimizing the loss in (3) At the second stage, we optimize ? and ? by minimizing the same loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">PartDR: Part-Level Differentiable Renderer</head><p>Human part segmentation provides effective 3D evidence, e.g. boundaries, occlusions and locations, to infer the relationship between body parts. We extend previous object-level differentiable neural mesh renderer (NMR) [OLPM * 18] to a Part-level Differentiable Renderer (PartDR). It is applicable to deal with the multiple parts, leading each part located and posed correctly. PartDR illustrates human parts independently and ignores the region which is occluded by other body parts during the computation of derivatives. We also design a depth-aware occlusion loss to revise the incorrectly occluded region. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, the target position of the red part is behind the blue one. The optimization process supervised by full segmentation fails to converge to the global optimum with a poor initialization. If supervision becomes part segmentation of two triangles, PartDR further changes the depth of them to reach the global optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Rendering the human parts</head><p>Forward propagation of PartDR outputs the face index map F and the alpha map A in common with the traditional rendering pipeline. The face index map indicates the correspondence between image pixels and faces of the human mesh.</p><formula xml:id="formula_6">F(u, v) = argmin i D (u,v) ( f i j ) at least one f i j ; ?1 otherwise.<label>(8)</label></formula><p>where f i j is the jth face of the ith part, and its projection covers the pixel at (u, v). The function D (u,v) (?) means the depth of specified face at (u, v). The alpha map A is defined as a binary map that A(u, v) = 1 indicates F(u, v) &gt; ?1. For each part, A i in (2) can be calculated by F and A.  </p><formula xml:id="formula_7">A i (u, v) = A(u, v) ifF(u, v) = i; 0 otherwise.<label>(9</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Approximate derivatives for part rendering</head><p>We take the insight that the differentiable rasterization is a linear function to compute approximate derivatives of each part. We use the function I P (x) ? 0, 1 to indicate the rendering function of pixel P at (u, v), thus I P = A i (u, v).</p><p>We define ?IP ?x (v i, j k ) as the derivative of the kth vertex of face f i j . If the face f i j will not be occluded by other parts,</p><formula xml:id="formula_8">?I P ?x (v i, j k?1,2,3 ) = ?I P x 0 ? x 1 .<label>(10)</label></formula><p>We only show the derivatives on the x-axis for simplicity. I P (x) is the rendered value of pixel P. ?I P is the residual between ground truth I P and I P (x 0 ). x 0 is the x-coordinate of the current vertex and x 1 is a new x-coordinate that makes P collide the edge of the rendered face. For example, in in <ref type="figure">Figure 4</ref>(b), moving v i, j k from x 0 to x 1 makes I P from 0 to 1. We approximate this moving process as a linear transformation, so that the gradient at x 0 is the slope of the linear function.</p><p>Considering the circumstance that one face is occluded by others partially or completely, change in x 1 will not change I P . As shown on the left side in <ref type="figure">Figure 4 (a)</ref>, the red triangle does not belong to the i-th part and occludes the blue face. The yellow pixel is P. Without occlusion, moving v i, j k from x 0 to x 1 will change the value of P, as illustrated by black line. If P is occluded, I P will not change in A i , so the gradient should be 0.</p><formula xml:id="formula_9">?I P ?x (v i, j k?1,2,3 ) = ? ? ? ? ? 0, if A(u, v) &gt; 0 and F(u, v) = i; ?IP x0?x1 , otherwise.<label>(11)</label></formula><p>We propose derivatives on the z-axis (direction on depth) as an extension for the part-level neural renderer. We omit the derivatives in the occluded regions and then design a new approximation of the derivatives on the z-axis to refine the incorrectly occluded part. As shown in <ref type="figure">Figure 4</ref>(e), we first find the occluded face. Then we compute the depth derivatives directly proportional to the distance between the occluded point and the one occluding it. Base on the triangle similarity, the derivative is</p><formula xml:id="formula_10">?I P ?z (v i, j k ) = ? ? ?I P ? log ?(M 0 , Q) ?(M 0 , v i, j k ) ? ?z + 1 .<label>(12)</label></formula><p>?z = z 0 ? z 1 is the distance between the two faces. ?(?, ?) is the length between two points. Q is the corresponding point whose projecting point is P. The line form v</p><formula xml:id="formula_11">i, j 0 to Q intersects v i, j 1 to v i, j 2 at M 0 .</formula><p>? is a variable to magnitude the term. Different from x-axis that x 1 ? x 0 at least one pixel, ?z could be a small value, so we use logarithmic function to avoid gradient explosion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">EllipBody: An Intermediate Representation</head><p>We proposed a light-weight and flexible intermediate representation to simplify the model and disentangle human body parts. The proposed representation EllipBody utilizes ellipsoids to represent body parts, therefore pose and shape parameters can be the position, orientation, length, and thickness of each ellipsoid. The Ellip-Body contains both the human mesh M and the skeleton S. This non-detailed model can be considered as the inscribed model of real human body and has following advantages. Firstly, the model composed of primitive geometries has fewer faces and simple typologies, which can accelerate the rendering and computation of interpenetration. Secondly, EllipBody is robust to imperfect part segmentation. Detailed models, such as SMPL, use shape parameters altogether to adjust the body. Thus all parameters may be affected by the error of one part. Lastly, in some circumstances, users may need to modify detailed models intuitively. EllipBody can be edited through its interpretable shape parameters, then SMPL will be correspondingly changed.</p><p>As shown in <ref type="figure">Figure 5</ref>, we deform an icosahedron to generate each ellipsoid then assemble them according to the bones of the skeleton, where the endpoints of the ellipsoids are located at human joints. We define the shape of the i-th ellipsoid E i with the bone length l i and part thickness along two other axes t 1 i ,t 2 i . The global rotation R i ? SO(3) of each ellipsoid is the pose parameter. In practice, EllipBody is made of 20 ellipsoids:</p><formula xml:id="formula_12">E = {E i |i = 1, ..., 20},<label>(13)</label></formula><p>where</p><formula xml:id="formula_13">E i = E(R i , C i , l i ,t 1 i ,t 2 i ).<label>(14)</label></formula><p>, E is the function to deform the original icosahedron. C i is the position of the centre of the i-th ellipsoid, which is the midpoint of two connected joints. By forward kinematics, S is inferred via l.</p><formula xml:id="formula_14">? 1 , 1 , ? ? 2 , ? 3 , 3 , 2 , z ( 1 , ) Q 0 Q? , P<label>( , ) 0 1</label></formula><formula xml:id="formula_15">S i = R parent(i) ? (l i O i ) + S parent(i) .<label>(15)</label></formula><p>where O i is an offset vector indicating the direction from its parent to the current joint. So l i O i denotes the local position of i-th joint in its parent joint's coordinate.</p><p>As the human body is symmetric, ellipsoids in EllipBody share the parameters when indicating the same category of human parts. Therefore, we reduce the number of semi-principal axes parameters</p><formula xml:id="formula_16">Part length Shape Part length Shape Ass l 0 t 0 t 1 Upper legs l 6 t 7 t 7 Abdomen l 1 t 0 t 2 Lower legs l 7 t 8 t 8 Chest l 2 t 0 t 3 Feet l 8 t 9 t 10 Neck l 3 t 4 t 4</formula><p>Upper arms l 9 t 11 t 11 Shoulders l 4 t 5 t 5 Fore arms l 10 t 12 t 12 Head l 5 t 6 t 6 Hands l 11 t 13 t 14 <ref type="table">Table 1</ref>: Shape Parameters of EllipBody Parts. l denotes the length of the ellipsoids. t denotes the thickness of them.</p><p>from R 20?3 to R 27 . <ref type="table">Table 1</ref> shows the indices of shape parameters for different parts.</p><p>EllipBody can be an intermediate representation for any detailed models. We use SMPL [LMR * 15] as an example to describe the conversion between them. We consider that EllipBody should inscribe the SMPL model when they represent the same objective.</p><p>Suppose v S are vertices of the SMPL M S . v E and M E are for El-lipBody. Because two models share the same pose parameters and different shape parameters, we minimize the following loss:</p><formula xml:id="formula_17">(?, ?) = argmin ?,? L ICP (M S , M E ) + L PEN (v S , M E ),<label>(16)</label></formula><p>where ? and ? are the SMPL parameters. L ICP is Iterative Closest Points (ICP) <ref type="bibr" target="#b6">[BM92]</ref> loss to make two meshes closer. L PEN is the penetration loss indicating the extent that vertices of SMPL penetrate into EllipBody. Suppose one vertex of SMPL v = (vx, vy, vz) T and one ellipsoid of EllipBody E i . The vector from the center of the ellipsoid to the vertex before rotation is d = R ?1 i (v ? C i ). We transform the ellipsoid to a sphere with radius 1, and</p><formula xml:id="formula_18">e(v, E i ) = 2 ? dx l i , 2 ? dy t 1 i , 2 ? dz t 2 i 2 (17)</formula><p>is the distance from the center to the vertex.</p><formula xml:id="formula_19">L PEN = ? v,E a(1 ? e(v, E i )) 2 .<label>(18)</label></formula><p>If e(v, E i ) &gt; 1, which means v is outside the E i , a becomes 0 or otherwise 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pose and Shape Reconstruction</head><p>We proposed an end-to-end pipeline to estimate the parameters of EllipBody. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, a CNN-based backbone extracts the features from a single image first. Base on the image feature, we regress the parameters of the pose and shape. After that, we optimize the model by minimizing the loss in <ref type="formula">(3)</ref>  The neural network could map all the images and human models in theory. However, due to lack of 3D annotations, the performance of the network may not be satisfied. To achieve more accurate results, we take the predicted parameters from the regressor as the initialization, then use part segmentation from the network to refine the EllipBody. We minimize the loss function</p><formula xml:id="formula_20">E(r, l, t, c) = ?segLseg + ? pro j L pro j + ? l L l + ?t Lt .<label>(19)</label></formula><p>Most terms are introduced in section 3.1. L l = l ?l 2 and Lt = t ?t 2 are regularization term wherel andt are mean shape parameters. Since the network provides an initialization, the optimization process only needs a small number of iterations to converge. As a result, the optimization fixes the minor errors of the model predicted from regressors.</p><p>In order to visualize the detailed body model, we fit SMPL model to circumscribe the EllipBody. Note that SMPL has 23 rotation angles, while ours have 20 angles. We decompose the angles where the angle in SMPL are almost fixed like the chest and clavicles.</p><p>Other joints related to limbs and the head share the same pose parameters. After that, we initialize ? by mean shape parameters and minimize the loss function proposed in (16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we show the evaluation of the results achieved by our method and perform experiments with ablation studies to analyze the effectiveness of the components in our framework. We also compare our method with state-of-the-art pose and shape reconstruction techniques. The qualitative evaluation adopts SMPL [LMR * 15] as the final model to keep the consistency with other methods, despite that any other model can be converted from Ellip-Body with the same scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Human3.6M: It is a large-scale human pose dataset that contains complete motion capture data which contain rotations angles. It also provides images, camera settings, part segmentation, and depth images. We use original motion capture pose data to train Ellip-Body and assemble the body part segmentation into 14 parts. We use data of five subjects (S1, S5, S6, S7, S8) for training and the rest two (S9, S11) for testing. We use the Mean Per Joint Position Error (MPJPE) and Reconstruction Error (PA-MPJPE) for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UP-3D:</head><p>It is an in-the-wild dataset, which collects high-quality samples from MPII [APGS14], LSP <ref type="bibr" target="#b18">[JE10]</ref>, and FashionPose [DGLVG14]. There are 8,515 images, 7,818 for training and 1389 for testing. Each image has corresponding SMPL parameters. We use this dataset to enhance the generalization of the network.</p><p>LSP: It is a 2D pose dataset, which provides part segmentation annotations. We use the test set of this dataset to evaluate the accuracy and f1 score of part segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The backbone ResNet-50 and 2D keypoints estimation network are pretrained by Xiao et al. <ref type="bibr" target="#b48">[XWW18]</ref>. The dimension of the regression model is 1024, and each regressor is stacked with two residual blocks. The input image size is 256 ? 256, while output size of segmentation is also 256 ? 256. We use Adam optimizer for network training and optimization. During the training process, we use Human3.6M data to warm up. It is an important fact that the somatotype of body and extrinsic camera parameters are coupling due to perspectives. Therefore, we adopt mean bone lengthsl and thicknessest, and train the regressors for r, c. we set the batch size to 128, with learning rate 10 ?3 to train the model for first 70 epochs without part segmentation loss Lseg. At the second stage, we add segmentation loss and reduce the learning rate to 10 ?4 for additional 30 epochs. We use UP-3D and Human3.6M together and fix the weights of regressor for c. During optimization, we use Adam optimizer with learning rate 10 ?2 to fit the predicted Ellip-Body at most 50 iterations. Base on the experimental results, we set ? 3D : ? pro j : ?seg : ? l : ?t = 1 : 1 : 10 ?2 : 10 ?3 : 10 ?3 .   We compare our approach with other state-of-the-art methods for both pose and shape reconstruction. For 3D pose estimation, we evaluate the results with Human3.6M dataset in <ref type="table" target="#tab_6">Table 2</ref>. We use the same protocols in [KBJM18] where the metric is reconstruction error after scaling and rigid transformation. In order to evaluate the results for body shape reconstruction, we compare our method with recent approaches on both 2D and 3D metrics. The results in <ref type="table" target="#tab_7">Table 3</ref> present per vertex reconstruction error on UP-3D. We also evaluate the results on the test set of LSP in <ref type="table" target="#tab_8">Table 4</ref> with the metric accuracy and F1 score. We adopt EllipBody as the representation to avoid the minor error caused by the detailed human template.</p><p>We refer the network prediction in our approach as EllipBo-dyNet. The EllipBody parameters predicted directly from Ellip-BodyNet can achieve competitive results among recent methods which reconstruct the full body in an end-to-end network. With further optimization, the error is close to the state-of-the-art approach [KPBD19], which also conducts the optimization to improve the results. We justify that our method balances the pose and shape. As shown in   <ref type="table">Table 5</ref>: Comparison on the protocol 1 of Human3.6M with different parametric model. We also compare the performance with or without different losses. The evaluation method is MPJPE. only provides faithful shape parameters but also improves the performance of the pose estimation as well. We find that optimization with ground-truth part labels even out-perform SMPLify [BKL * 16] who generates the part annotations for the UP-3D dataset. For a comprehensive comparison, we also optimize SMPL model on ground truth part segmentation. The result is roughly equivalent to EllipBody but does not perform better on part segmentation due to the heterogeneous mesh of SMPL template. Since UP-3D dataset has ground truth SMPL parameters, we evaluate registered SMPL results with vertex-to-vertex errors in <ref type="table" target="#tab_7">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablative Study</head><p>Model Representation. We investigate the effectiveness of our proposed model base on protocol 1 of Human3.6M. Different from section 4.3, this protocol only aligns the root of the models to compute MPJPE. The reason to adopt this protocol is that our Ellip-BodyNet predicts both pose and shape parameters, and scaling or applying rigid transformation on the model will neglect the global rotation and body somatotype. We first compare the SMPL and El-lipBody as the output of the network, respectively. While 3D anno-  <ref type="figure">Figure 6</ref>: Optimization Performance on LSP. The red line illustrates the time consumption of each iteration. The blue line is the accuracy of part segmentation. We mark the number of faces for our models and SMPL, and our models with repeat times of surface subdivisions from zero to three are denoted by terms from E0 to E3. The accuracy does not increase after one times subdivision.</p><p>tations such as joint angle, shape parameters are not precisely the same between two models, we train the network supervised by 2D keypoints and full/part segmentation. As shown in <ref type="table">Table 5</ref>, the performance is powered by EllipBody, which is composed of primitive ellipsoids that facilitate the training process.</p><p>Body Part Supervision. To prove that part segmentation provides more image cue, we take various loss functions to train the model respectively. Full body segmentation reduces error by 6.7mm, comparing with projection only. When we replace with part supervision, the result decreases further 1.9mm MPJPE. Even if we train the network with 3D supervisions, part segmentation still improves the performance of the human pose and shape reconstruction, as shown in <ref type="table">Table 5</ref>.</p><p>Speed of Part Differentiable Renderer. Body parts of EllipBody are deformed from the icosahedron, which means we can subdivide each face to provide more fine-grained EllipBody. In <ref type="figure">Figure 6</ref>, E i denotes the EllipBody which is subdivided by i times. Each face is divided into four identical faces at each time. With more faces, the model can fit the body boundary more compactly, but consume more time and may lead to over-fitting. The red line illustrates the iteration time for different models, and the blue line indicates the accuracy of part segmentation on LSP <ref type="bibr" target="#b18">[JE10]</ref>. We find that the performance will not increase after two subdivisions, so we adopt E 1 as the default EllipBody configuration. The speed is tested on GTX 1080TI GPU, and the size of the rendered image is 256 ? 256.   the parts of EllipBody by PartDR. The boundaries of EllipBody are close to the one in original images, proving that the non-detailed model could fit the real human body successfully. However, there are still a few failure cases that can be attributed to incorrect part segmentation, occlusions by multiple people, as well as challenge poses. As shown in <ref type="figure" target="#fig_5">Figure 7</ref>, though part segmentation with correct ordinal depth, the legs get crossing due to the wrong size of the part segmentation of right foot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Evaluation</head><p>Another advantage of EllipBody is interpretable shape parameters. For those unusual body shapes, EllipBody can construct longer limbs, more oversized heads, as well as shorter legs. <ref type="figure" target="#fig_6">Figure 8</ref> shows an interpolation from one EllipBody to another. We also convert the model to SMPL. It can be widely used for animation editing and avatar modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we focus on improving monocular human pose and shape reconstruction with a common image cue -part segmentation. We propose an intermediate geometry model EllipBody which has explicit parameters and light-weight structure. We also propose a differentiable mesh renderer to a depth-aware part-level renderer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>GraphCMR Part Seg EllipBody Ours SPIN that can recognize the occlusion between human parts. The partlevel differential neural renderer utilizes the part segmentation as effective supervision to improve the performance on both regression and optimization stage. Furthermore, any detailed parametric model like SMPL can be registered on EllipBody model, such that the neural network to predict EllipBody does not need retraining when using a new human template. Although our method still has some limitations such as uncontrollable face directions, self interpenetration and the heavy dependence of part segmentation quality, the EllipBody has great potential in many applications. As the future work, we would extend the approach to the scene involving multiple people, as well as other occlusion cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Effect of part segmentation. Predictions of 2D pose and full segmentation are identical in (a) and (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Framework. Our pipeline is divided into three stages: (1) A CNN-based backbone extracts features from the image, then regressors predict the parameters of EllipBody and camera. The supervision includes 2D keypoints and part segmentation predicted from networks. (2) Using PartDR further optimizes EllipBody to align with the predicted image cues. (3) Registering a detailed model on EllipBody.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Optimization with PartDR. The upper row illustrates the optimization process with full segmentation. The lower row shows the one with part segmentation. The optimization target is in the green box. However, optimization with full segmentation may stop at red box, while PartDR further pushes Part A to the right place.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Part-level Differentiable Renderer. The left side illustrates all four possible cases on approximate values of intensity and their corresponding derivatives along the axes in the image plane. The right side illustrates the case where rendered parts are occluded incorrectly, i.e. z-axis gradients take effect over the rendered parts. The yellow pixel P indicates where the rendering loss come from. The black line is the approximate function of I P and its derivative if there is no occlusion. The red line is the function that occluded by red triangle. Formulation of EllipBody. Ellipsoids are subdivided and deformed from icosahedrons. The skeleton of the human body is computed by forward kinematics. After that, ellipsoids are assembled along the skeleton to construct the mesh. Detailed models can be registered by optimization with ICP and penetration loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9</head><label>9</label><figDesc>illustrates the reconstruction results by our approach in LSP dataset. We present the part segmentation rendered by PartDR, and the mesh of both EllipBody and corresponding SMPL. The second and third columns show the results by recent the state-ofthe-art methods [KPD19, KPBD19]. Most pose ambiguities with precise part segmentation can be solved with PartDR and EllipBody as listed in the last column. The part segmentation is rendered with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Some failure cases of our reconstruction results are caused by wrong part segmentation and ill-posed problems. The human on the left side should be lean forward, but ours leans back. On the right side, the part segmentation of the right foot is too big, making the legs crossed. Both of them are correct at the frontal view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>SMPL with EllipBody. As the high flexibility in parameters of EllipBody, we can control the body parts independently. The upper row shows an interpolation between two totally different EllipBody models. The lower row shows the corresponding SMPL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative Results. Comparison with recent approaches for LSP datasets. Images from left to right indicate: (1) original image, (2) GraphCMR [KPD19], (3) SPIN [KPBD19] (4) registered SMPL on EllipBody, (4) part segmentation of EllipBody, (5) EllipBody.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>shape and textures successfully but ignore different parts of the object. Liu et al. [LLCL19</figDesc><table><row><cell>derers output the</cell></row><row><cell>Work</cell></row><row><cell>2.1. Differentiable Rendering</cell></row><row><cell>Rendering connects the image plane with the 3D space. Recent</cell></row><row><cell>works on inverse graphics [dLGPF08, LB14, KUH18] made great</cell></row><row><cell>efforts in the differentiable property, which makes the renderer sys-</cell></row><row><cell>tem as an optional module in machine learning. Loper et al. [LB14]</cell></row><row><cell>propose a differentiable renderer called OpenDR, which obtains</cell></row><row><cell>derivatives for the model parameters. Kato et al. [KUH18] present</cell></row><row><cell>a neural renderer which approximates gradient as a linear func-</cell></row><row><cell>tion during rasterization. These methods support recent approaches</cell></row><row><cell>[OLPM  *  18, PZZD18a] exploiting the segmentation as the super-</cell></row><row><cell>vised labels to improve their performance. These differentiable ren-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>17] take the silhouettes and dense 2D keypoints as additional features, and extend SMPLify method to obtain more accurate results. Recent expressive human model SMPL-X [PCG * 19] integrates face, hand, and full-body. Pavlakos et al.optimize the VPoser [PCG * 19], which is the latent space of the SMPL parameters, together with a collision penalty and a gender classifier. The regression-based method becomes the majority trend recently because of agility. Thus Pavlakos et al. [PZZD18b] use a CNN to estimate the parameters from the silhouettes and 2D joint heatmaps. Kanazawa et al. [KBJM18] present an end-to-end network, called HMR, to predict the parameters of the shape which employ a large dataset to train a discriminator to guarantee the available parameters. Kolotouros et al. [KPD19] propose a framework called GraphCMR which regresses the position of each vertex through a graph CNN. SPIN [KPBD19] combines both optimization and regression methods to achieve high performance.</figDesc><table /><note>Guan et al. [GWBB09] optimize the parameters of SCAPE [ASK* 05] model with the 2D keypoints annotation. With reliable 2D keypoints detection, Bogo et al. [BKL* 16] propose SMPLify to optimize the parameters of SMPL model [LMR* 15]. Lassneret al. [LRK*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Wang et al. / Monocular Human Pose and Shape Reconstruction using Part Differentiable Rendering</figDesc><table><row><cell></cell><cell>Rec. Error (mm)</cell></row><row><cell>Akhter &amp; Black [AB15]</cell><cell>181.1</cell></row><row><cell>Ramakrishna et al. [RKS12]</cell><cell>157.3</cell></row><row><cell>Zhou et al. [ZZLD16]</cell><cell>106.7</cell></row><row><cell>SMPlify [BKL  *  16]</cell><cell>82.3</cell></row><row><cell>Lassner et al. [LRK  *  17]</cell><cell>80.7</cell></row><row><cell>Pavlakos et al. [PZZD18b]</cell><cell>75.9</cell></row><row><cell>NBF [OLPM  *  18]</cell><cell>59.9</cell></row><row><cell>HMR [KBJM18]</cell><cell>56.8</cell></row><row><cell>GraphCMR [KPD19]</cell><cell>50.1</cell></row><row><cell>DenseRaC [XZT19]</cell><cell>48.0</cell></row><row><cell>SPIN [KPBD19]</cell><cell>41.1</cell></row><row><cell>EllipBodyNet</cell><cell>49.4</cell></row><row><cell>EllipBodyNet + Optimization</cell><cell>45.2</cell></row></table><note>M.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Quantitative</figDesc><table><row><cell cols="2">results on Human3.6M [IPOS13]. Numbers</cell></row><row><cell cols="2">are reconstruction errors (mm) of 17 joints on frontal camera (cam-</cell></row><row><cell cols="2">era 3). The numbers are taken from the respective papers.</cell></row><row><cell></cell><cell>Vertex-to-Vertex Error (mm)</cell></row><row><cell>Lassner et al. [LRK  *  17]</cell><cell>169.8</cell></row><row><cell>Pavlakos et al. [PZZD18b]</cell><cell>117.7</cell></row><row><cell>Bodynet et al. [VCR  *  18]</cell><cell>102.5</cell></row><row><cell>Ours</cell><cell>100.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Vertex-to-Vertex error on UP-3D. The numbers are mean per vertex errors (mm). We use the registered SMPL model of Stage 3 due to the ground truth are based on SMPL models.</figDesc><table><row><cell>4.3. Comparing with the State-of-the-Arts.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>, with predicted part segmentation from [OLPM * 18], optimized EllipBody outperforms others including SPIN [KPBD19] on both foreground/background segmentation and part segmentation. The rationale is that those optimization targets in previous methods [BKL * 16,KPBD19] are typically 2D keypoints. Leveraging part segmentation as the optimization goal not</figDesc><table><row><cell></cell><cell cols="2">FB Seg.</cell><cell>Part Seg.</cell><cell></cell></row><row><cell></cell><cell>acc.</cell><cell>f1</cell><cell>acc.</cell><cell>f1</cell></row><row><cell>SMPLify on GT [BKL  *  16]</cell><cell>92.17</cell><cell>0.88</cell><cell>88.82</cell><cell>0.67</cell></row><row><cell>SMPLify [BKL  *  16]</cell><cell>91.89</cell><cell>0.88</cell><cell>87.71</cell><cell>0.64</cell></row><row><cell>SMPLify on [PZZD18a]</cell><cell>92.17</cell><cell>0.88</cell><cell>88.24</cell><cell>0.64</cell></row><row><cell>HMR [KBJM18]</cell><cell>91.67</cell><cell>0.87</cell><cell>87.12</cell><cell>0.60</cell></row><row><cell>Bodynet [VCR  *  18]</cell><cell>92.75</cell><cell>0.84</cell><cell>-</cell><cell>-</cell></row><row><cell>DenseRaC [XZT19]</cell><cell>92.40</cell><cell>0.88</cell><cell>87.90</cell><cell>0.64</cell></row><row><cell>GarphCMR [KPD19]</cell><cell>91.46</cell><cell>0.87</cell><cell>88.69</cell><cell>0.66</cell></row><row><cell>SPIN [KPBD19]</cell><cell>91.83</cell><cell>0.87</cell><cell>89.41</cell><cell>0.68</cell></row><row><cell>PartDR+SMPL+GT.Part</cell><cell>94.03</cell><cell>0.91</cell><cell>91.91</cell><cell>0.79</cell></row><row><cell>PartDR+EllipBody+GT.Part</cell><cell>94.74</cell><cell>0.92</cell><cell>93.26</cell><cell>0.84</cell></row><row><cell>PartDR+EllipBody+Pred.Part</cell><cell>92.13</cell><cell>0.88</cell><cell>90.70</cell><cell>0.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Segmentation evaluation on the LSP. The numbers are accuracies and f1 scores of foreground-background segmentation and body part segmentation. SMPL or EllipBody indicates which body representation we used. We show the upper bound of our approach with ground truth part segmentation. Our approach with predicted part segmentation reaches the state-of-the-art.</figDesc><table><row><cell>Model</cell><cell>Loss</cell><cell>MPJPE (mm)</cell></row><row><cell>SMPL</cell><cell>L pro j</cell><cell>106.8</cell></row><row><cell>SMPL</cell><cell>L pro j + Lseg(full)</cell><cell>75.9</cell></row><row><cell>SMPL</cell><cell>L pro j + Lseg(part)</cell><cell>67.1</cell></row><row><cell cols="2">EllipBody L pro j</cell><cell>73.8</cell></row><row><cell cols="2">EllipBody L pro j + Lseg(full)</cell><cell>67.1</cell></row><row><cell cols="2">EllipBody L pro j + Lseg(seg)</cell><cell>65.2</cell></row><row><cell cols="2">EllipBody L 3D + L pro j + Lseg(full)</cell><cell>64.1</cell></row><row><cell cols="2">EllipBody L 3D + L pro j + Lseg(part)</cell><cell>62.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2020 The Author(s) Computer Graphics Forum ? 2020 The Eurographics Association and John Wiley &amp; Sons Ltd.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank for the support from National Natural Science Foundation of China (61972157, 61902129), Shanghai Pujiang Talent Program (19PJ1403100), Economy and Information Commission of Shanghai (XX-RGZN-01-19-6348), National Key Research and Development Program of China (No. 2019YFC1521104). We also thank the anonymous reviewers for their helpful suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-view pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amin S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akhter I</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele B</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<idno>ACM. 3</idno>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>ASK * 05</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belagiannis V</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amin S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilic S</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
	<note>BAA * 14</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bogo F</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanazawa A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
	<note>BKL * 16</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">J</forename><surname>Besl P</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">D</forename><surname>Mckay N</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor fusion IV: control paradigms and data structures</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1611</biblScope>
			<biblScope unit="page" from="586" to="606" />
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao Z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon T</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei S.-E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">B</forename><surname>Gragg W</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stewart</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reorthogonalization and stable algorithms for updating the gramschmidt qr factorization. Mathematics of Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="772" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Body parts dependent joint regressors for human pose estimation in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool L</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2131" to="2143" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modelbased hand tracking with texture, shading and self-occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorce</forename><forename type="middle">M</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">F</forename><surname>Felzenszwalb P</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[gwbb09] Guan P</forename><surname>Weiss A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">O</forename><surname>Balan A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Habibie I</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu W</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehta D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10905" to="10914" />
		</imprint>
	</monogr>
	<note>HXM * 19</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He K</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename><forename type="middle">S</forename><surname>Everingham M</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.12.2</idno>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanazawa A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">J</forename><surname>Black M</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">W</forename><surname>Jacobs D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via modelfitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolotouros N</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">J</forename><surname>Black M</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dani-Ilidis K</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolotouros N</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harada T</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Opendr: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">M</forename><surname>Loper M</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kinematic analysis of a stewart platform manipulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu K</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">L</forename><surname>Lewis F</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="282" to="293" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Soft rasterizer: A differentiable renderer for image-based 3d reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li T</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen W</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin T.-Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maire</forename><forename type="middle">M</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>LMB * 14</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loper M</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>LMR * 15. 248. 2, 3, 6</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><forename type="middle">G</forename><surname>Milan A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">J</forename><surname>Black M</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
	<note>LRK * 17</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kinematic analysis of a three-degreesof-freedom in-parallel actuated manipulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee K.-M</forename><forename type="middle">K</forename><surname>Shah D</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="354" to="360" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hossain R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">K</forename><surname>Newell A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
	<note>OLPM * 18. international conference on 3D vision (3DV). 1, 2, 4</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">A</forename><surname>Osman A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10975" to="10985" />
		</imprint>
	</monogr>
	<note>PCG * 19</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">G</forename><surname>Derpanis K</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramakrishna V</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Skeletal graph based human pose estimation in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauswiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?ther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Loose-limbed people: Estimating 3d human pose and motion using nonparametric belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigal L</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun K</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei F</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
	<note>SXW * 18</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">J</forename><surname>Russell B</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laptev I</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
	<note>VCR * 18</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Drpose3d: depth ranking in 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><forename type="middle">M</forename><surname>Chen X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu W</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin L</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma L</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="978" to="984" />
		</imprint>
	</monogr>
	<note>WCL * 18</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10965" to="10974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu S.-C</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung T</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7760" to="7770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The stitched puppet: A graphical model of 3d human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuffi S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3537" to="3546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang Q</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
	<note>ZHS * 17</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1648" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
