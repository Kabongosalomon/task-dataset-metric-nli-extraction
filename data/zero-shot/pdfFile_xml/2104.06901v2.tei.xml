<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Interpretable Clauses Semantically using Pretrained Word Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><forename type="middle">Kumar</forename><surname>Yadav</surname></persName>
							<email>rohan.k.yadav@uia.no</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence Research</orgName>
								<orgName type="institution">University of Agder</orgName>
								<address>
									<postCode>4879</postCode>
									<settlement>Grimstad</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Jiao</surname></persName>
							<email>lei.jiao@uia.no</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence Research</orgName>
								<orgName type="institution">University of Agder</orgName>
								<address>
									<postCode>4879</postCode>
									<settlement>Grimstad</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole-Christoffer</forename><surname>Granmo</surname></persName>
							<email>ole.granmo@uia.no</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence Research</orgName>
								<orgName type="institution">University of Agder</orgName>
								<address>
									<postCode>4879</postCode>
									<settlement>Grimstad</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Goodwin</surname></persName>
							<email>morten.goodwin@uia.no</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence Research</orgName>
								<orgName type="institution">University of Agder</orgName>
								<address>
									<postCode>4879</postCode>
									<settlement>Grimstad</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Interpretable Clauses Semantically using Pretrained Word Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based on propositional logic, which has demonstrated competitive performance in many Natural Language Processing (NLP) tasks, including sentiment analysis, text classification, and Word Sense Disambiguation. To obtain human-level interpretability, legacy TM employs Boolean input features such as bag-of-words (BOW). However, the BOW representation makes it difficult to use any pre-trained information, for instance, word2vec and GloVe word representations. This restriction has constrained the performance of TM compared to deep neural networks (DNNs) in NLP. To reduce the performance gap, in this paper, we propose a novel way of using pre-trained word representations for TM. The approach significantly enhances the performance and interpretability of TM. We achieve this by extracting semantically related words from pre-trained word representations as input features to the TM. Our experiments show that the accuracy of the proposed approach is significantly higher than the previous BOW-based TM, reaching the level of DNN-based models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tsetlin Machine (TM) is an explainable pattern recognition approach that solves complex classification problems using propositional formulas <ref type="bibr" target="#b11">(Granmo, 2018)</ref>. <ref type="bibr">Text-(Berge et al., 2019)</ref>, numerical data- <ref type="bibr">(Abeyrathna et al., 2019)</ref>, and image classification  are recent areas of application. In Natural Language Processing (NLP), TM has provided encouraging trade-offs between accuracy and interpretability for various tasks. These include Sentiment Analysis (SA) <ref type="bibr" target="#b27">Saha et al., 2020)</ref>, Word Sense Disambiguation (WSD) , and novelty detection <ref type="bibr" target="#b4">(Bhattarai. et al., 2021)</ref>. Because TM NLP models employ bag-ofwords (BOW) that treat each word as independent features, it is easy for humans to interpret them. The models can be interpreted simply by inspecting the words that take part in the conjunctive clauses. However, using a simple BOW makes it challenging to attain the same accuracy level as deep neural network (DNN) based models.</p><p>A key advantage of DNN models is distributed representation of words in a vector space. By using a single-layer neural network, <ref type="bibr">Mikolov et al.</ref> introduced such a representation, allowing for relating words based on the inner product between word vectors <ref type="bibr" target="#b22">(Mikolov et al., 2013)</ref>. One of the popular methods is skip-gram, an approach that learns word representations by predicting the context surrounding a word within a given window length. However, skip-gram has the disadvantage of not considering the co-occurrence statistics of the corpus. Later, <ref type="bibr">Pennington et al. developed</ref> GloVe -a model that combines the advantages of local window-based methods and global matrix factorization <ref type="bibr" target="#b24">(Pennington et al., 2014)</ref>. The foundation for the above vector representation of words is the distributional hypothesis that states that "the word that occurs in the same contexts tend to have similar meanings" <ref type="bibr" target="#b13">(Harris, 1954)</ref>. This means that in addition to forming a rich high-dimensional representation of words, words that are closer to each other in vector space tend to represent similar meaning. As such, vector representations have been used to enhance for instance information retrieval <ref type="bibr" target="#b21">(Manning et al., 2008)</ref>, name entity recognition <ref type="bibr" target="#b34">(Turian et al., 2010)</ref>, and parsing <ref type="bibr" target="#b31">(Socher et al., 2013)</ref>.</p><p>The state of the art in DNN-based NLP has been advanced by incorporating various pre-trained word representations such as GloVe <ref type="bibr" target="#b24">(Pennington et al., 2014)</ref>, word2vec <ref type="bibr" target="#b22">(Mikolov et al., 2013)</ref>, and fasttext . Indeed, building semantic representations of the words has been demonstrated to be a vital factor for improved performance. Most DNN-based models utilize the pre-trained word representations to initialize their word embeddings. This provides them with additional semantic information that goes beyond a traditional BOW.</p><p>However, in the case of TM, such word representations cannot be directly employed because they consist of floating-point numbers. First, these numbers must be converted into Boolean form for TM to use, which may result in information loss. Secondly, replacing the straightforward BOW of a TM with a large number of floating-point numbers in fine-grained Boolean form would impede interpretability. In this paper, we propose a novel preprocessing technique that evades the above challenges entirely by extracting additional features for the BOW. The additional features are found using the pre-trained distributed word representations to identify words that enrich the BOW, based on cosine similarity. In this way, TM can use the information from word representations for increasing performance, and at the same time retaining the interpretability of the model.</p><p>The rest of the paper is organised as follows. We summarize related work in Section 2. The proposed semantic feature extraction for TM is then explained in Section 3. In Section 4, we present the TM architecture employing the proposed feature extension. We provide extensive experiment results in Section 5, demonstrating the benefits of our approach, before concluding the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Conventional text classification usually focuses on feature engineering and classification algorithms. One of the most popular feature engineering approaches is the derivation of BOW features. Several complex variants of BOW have been designed such as n-grams <ref type="bibr" target="#b35">(Wang and Manning, 2012)</ref> and entities in ontologies <ref type="bibr" target="#b6">(Chenthamarakshan et al., 2011)</ref>. Apart from BOW approaches, Tang et al. demonstrated a new mechanism for feature engineering using a time series model for short text samples <ref type="bibr" target="#b33">(Tang et al., 2020)</ref>. There are also several techniques to convert text into a graph and sub-graph <ref type="bibr" target="#b26">(Rousseau et al., 2015;</ref><ref type="bibr" target="#b20">Luo et al., 2017)</ref>. In general, none of the above methods adopt any pre-trained information, hence have inferior performance.</p><p>Deep learning-based text classification either depends on initializing models from pre-trained word representations, or on jointly learning both the word-and document level representations. Various studies report that incorporating such word representations, embedding the words, significantly enhances the accuracy of text classification <ref type="bibr" target="#b29">Shen et al., 2018a)</ref>. Another approach related to pre-trained word embedding is to aggregate unsupervised word embeddings into a document embedding, which is then fed to a classifier <ref type="bibr" target="#b16">(Le and Mikolov, 2014;</ref><ref type="bibr" target="#b32">Tang et al., 2015)</ref>.</p><p>Despite being empowered with world knowledge through pre-trained information, DNNs such as BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> and XLNet <ref type="bibr" target="#b38">(Yang et al., 2019)</ref> can be very hard to interpret. One interpretation approach is to use attention-based models, relying on the weights they assign to the inputs. However, more careful studies reveal that attention weights in general do not provide a useful explanation <ref type="bibr" target="#b2">(Bai et al., 2020;</ref><ref type="bibr" target="#b28">Serrano and Smith, 2019)</ref>. Researchers are thus increasingly shifting focus to other kinds of machine learning, with the TM being a recent approach considered to provide human-level interpretability <ref type="bibr" target="#b11">Granmo, 2018;</ref>. It offers a very simple model consisting of multiple Tsetlin Automata (TAs) that select which features take part in the classification. However, despite promising performance, there is still a performance gap to the DNN models that utilize pre-trained word embedding. Yet, several TM studies demonstrate high degree of interpretability through simple rules, with a marginal loss in accuracy <ref type="bibr" target="#b27">Saha et al., 2020)</ref>.</p><p>A significant reason for the performance gap between TM-based and state-of-the-art DNN-based NLP models is that TM operates on Boolean inputs, lacking a method for incorporating pre-trained word embeddings. Without pre-trained information, TMs must rely on labelled data available for supervised learning. On the other hand, incorporating high-dimensional Booleanized word embedding vectors directly into the TM would significantly reduce interpretability. In this paper, we address this intertwined challenge. We propose a novel technique that boosts the TM BOW approach, enhancing the BOW with additional word features. The enhancement consists of using cosine similarity between GloVe word representations to obtain semantically related words. We thus distill information from the pre-trained word representations for utilization by the TM. To this end, we propose two methods of feature extension: (1) using the k nearest words in embedding space and (2) using words within a given similarity threshold, measured as cosine angle (?). By adopting the two methods, we aim to reduce the current performance gap between interpretable TM and black-box DNN, by achieving either higher or similar accuracy, relying on pre-trained word embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Boosting TM BOW with Semantically Related Words</head><p>Here, we introduce our novel method for boosting the BOW of TM with semantically related words. The method is based on comparing pretrained word representations using cosine similarity, leveraging distributed word representation. There are various distributional representations of words available. These are obtained from different corpora, using various techniques, such as word2vec, GloVe, and fastText. We here use GloVe because of its general applicability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Feature Extraction from Distributed Word Representation</head><p>Distributed word representation does not necessarily derive word similarity based on synonyms but based on the words that appear in the same context. As such, the representation is essential for NLP because it captures the semantics interconnecting words. Our approach utilizes this property to expand the range of features that we can use in an interpretable manner in TM. Consider a full vocabulary W of m words, W = [w 1 , w 2 , w 3 . . . , w m ]. Further consider a particular sentence that is represented as a Boolean BOW X = [x 1 , x 2 , x 3 , . . . , x m ]. In a Boolean BOW, each element x r , r = 1, 2, 3, . . . , m, refers to a specific word w r in the vocabulary W . The element x r takes the value 1 if the corresponding word w r is present in the sentence and the value 0 if the word is absent. Assume that n words are present in the sentence, i.e., n of the elements in X are 1-valued. Our strategy is to extract additional features from these by expanding them using cosine similarity. To this end, we use a GloVe embedding of each present word w r , r ? {z|x z = 1, z = 1, 2, 3 . . . , m}. The embedding for word w r is represented by vector w e r ? d , where d is the dimensionality of the embedding (typically varying from 25 to 300).</p><p>We next introduce two selection techniques to expand upon each word:</p><p>? Select the top k most similar words,</p><p>? Select words up to a fixed similarity angle cos(?) = ?.</p><p>For example, let us consider two contexts: "very good movie" and "excellent film, enjoyable". Figs. 1 and 2 list similar words showing the difference between top k words and words within angle cos(?), i.e., ?. In what follows, we will explain how these words are found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Similar Words based on Top k Nearest Words</head><p>We first boost the Boolean BOW of the considered sentence by expanding X with (k ? 1) ? n semantically related words. That is, we add k ? 1 new words for each of the n present words. We do this by identifying neighbouring words in the GloVe embedding space, using cosine similarity between the embedding vectors. Consider the GloVe embedding vectors W e G = [w e 1 , w e 2 , . . . , w e m ] of the full vocabulary W . For each word w r from the sentence considered, the cosine similarity to each word w t , t = 1, 2, . . . , m, of the full vocabulary is given by Eq. <ref type="formula" target="#formula_0">(1)</ref>,</p><formula xml:id="formula_0">? t r = cos(w e r , w e t ) = w e r ? w e t ||w e r || ? ||w e t || .<label>(1)</label></formula><p>Clearly, ? t r is the cosine similarity between w e r and w e t . By calculating the cosine similarity of w r to the words in the vocabulary, we obtain m values: ? t r , t = 1, 2, . . . , m. We arrange these values in a vector ? r :</p><formula xml:id="formula_1">?r = [? 1 r , ? 2 r , . . . , ? m r ].<label>(2)</label></formula><p>The k elements from ? r of largest value are then identified and their indices are stored in a new set A r . Finally, a boosted BOW, referred to as X mod , can be formed by assigning element x t value 1 whenever one of the A r contains t, and 0 otherwise:</p><formula xml:id="formula_2">X mod = [x1, x2, x3, . . . , xm],<label>(3)</label></formula><formula xml:id="formula_3">xt = 1 ?r, t ? Ar 0 r, t ? Ar.</formula><p>In addition, the vocabulary size for a particular task/dataset can be changed accordingly, which is usually less than m. Note that implementationwise, the GloVe library provides the top k similar words of w r without considering the word w r itself, having similarity score 1. Hence, using the GloVe library, w r must also be added to the boosted BOW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Similar Words within Cosine Angle Threshold</head><p>Another approach to enrich the Boolean BOW of a sentence is thresholding the cosine angle. This is  different from the first technique because the number of additional words extracted will vary rather than being fixed. Whereas the first approach always produces k ? 1 new features for each given word, the cosine angle thresholding brings in all those words that are sufficiently similar. The cosine similarity threshold is given by ? = cos(?), where ? is the threshold for vector angle, while ? is the corresponding similarity score. As per Eq.</p><p>(2), we obtain ? r , which consists of the similarity scores of the given word w r in comparison to the m words in the vocabulary. Then, for each given word w r , the indices of those scores ? t r that are greater than or equal to ? (? t r ? ?) are stored in the set A r . Similar to the first technique, the words in W with the indices in A r are utilized to create X mod as given by Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tsetlin Machine-based Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tsetlin Machine Architecture</head><p>A TM is composed by TAs that operate with literals -Boolean inputs and their negations -to form conjunctions of literals (conjunctive clauses). A dedicated team of TAs builds each clause, with each input being associated with a pair of TAs. One TA controls the original Boolean input whereas the other TA controls its negation. The TA pair selects a combination of "Include" or "Exclude" actions, which decide the form of the literal to include or exclude in the clause.</p><p>Each TA decides upon an action according to its current state. There are N states per TA action, 2N states in total. When a TA finds itself in states 1 to N , it performs the "Exclude" action. When in states N + 1 to 2N , it performs the "Include" action. How the TA updates its state is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. If it receives Reward, the TA moves to a deeper state thereby increasing its confidence in the current action. However, if it receives Penalty, it moves towards the centre, weakening the action. It may eventually jump over the middle decision boundary, to the other action. It is through this game of TAs that the TM shapes the clauses into frequent and discriminative patterns. With respect to NLP, TM heavily relies on the Boolean BOW introduced earlier in the paper. We now make use of our proposed modified BOW X mod = [x 1 , x 2 , x 3 , . . . , x m ]. Let l be the number of clauses that represent each class of the TM, covering q classes altogether. Then, the overall pattern recognition problem is solved using l ? q clauses.</p><formula xml:id="formula_4">Each clause C j i , 1 ? j ? q, 1 ? i ? l of the TM is given by C j i = ? ? k?I j i x k ? ? ? ? ? k?? j i ?x k ? ? , where I j i and? j i are non-overlapping subsets of the input variable indices, I i j ,? i j ? {1, . . . , m}, I i j ?? i j = ?.</formula><p>The subsets decide which of the input variables take part in the clause, and whether they are negated or not. The indices of input variables in I i j represent the literals that are included as is, while the indices of input variables in? i j correspond to the negated ones. Among the q clauses of each class, clauses with odd indexes are assigned positive polarity (+) whereas those with even indices are assigned negative polarity (-). The clauses with positive polarity vote for the target class and those with negative polarity vote against it. A summation operator aggregates the votes by subtracting the total number of negative votes from positive votes, as shown in Eq. (4).</p><formula xml:id="formula_5">f j (X mod ) = ? l?1 i=1,3,... C j i (X mod )? ? l i=2,4,... C j i (X mod ).<label>(4)</label></formula><p>For q number of classes, the final output y is given by the argmax operator to classify the input based on the highest sum of votes,? = argmax j f j (X mod ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Distributed Word Representation in TM</head><p>Consider two contexts for sentiment classification: "Very good movie" and "Excellent film, enjoyable". Both contexts have different vocabularies but some of them are semantically related to each other. For example, "good" and "excellent" have similar semantics as well as "film" and "movie". Such semantics are not captured in the BOW-based input. However, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, adding words to the BOWs that are semantically related, as proposed in the previous section, makes distributed word representation available to the TM.</p><p>The resulting BOW-boosted TM architecture is shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Here each input feature is first expanded using the GloVe representation, adding semantically related words. Each feature is then transferred to its corresponding TAs, both in original and negated form. Each TA, in turn, decides whether to include or exclude its literal in the clause by taking part in a decentralized game. The actions of each TA is decided by its current state and updated by the the feedback it receives based on its action. As shown in the figure, the TA actions produce a collection of conjunctive clauses, joining the words into more complex linguistic patterns.</p><p>There are two types of feedback that guides the TA learning. They are Type I feedback and Type II feedback, detailed in <ref type="bibr" target="#b11">(Granmo, 2018)</ref>. Type I feedback is triggered when the ground truth label is 1, i.e., y = 1. The purpose of Type I feedback is to include more literals from the BOW to refine the clauses, or to trim them by removing literals. The balance between refinement and trimming is controlled by a parameter called specificity, s. Type I feedback guides the clauses to provide true positive output, while simultaneously controlling overfitting by producing frequent patterns. Conversely, Type II feedback is triggered in case of false positive output. Its main aim is to introduce zero-valued literals into clauses when they give false positive output. The purpose is to change them so that they correctly output zero later in the learning process.</p><p>Based on these feedback types, each TA in a clause receives Reward, Penalty or Inaction. The overall learning process is explained in detail by . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>In this section, we evaluate our TM-based solution with the input features enhanced by distributed word representation. Here we use Glove pretrained word vector that is trained using CommonCrawl with the configuration of 42B tokens, 1.9M vocab, uncased, and 300d vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We have selected various types of datasets to investigate how broadly our method is applicable: R8 and R52 of Reuters, Movie Review (MR), and TREC-6. ? Reuters 21578 dataset include two subsets: R52 and R8 (all-terms version). R8 is divided into 8 sections while there are 52 categories in R52. ?MR is a movie analysis dataset for binary sentiment classification with just one sentence per review <ref type="bibr" target="#b23">(Pang and Lee, 2005)</ref>. In this study, we used a training/test split from <ref type="bibr" target="#b32">(Tang et al., 2015)</ref> 1 .</p><p>?TREC-6 is a question classification dataset <ref type="bibr" target="#b17">(Li and Roth, 2002)</ref>. The task entails categorizing a query into six distinct categories (abbreviation, description, entity, human, location, numeric value).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TM Parameters</head><p>A TM has three parameters that must be initialized before training a model: number of clauses l, voting target T , and specificity s. We configure these parameters as follows. For R8, we use 2,500 clauses, a threshold of 80, and specificity 9. The vocabulary size is 5,000. For R52, we employ 1,500 clauses, the voting target is 80, and specificity is 9.</p><p>Here, we use a vocabulary of size 6,000. For MR, the number of clauses is 3,000, the voting target is 80, and specificity is 9, with a vocabulary of size 5,000. Finally, for TREC, we use 2,000 clause, a voting target of 80, and specificity 9, with vocabulary size 6,000. These parameters are kept static as we explore various k and ? values for selecting similar words to facilitate comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance When Using Top k Nearest Neighbors</head><p>Here, we demonstrate the performance on each of the datasets, exploring the effect of different k-values, i.e., 3, 5 and 10. The performance of the proposed technique for selected datasets with various values of k is compared in <ref type="table">Table 1</ref>. It can be seen that by using feature extension, performance is significantly enhanced. Both k = 3 and k = 5 outperform the simple BOW (k = 0). However, for this particular dataset, k = 10 performs poorly because extending each word to its 10 nearest neighbors includes many unnecessary contexts that have no significant impact on the classification. In terms of accuracy, k = 5 performs best for the R8 dataset. For the R52 dataset, the feature extension with k = 5 and k = 10 performs poorly compared to using k = 0 and k = 3. Here, k = 3 is the best-performing parameter. The improvement obtained by moving from a simple BOW to a BOW enhanced with semantically similar features is obvious in the case of the R52 dataset. Similarly, in the case of the TREC dataset, the performance of simple BOW (k = 0) is markedly outperformed by the feature extension techniques for all the tested k-values, with k = 5 and k = 10 being good candidates. The advantage of k = 10 over k = 5 is that k = 10 reaches its peak accuracy in an earlier epoch. Lastly, the performance of the MR is again clear that the feature extension technique outper- forms the simple BOW (k = 0) with a high margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance When Using Neighbors Within a Similarity Threshold</head><p>This section demonstrates the performance of our BOW enhancement approach when using various similarity thresholds ? for feature extension. Here, ? refers to the cosine similarity between a word in the BOW and a target word from the overall vocabulary. Again, similarity is measured in the GloVe embedding space as the cosine of the angle ? between the embedding vectors compared, cos(?).</p><p>For ?, we here explore the values 0.5, 0.6, 0.7, 0.8, and 0.9, whose corresponding angles are 60 ? , 53.13 ? , 45.57 ? , 36.86 ? , and 25.84 ? , respectively. The performance of the various ?-values for the selected dataset is shown in <ref type="table" target="#tab_3">Table 2</ref>. For R8 dataset, feature extension using ? = 0.7, ? = 0.8, and ? = 0.9 outperforms the simple BOW (? = 0) where ? = 0.7 being the best. In case of the R52 dataset, all of the investigated ?-values outperform the simple BOW (? = 0) where ? = 0.5 and ? = 0.8 performs the best. Similar trend is observed in case of TREC and MR dataset where feature extension outperforms the simple BOW.</p><p>In most of the cases, however, a too strict similarity threshold ? tends to reduce performance be-  cause fewer features are added to the BOW. Even though using a looser similarity score thresholds also introduces unnecessary features, these do not seem to impact the formation of accurate clauses. Overall, our experiments show that using ?-values from 0.5 to 0.7 peaks performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison with Baselines</head><p>We here compare our proposed model with selected text classification-and embedding methods. We have selected representative techniques from various main approaches, both those that leverage similar kinds of pre-trained word embedding and those that only use BOW. The selected baselines are: ?TF-IDF+LR: This is a bag-of-words model employing Term Frequency-Inverse Document Frequency (TF-IDF) weighting. Logistic Regression is used as a softmax classifier. ?CNN:</p><p>The CNN-baselines cover both initialization with random word embedding (CNN-rand) as well as initialization with pretrained word embedding (CNNnon-static) <ref type="bibr" target="#b15">(Kim, 2014)</ref>. ? LSTM: The LSTM model that we employ here is from <ref type="bibr" target="#b18">(Liu et al., 2016)</ref>, representing the entire text using the last hidden state. We tested this model with and without pre-trained word embeddings. ? Bi-LSTM: Bidirectional LSTMs are widely used for text classification. We compare our model with Bi-LSTM fed with pre-trained word embeddings. ?PV-DBOW: PV-DBOW is a paragraph vector model where the word order is ignored. Logistic Regression is used as a softmax classifier <ref type="bibr" target="#b16">(Le and Mikolov, 2014)</ref>. ? PV-DM: PV-DM is also a paragraph vector model, however with word ordering taken into account. Logistic Regression is used as a softmax classifier <ref type="bibr" target="#b16">(Le and Mikolov, 2014)</ref>. ?fastText: This baseline is a simple text classification technique that uses the average of the word embeddings provided by fastText as document embedding. The embedding is then fed to a linear classifier . We evaluate both the use of uni-grams and bigrams. ? SWEM : SWEM applies simple pooling techniques over the word embeddings to obtain a document embedding <ref type="bibr" target="#b30">(Shen et al., 2018b)</ref>.</p><p>?Graph-CNN-C: A graph CNN model uses convolutions over a word embedding similarity graph <ref type="bibr" target="#b7">(Defferrard et al., 2016)</ref>, employing a Chebyshev filter. ?S 2 GC: This technique uses a modified Markov Diffusion Kernel to derive a variant of Graph Convolutional Network (GCN) <ref type="bibr" target="#b39">(Zhu and Koniusz, 2021</ref>). ?LguidedLearn: It is a label-guided learning framework for text classification. This technique is applied to BERT as well , which we use for comparison purposes here.</p><p>?Feature Projection (FP): It is a novel approach to improve representation learning through feature projection. Existing features are projected into an orthogonal space <ref type="bibr" target="#b25">(Qin et al., 2020)</ref>.</p><p>From <ref type="table" target="#tab_5">Table 3</ref>, we observe that the TM approaches that employ either of our feature extension techniques outperform several word embedding-based Logistic Regression approaches, such as PV-DBOW, PV-DM, and fastText. Similarly, the legacy TM outperforms sophisticated models like CNN and LSTM based on randomly initialized word embedding. Still, the legacy TM falls of other models when they are initialized by pre-trained word embeddings. By boosting the Boolean BOW with semantically similar features using our proposed technique, however, TM outperforms LSTM (pretrain) on the R8 dataset and performs similarly on R52 and MR. In addition to this, our proposed approach achieves quite similar performance compared to BERT, even though BERT has been pre-trained on a huge text corpus. However, it falls slightly short of sophisticated finetuned models like Lguided-BERT-1 and Lguided-BERT-3. Overall, our results show that our proposed feature extension technique for TMs significantly enhances accuracy, reaching state of the art accuracy. Importantly, this accuracy enhancement does not come at the cost of reduced interpretability, unlike DNNs, which we discuss below. The state of the art for the TREC dataset is different from the other three datasets, hence we report results separately in <ref type="table">Table 4</ref>. These results clearly show that although the basic TM model does not outperform the recent DNN-and transformer-based models, the feature-boosted TM outperforms all of those models except understandably BAE:BERT <ref type="bibr" target="#b10">(Garg and Ramakrishnan, 2020)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Interpretation</head><p>The proposed feature extension-based TM does not only impact accuracy. Perhaps surprisingly, our proposed technique also simplify the clauses that the TM produces, making them more meaningful in a semantic sense. To demonstrate this property, let us consider two samples from the MR dataset: S 1 ="the cast is uniformly excellent and relaxed" and S 2 ="the entire cast is extraordinarily good". Let the vocabulary, in this case, be <ref type="bibr">[cast, excellent, relaxed, extraordinarily, good, bad, boring, worst]</ref> as shown in <ref type="figure">Fig. 6</ref>.</p><p>the cast is uniformly excellent and relaxed the entire cast is extraordinarily good the cast is uniformly excellent/good and relaxed the entire cast is extraordinarily good/excellent TM with a simple BOW TM with a feature extended BOW <ref type="figure">Figure 6</ref>: Clause learning semantic for multiple examples compared to simple BOW based TM.</p><p>As we can see, that the TM initialized with normal BOW uses two separate clauses to represent two examples. However, augmenting feature on TM uses only one clause that learns the semantic for multiple examples.This indeed makes interpre-  <ref type="bibr" target="#b9">(Dragos , et al., 2021)</ref> 87.20 TM 88.05? 1.52 TM with k 89.82? 1.18 TM with ? 90.04? 0.94 <ref type="table">Table 4</ref>: Comparison of feature extended TM with the state of the art for TREC. Reported accuracy of TM is the mean of last 50 epochs of 5 independent experiments with their standard deviation.</p><p>tation of TM more powerful and meaningful as compared to simple BOW based TM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we aimed to enhance the performance of Tsetlin Machines (TMs) by introducing a novel way to exploit distributed feature representation for TMs. Given that a TM relies on Bag-of-words (BOW), it is not possible to introduce pre-trained word representation into a TM directly, without sacrificing the interpretability of the model. To address this intertwined challenge, we extended each word feature by using cosine similarity on the distributed word representation. We proposed two techniques for feature extension: (1) using the k nearest words in embedding space and (2) including words within a given cosine angle (?). Through this enhancement, the TM BOW can be boosted with pre-trained world knowledge in a simple yet effective way. Our experiment results showed that the enhanced TM not only achieve competitive accuracy compared to state of the art, but also outperform some of the sophisticated deep neural network (DNN) models. In addition, our BOW boosting also improved the interpretability of the model by increasing the scope of each clause, semantically relating more samples. We thus believe that our proposed approach significantly enhance the TM in the accuracy/interpretability continuum, establishing a new standard in the field of explainable NLP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Similar words for an example "very good movie" using 300d GloVe word representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>A TA with two actions: "Include" and "Exclude".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>(a) BOW input representation without distributed word representation. (b) BOW input using similar words based on distributed word representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Architecture of TM using modified BOW based on word similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of feature extended TM with several parameters for ?.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of feature extended TM with the state of the art for R8, R52 and MR. Reported accuracy of TM is the mean of last 50 epochs of 5 independent experiments with their standard deviation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/mnqu/PTE/tree/master/data/mr.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abeyrathna</forename><surname>Kuruge Darshana</surname></persName>
		</author>
		<imprint>
			<pubPlace>Ole-Christoffer</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A scheme for continuous input to the Tsetlin machine with applications to forecasting disease outbreaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances and Trends in Artificial Intelligence. From Theory to Practice</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="564" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Why is attention not so interpretable. arXiv: Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using the tsetlin machine to learn human-interpretable rules for highaccuracy text categorization with medical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole-Christoffer</forename><surname>Geir Thore Berge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Tor Oddbj?rn Tveit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viggo Matheussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="115134" to="115146" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring the novelty of natural language text using the conjunctive clauses of a tsetlin machine text classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bimal</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ole-Christoffer Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Agents and Artificial Intelligence</title>
		<meeting>the 13th International Conference on Agents and Artificial Intelligence</meeting>
		<imprint>
			<publisher>SciTePress</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="410" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Concept labeling: Building text classifiers with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Vijil Chenthamarakshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">D</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1225" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: Human Language Technologies</title>
		<meeting><address><addrLine>Minneapolis, Minnesota. ACL</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Question classification using interpretable tsetlin machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Dragos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Nicolae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop of Machine Reasoning. ACM International Conference on Web Search and Data Mining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bae: Bert-based adversarial examples for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutham</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The tsetlin machinea game theoretic bandit driven approach to optimal pattern recognition with propositional logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole-Christoffer</forename><surname>Granmo</surname></persName>
		</author>
		<idno>abs/1804.01508</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole-Christoffer</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sondre</forename><surname>Glimsdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">W</forename><surname>Omlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geir Thore</forename><surname>Berge</surname></persName>
		</author>
		<title level="m">The convolutional tsetlin machine. arXiv, 1905</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9688</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. WORD</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
			<pubPlace>Short Papers; Valencia, Spain. ACL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2873" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Label-guided learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
		<idno>abs/2002.10772</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bridging semantics and syntax with graph algorithms -state-of-the-art of extracting biomedical relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?zlem</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="160" to="178" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Nevada, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Michigan, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature projection for improved text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Online. ACL</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8161" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text categorization as a graph classification problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanouil</forename><surname>Kiagias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Long Papers)</title>
		<meeting><address><addrLine>Beijing, China. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1702" to="1712" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mining interpretable rules for sentiment and semantic relation analysis using tsetlin machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupsa</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole-Christoffer</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Goodwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence XXXVII</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofia</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<title level="m">Is attention interpretable? In ACL</title>
		<meeting><address><addrLine>Florence, Italy. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2931" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Melbourne, Australia. ACL</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="450" />
			<date type="published" when="2018" />
			<publisher>Long Papers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pte: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;15</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;15<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Enriching feature engineering for short text samples by language time series analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Blincoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kempa-Liehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPJ Data Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="59" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
			<date type="published" when="2012" />
			<publisher>Short Papers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Humanlevel interpretable learning for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Rohan Kumar Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole-Christoffer</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Interpretability in word sense disambiguation using tsetlin machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohan Kumar Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lei Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Ole-Christoffer Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Agents</title>
		<meeting>the 13th International Conference on Agents</meeting>
		<imprint>
			<publisher>INSTICC, SciTePress</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="402" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simple spectral graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
