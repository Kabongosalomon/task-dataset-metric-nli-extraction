<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Gesture Recognition in Robot-assisted Surgery with Reinforcement Learning and Tree Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
						</author>
						<title level="a" type="main">Automatic Gesture Recognition in Robot-assisted Surgery with Reinforcement Learning and Tree Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Surgical gesture recognition</term>
					<term>Deep reinforce- ment learning in robotics</term>
					<term>Tree search</term>
					<term>Robotic surgery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic surgical gesture recognition is fundamental for improving intelligence in robot-assisted surgery, such as conducting complicated tasks of surgery surveillance and skill evaluation. However, current methods treat each frame individually and produce the outcomes without effective consideration on future information. In this paper, we propose a framework based on reinforcement learning and tree search for joint surgical gesture segmentation and classification. An agent is trained to segment and classify the surgical video in a human-like manner whose direct decisions are re-considered by tree search appropriately. Our proposed tree search algorithm unites the outputs from two designed neural networks, i.e., policy and value network. With the integration of complementary information from distinct models, our framework is able to achieve the better performance than baseline methods using either of the neural networks. For an overall evaluation, our developed approach consistently outperforms the existing methods on the suturing task of JIGSAWS dataset in terms of accuracy, edit score and F1 score. Our study highlights the utilization of tree search to refine actions in reinforcement learning framework for surgical robotic applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Robot-assisted surgery facilitates surgeons to perform a variety of complex operations in minimally invasive surgery and improves the precision of surgical manipulation in the meanwhile. For example, the da Vinci surgical system is designed to assist certain surgery and widely used in nowadays clinical procedures, with a large amount of video visual and kinematic data recorded <ref type="bibr" target="#b0">[1]</ref>. Towards an intelligent operating theatre, developing data-driven methods which can learn to recognize the surgical gestures is a fundamental task. The goal is to segment a sequence of surgical operations given in video format, i.e., classifying each frame into a specific type of surgical gesture, such as positioning needle, orienting needle, and pulling the suture, etc.</p><p>Automatically recognizing the robotic gestures in surgical process plays an important role for surgery surveillance <ref type="bibr" target="#b1">[2]</ref>, automatic skill assessment <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, and surgery training <ref type="bibr" target="#b5">[6]</ref>. Identifying which action is being operated is also crucial for developing the context-aware theatre <ref type="bibr" target="#b6">[7]</ref> and autonomous robotic surgery systems <ref type="bibr" target="#b7">[8]</ref>. These applications help reduce the mental cognitive workload of surgeons and improve reliability and safety of the robot-assisted surgery. However, the development of automatic surgical gesture recognition method is challenging, as the gesture usually includes complex multi-step actions and sometimes intricate maneuvers <ref type="bibr" target="#b8">[9]</ref>. Also, variability in users' manipulation habits and proficiency makes the problem even more complicated.</p><p>Some studies have been conducted for surgical recognition tasks, ranging from phase recognition to fine-grained gesture and action recognition. Classical approaches have been based on statistical models and unsupervised learning methods, e.g., Hidden Markov Model (HMM) was exploited for automatic phase recognition <ref type="bibr" target="#b9">[10]</ref>. A sparse HMM was proposed to improve the expressive power of discrete or Gaussian observations <ref type="bibr" target="#b10">[11]</ref>. Fusing video and kinematic data, a combined Markov/semi-Markov conditional random field (MsM-CRF) model was exploited for joint segmentation and recognition of surgical gestures <ref type="bibr" target="#b11">[12]</ref>. In <ref type="bibr" target="#b12">[13]</ref>, a temporal CRF model was combined with a frame-level representation based on discriminative sparse coding. However, these methods produce suboptimal outcomes because of either loss of long-term dependency or severe over-segmentation problems <ref type="bibr" target="#b13">[14]</ref>. Gaussian Mixture Model (GMM) initialized by the k-means clustering algorithm was used to estimate segmentation points <ref type="bibr" target="#b14">[15]</ref>. A hierarchical Dirichlet Process GMM was proposed to learn the segmentation criteria <ref type="bibr" target="#b15">[16]</ref>. To avoid tedious parameter tuning, a GMM-based algorithm was designed under weak supervision <ref type="bibr" target="#b0">[1]</ref>. However, the manual feature extraction used in these methods is relatively subjective with limited representation capability.</p><p>For better feature extraction, methods using deep learning (DL) techniques have achieved impressive results in surgical recognition. Long Short Term Memory (LSTM) network was used to maintain the temporal information among frames <ref type="bibr" target="#b16">[17]</ref>. Designed to better extract low-level features, segmental spatiotemporal convolutional neural network (Seg-ST-CNN) outperformed temporal models like LSTM <ref type="bibr" target="#b17">[18]</ref>. In <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, Temporal Convolutional Network (TCN) learned a hierarchy of intermediate feature representations and formed an encoder-decoder framework. Combining a deep ResNet <ref type="bibr" target="#b20">[21]</ref> with an RNN network, recurrent convolutional network was proposed to identify the surgical phase from videos <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. These methods concentrate on frame-wise accuracy, however, segment-level performance is not fully focused which is limited by their training loss functions <ref type="bibr" target="#b13">[14]</ref>.</p><p>Recently, deep reinforcement learning (RL) was applied to gesture recognition. An agent learned its policy by interacting with the environment, i.e., surgical data, and achieved a stateof-the-art segment-level performance <ref type="bibr" target="#b13">[14]</ref>. Since the agent relies on a neural network to produce decisions, the network confusion <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref> problem remains unsolved for some rarely appeared gestures. Considering information from different stages might be a promising solution to this problem. This group of works target at generating more robust decisions than direct outputs for sequential decision problems. In <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, the capacity of AphaGo was enhanced by a large margin over the raw network, since Monte Carlo tree search (MCTS) <ref type="bibr" target="#b26">[27]</ref> provides a look-ahead approach to bring statistics from future states. Although MCTS was designed for two-player games originally, Schadd et al. raised singleplayer MCTS, where the average and best scores were both considered <ref type="bibr" target="#b27">[28]</ref>.</p><p>In this paper, we come up with a novel and generic reinforcement learning framework for surgical video segmentation and classification through tree search. The proposed method is a search-based algorithm which produces current decisions by looking ahead into the future. We claim that it is only necessary to search for the frames that the policy agent produces uncertain decisions, which saves a lot of computing time. Hence, we design a gateway component to decide when to think carefully with the value network. More specifically, if the agent feels very confident about the output decisions, i.e., the maximum probability is above a threshold, decisions from a policy network are directly used to segment the input sequences; if not, tree search is invoked by considering the outputs from a policy and a value network. Our main contributions are summarized as follows:</p><p>? We propose a novel reinforcement learning based framework which performs in a human-like manner to generate decisions by jointly leveraging a policy network and a value network.</p><p>? We present a new tree search algorithm for decision refinement by potentially considering the future frames. This is crucial for accurate prediction of surgical gesture in online mode.</p><p>? We evaluate our proposed method on the public robotic surgery dataset of JIGSAWS. Our agent outperforms state-of-the-art results on surgical gesture recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARY</head><p>1) Reinforcement learning: RL is about an agent interacting with the environment and learning to behave tactfully according to rewards in essence. This sequential decision problem can be formalized into a finite Markov Decision Process (MDP) <ref type="bibr" target="#b28">[29]</ref> where the sets of states, actions, and rewards have a finite number of elements. At each step t, the agent faces some state S t ? S and selects an action A t ? A(s) based on S t . One time step later, the agent receives a numerical reward R t+1 ? R ? R. Then, the environment transfers to a new state S t+1 at probability p(S t+1 |S t , A t ) and the agent wants to learn the optimal policy ?(S t ) which is a distribution towards action set. The trajectory of agent is S 0 , A 0 , R 1 , S 1 , A 1 , R 2 , ... The goal is to maximize the accumulated rewards in an episode.</p><p>2) Monte Carlo tree search: MCTS grows a search tree by asserting newly gained information asymmetrically <ref type="bibr" target="#b29">[30]</ref>. It includes four steps in each simulation process <ref type="bibr" target="#b26">[27]</ref>: selection, expansion, simulation, and backup. Firstly, the most urgent node is selected by a tree policy. Then, one or more child nodes are expanded by the selected node. Thirdly, a complete episode is run under rollout policy from one of its newlyadded child nodes. Eventually, the simulation outcome is used to update the statistics of its ancestors. After a certain number of simulations, the next action can be chosen according to the statistics from the simulated outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>In this section, we formulate our problem using an MDP model <ref type="bibr" target="#b13">[14]</ref>, in which the agent regards the visual features as environment states and surgical gesture recognition as decisions. The <ref type="figure" target="#fig_2">Fig. 1</ref> depicts an overview of our framework which is mainly consisted of a policy network and a value network. These two networks work together in a hierarchical way. We will describe each component in details, and particularly for the proposed tree search algorithm which is key to our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem setup</head><p>To segment and recognize the surgical gestures from the video data, the agent starts at the beginning of the video sequence {x t } T t=0 and moves towards the end of the sequence. Based on the observation at some position, i.e., the environment state, the agent selects an action a i = (k, c) consisting of a step size k and a gesture class c for the frames stepped over. The length of k can choose a small step k s or a large step k l based on the confidence in the gesture prediction to give. When the agent goes over all the data sequence {x t } T t=0 , the episode ends and this trajectory can be evaluated by user-preferred criteria.</p><p>This problem can also be regarded as a path-finding problem that the agent wants to give a sequence of actions to get the maximum scores given pre-defined criteria. At each state, there are b actions to be chosen from and the complexity is exponential. In fact, with the ground truth label sequence {y t } T t=0 , the optimal path is definite. In this paper, the proposed approach helps the agent consider the best path to go ahead.</p><p>A high-quality feature base is crucial for taking good advantage of our method. In this regard, we exploit the highlevel representative TCN features as the input of our neural networks <ref type="bibr" target="#b18">[19]</ref>. Specifically, the video data are first processed by a spatial CNN <ref type="bibr" target="#b17">[18]</ref> to generate raw features. Then, we retrain the TCN model with modifying the original loss to the weighted cross-entropy loss <ref type="bibr" target="#b13">[14]</ref>. Finally, the TCN features {s t tcn } can be obtained from the last hidden layer of our well-trained TCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Policy network</head><p>The policy network takes input as the environment state and outputs a distribution over action space <ref type="bibr" target="#b13">[14]</ref>. Then, the  observation at frame t is defined as</p><formula xml:id="formula_0">s t p := (s t tcn , s t+ks tcn , s t+k l tcn , s trans , s hot ),<label>(1)</label></formula><p>where s trans are probabilities from a statistical language model <ref type="bibr" target="#b30">[31]</ref> and s hot is a one-hot vector indicating the gesture class given by the last action. The reward for each action is given by</p><formula xml:id="formula_1">r(s t p , (k, c)) := ?k ? t+k?1 t =t 1(y t = c),<label>(2)</label></formula><p>where ? is a weight parameter. This reward definition incites larger steps while penalizes wrong predictions. Then, the policy is optimized by using the Trust Region Policy Optimization <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Value network</head><p>The value network takes as an input the representation of the environment state and the action choice of the agent, and outputs an advantage score of each action. The observation at frame t is defined as</p><formula xml:id="formula_2">s t v := (s t tcn , s t hot ),<label>(3)</label></formula><p>where s t hot is a one-hot vector indicating the conjectured gesture class of the frame t. Note that s t hot and s hot share the same kind of representation. Here, we regard {s t v } as a new sequence data to jointly consider the surgical data and candidate gesture categories, which is then sent to a recurrent neural network. The reward for each action is</p><formula xml:id="formula_3">r(s t v , (k, c)) := t+k?1 t =t 1(y t = c) ? t+k?1 t =t 1(y t = c). (4)</formula><p>Thus, the range of the global mean reward is [?1, 1].</p><p>The sketchy structure of the value network is demonstrated in <ref type="figure" target="#fig_2">Fig. 1</ref>. The input layer of the value network receives a sequence vector by concatenating TCN feature and its possible class. It connects to one hidden layer of LSTM and a fully connected layer, both with 32 neurons. In the output layer, we use a Tanh nonlinearity to produce a scalar.</p><p>Since the ground truth label sequence is available, we can directly employ supervised learning approach to train the network. The expert experiences are created with the ground truth labels. To be mentioned, we utilize training data augmentation to alleviate overfitting, in which the agent generates non-expert predictions using random strategy.</p><p>Until an episode ends, the overall mean rewardr * for each frame t is obtained and the data is stored as (s t v ,r * ). Randomly chosen sequences with length K are sent to LSTM <ref type="bibr" target="#b21">[22]</ref> and the mean square loss function is optimized by Adam optimizer <ref type="bibr" target="#b32">[33]</ref>. To establish longer dependence over each frame, the model is trained with increasing K <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Tree search algorithm</head><p>We design a single-player tree search algorithm to fuse the outputs from a policy network and a value network. Since the environment behavior is deterministic, the constructed search tree does not contain the nodes for the environment. The purpose of the tree search is to return the best possible path starting from the current frame and help the agent make decisions. Since the prior probability of each child node is conditional on its parent node, a tree structure is applied to preserve this information. To facilitate the retrieval of the best path in the tree, we design a tree structure that each parent node preserves the maximum action value among all its child nodes. It is trivial that the node with maximum value can be chosen greedily from the root node.</p><p>The prior probabilities of child nodes are computed using a policy network:</p><formula xml:id="formula_4">p(s) = f p (s t p ).<label>(5)</label></formula><p>Due to the step length k of each action, we use the average of the k sequential outputs from the value network f v as the estimation of the global mean reward, i.e.,</p><formula xml:id="formula_5">r(s) = 1 k f v ([s t v : s t+k v ]),<label>(6)</label></formula><p>where [s t v : s t+k v ] is the observation sequence from current frame to the next kth frame. If s is a leaf node, its state value is calculated by v(s) = mean{r(s )}, for s in the path s 0 ? s,</p><p>where s 0 is the root node. Each leaf node re-evaluates the global mean reward by averaging all the exiting estimations <ref type="figure">Fig. 2</ref>. Tree search pipeline. a. Each search traverses the tree following the selection criterion from the root node. b. The leaf node s L expands all its child nodes and the prior probabilities are delivered to child nodes; each child node is evaluated to store its state value. c. All action values of the nodes in this search trajectory is updated to reflect the maximum value in its branches. from the same path because average lowers the variance. Because of the tree property stated above, the action value of each node is equivalent to</p><formula xml:id="formula_7">... ... s 0 ... max(Q + U ) max(Q + U ) s L ... p 1 p i a. Select b. Expand and evaluate c. Backup Repeat . . . s 1 L {v i } v 1 v i ... ... s 0 ... s L . . . ... ... s 0 ... s L . . . maxQ maxQ maxQ ... s i L ... ... v n s n L p n</formula><formula xml:id="formula_8">Q(s, a) = max s |s,a?s v(s ),<label>(8)</label></formula><p>where s, a?s indicates that a path eventually reached a leaf node s after taking an action a from s. Actually, each node stores the estimation of the optimal path through itself. The overall tree search process is illustrated as follows. In <ref type="figure">Fig. 2</ref>-a, the selection starts from the root node s 0 and ends until a leaf node s L is encountered. At each of these steps, an action with the maximum sum of Q(s, a) and upper confidence bound (UCB) is selected: a = arg max a (Q(s, a) + U (s, a)) .</p><p>The policy distribution from the policy network is added into U (s, a) to help narrow the search space,</p><formula xml:id="formula_10">U (s, a) = c puct (1 + p(s, a)) b N (s, b) 1 + N (s, a) ,<label>(10)</label></formula><p>where c puct is a constant determining the level of exploration and N (s, a) is the visit count of edge (s, a). In this phase, the optimal information is used to guide the search due to the reachability of each state and UCB enforces the agent to consider the rarely visited nodes. In <ref type="figure">Fig. 2-b</ref>, all child nodes {s i L } of s L is expanded in the tree. The policy network evaluates s L to deliver prior probability p i to each child node s i L by Eq. <ref type="formula" target="#formula_4">(5)</ref>, while state values of the newly expanded nodes are generated using Eq. (6) and Eq. <ref type="bibr" target="#b6">(7)</ref>. Note that the value network is utilized in a batch style and actions with the same step length are included in the same batch. After this phase, s i L stores the evaluation v i of itself. In <ref type="figure">Fig. 2-c</ref>, the action values of s L is given by:</p><formula xml:id="formula_11">Q(s L , a) = max v(s i L ),<label>(11)</label></formula><p>and its ancestor nodes with each step j &lt;= L are updated in a backward pass by:</p><formula xml:id="formula_12">Q(s j , a j ) = max sj+1|sj ,aj ?sj+1 Q(s j+1 , a j+1 ),<label>(12)</label></formula><p>and the visit count N (s, a) of the each edge in this path is also increased. When a certain number of simulations are implemented, the action in the first edge with most visit times from s 0 is chosen. If more than one action have the same maximum visit count, the edge with the maximum action value is chosen.</p><p>In our search algorithm, we do not choose a big simulation times to make U (s, a) approach zero because the dependence of frames decays with the increasing distance. Thus, we let p(s, a) always play a role in the selection phase and help prune away some inferior branches. Furthermore, our method ensures that each leaf node is judged by prior probability and value together. The pseudo code of our method describes the overall tree search process, as shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate our proposed deep reinforcement learning method for surgical gesture recognition on the popular public JIGSAWS <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b34">[35]</ref> dataset. We design experiments to answer the following two questions: 1) Does the tree search algorithm produce a better testing outcome? 2) What role does each component in the framework play in the performance boost?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We use the JIGSAWS, a public dataset captured by the da Vinci Surgical System (dVSS, Intuitive Surgical Inc., CA, USA). It consists of video and kinematic data from eight surgeons in three different levels of robotic surgical experience. The manual annotations describing the ground truth gesture classes for each frame are available. We use the video data from the suturing task with total 10 different gestures, i.e., reaching for the needle with right hand (G1), positioning the tip of the needle (G2), etc. There are 39 sequences in total and lengths are a few minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metrics</head><p>We examine three different evaluation metrics for different approaches: (i) Accuracy, i.e., the percentage of correctly recognized frames in a video. (ii) Edit score <ref type="bibr" target="#b17">[18]</ref>, the normalized  <ref type="bibr" target="#b19">[20]</ref> with different thresholds. It penalizes over-segmentation errors while does not for minor temporal shifts between the predictions and ground truth. Under this criterion, each predicted gesture segment is considered true positive if its Intersection over Union (IoU) towards the corresponding ground truth is above the threshold and vice versa. Then, F1 score is computed using precision and recall by: F1 = 2 prec * recall prec+recall .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>We follow the leave-one-user-out (LOUO) setup for crossvalidation, which is the same as used in <ref type="bibr" target="#b5">[6]</ref>. And one model is trained for each experiment with one left-out user. The final evaluation metrics are calculated for each video in the test dataset and then averaged. A TCN is trained to generate features for the policy and value network.</p><p>As a baseline for the proposed approach, we re-implement the method in <ref type="bibr" target="#b13">[14]</ref> as policy network with one minor change, where actions are chosen deterministically, i.e., the action with maximum probability is executed, rather than using a stochastic strategy in testing stages. As for the value network, we also adopt the same parameters for (k s , k l ) and ? as the policy network, i.e., (4, 21) and 0.1. As there are 10 gesture classes in this dataset, the search tree expands 20 child nodes each time. The length K of training sequences increases from 20 to 100 with an interval of 10. Half of the training data for the value network are generated using random actions. We set the threshold of conducting tree search as 0.98. The constant c puct and search times are set to 1.5 and 10 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental results and ablation analysis</head><p>We perform experiments to test the abilities of the policy network, value network, and our combined network. The action with the biggest probability output by the policy network is directly used to segment an episode. Note that the tree search algorithm cannot be carried out with only the policy network because its output is conditional probability precisely. By contrast, the value network gives the estimated rewards for all the elements in the action set and the action with the maximum estimation is executed. Since the relationships between the current frame and future frames are limited and indefinite, we apply different search times for the proposed method. In fact, with only the value network, tree search can also be realized by assigning each action an equal prior probability. Thus, we also evaluate the tree search model with information only from the value network.</p><p>1) Testing of each component: <ref type="table" target="#tab_1">Table I</ref> summarizes the segmentation results respect to different search times. When searching 10 times for each consideration, the combined method achieves the highest segment-level edit score at a negligible cost of accuracy. Combining the policy and value network always outperforms the two individual networks concerning the frame-level accuracy. Also, the results of the value network indicate the importance of the policy network. The scores are not always going up with the increment of search times, which implies the limited effectiveness of the temporal information. Although it seems that the decisions of past frames are independent of the future choice of action, results of the value network show that they do help to choose a right class, at least on a segment level. By the way, the value network trained by supervised learning also achieves reasonable performance since this is a special reinforcement learning problem with expert data available.</p><p>2) Behaviors of the policy network: To inspect the detailed behavior of the policy network, a prediction example generated is visualized. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, the predicted classes follow the trend of ground truth (Please refer to <ref type="bibr" target="#b5">[6]</ref> for gesture classification numbers). We also plot the maximum probability for each frame to examine its confidence in an episode. It is interesting that the policy network tends to be ambiguous at gesture boundaries. Most of the time, it is certain to make the right decision although it misses the right actions for some parts with high confidence. Since the tree search is only applied at the uncertain frames, the improvement relative to the policy network mainly depends on the more accurate recognition of boundaries. The oversegmentation problem is also alleviated as the edit score is raised. Furthermore, we show a prediction example of the segmentation outcomes of direct decisions by the policy network and rectified decisions using tree search. As shown in <ref type="figure">Fig. 4</ref>, one gesture is missed in the second half of the episode by the policy network. By contrast, the missing gesture is recognized by our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3)</head><p>Overall performances: <ref type="table" target="#tab_1">Table II</ref> shows the experimental results on video data. Our tree search based method is compared with the original TCN, RL based method, and other recent works. The evaluation metrics are accuracy, edit score and F1 score with the IoU threshold set to 10%, 25%, 50% respectively. Compared with existing works, our approach achieves higher scores given all evaluation criterion. The reproduced outcome using RL based method attains almost the same performances as reported in <ref type="bibr" target="#b13">[14]</ref>. For the value network, its performance is obtained without tree search. Interestingly, the policy network enjoys a superior ability on accuracy than the value network, while the value network behaves better on edit score. Through tree search for 10 times, the two cooperate to create an even better outcome. The processing time for the 10 times search setting and <ref type="bibr" target="#b13">[14]</ref> are 25.4 s and 6.2 s, respectively, which are completely enough for an online mode given the total length of videos (73 mins). The additional time of our method is caused by re-considerations of 13% frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSIONS</head><p>The proposed method is a search-based algorithm which makes it more generic for decision problems. The problem formulated using RL framework here is a special case that the agent's actions do not affect the transfer of the environment. Therefore, the interactions of adjacent actions are <ref type="bibr" target="#b0">(1)</ref> (2)</p><p>(3) <ref type="figure">Fig. 4</ref>. Color-coded ribbon illustration of surgical gesture from a complete video. We present (1) the ground truth (2) recognition results from the policy network (3) rectified predictions by tree search. Each color stands for a gesture class. The method based on tree search could recognize the missing gesture of the policy network. quite limited. Our method makes the agent act in a humanlike manner by jointly leveraging two networks. The policy network gives quick decisions and the value network offers meticulous selections. The intuition is that the value network could rectify the policy network's faults by providing advice from a different perspective.</p><p>Our work is novel in terms of formulating the surgical video gesture recognition task into a path searching problem. Rather than purely relying on a policy network as Liu and Jiang <ref type="bibr" target="#b13">[14]</ref> recently investigated, we further introduce a value network into the framework, by borrowing the spirit of AlphaGo <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. More importantly, we develop a tree search algorithm associated with the value network for leveraging global information. In the problem setting of online prediction for the surgical gesture, future frames are not available to the system. By taking advantage of our proposed tree search algorithm, predictions of future frames can be explosively considered for helping make decisions on the current time step. As a nearly online mode, another alternative solution of missing future frames is to output slightly delayed gesture predictions, which makes the proposed tree search feasible. In practice, such a value network functions together with the policy network, when a frame receives a less confident decision by the policy network. In other words, the value network is an inseparable module to compensate defective predictions from the policy network. We think our introduced insight will inspire more future studies on using reinforcement learning for surgical video analysis. For our future work, we plan to employ disparate features (e.g., visual and kinematic data) to train the two networks respectively, for further stimulating complementarity of the policy and value networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we propose a novel method based on tree search for surgical video segmentation problems. The tree search algorithm unites the outputs from different neural networks. During tree search, prior probabilities from the policy network narrow down the search space and guide the search directions together with evaluations from the value network. Due to a more comprehensive consideration of action selections, the suggested approach outperforms the baseline methods as well as the existing works on JIGSAWS dataset in terms of different metrics. To conclude, we highlight the benefit of introducing the contemplation ability for the agent when getting confused about the direct decisions, which is of great importance in the medical field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was partially supported by HK RGC TRS project T42-409/18-R, and a grant from the National Natural Science Foundation of China (Project No. U1813204) and CUHK T Stone Robotics Institute. X. Gao, Y. Jin, Q. Dou and P. A. Heng are with the Department of Computer Science and Engineering, The Chinese University of Hong Kong. Q. Dou and P. A. Heng are also with the CUHK T Stone Robotics Institute. Corresponding author at: qdou@cse.cuhk.edu.hk (Qi Dou).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of our proposed deep reinforcement learning based method for automatic surgical gesture recognition. Our framework consists of a policy network and a value network with tree search, which work together in a complementary manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>In the upper part, we present the recognition results from the policy network and ground truth in one complete video; in the lower part, the corresponding predicted probabilities are shown which are used to choose the executed action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>Tree search algorithm Input: Current states, search times n Output: Refined action a * Initialize the root node s 0 according to current states for i = 1, n do Start from s 0 Go through the tree using Eq. (9) until a leaf node s L if s L is not end state then Expand all its child nodes Evaluate s L to output {p i } using Eq. (5) Compute {v i } of its child nodes using Eq. (6-7) end if Update visit count of these nodes Update action values of the visited nodes by Eq. (12) end for if only one N (s 0 , a) is maximum then a</figDesc><table><row><cell>N (s 0 , a)</cell></row><row><cell>else</cell></row><row><cell>a  *  ? arg max (N (s 0 , a) + Q(s 0 , a)) a end if return a</cell></row></table><note>* ? arg maxa* Levenshtein distance between predicted gesture sequence and ground truth. (iii) F1@k score</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ABLATION</head><label>I</label><figDesc>EXPERIMENTS ON TREE SEARCH WITH DIFFERENT SEARCH TIMES.</figDesc><table><row><cell>Search times 0 10 20 30 40</cell><cell>Policy Acc Edit 81.52 87.87 80.91 88.34 81.67 87.94 Value Policy+Value Acc Edit Acc Edit -80.99 87.74 81.67 88.53 -80.99 87.74 81.70 88.24 -81.02 88.16 81.61 88.22 -81.01 88.16 81.69 87.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II RESULTS</head><label>II</label><figDesc>ON THE SUTURING TASK OF JIGSAWS.</figDesc><table><row><cell>Method MsM-CRF [12] Seg-ST-CNN [18] TCN [19] TCN+Deep RL [14] Policy Net Value Net Policy+Value (Ours) 81.67 88.53 92.68 90.99 83.15 Acc Edit F1@{10,25,50} 71.78 --74.22 66.56 -81.4 83.1 -81.43 87.96 92.0 90.5 82.2 81.52 87.87 92.20 90.86 82.77 80.91 88.34 92.32 90.10 81.36</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised recognition of surgical gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Amsterdam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">De</forename><surname>Momi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9565" to="9571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmenting and classifying activities in robot-assisted surgery with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waldram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Task versus subtask surgical skill evaluation of robotic minimally invasive surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Reiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-derived models for segmentation with application to surgical assessment and training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis of energy-based metrics for laparoscopic skills assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poursartip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Lebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Naish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Trejos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1532" to="1542" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A dataset and benchmarks for segmentation and recognition of gestures in robotic surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sefati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2025" to="2041" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic data-driven real-time segmentation and recognition of surgical workflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dergachyova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huaulm?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Morandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1081" to="1089" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A cognitive robot control architecture for autonomous execution of surgical tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Preda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferraguti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">De</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Secchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Muradore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fiorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bonf?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Robotics Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page">1650008</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transfer learning for surgical task segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9166" to="9172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Phase segmentation methods for an automatic surgical workflow analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sakurai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamazoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Biomedical Imaging</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse hidden markov models for surgical gesture classification and skill evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Computer-Assisted Interventions</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="167" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Surgical gesture segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-toend fine-grained action segmentation and recognition using conditional random field models and discriminative sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mavroudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bhaskara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sefati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1558" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for surgical gesture segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="247" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autonomous framework for segmenting robot trajectories of manipulation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="141" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Research</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="91" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognizing surgical activities with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="551" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal CNNs for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SV-RCNet: workflow recognition from surgical videos using recurrent convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1114" to="1126" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepPhase: surgical phase recognition in CATARACTS videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zisimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Flouty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giataganas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nehme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using augmentation to improve the robustness to rotation of deep learning segmentation in robotic-assisted surgical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Itzkovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jarc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Refaely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5068" to="5075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page">484</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient selectivity and backup operators in Monte-Carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers and Games</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single-player Monte-Carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Schadd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Herik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-B</forename><surname>Chaslot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Uiterwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers and Games</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of Monte Carlo tree search methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rohlfshagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tavener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling deep temporal dependencies with recurrent grammar cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1925" to="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">JHU-ISI gesture and skill assessment working set (JIGSAWS): A surgical activity dataset for human motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Reiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>B?jar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Yuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling and Monitoring of Computer Assisted Interventions (M2CAI)-MICCAI Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
