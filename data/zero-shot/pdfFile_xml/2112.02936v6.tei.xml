<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pairwise Learning for Neural Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Wang</surname></persName>
							<email>zhitaowang@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Tencent</roleName><forename type="first">Wechat</forename><surname>Pay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litao</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Tencent</roleName><forename type="first">Wechat</forename><surname>Pay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zou</surname></persName>
							<email>yuanhangzou@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjing</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Tencent</roleName><forename type="first">Wechat</forename><surname>Pay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouzhi</forename><surname>Chen</surname></persName>
							<email>easychen@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Tencent</roleName><forename type="first">Wechat</forename><surname>Pay</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">WeChat Search</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pairwise Learning for Neural Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we aim at providing an effective Pairwise Learning for Neural Link Prediction (PLNLP) framework. The framework treats link prediction as a pairwise learning to rank problem and consists of four main components, i.e., neighborhood encoder, link predictor, negative sampler and objective function. The framework is flexible that any generic graph neural convolutions or link prediction specific neural architectures could be employed as neighborhood encoder. For link predictor, we design different scoring functions, which could be selected based on different types of graphs. In negative sampler, we provide several sampling strategies, which are problem specific. As for objective function, we propose to use an effective ranking loss, which approximately maximizes the standard ranking metric AUC. We evaluate the proposed PLNLP framework on 4 link property prediction datasets of Open Graph Benchmark (OGB), including ogbl-ddi, ogbl-collab, ogbl-ppa and ogbl-ciation2. PLNLP achieves top 1 performance on ogbl-ddi and ogbl-collab, and top 2 performance on ogbl-ciation2 only with basic neural architecture. The experimental results demonstrate the effectiveness of PLNLP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>knowledge or expensive trial and error are inevitable in choosing appropriate heuristics for different networks. Thanks to effective feature learning ability of neural networks, a series of neural link prediction models <ref type="bibr" target="#b24">[Zhang and Chen, 2017</ref><ref type="bibr" target="#b9">, Kipf and Welling, 2016b</ref><ref type="bibr" target="#b21">, Wang et al., 2020</ref><ref type="bibr" target="#b20">, 2019</ref><ref type="bibr" target="#b22">, 2021</ref> were proposed, of which the generalization ability was successfully improved.</p><p>Existing neural link prediction methods pay much attention on designing more expressive neural architectures, while some basic properties of the problem are often neglected. For example, most neural models treat link prediction as a binary classification problem and naturally adopt a cross entropy loss function. However, this learning schema seems not to be suitable for the link prediction problem. First, link classification is extremely imbalanced due to the natural sparsity of most graphs. Although under-sampling could be adopted, there would be information loss during sampling process and what ratio of sampling is hard to decide. Second, most link prediction evaluation protocols do not aim at labeling positive pairs as 1 while negative pairs as 0, but ask for ranking positive pairs higher than negative pairs. Therefore, employing cross-entropy function seems not to be so direct to the objective of the link prediction task.</p><p>Based on above understanding and our previous research <ref type="bibr" target="#b21">[Wang et al., 2020</ref><ref type="bibr" target="#b20">[Wang et al., , 2019</ref><ref type="bibr" target="#b22">[Wang et al., , 2021</ref>, we provide an effective and generic pairwise learning neural link prediction framework in this paper, named PLNLP. The framework adopts a pairwise learning to rank schema and consists of four main components, i.e., neighborhood encoder, link predictor, negative sampler and objective function. The neighborhood encoder aims at extracting expressive neighborhood information of input node-pair. Any generic graph neural convolution, such as <ref type="bibr">GCN [Kipf and Welling, 2016a]</ref> and SAGE <ref type="bibr" target="#b5">[Hamilton et al., 2017]</ref>, or link prediction specific neural architecture, such as SEAL , NANs <ref type="bibr" target="#b21">[Wang et al., 2020]</ref> and HalpNet <ref type="bibr" target="#b22">[Wang et al., 2021]</ref>, could be employed as neighborhood encoder. For link predictor, we design different scoring functions, which could be selected based on different types of graphs. In negative sampler, we provide several negative sampling strategies, which are problem specific. As for objective function, we propose to use an effective ranking loss, which approximately maximizes the standard ranking metric AUC. We evaluate the proposed PLNLP framework on 4 link property prediction datasets of Open Graph Benchmark (OGB) <ref type="bibr" target="#b6">[Hu et al., 2020]</ref>, including ogbl-ddi, ogbl-collab, ogbl-ppa and ogbl-ciation2. PLNLP with basic neural architecture achieves top 1 performance on ogbl-ddi and ogbl-collab, and top 2 performance on ogbl-ciation2. The performance demonstrates the effectiveness of PLNLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Existing link prediction approaches can be categorized into three families: heuristic feature based, latent embedding based and neural network based.</p><p>Heuristic Methods: Most heuristics measure node similarity with neighborhood information. Popular heuristics include first-order methods common neighbors, Jaccard index <ref type="bibr" target="#b16">[Salton and McGill, 1986]</ref> and preferential attachment <ref type="bibr" target="#b11">[Liben-Nowell and Kleinberg, 2007]</ref>; second-order methods, i.e., <ref type="bibr" target="#b0">Adamic-Adar [Adamic and Adar, 2003]</ref>, resource allocation <ref type="bibr" target="#b27">[Zhou et al., 2009]</ref>; and high-order heuristic SimRank <ref type="bibr" target="#b7">[Jeh and Widom, 2002]</ref>. These heuristics often fail to capture complex latent formation features.</p><p>Embedding-based Methods: Embedding based methods aim at learning latent node features. The most classical one is matrix factorization (MF) method <ref type="bibr" target="#b14">[Menon and Elkan, 2011]</ref>, which aims at reconstructing adjacency matrix. Besides, a series of unsupervised network representation learning models <ref type="bibr" target="#b15">[Perozzi et al., 2014</ref><ref type="bibr" target="#b17">, Tang et al., 2015</ref><ref type="bibr" target="#b4">, Grover and Leskovec, 2016</ref><ref type="bibr" target="#b5">, Hamilton et al., 2017</ref>, are also applicable for link prediction. These methods learn generic latent embeddings by preserving structure proximities from a probabilistic view and predict links by composing node embeddings as edge features. PNRL  is a state-of-the-art link prediction specific embedding method, which simultaneously preserves proximities of observed structure and infers hidden links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NN-based Methods:</head><p>Recently, some neural network-based link prediction models were developed, which explore non-linear deep structural features with neural layers. Variational graph auto-encoders <ref type="bibr" target="#b9">[Kipf and Welling, 2016b]</ref> predict links by encoding graph with graph convolutional layer <ref type="bibr" target="#b8">[Kipf and Welling, 2016a]</ref>. Another two state-of-the-art neural models WLNM <ref type="bibr" target="#b24">[Zhang and Chen, 2017]</ref> and SEAL  use graph labeling algorithm to transfer union neighborhood of two nodes (enclosing subgraph) as meaningful matrix and employ convolutional neural layer or a novel graph neural layer DGCNN  for encoding.</p><p>Besides, in our previous work, we proposed a series of neighborhood attention neural networks <ref type="bibr" target="#b21">[Wang et al., 2020</ref><ref type="bibr" target="#b20">[Wang et al., , 2019</ref><ref type="bibr" target="#b22">[Wang et al., , 2021</ref>, in which different attention mechanisms were designed to encode neighborhood information specific for link prediction problem. For instance, in <ref type="bibr" target="#b21">[Wang et al., 2020</ref><ref type="bibr" target="#b20">[Wang et al., , 2019</ref>, we proposed cross neighborhood attention and interactive attention mechanisms to capture structural interactions between neighborhoods of the target node-pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graphs</head><p>Generally, a graph (network) is represented as G = (V, E), where V = {v 1 , ..., v N } is the set of nodes, E ? V ? V is the set of links, and the total number of distinct nodes is N . Also, a graph is often denoted as an adjacency matrix A, where A i,j = 1 if there is a link from node v i to v j , otherwise A i,j = 0. A will be symmetric, if the graph is undirected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neighborhood of Node</head><p>We use N h (v i ) to represent the h-hop neighborhood of node v i ? V , which is the set of nodes whose distance to v i (represented as d(v i , v j )) is not greater than h. In this paper, we focus on unweighted graph, thus the distance function d(v i , v j ) is directly computed as the length of the shortest path between v i and v j . We call v i the center node and v j ? N h (v i ) the neighboring node within h-hop.</p><p>To make the neighborhood also include the unique information of the center node, we define that the center node v i is a neighboring node of itself, such that v i ? N h (v i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neighborhood Subgraph of Node-Pair</head><p>We</p><formula xml:id="formula_0">use G h (v i , v j ) to represent the h-hop neighborhood subgraph of the node pair (v i , v j ), which is extracted from the whole graph G. Formally, for any node v k in the neighborhood subgraph G h (v i , v j ), it should satisfy d(v k , v i ) ? h and d(v k , v j ) ? h, i.e., v k ? N h (v i ) ? N h (v j ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Link Prediction</head><p>Link prediction problems are categorized as temporal link prediction which predicts potential new links on an evolving network, and structural link prediction which infers missing links on a static network. In this paper, we focus on structural link prediction. Given the partially observed structure of a network, the goal of it is to predict the unobserved links. Formally, given a partially observed network G = (V, E), we represent the set of node-pairs with unknown link status as E ? = V ?V ?E, then the goal of structural link prediction is to infer link status of node-pairs in E ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PLNLP Framework</head><p>The proposed framework is illustrated as <ref type="figure" target="#fig_0">Figure 1</ref>. Given an input graph, negative sampler aims to draw negative samples and form training pairs. A training pair consists of a positive sample, which is a node-pair with an observed edge in input graph, and a negative sample, which is a node-pair drawn by negative sampler. Neighborhood encoder is used to extract neighborhood information of both positive and negative samples as the hidden representations. Given the hidden representations, link predictor will calculate link scores of both samples. With link scores of training pairs, the model parameters will be optimized based on the pairwise ranking objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neighborhood Encoder</head><p>Neighborhood information has proved crucial for link prediction. Therefore, we propose to use neighborhood neural encoder to extract structural information of input samples. We consider two kinds of neighborhood encoder in this paper. One is Node Neighborhood Encoder (NNE), which encodes the two nodes of a input sample with their own neighborhood as two hidden representations, separately. Any generic graph neural networks (GNN), e.g, GCN, GraphSAGE and GAT, could be employed as NNE. Assume that input sample is (v i , v j ), NNEs aim to extract hidden representations </p><formula xml:id="formula_1">h i = NNE(x i , {x k |v k ? N h (v i )}), h j = NNE(x j , {x l |v l ? N h (v j )})<label>(1)</label></formula><p>where x i generally represents the input feature of node v i . If there is no input features, x i represents a embedding vector of node v i , which is trainable parameter. x i could also represent the concatenation of input feature and node embedding. In this framework, we only consider homogeneous graph, which means that all nodes share a same NNE.</p><p>The other kind of neighborhood encoder is Edge level Neighborhood Encoder (ENE), or called nodepair neighborhood encoder. Recently, a series of ENEs, such as SEAL, NIAN and HalpNet, were proposed specifically for link prediction problem. The main advantage of ENEs is capturing structural interactions between the neighborhoods, which are ignored in NENs. ENEs often consider the neighborhood subgraph of a sample (node-pair) as input, and encode it as one hidden representation. Assume that input sample is (v i , v j ), ENEs derive a hidden representation of the input sample as follows:</p><formula xml:id="formula_2">h ij = ENE(x i , x j , {x k |v k ? G h (v i , v j )})</formula><p>(2) Similarly, x i represents the input feature, or trainable embedding, or the concatenation of input feature and embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Link Score Predictor</head><p>After deriving the hidden representations either in node level or node-pair (edge) level, the framework will calculate a linking score of the input sample. We provide several selections of the scoring function.</p><p>Dot Predictor. If we use NNE to derive h i and h j of the input sample (v i , v j ), we can simply use a dot operator to derive the score:</p><formula xml:id="formula_3">s ij = h i ? h j<label>(3)</label></formula><p>Bilinear Dot Predictor. Dot operator can be only used for undirected graph due to its commutative property. For directed graph, we can adopt bilinear dot operator to make the scoring function not commutative:</p><formula xml:id="formula_4">s ij = h i Wh j (4) where W is a learnable matrix.</formula><p>MLP Predictor. We can also employ a multi-layer perceptron (MLP) as the link predictor. If we use ENEs to obtain hidden representation of the input sample, the predictor is as follow:</p><p>s ij = MLP(h ij ) (5) If we use NNEs to obtain hidden representations, there are several possible forms of MLP's input. If the graph is undirected, we can adopt a widely used commutative operator, i.e, hadamard product :</p><formula xml:id="formula_5">s ij = MLP(h i h j )<label>(6)</label></formula><p>If the graph is directed, we would prefer a non-commutative operator, such as concatenation ||:</p><formula xml:id="formula_6">s ij = MLP(h i ||h j )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pairwise Learning with Ranking Objective</head><p>Due to the sparsity of networks, there often exists extreme imbalance between linked pairs and non-linked pairs. Meanwhile, most link prediction tasks do not aim at labeling positive pairs as 1 while negative pairs as 0, but ask for ranking positive pairs higher than negative pairs. To be consistent with the general objective of link prediction, we adopt the ranking idea for model learning, which can be formalized as:</p><formula xml:id="formula_7">s ij &gt; s kl , ?(v i , v j ) ? E and ?(v k , v l ) ? E ?<label>(8)</label></formula><p>where s ij and s kl are the output scores of link predictor, E ? is the set of true non-linked pairs. In fact, the above learning objective is equivalent to maximize the Area Under the Curve (AUC), which is interpreted as the probability of a positive sample ranking higher than a negative sample. The empirical AUC value is defined as follow:</p><formula xml:id="formula_8">AUC = (vi,vj )?E (v k ,v l )?E ? 1[f ? (v i , v j ) &gt; f ? (v k , v l )] |V ? V | (9) where 1[?] is an indicator function that equals to 1 if f ? (v i , v j ) &gt; f ? (v k , v l ), otherwise equals to 0. f ? (v i , v j ) = s ij</formula><p>represents the output of the neural link prediction model, where ? denotes all parameters of the model. Optimizing AUC is not straightforward since the gradient of this function is either zero or not defined. Various techniques have been proposed to approximate the AUC with a surrogate function. There are several possible selections of surrogate functions, such as pairwise hinge loss, logistic loss or exponential loss. In this paper, we simply select the squared least surrogate loss, which is proved consistent with AUC theoretically <ref type="bibr" target="#b3">[Gao and Zhou, 2015]</ref>. Our framework is flexible to adopt any other surrogate function that approximates AUC. The base AUC-optimization objective function is defined as follow:</p><formula xml:id="formula_9">O AUC = min ? (vi,vj )?E,(vi,v k )?E ? (1 ? f ? (v i , v j ) + f ? (v i , v k )) 2 + ? 2 ||?|| 2<label>(10)</label></formula><p>The above function forces the margin between positive samples and negative samples to be 1. In some situations, this constraint is too strict for the optimization. It can be relaxed by combining the squared hinge loss with above function:</p><formula xml:id="formula_10">O Hinge-AUC = min ? (vi,vj )?E,(vi,v k )?E ? (max (0, 1 ? f ? (v i , v j ) + f ? (v i , v k ))) 2 + ? 2 ||?|| 2 (11)</formula><p>The above function only forces the margin between positive samples and negative samples to be larger than 1.</p><p>Furthermore, the margin may not be fixed as 1 if weights on training edges (positive sample) are expected to be modeled. A straightforward way of introducing sample weights is as follows:</p><formula xml:id="formula_11">O Weighted-Hinge-AUC = min ? (vi,vj )?E,(vi,v k )?E ? ? ij (max (0, ? ij ? f ? (v i , v j ) + f ? (v i , v k ))) 2 + ? 2 ||?|| 2</formula><p>(12) where ? is an adaptive margin, which may correspond to normalized weights of training edges.</p><p>In above objective functions, to prevent over-fitting problem, we use the L2 regularization on parameters with a weight ?. Given a positive pair (v i , v j ) and a sampled negative pair (v k , v l ), the parameters ? of the model are optimized by the stochastic gradient descent (SGD) method. For most cases, we use the basic objective function in Eq. 10. When sample weights are considered, the objective function of Eq. 12 is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Negative Sampling</head><p>In practice, true non-linked set E ? is not available in the training data. A conventional strategy is randomly sampling a negative node-pair (v k , v j ), which has unknown link status and is assumed as negative samples. For different problems or types of graphs, we may have different sampling strategies.</p><p>Global Sampling. Global sampling represents that, for each positive sample, we uniformly sample a negative node-pair from the set E ? = V ? V ? E. This strategy is suitable for the problem seeking for global ranking performance. For example, in the protein-protein interaction, we are interested in potential node-pairs, which are worth performing further analysis on, among all possible node-pairs. Local Sampling. Local sampling represents that, for a positive sample (v i , v j ), we firstly select an anchor node saying v i , then uniformly sample a node v k and regard (v i , v k ) as the negative sample. Instead of uniform distribution, other distribution, e.g, the power of node degrees, can be applied to sample the negative node v k . This strategy is appropriate to the situation that aims to obtain good ranking for individual nodes. For example, in a recommendation system, we would like to recommend a good ranking list of items to each individual user.</p><p>Adversarial Sampling. The performance of random sampling strategy is not always stable due to complete randomness. Similar problems of random negative sampling have also been found in other tasks, e.g., knowledge graph embedding <ref type="bibr">[Wang et al., 2018, Cai and</ref> and image retrieval <ref type="bibr" target="#b23">[Wu et al., 2017]</ref>. In our previous work <ref type="bibr" target="#b21">[Wang et al., 2020]</ref>, we proposed to use adversarial learning technique to generate negative samples instead of random sampling. We designed a generative model to generate high quality negative samples, which aims at making difficulties to link prediction model. In this way, link prediction model and negative sample generator play an adversarial game. By continuously providing high quality negative samples, adversarial sampling more robust than random sampling. We leave the evaluation of adversarial sampling on ogb datasets as future work.</p><p>Negative Sample Sharing. Since the framework adopts pairwise schema, each negative sample can only be used for one positive sample once, which is not efficient. To make better use of negative samples, we propose a negative sample sharing mechanism. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, assume that the total number of positive sample is m, we firstly draw m negative samples and construct m training pairs with same indexes. Given the negative samples, the sharing mechanism will random permute the indexes of negative samples, and form m new training pairs. The hyper-parameter num_neg indicates the mechanism will random permute (num_neg-1) times of negative samples. By using this sharing mechanism, we could create m ? num_neg training pairs by only sampling m negative samples at each training epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Data Augmentation with Random Walk</head><p>In some graphs, high-order structure information play a important role. Although increasing the number of GNN layers could model the high-order information, it also may leads to over-smoothing problem and low efficiency. To this end, we propose to use data augmentation to introduce high-order information at the input. A general technique to sample high order information is the Random Walk. Given all nodes in the graph, we use the basic random walk method to sample the high-order pairs. Assume the start point node is v i and its random walk is RW(v i ) = {v k+1 , ..., v k+l }, where l represents the walk length , then the set of positive samples is augmented as :</p><formula xml:id="formula_12">E aug = E ? {(v i , v j )|v j ? RW(v i ), ?v i ? V }<label>(13)</label></formula><p>Meanwhile, the augmented pairs are associated with weights based on the steps of walks. For example, in the walk, RW(v i ) = {v k+1 , ..., v k+l }, the augmented pair (v i , v k+l ) is associated with the weight 1/l. With different weights of augmented pairs in E aug , we find that using the weight-adaptive objective function in Eq. 12 is more effective. Therefore, it is suggested using this objective function when random walk augmentation is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation on OGB</head><p>Our code for evaluation is available at https://github.com/zhitao-wang/PLNLP.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Evaluation Metrics</head><p>We evaluate the link prediction ability of PLNLP on Open Graph Benchmark (OGB) data <ref type="bibr" target="#b6">[Hu et al., 2020]</ref>. Four data sets with different graph types are evaluated, including ogbl-ddi, ogbl-collab, ogbl-citation2 and ogbl-ppa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ogbl-ddi:</head><p>The dataset is a homogeneous, unweighted, undirected graph, representing the drug-drug interaction network. Each node represents a drug. Edges represent interactions between drugs.</p><p>The task is to predict drug-drug interactions given information on already known drug-drug interactions. The performance is evaluated by Hits@20: each true drug interaction is ranked among a set of approximately 100,000 randomly-sampled negative drug interactions, and count the ratio of positive edges that are ranked at 20-place or above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ogbl-collab:</head><p>The dataset is an undirected graph, representing a subset of the collaboration network between authors indexed by MAG. Each node represents an author and edges indicate the collaboration between authors. All nodes come with 128-dimensional features.</p><p>The task is to predict the future author collaboration relationships given the past collaborations. Evaluation metric is Hits@50, where each true collaboration is ranked among a set of 100,000 randomly-sampled negative collaborations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ogbl-ppa:</head><p>The dataset is an undirected, unweighted graph. Nodes represent proteins from 58 different species, and edges indicate biologically meaningful associations between proteins.</p><p>The task is to predict new association edges given the training edges. Evaluation metric is Hits@100, where each positive edge is ranked among 3,000,000 randomly-sampled negative edges.</p><p>ogbl-citation2: The dataset is a directed graph, representing the citation network between a subset of papers extracted from MAG. Each node is a paper with 128-dimensional word2vec features.</p><p>The task is to predict missing citations given existing citations. The evaluation metric is Mean Reciprocal Rank (MRR), where the reciprocal rank of the true reference among 1,000 negative candidates is calculated for each source paper, and then the average is taken over all source papers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Settings and Results</head><p>The detailed settings of PLNLP in this paper are shown in <ref type="table" target="#tab_1">Table 1</ref>. We only employ basic node level neighborhood encoder, e.g., GCN or SAGE, to demonstrate the effectiveness of the proposed framework. Some well-design edge level neighborhood encoder specific for link prediction, such as SEAL, NANs and HalpNet, may further improve the performance. But due to low efficiency of edge level neighborhood encoders, we leave this part in the future work. We treat all training datasets as unweighted and undirected graphs. As for MLP predictor, we use hadamard product to get the input of MLP. It is worth noting that we use validation set for training on ogbl-collab, which is allowed by OGB. Meanwhile, we employ the trick from HOP-REC that we only use training edges after year 2010 in ogbl-collab. Furthermore, we use random walk augmentation with a walk length 10 for ogbl-collab.</p><p>Following OGB rules, we evaluate PLNLP with 10 runs, without fixing random seed. As for other state-of-the-art methods, we just copy the results from OGB official leader board.</p><p>The averaged results with standard deviation are reported in the <ref type="table">Table 2</ref>. Only with basic graph neural architectures, PLNLP achieves top 1 performance on ogbl-ddi and ogbl-collab, and top 2 performance on ogbl-ciation2. This significantly demonstrates the effectiveness of PLNLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>Furthermore, we compare PLNLP against the generic classification learning framework where the loss function is cross-entropy. In this ablation study, we keep same neural architecture (same encoder, predictor with same parameters as reported in <ref type="table" target="#tab_1">Table 1</ref>) in the two frameworks. We use basic AUC objective function Eq.10 for all datasets and do not use random walk augmentation for ogbl-collab in this study. To guarantee fairness, we use same negative sampling strategies and use the same number of negative samples at each epoch. The results are shown in <ref type="table">Table 3</ref>. It is found that PLNLP remarkably outperforms the generic classification learning schema, which indicates the proposed pairwise learning could maximize the performance of graph neural models on link prediction problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>: PLNLP Framework of the input sample as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Negative Sample Sharing Mechanism</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Settings of PLNLP on OGB Datasets</figDesc><table><row><cell></cell><cell>Loss Func.</cell><cell>Encoder</cell><cell>Predictor</cell><cell>Neg. Sampler</cell><cell>Other Parameters</cell></row><row><cell></cell><cell></cell><cell>SAGE</cell><cell>MLP</cell><cell></cell></row><row><cell>ddi</cell><cell>O AUC</cell><cell>layer = 2 dim = 512</cell><cell>layer = 2 dim = 512</cell><cell></cell></row><row><cell></cell><cell></cell><cell>dropout = 0.3</cell><cell>dropout = 0.3</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors greatly thank the great support for advanced research from departments of WeChat Pay and WeChat Search.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Friends and neighbors on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="230" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixed membership stochastic blockmodels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1981" to="2014" />
			<date type="published" when="2008-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kbgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04071</idno>
		<title level="m">Adversarial learning for knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the consistency of auc pairwise optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simrank: a measure of structural-context similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="538" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Link prediction in complex networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Physica A: statistical mechanics and its applications</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="page" from="1150" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of link prediction in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?ctor</forename><surname>Mart?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Berzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Carlos</forename><surname>Cubero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Link prediction via matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint european conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Introduction to modern information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Incorporating gan for negative sampling in knowledge representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangyin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predictive network representation learning for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="969" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neighborhood interaction attention network for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2153" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neighborhood attention networks with adversarial learning for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Hierarchical attention link prediction neural network. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjing</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2021.107431.URLhttps:/www.sciencedirect.com/science/article/pii/S0950705121006936</idno>
		<idno>0950-7051. doi</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2021.107431.URLhttps://www.sciencedirect.com/science/article/pii/S0950705121006936" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">232</biblScope>
			<biblScope unit="page">107431</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman neural machine for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting missing links via local information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Cheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="630" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
