<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A TWO-STAGE APPROACH TO DEVICE-ROBUST ACOUSTIC SCENE CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Hu</surname></persName>
							<email>huhu@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Han</forename><surname>Huck</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjun</forename><surname>Xia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Media Lab</orgName>
								<orgName type="institution">Tencent Corporation</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Bai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>HeFei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>HeFei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajian</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>HeFei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutong</forename><surname>Niu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>HeFei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>HeFei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanjuan</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Media Lab</orgName>
								<orgName type="institution">Tencent Corporation</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Media Lab</orgName>
								<orgName type="institution">Tencent Corporation</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Bao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Media Lab</orgName>
								<orgName type="institution">Tencent Corporation</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Media Lab</orgName>
								<orgName type="institution">Tencent Corporation</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabato</forename><forename type="middle">Marco</forename><surname>Siniscalchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Engineering School</orgName>
								<orgName type="institution">University of Enna Kore</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannan</forename><surname>Wang</surname></persName>
							<email>yannanwang@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent Media Lab</orgName>
								<orgName type="institution">Tencent Corporation</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Du</surname></persName>
							<email>jundu@ustc.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>HeFei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
							<email>chinhui.lee@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A TWO-STAGE APPROACH TO DEVICE-ROBUST ACOUSTIC SCENE CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Acoustic scene classification</term>
					<term>robustness</term>
					<term>con- volutional neural networks</term>
					<term>data augmentation</term>
					<term>class activation map- ping</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To improve device robustness, a highly desirable key feature of a competitive data-driven acoustic scene classification (ASC) system, a novel two-stage system based on fully convolutional neural networks (CNNs) is proposed. Our two-stage system leverages on an ad-hoc score combination based on two CNN classifiers: (i) the first CNN classifies acoustic inputs into one of three broad classes, and (ii) the second CNN classifies the same inputs into one of ten finergrained classes. Three different CNN architectures are explored to implement the two-stage classifiers, and a frequency sub-sampling scheme is investigated. Moreover, novel data augmentation schemes for ASC are also investigated. Evaluated on DCASE 2020 Task 1a, our results show that the proposed ASC system attains a state-of-theart accuracy on the development set, where our best system, a twostage fusion of CNN ensembles, delivers a 81.9% average accuracy among multi-device test data, and it obtains a significant improvement on unseen devices. Finally, neural saliency analysis with class activation mapping (CAM) gives new insights on the patterns learnt by our models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Acoustic scene classification (ASC) refers to the task of identifying real-life sounds into environment classes, such as metro station, street traffic, public square, etc. An acoustic scene sound contains much information and rich content, which makes accurate scene prediction difficult and an intriguing research problem at the same time. In recent years, the organizers of the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> provided both the benchmark data and a competitive platform to promote acoustic scene research and analysis. If we are to analyze top ASC systems reported in the challenge, we will find that most of them are built on the deep neural networks (DNNs) framework, and the key ingredient of their success is the use of convolutional layers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Advanced deep learning techniques, such as attention mechanism <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, mix-up <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, generative adversarial network (GAN) and variational auto encoder (VAE) based data augmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, and deep feature learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> can further enhance ASC results. Although those ASC systems work well  <ref type="figure">Fig. 1</ref>. The proposed two-stage ASC system. on in-domain data, a generalization issue is observed in the presence of audio recording collected in mismatched testing conditions, e.g., audio segments recorded with devices different from those used in the training phase <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Device robustness is an inevitable issue in a real production environment, and it is an important aspect to handle when designing an ASC system.</p><p>In this work, we have three main contributions: (i) we propose a novel two-stage ASC system; (ii) we investigate three different fully CNN models in the proposed two-stage system; (iii) we explore novel data augmentation strategies to reduce the device dependency of our models. Our experimental results are gathered on the DCASE 2020 task1a development data set, and the proposed CNN models achieve competitive results. Specifically, we attained a 79.4% overall ASC accuracy by ensemble of our three CNN models, and it obtains a significant improvement on unseen devices when compared with the baseline. The two-stage approach further boosts the accuracy of the ensemble model, and an ASC classification accuracy of 81.9% is attained. To better understand the key characteristics of our CNN based systems, we carry out a neural saliency analysis based on the class activation mapping (CAM) <ref type="bibr" target="#b17">[18]</ref> on input data and the final output. Such analysis shows that our CNN models actually focus more on the meaningful audio segments containing acoustic events for the final classification task, such as bird sounds in a park or car horn sounds from street traffic, rather than leveraging on the background sound, as instead reported in <ref type="bibr" target="#b18">[19]</ref>. <ref type="figure">Figure 1</ref> shows the proposed two-stage ASC system. The overall system consists of two independent classifiers and outputs the class of the input audio scene choosing among ten classes. Different from arXiv:2011.01447v1 [cs.SD] 3 Nov 2020 a more conventional one-stage ASC systems, which directly feed acoustic features into the 10-class classifier and get final scene class, an extra general classifier, namely a 3-class classifier is introduced, which we expect it to enhance the classification process and leverage the over-fitting issue of 10-class classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TWO-STAGE ASC SYSTEM DESIGN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Two-Stage Classification Procedure</head><p>In our setup, the 3-class classifier classifies an input scene audio into one of three broad classes: in-door, out-door, and transportation. This 3-class classification way is from our prior knowledge that scene audios can be roughly categorized into such three classes. The 10-class classifier is actually the main classifier, which assigns a given input audio clip into one of ten target acoustic scene classes, including airport, shopping mall, metro station, pedestrian street, public square, street traffic, tram, bus, metro, park. Each audio clip should belong to one of the three / ten classes. The final acoustic scene class is chosen by the score fusion of those two classifiers. If we let C 1 and C 2 denote the set of three broad classes, and ten classes, respectively, and let F 1 and F 2 indicate the output of the first and second classifier, respectively. The final predicted class Class(x) for the input x is:</p><formula xml:id="formula_0">Class(x) = argmax q,(p?C 1 ,q?C 2 ,p?q) F 1 p (x) * F 2 q (x),</formula><p>where p ? q means that p can be thought of a super set of q. For example, transportation class is the super set for bus, tram, and metro classes. Therefore, the probability of an input audio clip to be from the public square scene is equal to the product of the probability of out-door place, F 1 p (x), and that of public square, F 2 q (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">CNN based Classifiers and Ensemble</head><p>Three CNN based models are investigated to build our ASC system, namely Resnet, FCNN and fsFCNN. Resnet is a deep residual neural network <ref type="bibr" target="#b19">[20]</ref> empowered by residual learning. Residual learning is a major efficient method to avoid gradient vanish issues while increasing the depth of networks to allow high-level feature learning. Our Resnet structure is based on the model in <ref type="bibr" target="#b20">[21]</ref>, which has no frequency sub-sampling throughout the whole network. Each input feature map is divided into two sub-feature mapping along the frequency dimension. To be specific, if we have N frequency bins, the first N/2 and the second half are processed by two parallel stacked residual layers. A global pooling layer and 10-way soft-max are used to get the final utterance level prediction results. The FCNN (fully convolutional neural network) model is a VGG <ref type="bibr" target="#b21">[22]</ref>-like model which is built with 9 stacked convolutional layers with small-size kernel. Each convolutional layer is followed by a batch normalization operation and ReLU activation function. Dropout is also used in order to alleviate over-fitting issues. Before the final global average pooling layer, channel attention <ref type="bibr" target="#b22">[23]</ref> is applied to each output channel of the last layer. fsFCNN (frequency sub-sampling FCNN) is an extension of FCNN model which mainly has 2 more convolutional layers and reduces max-pooling size in the frequency axis. In our experiments we notice that it can help to leverage over-fitting issue.</p><p>Finally, we build an ensemble using those CNN models and make the final predictions. The output probabilities after softmax are summed by equal weights, and then the scene class with the maximum posterior represents the final classification decision. For the 3-class classifier, only Resnet and FCNN are used; whereas, for the 10-class classifier, all three CNN models are used. The two-stage score fusion is performed after model ensemble of classifiers in each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Data Augmentation Strategies for Training</head><p>When training the CNN models, 9 different data augmentation methods are investigated. The training for 3-class models and 10-class models adopt the same data augmentation strategy. They can be categorized into two main classes: generating extra data or not.</p><p>Data augmentation strategies, which do not generate any extra data, include: (i) Mixup <ref type="bibr" target="#b23">[24]</ref>, which randomly mixes data batches with corresponding labels; (ii) Random cropping <ref type="bibr" target="#b20">[21]</ref>, which randomly crops input features into ones with a smaller length along the time axis; (iii) SpecAugment <ref type="bibr" target="#b24">[25]</ref>, which randomly masks input features by zero along both time and frequency axes. SpecAugment is performed on batch level, and the parameter of mask is set to 10% of the time and frequency dimensions, respectively.</p><p>Strategies that generate extra data include: (i) Spectrum correction <ref type="bibr" target="#b25">[26]</ref>, which aims at transforming a given input spectrum to that of a reference, possibly ideal, device. Different from the original idea, we newly employ spectrum correction as a data augmentation technique, where we create a reference device spectrum, by averaging the spectrum from all training devices except that from device A, and then correct the spectrum of each training waveform collected with device A to obtain extra data. (ii) Reverberation with Dynamic Range Compression (DRC), where we simulate more audio clips from 'new devices' by introducing reverberation and applying DRC -this data-agumentation solution is also new for ASC. Room impulse responses <ref type="bibr" target="#b26">[27]</ref> are used to create reverberation into the device A data, and DRC is then applied to dynamically compress the amplitude range. Room impulse responses and DRC settings are randomly chosen to generate new training waveforms. (iii) Pitch shift, where we randomly shift the pitch of each audio clip based on the uniform distribution. (iv) Speed change, where we randomly change the audio speed based on the uniform distribution. If output waveform is longer than the original one, extra samples are dropped from the end, otherwise, padding is applied till attaining the same input length. (v) Random noise, we randomly add Gaussian noise to each training waveform. (vi) Mix audios, where we randomly mix two audios from the same acoustic scene class. It's device-independent so it may help simulate a 'new device'. Although the solutions from (iii) to (vi) are not new strictly speaking, their application for ASC device robustness has never been evaluated before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS &amp; ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setup</head><p>All ASC systems are evaluated on the DCASE 2020 task1a development data set <ref type="bibr" target="#b2">[3]</ref>, which consists of ?14K 10-second single-channel train audio clips and ?3K test audio clips recorded by 9 different devices, including real devices A, B, C, and simulated device s1-s6. Only device A, B, C, s1-s3 are in the training set; whereas, devices s4-s6 are not available in the training phase. The greatest amount of training audio clips are recorded with device A, namely over 10K audio clips. In the test set, the number of audio clips from each device is the same. For each input audio clip, a short-time Fourier transform (STFT) with 2048 FFT points is applied, using a window size of 2048 samples and a hop length of 1024 samples. The Librosa <ref type="bibr" target="#b27">[28]</ref> library is employed to extract log-mel filter bank (LMFB) features. Log-mel deltas and delta-deltas without padding are also computed, which finally generates an input feature tensor with size of 423?128?3. Before feeding the input tensors into the CNN classifier, we perform utterance-level scaling operation to scale LMFB coefficients into [0,1]. All deep models in this work are built with Keras <ref type="bibr" target="#b28">[29]</ref>. Stochastic gradient descent (SGD) with a cosine-decayrestart learning rate scheduler is used to train all deep models. Max- imum and minimum learning rates are 0.1, and 1e-5, respectively. 1 <ref type="table">Table 1</ref> shows the accuracy for 3-class classifiers, namely Resnet, FCNN and Ensemble of these two models. The 3-class classification task is relatively easy since an accuracy above 90% is attained by each of the three models. FCNN shows a better accuracy than Resent, and the ensemble of these two models allows a further boost of the classification result. In the following experiments with the two-stage system, we use Ensemble system as the 3-class classifier. All main experimental results concerning the 10-class ASC task are shown in <ref type="table">Table 2</ref>. In the training set, device A data accounts for around 75%, and device B, C, s1-s3 accounts for around 5%, respectively. Thus, we can think of device A as a source device, and the remaining ones as target devices. Based on the device information of data, we divide the test set into four different subsets, which represent real source data (device A), real target data (device B &amp; C), target seen-simulated data (device s1-s3), and target unseen-simulated data (device s4-s6). The first row in <ref type="table">Table 2</ref> gives ASC accuracy of the official baseline system <ref type="bibr" target="#b2">[3]</ref>, which uses a two-fully connected layer neural classifier, and OpenL3 <ref type="bibr" target="#b14">[15]</ref> to extract input audio embedding. From baseline results, we can argue that good ASC accuracy can be attained on real source data (device A), but a severe performance drop is observed on other devices. Specifically, a severe ASC accuracy drop is reported on unseen devices (s4-s6), where the ASC accuracy is as low as 44.3%. Rows from 6th through 9th in Table 2 display ASC results attained with a 10-class classifier. Resnet attains a significant improvement on all test devices over the baseline system. FCNN and fsFCNN outperform Resnet on all testing scenarios, and FCNN achieves slightly better results than fsFCNN. Finally, the ensemble of these three models attains a 79.4% overall ASC accuracy, which is a meaningful improvement compared to any of the single-stage models reported in <ref type="table">Table 2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Results</head><p>The experimental results of two-stage systems are shown in the 1 Code available: https://github.com/MihawkHu/DCASE2020_task1 Those results confirm the effectiveness of the proposed two-stage system with respect to improving device robustness.</p><p>To better compare our results, the results of top-5 ranking systems (excluding ours) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> are also listed from 2st to 5th rows in <ref type="table">Table 2</ref>. The experimental results clearly demonstrated that the proposed two-stage system achieves a competitive performance with all benchmark models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation of Data Augmentation Strategies</head><p>We now investigate the effectiveness of different data augmentation strategies in combination. We test only the Resnet-based ASC system to keep the experimental setup consistent. <ref type="table" target="#tab_1">Table 3</ref> shows the classification accuracy attained with different data augmentation schemes. It should be noted that mixup and random cropping are always employed in the training phase.</p><p>When comparing the first two rows, we can see that specAugment can boost the overall ASC accuracy from 71.0% to 71.6%, but on unseen data the accuracy becomes worse (from 69.5% to 67.7%). Next, we add spectrum correction, which results in an increase in the amount of training data. As described before, device A data is used to generate data from other devices, in which we get the same size extra data with device A data. From the results in <ref type="table" target="#tab_1">Table 3</ref>, we can observe that spectrum correction has a beneficial effect on overall classification results, but the accuracy on device A decreases. That outcome is not unexpected since there is now much more training data from other devices than from device A. New extra data is generated by reverberation with DRC, which aims to simulate data from more different devices. From the results we can see when extra data with reverberation and DRC is added in the training set, the overall ASC accuracy is further boosted (72.8%), where most of the gain is imputable to an increse of the accuracy on the unseen test data. Finally, we employ four more data augmentation approaches, including pitch shift, speech change, random noise and mix audios. As shown in the last row of <ref type="table" target="#tab_1">Table 3</ref>, performance can be further boosted. Using all strategies, a 74.6% accuracy on Resnet is attained 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Neural Saliency Analysis via Class Activation Mapping</head><p>Neural saliency methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref> aim to provide interpretable analyses on the weight distribution over hidden neurons of a well-trained artificial neural network model. Benchmark saliency methods, such as class activation mapping (CAM) <ref type="bibr" target="#b17">[18]</ref>, have shown a correlation between model performance and ability in spotting physical pattern in DNN-based acoustic models <ref type="bibr" target="#b33">[34]</ref>.</p><p>In this work, we use CAM <ref type="bibr" target="#b17">[18]</ref> to gain a better understanding about what sound patterns are found by our CNNs to accomplish the ASC task. Indeed, CAM can highlight the class-specific discriminative regions in the input feature map. In other words, CAM helps generating a two-dimension activation map that can be used to better interpret the prediction decision made by deep architectures. In our case, where an audio clip is transformed into a time-frequency representation, CAM reveals whether the ASC decision is triggered by CNN's attention posed on time and frequency regions of input audio clip having a meaningful semantic content with respect to the target acoustic scene. Examples of some analysis results obtained with CAM are shown in <ref type="figure" target="#fig_1">Figures 2 and 3</ref>, where the spectrogram of the input audio clip (top panel), and the CAM for the 3-class classifier (center panel), and the 10-class classifier (bottom panel) are shown. Regions in CAM with deep red color imply that the CNN pays more attention to them, and those regions have a higher classspecific discriminative power. As the size of FCNN and fsFCNN's output feature map is too small to map back to the input, only Resnet model is used for neural saliency analysis.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, a 10-second audio clip referring to a metro station scene is visualized; the audio clip contains brake and horn sounds from 0s to around 5s. After 5s, only reverberation remains in the audio. The CAM result indeed reveals that the CNN pays more attention to the segment between 0s and 5s. Moreover, 3-class classifier and 10-class classifier have similar activation regions, and the 3-class classifier seems to give more weight to the information in that region. Indeed, if we check the confidence in making the prediction, i.e., scaled class posterior probability from the CNN, the proposed two-stage fusion can increase it from 70.7% to 85.0%. The same pattern can be observed in the results of <ref type="figure" target="#fig_2">Figure 3</ref>, in which it contains a 10-second audio from a bus station. This audio has brake sound between around 2s and 5s, and human speech starts around 5s till 7s. From the CAM results, we can see that CNN pays more attention to the segment from 2s to 5s, which contains a brake sound. However, the CNN is not interested in the human speech although it is clearly audible and related to the bus arriving announcement. This result is indeed interesting: Human beings would use the bus-station announcement to make ASC decision; however, the CNN cannot perform speech recognition, so it may simply classify that region as belonging to an ideal human speech class, which however appears in many scenes and has therefore a low discriminative power. We can thus argue that CNNs use acoustic events to classify the input scene audios. Specifically, CNNs pay more attention to audio segments containing acoustic events, such as birds sound from park, or car horn sound from street traffic. It should be noted that our conclusion with CAM is different from that in <ref type="bibr" target="#b18">[19]</ref>, where the authors claim that CNNs use background sound rather than acoustic events. By comparing the experimental setups, the main difference is that each audio clip is chunked into 10 1-second segments in their setup; in contrast, we use the whole 10-second audio clip as the input. Since acoustic events usually last for several seconds, it's difficult for CNNs to capture them when inputs are very short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>This work focuses on device robustness in acoustic scene classification. We propose a novel two-stage ASC framework based on CNNs. Three different fully CNN models and several novel data augmentation strategies are investigated to improve device robustness. A general 3-class classifier and a specific 10-class classifier are combined through score fusion. Experiments on the DCASE 2020 task1a development set show the effectiveness of our solution. Specifically, our best system, a two-stage fusion of a CNN-based ensemble, obtains a state-of-the-art 81.9% average ASC accuracy. Moreover, our CAM-based neural saliency analysis demonstrates that CNNs pay particular attention to audio segments strictly related acoustic events rather than simply fetching information from the background environmental sound.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Nerual saliency analysis via CAM of an audio clip from a metro station (metro station vienna 87 2389 a.wav). The three plots are: (a) spectrogram, (b) CAM of 3-class classifier, (c) CAM of 10-class classifier. In this 10-second audio, brake and horn sound starts from 0s to around 8s and only reverberation remains after 5s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Nerual saliency analysis via CAM of an audio clip from a bus (bus prague 1102 42431 a.wav). The three plots are (a) spectrogram, (b) CAM of 3-class classifier, (c) CAM of 10-class classifier, respectively. In this 10-second audio, brake sound starts from around 2s to around 5s, human talk starts from around 5s to 7s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Classification accuracy of different 3-class models. Overall results on DCASE 2020 task1a development set. All proposed data augmentation strategies are used in all our models.</figDesc><table><row><cell cols="5">3-class Model Resnet FCNN Ensemble</cell><cell></cell></row><row><cell>Acc. %</cell><cell cols="2">91.4</cell><cell>92.9</cell><cell>93.2</cell><cell></cell></row><row><cell>System</cell><cell>A %</cell><cell>B&amp;C %</cell><cell>s1-s3 %</cell><cell>s4-s6 %</cell><cell>Avg. %</cell></row><row><cell>Baseline [3]</cell><cell>70.6</cell><cell>61.6</cell><cell>53.3</cell><cell>44.3</cell><cell>54.1</cell></row><row><cell>Suh et al. [30]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.4</cell></row><row><cell>Gao et al. [31]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.5</cell></row><row><cell>Liu et al. [32]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.1</cell></row><row><cell>Koutini et al. [33]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.3</cell></row><row><cell>Resnet</cell><cell>83.0</cell><cell>76.1</cell><cell>73.6</cell><cell>71.0</cell><cell>74.6</cell></row><row><cell>FCNN</cell><cell>87.3</cell><cell>79.5</cell><cell>75.7</cell><cell>73.0</cell><cell>76.9</cell></row><row><cell>fsFCNN</cell><cell>83.9</cell><cell>78.6</cell><cell>75.4</cell><cell>72.8</cell><cell>76.2</cell></row><row><cell>Ensemble</cell><cell>87.0</cell><cell>81.5</cell><cell>78.0</cell><cell>76.9</cell><cell>79.4</cell></row><row><cell>2-stage Resnet</cell><cell>84.5</cell><cell>78.6</cell><cell>76.2</cell><cell>76.4</cell><cell>77.7</cell></row><row><cell>2-stage FCNN</cell><cell>89.1</cell><cell>82.9</cell><cell>78.5</cell><cell>76.9</cell><cell>80.1</cell></row><row><cell>2-stage fsFCNN</cell><cell>83.9</cell><cell>81.2</cell><cell>78.6</cell><cell>76.4</cell><cell>79.0</cell></row><row><cell cols="2">2-stage Ensemble 87.9</cell><cell>84.1</cell><cell>80.4</cell><cell>79.9</cell><cell>81.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Accuracy comparison of different data augmentation strategies on Resnet. Mixup and random cropping are always employed. 'sa' indicates specAugment. 'sc' indicates spectrum correction. 'r' indicates reverberation with DRC. 'aug' indicates another four augmentation methods, including pitch shift, speed change, random noise and mix audios. 8% for fsFCNN, and 2.5% for the ensemble. The best performance is attained by the two-stage of three models' ensemble, which achieves 81.9% overall ASC accuracy. Although the top performance is still delivered on the source device A (87.9%), the average ASC accuracy on the remaining test devices is above 79%, which compares favourably against the baseline system. When comparing ASC accuracy between seen-simulated test set (device s1-s3) and unseen-simulated test set (device s4-s6), we can argue that the gap in performance is mild (80.4% vs. 79.9%), and they are close to the overall average accuracy (80.4% vs. 79.9% vs. 81.9%).</figDesc><table><row><cell>System</cell><cell>A %</cell><cell>B&amp;C %</cell><cell>s1-s3 %</cell><cell>s4-s6 %</cell><cell>Avg. %</cell></row><row><cell>Resnet</cell><cell>78.8</cell><cell>72.1</cell><cell>69.3</cell><cell>69.5</cell><cell>71.0</cell></row><row><cell>+sa</cell><cell>80.3</cell><cell>73.5</cell><cell>71.4</cell><cell>67.7</cell><cell>71.6</cell></row><row><cell>+sa+sc</cell><cell>79.1</cell><cell>75.0</cell><cell>70.7</cell><cell>68.9</cell><cell>72.0</cell></row><row><cell>+sa+sc+r</cell><cell>80.3</cell><cell>74.7</cell><cell>71.4</cell><cell>70.3</cell><cell>72.8</cell></row><row><cell cols="2">+sa+sc+r+aug  *  83.0</cell><cell>76.1</cell><cell>73.6</cell><cell>71.0</cell><cell>74.6</cell></row><row><cell cols="6">last four rows of Table 2. When comparing single-stage 10-class</cell></row><row><cell cols="6">classifier with the two-stage system, a significant improvement can</cell></row><row><cell cols="6">be attained. For example, comparing 'Resnet' with '2-stage Resnet',</cell></row><row><cell cols="6">3.1% absolute improvement on overall accuracy is observed (74.6%</cell></row><row><cell cols="6">vs. 77.7%). Accuracies on all the test sets have indeed improved,</cell></row><row><cell cols="6">especially on the unseen-simulated test set (device s4-s6), where</cell></row><row><cell cols="6">a 5.4% absolute improvement is obtains. The overall absolute ac-</cell></row><row><cell cols="6">curacy gains with the proposed two-stage technique are 3.2% for</cell></row><row><cell>FCNN, 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also increased amount of Resnet parameter, i.e, the channel number, when all data-augmentation schemes are used.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">DCASE 2017 challenge setup: Tasks, datasets and baseline system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multi-device dataset for urban acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in DCASE</title>
		<imprint>
			<biblScope unit="page" from="9" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Acoustic scene classification in dcase 2020 challenge: generalization across devices and low complexity solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detection and classification of acoustic scenes and events: Outcome of the dcase 2016 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM T-ASLP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="379" to="393" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Acoustic scene classification using convolutional neural network and multiple-width frequency-delta data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02383</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Acoustic scene classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Battaglino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lepauloux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Device-robust acoustic scene classification based on two-stage categorization and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note>Tech. Rep., DCASE2020 Challenge</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Robust Framework for Acoustic Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huy</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramaswamy</forename><surname>Palaniappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3634" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-attention mechanism based system for dcase2018 challenge task1 and task4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in DCASE</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Attention based cldnns for short-duration acoustic scene classification.,&quot; in Interspeech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alwan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="469" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mixup-based acoustic scene classification using multi-channel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Acoustic scene classification with mismatched devices using cliquenets and mixup data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Integrating the data augmentation scheme with various classifiers for acoustic scene modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
	<note>Tech. Rep., DCASE2019 Challenge</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial network based acoustic scene training set augmentation and selection using svm hyper-plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE</title>
		<imprint>
			<biblScope unit="page" from="93" to="97" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Look, listen, and learn more: Design choices for deep audio embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="3852" to="3856" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep neural network bottleneck features for acoustic event recognition.,&quot; in Interspeech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2954" to="2957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ase: Acoustic scene embedding using deep archetypal analysis and gmm.,&quot; in Interspeech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Abrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thakur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3299" to="3303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapedriza</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Learning Deep Features for Discriminative Localization.,&quot; CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhancing sound texture in cnn-based acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="815" to="819" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Acoustic scene classification using deep residual networks with late fusion of separated high and low frequency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="141" to="145" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A multiple-instance densely-connected convnet for aerial scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4911" to="4926" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Acoustic scene classification for mismatched recording devices using heatedup softmax and spectrum correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kosmider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="126" to="130" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A summary of the reverb challenge: state-of-the-art and remaining challenges in reverberant speech processing research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Emanu?l</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Habets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Kellermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Leutnant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Designing acoustic scene classification models with CNN variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE2020 Challenge</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Acoustic scene classification using deep residual networks with focal loss and mild domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcdonnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE2020 Challenge</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Acoustic scene classification with residual networks and attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep., DCASE2020 Challenge</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">CP-JKU submissions to DCASE&apos;20: Low-complexity cross-device acoustic scene classification with RF-regularized CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Henkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note>Tech. Rep., DCASE2020 Challenge</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="631" to="648" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
