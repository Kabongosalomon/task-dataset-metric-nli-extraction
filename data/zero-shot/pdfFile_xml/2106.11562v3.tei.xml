<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Cha</surname></persName>
							<email>sungmin.cha@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomyoung</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Face</orgName>
								<orgName type="institution">NAVER Clova</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
							<email>youngjoon.yoo@navercorp.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Face</orgName>
								<orgName type="institution">NAVER Clova</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Moon</surname></persName>
							<email>tsmoon@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a solid state-of-the-art baseline for a class-incremental semantic segmentation (CISS) problem. While the recent CISS algorithms utilize variants of the knowledge distillation (KD) technique to tackle the problem, they failed to fully address the critical challenges in CISS causing the catastrophic forgetting; the semantic drift of the background class and the multi-label prediction issue. To better address these challenges, we propose a new method, dubbed SSUL-M (Semantic Segmentation with Unknown Label with Memory), by carefully combining techniques tailored for semantic segmentation. Specifically, we claim three main contributions. (1) defining unknown classes within the background class to help to learn future classes (help plasticity), (2) freezing backbone network and past classifiers with binary cross-entropy loss and pseudo-labeling to overcome catastrophic forgetting (help stability), and (3) utilizing tiny exemplar memory for the first time in CISS to improve both plasticity and stability. The extensively conducted experiments show the effectiveness of our method, achieving significantly better performance than the recent state-of-the-art baselines on the standard benchmark datasets. Furthermore, we justify our contributions with thorough ablation analyses and discuss different natures of the CISS problem compared to the traditional class-incremental learning targeting classification. The official code is available at https://github.com/clovaai/SSUL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Class incremental learning (CIL) problem, in which a learner should incrementally learn newly arriving class objects while not "catastrophically" forgetting the past learned classes, is one of the fundamental, yet still open, problems in machine learning. After the seminal work, <ref type="bibr" target="#b27">[27]</ref>, most of the recent neural network-based CIL research has focused on the classification setting, and various approaches have been proposed to address the main challenge of the problem, the so-called plasticitystability dilemma, e.g., <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr">1,</ref><ref type="bibr">2,</ref><ref type="bibr">8]</ref>, to name a few.</p><p>The CIL framework has been extended to more complex semantic segmentation tasks <ref type="bibr" target="#b24">[24,</ref><ref type="bibr">3,</ref><ref type="bibr">7]</ref>, motivated by the practical need in various applications such as autonomous driving. One of the key additional difficulties of the class-incremental semantic segmentation (CISS) problem lies in the semantic drift of the background class present in the incrementally arriving training data. Namely, the label 'background (BG)" is assigned to all the pixels not included in the current class object region. The "BG" pixels belong to three categories: future object classes that the model does not yet observes, past object classes that are already learned, and the true background.</p><p>A few recent works attempted to address the above semantic drift issue by leveraging and modifying the knowledge-distillation (KD) <ref type="bibr" target="#b20">[20]</ref> technique, popular for CIL in standard image classification. Namely, the initiative study <ref type="bibr" target="#b24">[24]</ref> modified the KD for CISS straightforwardly, and <ref type="bibr">[3]</ref> proposed a strategy to incorporate the BG class probability in computing the cross-entropy and distillation losses. Furthermore, <ref type="bibr">[7]</ref>, the current state-of-the-art, utilized the pseudo-labeling of the BG pixels of the current task data with the model of the previous task using the cross-entropy loss, and it applies the multi-scale feature distillation scheme adopted from <ref type="bibr">[8]</ref>. However, we argue that above works only partially addressed the BG label issue. That is, <ref type="bibr">[3]</ref> naively added the class probabilities to modify the cross-entropy and distillation losses making it hard to have fine-grained learning of prediction probability for each class, each pixel. Moreover, <ref type="bibr">[7]</ref> could only handle the BG class pixels to the past classes via pseudo-labeling and lacked any mechanism for handling the future class case, one possible option for the the BG class. Consequently, their CIL segmentation performance, measured by mean Intersection-over-Union (mIoU), has been significantly lower than the upper-bound, the case of joint-training with all the labels.</p><p>This paper first identifies that the multi-label prediction of semantic segmentation is another critical challenge of CISS and proposes SSUL-M (Semantic Segmentation with Unknown Label with Memory) to address the challenge. Specifically, Our contributions are summarized as follows. First, we introduce an additional "Unknown" class label assigned to the objects in the background, detected by the off-the-shelf saliency-map detector. We let the base feature extractor distinguish the representations of the potential future class objects and the actual background region by augmenting the BG label with this additional class. Second, we adopt the pseudo-labeling strategy as in <ref type="bibr">[7]</ref> and further augment the BG &amp; Unknown class labels with the past class labels, but with two essential differences in concrete learning strategies to stabilize the classification scores and improve the precision of the prediction. One is using the separate sigmoid, instead of the softmax, output classifier for each class so that the model can learn the logit score in an absolute sense per class. The other is freezing the base feature extractor and the classifiers for past classes after initial learning to strictly maintain the past classes' knowledge. Third, we utilize an exemplar memory to store a tiny portion of training data, including past classes, as anchors and further improve the mIoU. Note that using the exemplar memory is a standard practice for CIL in classification but has been overlooked in CISS. Moreover, we show that the memory helps improve the mIoU for the current classes, in contrast to a common belief that it is a tool to prevent forgetting of past classes.</p><p>By integrating the above contributions, SSUL-M achieved the state-of-the-art performance on popular benchmarks with a significantly large margin over the recent baselines <ref type="bibr">[3,</ref><ref type="bibr">7]</ref>, particularly when the number of incremental tasks gets larger. Furthermore, we conduct extensive ablation studies and both quantitative and qualitative analyses to convincingly highlight the strength of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Class Incremental Learning (CIL) CIL <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b25">25]</ref> considers the setting in which new class objects arrive incrementally and the model needs to classify all the classes without storing all the past class data. It is well known that neural network-based CIL suffers from catastrophic forgetting <ref type="bibr" target="#b23">[23]</ref>, caused by the score bias toward the new classes due to the training data imbalance. Most CIL studies have focused on the classification tasks, and the exemplar-memory based methods combined with KD <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr">1,</ref><ref type="bibr">2,</ref><ref type="bibr">8]</ref> achieved the state-of-the-art performance by re-balancing the biased predictions of the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Incremental Semantic Segmentation (CISS)</head><p>Contrary to the current trend of CIL for classification, CIL for semantic segmentation have only focused on the setting without utilizing the exemplar-memory <ref type="bibr" target="#b24">[24,</ref><ref type="bibr">3,</ref><ref type="bibr">7]</ref>. <ref type="bibr">[3]</ref> first addressed the semantic drift of BG label, and <ref type="bibr">[7]</ref> proposed to use pseudo-labels to augment the BG label, all without exemplar-memory. To our knowledge, we firstly use exemplar-memory and potential future classes in BG label to solve CISS.</p><p>Saliency Map Detection Salient object detection is a fundamental computer vision task that identifies the most visually distinctive objects in an image. We use the off-the-shelf saliency-map detector to define the Unknown class in the BG label. The early salient object detection method, DRFI <ref type="bibr" target="#b13">[13]</ref>, conducts a multi-level segmentation with a random forest regressor to detect a salient object. Recent deep neural network based method exploits rich multi-scale feature maps with short connections <ref type="bibr">[11]</ref> and pooling-based modules <ref type="bibr" target="#b21">[21]</ref>. The saliency information has widely been utilized on various tasks. For example, the weakly-supervised semantic segmentation approaches <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b14">14]</ref> generate pseudo-labels filtering out the background regions using saliency map, and recent data augmentations <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b16">16]</ref> also exploit the saliency information to find the optimal mixing of mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations and Problem Setting</head><p>We consider exactly the same setting as considered in <ref type="bibr">[3,</ref><ref type="bibr">7]</ref>. In CISS, the learning happens with t = 1, . . . , T incremental tasks. For each incremental state t, we observe a training dataset D t that consists of pairs (x t , y t ), in which x t ? X denotes an input image of size N , and y t ? Y N t denotes the corresponding ground-truth (GT) pixel labels. The label space Y t = {c b } ? C t consists of the current classes in task t, C t , and the dummy background class c b , that is assigned to all pixels that do not belong to C t . Thus, the c b label can be assigned to the objects with the past classes C 1:t?1 , the objects with the future classes C t+1:T , or the true background pixels.</p><p>After learning task t, the semantic segmentation model f t ? is required to predict whether a pixel belongs to all the classes learned so far, C 1:t = C 1:t?1 ? C t , or the true background. As in other work, we assume the classes in each C t are disjoint. Typically, the model is defined to be a mapping f t ? : X ? R N ?|Y t | , in which Y t = C d ? C 1:t with C d containing dummy labels. All previous work <ref type="bibr">[3,</ref><ref type="bibr">7]</ref> simply set C d = {c b }, but in our work, we also add the separate "Unknown" class label, c u , to C d , hence, set C d = {c b , c u }. Later, we show defining this additional label c u in our model output plays a critical role in improving the learning capability for future classes. The architecture of f t ? is typically a fully-convolutional network, which consists of a convolutional feature extractor, h t ? , followed by the final 1 ? 1 classifier filters, {? t c } c?Y t , one for each output class in Y t . The learning of f t ? is done in conjunction with the previous model f t?1 ? : X ? R N ?|Y t?1 | to prevent forgetting during incrementally updating the model. Determining which output classifier (e.g., softmax or sigmoid) to use for each pixel as well as how to transfer the knowledge of f t?1 ? to f t ? (e.g., knowledge distillation or model freezing) are design choices, and we elaborate our choices more in details in the subsequent sections. Furthermore, we denote M as the exemplar memory, which can store a small number of samples from past training data, D 1:t?1 , and use it for learning f t ? . Once the learning of f t ? is done, the prediction for pixel i of an input image x is obtained b?</p><formula xml:id="formula_0">y t i = arg max c?Y t f t ?,ic (x),</formula><p>and the performance is measured by the mean intersection-over-union (mIoU) metric for the classes in Y t . (Only in the evaluation phase, we merge c b and c u for computing mIoU of the BG class.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Two Additional Challenges of CISS</head><p>In addition to the typical reason for causing catastrophic forgetting in CIL for classification, i.e., the prediction bias due to the data imbalance, we note there are two additional unique challenges in CISS.</p><p>The first challenge, as mentioned in the Introduction and in previous work <ref type="bibr">[3,</ref><ref type="bibr">7]</ref>, comes from the semantic drift of the background (BG) label. Namely, the ground-truth label associated with the pixels of the object can change depending on the incremental state. For example, when a pixel is labeled as "BG" at state t, it is possible that the corresponding pixel would be labeled as " sofa" at state t ? 1 (i.e., past class) or labeled as "dog" at state t + 1 (i.e., future class), depending on the true object to which the pixel belongs. Therefore, naively learning with the "BG" target label for the pixel at state t could cause either severe forgetting of the past class (i.e., hurting stability) or interfering the learning of the future class (i.e., hurting plasticity).</p><p>The second challenge stems from the fact that semantic segmentation is a multi-label prediction problem. Namely, for a given image, the segmentation model needs to output a set of classes, in contrast to the classification model which outputs only a single class. Therefore, the precision of the prediction for each pixel becomes important in addition to the recall as is reflected in the mIoU performance metric; i.e., not only predicting a correct class for a pixel is important, but also not predicting a wrong class is important for the overall metric. That is, if the prediction for every pixel <ref type="figure" target="#fig_2">Figure 1</ref>: Overall procedure of SSUL. Given (x t , y t ) ? D t , the augmented label? t is first obtained by the "Unknown" class modeling and pseudo-labeling. Then, using? t as a target, we update f t ? with model freezing and BCE loss. The exemplar-memory is also updated with class-balanced sampling.</p><p>gets biased toward the current classes in C t , the mIoU's for the past classes in C 1:t?1 as well as the current classes would significantly drop jointly. This point is exactly why even the mIoU's for the newly learned classes are very low in <ref type="bibr">[3,</ref><ref type="bibr">7]</ref>. Note this is in a stark difference with the classification, in which the accuracy of the current classes would remain high even with the severe bias and forgetting, since it is a single-label prediction problem.</p><p>To address above unique challenges of CISS, we devise our SSUL-M by carefully combining several ideas, of which overall procedure is outlined in <ref type="figure" target="#fig_2">Figure 1</ref>. We now elaborate on our three main contributions in details: 1) label augmentation for BG class with Unknown class and pseudo-labels, 2) stable score learning with model freeze and sigmoid output, and 3) usage of tiny exemplar memory with class-balanced sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contribution 1: Label Augmentation for BG Class</head><p>Here, we describe how we generate an augmented</p><formula xml:id="formula_1">target label? t ? (Y t ) N = {c b , c u , C 1:t } N given a training sample (x t , y t ) in D t . Recall that y t ? (Y t ) N = {c b , C t } N .</formula><p>Unknown class modeling In order to handle the case in which the potential future class objects are labeled as BG, we propose to use a novel unknown class label, c u , that is defined to designate any foreground objects that are not the current classes in C t . Specifically, as depicted in the top part of <ref type="figure" target="#fig_2">Figure 1</ref>, we first apply an off-the-shelf salient object detector <ref type="bibr">[11]</ref> S : X ? {0, 1} N to the input image x t , which assigns 1 to the pixels if they are salient (i.e., part of a foreground object) and 0 otherwise. Then, we assign c u to the pixels that are BG-labeled but salient; namely, we set?</p><formula xml:id="formula_2">t,i = c u if (y t,i = c b ) ? (S(x t ) i = 1) for pixel i.</formula><p>In our experiments, we show this Unknown class label plays a critical role so that the base feature extractor can distinguish the representations of the potential future class objects and the true background, even before observing the class labels.</p><p>Pseudo-labeling Once augmenting with c u is done, we further augment with pseudo-labels generated from the previous model f t?1 ? to maintain the knowledge of the past classes, similarly as in <ref type="bibr">[7]</ref> and as shown in the middle part of <ref type="figure" target="#fig_2">Figure 1</ref>. Namely, we denote? t?1,i = arg max c?Y t?1 f t?1 ?,ic (x t ) as the prediction of f t?1 ? for the i-th pixel and set? t,</p><formula xml:id="formula_3">i =? t?1,i if (y t,i = c b ) ? (? t?1,i ? C 1:t?1 ) ? (? i &gt; ? ), in which ? i = max c?C 1:t?1 ?(f t?1</formula><p>?,ic (x t )) stands for the confidence of prediction for? t?1,i ? C 1:t?1 where ?(?) is the sigmoid function, and ? is a threshold (set to 0.7). In words, we assign the pseudolabels (i.e., the predictions from the previous model) to the pixels that are BG-labeled only when the predictions are made to be the past object classes (excluding c b and c u ) with enough confidence.  <ref type="figure">Figure 2</ref>: Comparison of the effect on the classification scores for sigmoid with binary cross-entropy (BCE) and softmax with cross-entropy (CE). For a pixel with noisy label? t,i = c b (i.e., BG label (black) when the true is "sofa" (green), see the red dashed circle at task t), even when the classifier for "sofa" ({? c } c?C 1:t?1 ) is frozen, the softmax would cause the BG class score to rise above the score for "sofa" (see the red dashed arrow), whereas the sigmoid would let the BG score rise only moderately (see the blue dashed arrow). Thus, the classification for the pixel could still become "sofa" for sigmoid, whereas softmax would make a false prediction as BG.</p><formula xml:id="formula_4">!"# # !"# Freeze Freeze Freeze Freeze ! # ! 0.</formula><p>In summary, at the incremental state t, the augmented target label for the i-th pixel becomes</p><formula xml:id="formula_5">y t,i = ? ? ? ? ? ? ? ? ? y t,i if y t,i ? C t y t?1,i if (y t,i = c b ) ? (? t?1,i ? C 1:t?1 ) ? (? i &gt; ? ) c u if (y t,i = c b ) ? (S(x t ) i = 1) ? {(? t?1,i ? C d ) ? (? i ? ? )} c b else,<label>(1)</label></formula><p>in which the pseudo-label is generated only for the incremental state t ? 2. <ref type="figure" target="#fig_2">Figure 1</ref> shows a concrete example of augmentating y t to? t , in which C t?1 = {sofa} and C t = {person}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Contribution 2: Stable Score Learning</head><p>We argue that simply using? t as a target and training f t ? with softmax output layer would make the output scores too unstable as the incremental learning continues. The reason is because the augmented labels are noisy and the softmax computes the prediction probability in a relative way. As shown in our experiments, such instability significantly hurts the precision of the multi-label prediction. To that end, we propose the following three strategies for the stable learning of output scores.</p><p>Model freezing Instead of updating the full model with? t at every state t, we freeze the convolutional feature extractor, h ? , after initial learning (t = 1) as well as the classifiers for the past classes, {? c } c?C 1:t?1 , and only learn the 1 ? 1 filters ? t c b , ? t cu and {? t c } c?Ct . Such strict model freezing certainly can prevent forgetting, but is counter-intuitive from the plasticity viewpoint. However, thanks to the unknown class c u , it turns out the feature extractor h ? can roughly learn the representations for the potential future class objects present with BG label in D 1 . Thus, it becomes sufficient to learn the decision boundaries for c b , c u , and C t on those representations at state t.</p><p>Sigmoid with binary cross entropy loss For learning ? t c b , ? t cu and {? t c } c?Ct , the choice of output layer and loss function becomes important to make the output score stable. A common choice is the softmax with cross-entropy loss, however, since the target labels for c b and c u in? t are noisy, we observe the relative scoring of softmax could cause significant forgetting of past classes C 1:t?1 . To see this, let s ic = [f t ? (x t )] ic be the score for class c at pixel i. When the softmax with cross-entropy (CE) is used, the gradient of the loss at s ic becomes ?L t</p><formula xml:id="formula_6">CE (?)/?s ic = p ic ? 1{c =? t,i }, in which p ic = exp(s ic )/( c exp(s ic )).</formula><p>The issue occurs when? t,i = c b or c u , while the true label for the pixel should be the past classes in C 1:t?1 . (Thus, the pseudo-label? t?1,i missed the pixel, which could often happen.) Then, the gradient descent learning will force s ic b or s icu to become much higher than {s ic } c?C 1:t?1 , the scores obtained from the frozen classifiers for past classes. Thus, at test time, for a similarly confusing pixel, the model would tend to predict as c b or c u , hence, causing the forgetting of past classes even though their classifiers are fixed.</p><p>Therefore, we instead use the sigmoid output with binary cross-entropy (BCE) loss independently for each class. In that case, the gradient of the loss at s ic becomes ?L t BCE (?)/?s ic = ?(s ic )?1{c =? t,i }, hence, even for above noisy target label case, the scores s ic b or s icu will only increase to a certain absolute level regardless of other class score values, {s ic } c?C 1:t?1 . Thus, at test time, a similar pixel still may be predicted as a past class in C t?1 since the frozen past classifiers would still output a considerable score -this subtle difference between the loss functions is illustrated in <ref type="figure">Figure 2</ref>.</p><p>Weight transfer from unknown class classifier Finally, for learning {? t c } c?Ct , we initialize all the filters with ? t?1 cu , the classifier learned for the unknown class c u at the previous state. The reasoning is that the classes in C t would have been labeled as c u (as the potential future classes) in state t ? 1, hence, such weight transfer from ? t?1 cu would yield stable and fast learning of {? t c } c?Ct .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Contribution 3: Tiny Exemplar Memory</head><p>Using exemplar-memory to store a small portion of past training data for CIL is backed with a theoretical finding <ref type="bibr" target="#b19">[19]</ref> as well as strong empirical results <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr">2,</ref><ref type="bibr">1,</ref><ref type="bibr">4]</ref> for classification.</p><formula xml:id="formula_7">Algorithm 1: Pseudo code of SSUL-M 1: Require f ? , M, T , n epochs , batch size K 2: f ? ? Initialize h ? , ? c b , ? cu 3: Initialize M 4: for t ? 1, ..., T do 5: f t ? ? Initialize {? t c } c?Ct 6: if t = 1 then 7: Freeze h ? , {? t c } c?C 1:t?1 8: {? t c } c?Ct ? Weight Transfer(? t?1 cu ) 9:</formula><p>end if 10:</p><p>for n ? 1, ..., n epochs do 11: Hence, we propose to use it for CISS as well with a tailored class-balanced sampling strategy. The main rationale of using the memory is to make sure to include at least one sample with correct GT label per each class in C 1:t?1 in the training set for state t.</p><formula xml:id="formula_8">for B n K/2 ? D t do 12: B M K/2 ? M 13:B n ,B M ? Label.Aug.(B n , B M ) 14: ? ?SGD(B n ?B M , L t BCE (?))</formula><p>Namely, even though the pseudo-label? t?1 can provide labels for the classes in C 1:t?1 , it is also possible that the given image x t would never contain object cues for C 1:t?1 . In such a case, even with model freezing and stable score learning, when the confidence for the new class is learned to be high for a pixel (potentially for an old class), the prediction for the pixel could get biased toward the new class, causing the forgetting of the old class. Therefore, by denoting M = |M|, after learning incremental state t?1, we sample M/|C 1:t?1 | data points from D t?1 , and store them in the memory M by removing equal number of samples per each class in C 1:t?2 from M. In this way, M always contains at least one sample from each class in C 1:t?1 , and we show in the experiments that this classbalanced strategy is more helpful than random sampling <ref type="bibr">[4]</ref> as is done for CIL for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Summary</head><p>We summarize our SSUL-M algorithm in Algorithm 1, in which our contributions described in Section 3.3? Section 3.5 are denoted in the typewriter font. Namely, Label.Aug. stands for generating? t as in <ref type="formula" target="#formula_5">(1)</ref> for the selected batches B n and B M , "Freeze, L t BCE , and Weight Transfer" denote the methods for the stable score learning described in Section 3.4, and Update M denote the exemplarmemory maintenance with the class balanced sampling as mentioned in Section 3.5. Note we are constructing a mini-batch by sampling equal amount of data from D t and M, hence, the samples in M act as anchor points to improve the precision of the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Dataset We followed the experimental setting of <ref type="bibr">[3]</ref> and evaluated our method using Pascal-VOC 2012 <ref type="bibr">[9]</ref> and ADE20K <ref type="bibr" target="#b32">[32]</ref> datasets. Originally, <ref type="bibr">[3]</ref> set two experimental protocols, disjoint and overlapped, but we believe the latter is more realistic and challenging. Therefore, we evaluated on the overlapped setup only. Pascal VOC 2012 contains 20 foreground object classes and one background class, and ADE20K consists of 150 classes of both stuff and objects. We consider several incremental learning scenarios for each dataset, from the scenarios considered by the other baselines to newly proposed challenging scenarios with larger number of incremental states. A more detailed description on the datasets is introduced in the Supplementary Material (S.M.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We use the mean Intersection-over-Union (mIoU) as our evaluation metric, which computes the IoU for each class then computes the average over the classes. The IoU is defined as IoU = true-positive/(true-positive + false-positive + false-negative).</p><p>Implementation Details For all experiments, following other works <ref type="bibr">[7,</ref><ref type="bibr">3]</ref>, we use a Deeplab-v3 segmentation network <ref type="bibr">[5]</ref> with a ResNet-101 <ref type="bibr">[10]</ref> backbone pre-trained on ImageNet <ref type="bibr">[6]</ref>. We optimize the network using SGD with an initial learning rate of 10 ?2 and a momentum value of 0.9 for all CISS steps. Also, we set the learning rate schedule, data augmentation, and output stride of 16 following <ref type="bibr">[5]</ref> for all experiments. For each incremental state t, we train the network for 50 epochs for Pascal VOC with a batch-size of 32 and 60 epochs for ADE20K with a batch-size of 24. We tune the hyperparameters using 20% of the training set as a validation set and report the final results on the standard test set. For the exemplar memory, we utilized memory with a fixed size of |M| = 100 for Pascal VOC and |M| = 300 for ADE20K. To highlight the effect of the exemplar-memory, we report the results of the two versions of our method -SSUL (without memory) and SSUL-M (with memory). For the saliency-map detector to generate the Unknown class label, we employed DSS <ref type="bibr">[11]</ref> pretrained on MSRA-B dataset <ref type="bibr" target="#b22">[22]</ref>, which contains 5,000 labels for salient objects. The experiments were implemented in PyTorch [26] 1.7 with CUDA 10.1 and CuDNN 7 using two NVIDIA V100 GPUs, and all experiments were conducted with NSML <ref type="bibr" target="#b15">[15]</ref> framework. More information on the experimental details are in the S.M.</p><p>Baselines For a representative of the general regularization-based continual learning method, we select EWC <ref type="bibr" target="#b18">[18]</ref> and LWC <ref type="bibr" target="#b20">[20]</ref>-MC and applied them to each experimental setup of CISS. For CISS specific baselines, we compared with ILT <ref type="bibr" target="#b24">[24]</ref>, MiB <ref type="bibr">[3]</ref> and PLOP <ref type="bibr">[7]</ref>, and the Joint Training (Joint) result is also given as an upper bound. Note that PLOP <ref type="bibr">[7]</ref> is the current state-of-the-art. We reproduced the results of all baselines using the official code provided by the authors of <ref type="bibr">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results on benchmark dataset</head><p>Pascal VOC 2012 Following <ref type="bibr">[7,</ref><ref type="bibr">3]</ref>, we evaluate our method on four different scenarios, i.e., {10-1, 15-1, 15-5, and 19-1} as well as a more challenging scenario, {5-3}. The numbers in each scenario denote the number of classes to be trained for each state. For example, VOC 5-3 means learning 5 classes at the base task (t = 1), and then incrementally learning 3 classes five times (hence, T = 6).</p><p>In <ref type="table" target="#tab_2">Table 1</ref>, we observe our SSUL consistently outperforms the baselines with huge margin in all scenarios, even without using the exemplar-memory. Furthermore, the gap widens particularly for more challenging scenarios, namely, for the cases in which the base task has fewer classes and the number of tasks is larger. Note although MiB <ref type="bibr">[3]</ref> and PLOP <ref type="bibr">[7]</ref> show robustness for simple 2 tasks scenarios (19-1 and 15-5), their performance are rapidly dropped in more challenging and practical  scenarios (10-1, 5-3, 15-1). More specifically, <ref type="figure" target="#fig_0">Figure 3</ref>(a) shows the mIoU evolution for 15-1 scenario at each incremental step, and the baselines suffers from a significant drop of mIoU, as the new classes are incrementally learned. In contrast, SSUL improves mIoUs for both base (0-15) and newly learned <ref type="bibr" target="#b16">(16)</ref><ref type="bibr" target="#b17">(17)</ref><ref type="bibr" target="#b18">(18)</ref><ref type="bibr" target="#b19">(19)</ref><ref type="bibr" target="#b20">(20)</ref> classes significantly, showing much slower drop of mIoU. These results show that as long as the label augmentation of BG class with Unknown class and pseudo-labels is properly done, our stable score learning is very effective for CISS. Particularly, we observe that model freeze, which is believed to be not effective in CIL, is much more effective than the KD used in other baselines.</p><p>Furthermore, we observe our SSUL-M, which uses exemplar-memory, further strengthens SSUL significantly, particularly for the newly learned classes (i.e., for t ? 2). For example, in VOC 15-1, by storing only 5 ? 6 images per class in M, the mIoU for classes 16-20 improved about 12%. This confirms our intuition that the samples in the memory act as "anchors" to improve the precision of the predictions, hence, prevent forgetting. We can also clearly observe this improvement in <ref type="figure" target="#fig_1">Figure  4</ref>. As an additional experiment, we also conducted the experiment on the case of further reducing the number of base classes, such as 5-1 and 2-1, and we again observe that our SSUL and SSUL-M surpass other baselines. A detailed result on this additional experiment can be found also in the S.M.</p><p>To verify the robustness of each method on various class orderings, we experimented on the 20 difference orderings on VOC 15-1 scenario, as proposed in <ref type="bibr">[7]</ref>. <ref type="figure" target="#fig_0">Figure 3</ref>(b) plots the mIoU distributions for different methods, and we clearly observe both SSUL and SULL-M not only achieve higher mIoUs, but also have smaller variations compared to the baselines. In addition, we note SULL achieves ?1.5 faster training time compared to PLOP due to network freezing.</p><p>Qualitative analysis In <ref type="figure" target="#fig_1">Figure 4</ref>, we visualized the qualitative results for four images from VOC 15-1 task. We observe that PLOP partly maintains knowledge learned from the base task (bird and person), however, it becomes fatal in forgetting the classes learned in the intermediate steps, such as plant and sheep. In addition, PLOP frequently produces many false-positive predictions, lowering the mIoU of several classes. (E.g., see bird in Step 5.) On the other hand, we observe SSUL and SSUL-M both maintains the previously learned classes with high stability and effectively learns new classes. For example, the sheep class is accurately learned at Step 3 and not forgotten afterwards. Moreover, we observe SULL-M achieved further improvement of both plasticity and stability over SULL, especially by reducing the false-positive predictions.</p><p>ADE20K Unlike VOC 2012, ADE20K is densely labeled for both stuff and thing with 150 classes. It means that the class definition in ADE20K is clearer therefore, it naturally make reduce the concern about the semantic drift of BG label. To make efficient use of this prior knowledge of dataset, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSUL-M</head><p>Step 1 0-15</p><p>Step 2 <ref type="bibr">16 ( plant )</ref> Step 3 <ref type="bibr">17 ( sheep )</ref> Step 4 <ref type="bibr">18 ( sofa )</ref> Step 5 <ref type="bibr">19 ( train )</ref> Step <ref type="formula">6</ref>  consider unlabeled pixels as unknown class without using the saliency detector and enlarge the size of memory to |M| = 300. We evaluated our method in four different scenarios, i.e., {100-5, 100-10, 50-50, 100-50} as in <ref type="bibr">[7,</ref><ref type="bibr">3]</ref>. <ref type="table" target="#tab_3">Table 2</ref> again shows that SSUL achieves superior performance in the more challenging tasks (100-5, 100-10) than other methods. We believe that, this result demonstrates SSUL is also quite effective in CISS for a densely labeled dataset, without any extra saliency detector. The qualitative analysis on ADE20K is provided in the S.M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Ablation study on proposed components of SSUL Here, we analyze the effect of each proposed component of SSUL on VOC 15-1 scenario. <ref type="table" target="#tab_4">Table 3</ref> compares the mIoU of each ablation case, without memory, and the first low shows the result of SSUL with full components. Firstly, when the Unknown class is removed, we clearly observe that the mIoU's of both the first task (0-15) and subsequent classes <ref type="bibr" target="#b16">(16)</ref><ref type="bibr" target="#b17">(17)</ref><ref type="bibr" target="#b18">(18)</ref><ref type="bibr" target="#b19">(19)</ref><ref type="bibr" target="#b20">(20)</ref> decrease. Hence, this clearly demonstrates the advantage of using the Unknown class label, in terms of increasing both the plasticity and stability. Secondly, note that model freezing has a significant impact on the performance of CISS. Specifically, it not only prevents catastrophic forgetting on the first task, but also plays a critical role to learn new classes well. Finally, we observe when Softmax+CE instead of Sigmoid+BCE is used (the last ablation), the forgetting of the first task (0-15) drops more significantly despite model freezing. We believe this confirms our intuition in Section 3.4 on why Sigmoid+BCE can lead to more stable score learning.</p><p>Saliency-map detector and weight transfer The first two columns in <ref type="table" target="#tab_5">Table 4</ref> shows the ablation study on saliency-map detector for Unknown class and weight transfer. We compare the result of a default neural network-based saliency map extractor, DSS <ref type="bibr">[11]</ref> with ground-truth and a random forest-based DRFI <ref type="bibr" target="#b13">[13]</ref>. We observe the differences of the mIoU's among them are quite small, therefore, we believe that the quality of saliency map is not a significant factor of our method. The ablation study on weight transfer demonstrates weights for new classes initialized with ? t?1 cu is most effective to learn it. This result may at first look counter-intuitive since learning the final linear layer ? t c should not be sensitive on the initialization. However, given the noisy pseudo-label for learning, a quicker convergence of ? t c using the warm-start weight ? t?1 cu would effectively prevent the forgetting of past class, which could be caused by fitting the noisy pseudo-label for many epochs. More details on the effect of weight transfer is proposed in S.M.  Memory size and sampling rule The right column row of <ref type="table" target="#tab_5">Table 4</ref> shows the result of the mIoU on two sampling rules. We observe that, compared to random sampling, our proposed class-balanced sampling achieves better mIoU, particularly for the newly learned classes. We believe the reason is class-balanced sampling ensures at least one sample per class, therefore, it prevents the forgetting of minority classes than random sampling, which can miss certain classes in M. Finally, <ref type="figure" target="#fig_0">Figure 3</ref>(c) shows the dependency of SSUL-M on the memory size. It illustrates that tiny exemplar-memory for CISS significantly helps to increase the mIoU for the newly learned classes <ref type="bibr" target="#b16">(16)</ref><ref type="bibr" target="#b17">(17)</ref><ref type="bibr" target="#b18">(18)</ref><ref type="bibr" target="#b19">(19)</ref><ref type="bibr" target="#b20">(20)</ref> than the base classes (0-15). Moreover, we observe that after sufficiently large |M| the mIoU performance becomes robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks and Limitation</head><p>We proposed a new class-incremental learning method SSUL-M (Semantic Segmentation with Unknown Label with Memory) for semantic segmentation. In order to address two additional challenges of CISS, we made three main contributions -label augmentation with Unknown class labels, stable score learning, and tiny exemplar memory. They all were convincingly shown to be very effective in various CISS scenarios and our SSUL-M significantly outperformed baselines.</p><p>While promising, we admit our work also has some limitations as follows. First, our unknown class modeling may not be satisfactory for some "stuff" segmentation tasks since saliency-maps are mainly targeted for "things" (or objects). Second, our model freezing may harm plasticity and cause the model suffer from learning new classes especially when they are not captured by the unknown class label during the base task. Together with our insights from the analyses of SULL-M, we believe attempting to address above limitations would lead to a fruitful future research directions for the CISS problem. The model freezing may seem too restrictive as it may focus too much on maintaining the performance of the old classes and limit the capability of learning new classes. However, in our ablation study ( <ref type="table" target="#tab_4">Table 3</ref> in the manuscript), by comparing 1st and 3rd line of the table, we observe that model freezing significantly improves the mIoU not only for the old classes, but also for the new classes. This somewhat counterintuitive phenomenon mainly has to do with the second challenge mentioned abovei.e, since the predictions for the old classes remain accurate, the mIoU for the new classes also improves by limiting the false positives.</p><p>With such a positive effect of model freezing on the mIoU for the new classes, the unknown class label is introduced to compensate for the restrictiveness of complete freezing of the feature extractor in learning new classes and deal with the BG shift issue. Comparing the 1st and 2nd line of <ref type="table" target="#tab_4">Table  3</ref> in the manscript shows that the unknown class clearly further improves the performance for the new classes by effectively learning the representations of potential future classes (embedded in the unknown class). Note the result of SSUL (for VOC 15-1 task) is better than that of PLOP not only for the old classes, but also for the new classes when the unknown class label is used.</p><p>Finally, the learning capability for the new classes significantly gets boosted with the usage of the exemplar-memory. That is, for the VOC 15-1 task in <ref type="table" target="#tab_2">Table 1</ref>, the mIoU for the new classes improves from 36.59% (SSUL) to 49.01% (SSUL-M). This again is due to the fact that the ground-truth labels for the old classes in the memory help make less false positive errors for the new classes.</p><p>In summary, although the model freezing may seem somewhat counterintuitive for achieving the plasticity, we convincingly show that it is very effective not only for maintaining the performance of the old classes, but also for learning the new classes in the CISS problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">The case for stuff (area-like) and thing (object-like) classes in CISS</head><p>One of our main target issues is the semantic drift between true background and future class, and we alleviate the issue using the unknown class modeling with saliency maps. We believe that the applicability of the saliency maps depends on the dataset condition, that is, the matter is how many true backgrounds are included in the dataset.</p><p>Firstly, the segmentation datasets including only the object/thing (e.g., Pascal VOC) contain a huge number of true backgrounds. In this case, we showed that our unknown modeling with saliency maps is greatly helpful to alleviate the semantic drift between true background and future class (in <ref type="table" target="#tab_2">Table 1</ref>, <ref type="figure" target="#fig_0">Figure 3</ref> and <ref type="figure" target="#fig_1">Figure 4</ref> of the manuscript). Secondly, if the segmentation dataset including both the thing and stuff contains few true background pixels (e.g., ADE20K), the semantic drift rarely occurs.</p><p>In this case, we can apply our unknown modeling without saliency maps and achieve a reasonable performance as in <ref type="table" target="#tab_3">Table 2</ref> of the manuscript. Finally, if the segmentation dataset including the thing and stuff contains many true backgrounds, the saliency maps may not be helpful.</p><p>Nevertheless, we respectfully argue that the final case, that is, the dataset supports stuff segmentation yet contains many true backgrounds is not a common condition. Most famous datasets (e.g., Cityscape <ref type="bibr">[4]</ref>, COCO-stuff <ref type="bibr">[2]</ref>, Pascal Context <ref type="bibr">[10]</ref>, and NYU <ref type="bibr">[11]</ref>) contain few true backgrounds, therefore, our unknown modeling can widely be utilized in the same way as applied to ADE20K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">The details on sampling strategy for tiny examplar memory</head><p>Here, we describe the details of examplar memory sampling strategy. We denoted that the examplar memory as M and the size of examplar memory as M = |M|. After learning the incremental step t ? 1 (t &gt; 0), we sample the M for C 1:t?1 in D t?1 . For the random sampling strategy, we randomly select the data from D t?1 until the size of M becomes M . However, as mentioned in the ablation study section of the main paper, majority classes (e.g., person, dog, car) are mainly sampled in M, and none of the minority classes (e.g., TV, plant) may be sampled in M, inducing the catastrophic forgetting of the minority classes. To alleviate the problem, we designed the class-balanced sampling strategy; for each class in C 1:t?1 , we collect M/|C 1:t?1 | samples in D t?1 and store them in the M. When the |M| is not equal to M , we randomly drop or supplement the samples in M until |M| = M .</p><p>In addition, at the t + 1 step, we remove some samples of each class (i.e., C 1:t?1 ) in M and add samples for C t to M, so that M contains M/|C 1:t | samples per each class. This strategy ensures that M always contains at least one sample for each class, and we verified that the effectiveness of the class-balanced sampling strategy in the <ref type="table" target="#tab_5">Table 4</ref> of the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Additional Details of Datasets</head><p>Pascal VOC 2012 consists of 13,487 images, and it is divided as 10,582 images for training, 1,449 images for validation and 1,456 images for test dataset. ADE20K is a large scale dataset for semantic segmentation of scenes, including 25,210 images. It is also grouped as 20,210 images for the training set, 2,000 images for the validation set, and 3,000 images for the testing set. As stated in the manuscript, we followed exactly same experimental settings with PLOP <ref type="bibr">[5]</ref>.</p><p>3 The More Details of Experiments on Pascal VOC 2012</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The experimental result on reducing the initial number of classes</head><p>The result for the case in which the initial class set is small can be found in <ref type="table" target="#tab_2">Table 1</ref> (VOC 5-3 task) in the manuscript, and we observe our method still significantly surpasses the baselines. Furthermore, we carried out additional experiments with small initial classes (i.e., VOC 5-1, 2-1, and 2-2 tasks), as shown in <ref type="table" target="#tab_2">Table 1</ref>. We again observe that our SSUL consistently outperforms other baselines by a large margin even for such extreme cases, which confirms the effectiveness of our SSUL method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The additional ablation study on weight transfer</head><p>One may think the initialization of the final linear layer would not matter since the feature extractor is frozen. However, as also observed in prior work on CIL <ref type="bibr">[1,</ref><ref type="bibr">6]</ref>, the number of training iterations or the level of overfitting during learning seems to play an important role in finding the right trade-off between learning new classes and not forgetting the old classes. To that regard, we did additional experiments checking the effect of initialization and the number of training iterations in <ref type="table" target="#tab_3">Table 2</ref>. Namely, we simply did the random initialization for ? t c and increased the number of iterations (?1, ?2, ?4) and compared with our weight transfer (?1) results. From the table, we observe that the random initialization cannot reach the performance of weight transfer even with a larger number of iterations. We argue this is a similar phenomenon as what we observe in ordinary CIL. Namely, for the new class sigmoid, it is true that the linear weight would converge to the same solution regardless of the initialization. However, the number of iterations to reach the solution would differ -if we do the weight transfer, it will converge quicker, whereas if randomly initialized, a larger epoch would be necessary. Now, in the latter case, running with a larger epoch would result in the overfitting of the weights for c u and c b to the noisy pseudo label. Then, it will cause overconfident predictions for the Unknown or Background classes, which would penalize the overall mIoU. On the other hand, when only a small number of iterations are used for the random initialization, the prediction for the new class would become inaccurate, which again would hurt the overall mIoU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The effect of using the saliency-detector</head><p>In order to observe the effect of using the saliency-detector on the performance of other baselines, we additionally experimented with a variant of PLOP. That is, we implemented PLOP with saliency map, which only used the pseudo-labels that exist in the salient regions, similarly as in ours. As we can see in <ref type="table" target="#tab_4">Table 3</ref>, PLOP with saliency map slightly improves the original PLOP by 2.37% (i.e., 54.64% ? 57.01%). However, such result is still significantly lower than the result of SSUL (67.61%), underscoring the fact that our gain is not merely stemming from the additional data required for training the saliency detector.</p><p>Furthermore, even in our SSUL, we note that just naively using the saliency map would not automatically bring the mIoU gain. Namely, the saliency detector is closely used with our unknown class modeling, however, as we can see in <ref type="table" target="#tab_4">Table 3</ref> (2nd line) and <ref type="table" target="#tab_5">Table 4</ref> (weight transfer column) in the manuscript, if the correct weight transfer is not used, the mIoU becomes 61.89% even with the unknown class, which is not much higher than the result without the saliency detector and unknown class, i.e., 61.12%. Note we can enjoy the improvement to 67.61% (+6.49%) only when the proper weight transfer is applied. When we generate the pseudo-label, we set the threshold ? on the output of previous model ? i = max c?C 1:t?1 ?(f t?1 ?,ic (x t )) for the confident pseudo-label. Here, we conduct an experiment to analyze the effect of the hyperparameter ? in <ref type="table" target="#tab_5">Table 4</ref>. When ? = 0, which means without the thresholding, we can still achieve a competitive performance of 67.44% all mIoU, however, we found that some noisy labels are included in the pseudo-label. To increase the confidence of the pseudo-label and reduce the noisy labels, we set the ? and found that enough high ? of 0.7 can slightly boost the performance. In addition, as in <ref type="table" target="#tab_5">Table 4</ref>, we found that the proposed method is robust to the ? . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The experimental result on disjoint setup</head><p>As mentioned in seminar works <ref type="bibr">[3,</ref><ref type="bibr">5]</ref>, the disjoint setup has an additional constraint that only the classes observed so far are present in the input image, which is not necessarily practical. However, in this section, we provide an additional experiment result on the disjoint setup on <ref type="table" target="#tab_11">Table 5</ref>. We clearly observe that SSUL and SSUL-M again achieve superior results compared to other baselines.  <ref type="table" target="#tab_12">Table 6</ref> shows the summarized results of Pascal VOC 2012 by each class.  <ref type="table" target="#tab_13">Table 7</ref> shows the numerical details for SSUL and SSUL-M of <ref type="figure" target="#fig_0">Figure 3</ref>(b) in the manuscript. Note that we strictly followed the class orderings of Pascal VOC 2012 as done in PLOP [5]: 4 Qualitative analysis on ADE20K</p><p>In <ref type="figure" target="#fig_2">Figure 1</ref>, we visualized the qualitative results from ADE20K 100-10 (6 tasks) scenario. We argue that we seldom suffer from the background semantic shift issue on ADE20K because its clear and dense labels for both things and stuff. Consequently, the false-positive predictions are noticeably reduced compared to the results on Pascal VOC 2012. As in <ref type="figure" target="#fig_2">Figure 1</ref>, the unknown label (i.e., black pixels) is correctly transformed to the label to be learned in the future (e.g., fan in step-5 and plate in step-6) while keeping the previously learned knowledge.</p><p>Step 1 0-100</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Step 2 <ref type="bibr">[101]</ref><ref type="bibr">[102]</ref><ref type="bibr">[103]</ref><ref type="bibr">[104]</ref><ref type="bibr">[105]</ref><ref type="bibr">[106]</ref><ref type="bibr">[107]</ref><ref type="bibr">[108]</ref><ref type="bibr">[109]</ref><ref type="bibr">[110]</ref> Step 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>111-120</head><p>Step 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>121-130</head><p>Step 5 <ref type="bibr">[131]</ref><ref type="bibr">[132]</ref><ref type="bibr">[133]</ref><ref type="bibr">[134]</ref><ref type="bibr">[135]</ref><ref type="bibr">[136]</ref><ref type="bibr">[137]</ref><ref type="bibr">[138]</ref><ref type="bibr">[139]</ref><ref type="bibr">[140]</ref> Step 6 141-150 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>(a): mIoU evaluation on VOC 15-1, (b): mIoU distributions for 20 different class-orderings for VOC 15-1, (c): mIOU on VOC 15-1 with varying memory size |M|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of SSUL-M, SSUL and PLOP on VOC 15-1 scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Qualitative results of SSUL-M on ADE20K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on Pascal VOC 2012. 20.91 36.67 24.66 64.40 13.30 61.90 58.10 35.00 52.30 22.51 31.66 29.04 67.75 10.88 65.05 67.08 39.23 60.45 MiB [3] 12.25 13.09 12.65 34.22 13.50 29.29 57.10 42.56 46.71 71.43 23.59 69.15 76.37 49.97 70.08 PLOP [7] 44.03 15.51 30.45 65.12 21.11 54.64 17.48 19.16 18.68 75.35 37.35 73.54 75.73 51.71 70.09 SSUL 71.31 45.98 59.25 77.31 36.59 67.61 72.44 50.67 56.89 77.73 29.68 75.44 77.82 50.10 71.22 SSUL-M 74.02 53.23 64.12 78.36 49.01 71.37 71.27 53.21 58.37 77.83 49.76 76.49 78.40 55.80 73.02 Joint 78.41 76.35 77.43 79.77 72.35 77.43 76.91 77.63 77.43 77.51 77.04 77.43 79.77 72.35 77.43</figDesc><table><row><cell>VOC 10-1 (11 tasks) 0-10 11-20 all ---LwF-MC [20] 4.65 Method EWC [18] 5.90 4.95 6.90 ILT [24] VOC 15-1 (6 tasks) 0-15 16-20 all 0.30 4.30 1.30 6.40 8.40 7.15 3.67 5.50 8.75 7.99 8.56 (a)</cell><cell>VOC 5-3 (6 tasks) 0-5 6-20 all ---(b)</cell><cell>VOC 19-1 (2 tasks) 0-19 20 all 26.90 14.00 26.30 24.30 35.50 27.10 VOC 15-5 (2 tasks) 0-15 16-20 all (c)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on ADE20K.</figDesc><table><row><cell>ADE 100-5 (11 tasks) 0-100 101-150 all 0.08 1.31 0.49 36.01 5.66 25.96 38.21 ADE 100-10 (6 tasks) 0-100 101-150 all 0.11 3.06 1.09 18.29 ADE 100-50 (2 tasks) 0-100 101-150 all 14.40 17.00 3.53 ADE 50-50 (3 tasks) 0-50 511-150 all 12.85 9.70 11.12 29.24 40.52 17.17 32.79 45.57 21.01 29.31 PLOP [7] 39.11 Method ILT [24] MiB [3] 7.81 28.75 40.48 13.61 31.59 41.87 14.89 32.94 48.83 20.99 30.40 SSUL 39.94 17.40 32.48 40.20 18.75 33.10 41.28 18.02 33.58 48.38 20.15 29.56 SSUL-M 42.86 17.78 34.56 42.86 17.66 34.46 42.79 17.54 34.37 49.12 20.10 29.77 Joint 44.30 28.20 38.90 44.30 28.20 38.90 44.30 28.20 38.90 51.10 33.30 38.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study for SSUL on VOC 15-1 about PL (pseudo labeling), Unknown (unknown class modeling &amp; weight transfer), Freeze (model freezing), and Sigmoid+BCE (stable score learning).</figDesc><table><row><cell>PL Unknown Freeze Sigmoid+BCE</cell><cell>15-1 (6 tasks) 0-15 16-20 77.31 36.59 67.61 all 73.42 21.79 61.12 53.56 14.48 44.25 61.42 22.97 52.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for saliency-map detector (left), weight transfer (mid), and memory sampling (right) on VOC 15-1.</figDesc><table><row><cell>Saliency-map detector Methods 0-15 16-20</cell><cell>all</cell><cell>Methods</cell><cell cols="2">Weight transfer 0-15 16-20</cell><cell>all</cell><cell>Memory sampling Methods 0-15 16-20</cell><cell>all</cell></row><row><cell cols="4">DRFI [13] DSS [11] ground-truth 78.42 40.41 69.36 76.46 32.53 66.00 random ? ? t c 77.31 36.59 67.61 ? t?1 cb ? ? t c ? t?1 cu ? ? t c</cell><cell cols="4">73.73 23.99 61.89 73.29 23.70 61.48 class-balanced 78.36 49.01 71.37 random 78.61 38.87 69.15 77.31 36.59 67.61 ----</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 :</head><label>1</label><figDesc>The experiment result on the small initial number of classes. 45.98 59.25 69.32 40.38 48.65 62.35 34.32 38.32 62.38 42.46 45.31 SSUL-M 74.02 53.23 64.12 71.86 48.41 55.11 60.49 42.11 44.74 58.85 45.82 47.68</figDesc><table><row><cell>VOC 10-1 (11 tasks) 0-10 11-20 all 12.25 13.09 12.65 11.47 9.45 10.03 21.57 7.93 VOC 5-1 (16 tasks) VOC 2-1 (19 tasks) 0-5 6-20 all 0-2 3-20 all 9.88 41.11 23.35 25.89 VOC 2-2 (10 tasks) 0-2 3-20 all PLOP [5] 44.03 15.51 30.45 0.12 Method MiB [3] 9.00 6.46 0.01 5.22 4.47 24.05 11.92 13.66 SSUL 71.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>The additional ablation study for weight transfer</figDesc><table><row><cell>Head Init Random Random Random Weight Transfer</cell><cell>VOC 15-1 (6 tasks) Iterations 0-15 16-20 all ?1 73.73 23.99 61.89 ?2 73.89 23.08 62.45 ?4 72.22 22.38 61.14 ?1 77.31 36.59 67.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>The additional experiments using the saliency-detector.</figDesc><table><row><cell>PLOP PLOP SSUL</cell><cell>VOC 15-1 (6 tasks) Saliency 0-15 16-20 65.12 21.11 54.64 all 67.25 24.22 57.01 77.31 36.59 67.61</cell></row><row><cell cols="2">3.4 Additional hyperparameter search for ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Effect of threshold ? . 77.31 35.85 67.44 0.3 77.51 36.06 67.64 0.5 77.34 35.96 67.49 0.7 77.28 37.51 67.81 0.9 77.31 36.59 67.61 1.0 75.08 40.08 66.74</figDesc><table><row><cell>? 0.0</cell><cell>VOC 15-1 (6 tasks) 0-15 16-20 all</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Experimental results on Pascal VOC 2012 for disjoint setup. 13.67 46.48 75.37 38.89 73.64 71.00 42.82 64.29 SSUL 65.39 34.90 50.87 73.97 32.15 64.01 77.38 22.43 74.76 76.44 45.60 69.10 SSUL-M 65.02 40.82 53.50 76.46 43.37 68.58 77.58 43.89 75.98 76.47 48.55 69.83 Joint 78.41 76.35 77.43 79.77 72.35 77.43 77.51 77.04 77.43 79.77 72.35 77.43 3.6 The details of experimental results of Pascal VOC 2012</figDesc><table><row><cell>Method EWC [7] LwF-MC [8] ILT [9] MiB [3] PLOP [5]</cell><cell>VOC 10-1 (11 tasks) 0-10 11-20 all ---------------</cell><cell>VOC 15-1 (6 tasks) 0-15 16-20 all 0.30 4.30 1.30 23.20 16.00 22.90 26.70 37.70 29.40 VOC 19-1 (2 tasks) VOC 15-5 (2 tasks) 0-19 20 all 0-15 16-20 all 4.50 7.00 5.20 63.00 13.20 60.50 67.20 41.20 60.70 3.70 5.70 4.20 69.10 16.40 66.40 63.20 39.50 57.30 46.20 12.90 37.90 69.60 25.60 67.40 71.80 43.30 64.70 57.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Details of Pascal VOC 2012 mIoU performance per class. The details of experimental results of class orderings</figDesc><table><row><cell>10-1 (11 tasks) SSUL 3.7</cell><cell>bg</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse mbike person plant sheep sofa</cell><cell>train</cell><cell>tv</cell><cell>mIoU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Details of the performances for each class ordering on VOC 15-1 scenario. 75.48 72.82 76.45 77.13 72.39 76.66 81.43 74.61 73.98 77.41 79.76 78.42 78.47 73.66 78.42 75.11 74.36 74.29 72.19 16-20 36.59 51.37 53.69 54.70 47.08 50.99 40.70 29.67 49.09 47.94 38.89 44.86 49.07 40.22 50.90 44.75 50.34 55.24 47.65 61.40 all 67.61 69.74 68.26 71.27 69.98 67.29 68.10 69.10 68.53 67.78 68.24 71.45 71.44 69.37 68.24 70.41 69.21 69.81 67.95 69.62 SSUL-M 0-15 78.36 75.44 73.01 76.42 77.12 72.75 77.17 77.41 74.85 73.68 77.41 79.83 77.66 78.87 74.72 78.27 75.61 74.65 72.81 70.85 16-20 49.01 53.99 72.01 68.77 58.78 65.58 55.74 52.88 57.09 55.45 52.88 48.33 55.13 50.22 64.42 54.00 63.46 62.55 47.93 61.27 all 71.37 70.33 72.77 74.60 72.75 71.01 72.07 71.57 70.62 69.34 71.57 72.33 72.29 72.05 72.27 72.49 72.72 71.77 66.89 68.57</figDesc><table><row><cell>15-1 (6 tasks) SSUL 0-15</cell><cell>1 77.31</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>Class Ordering 10 11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell><cell>20</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was done while Sungmin Cha did a research internship at NAVER AI Lab. This work was supported in part by the New Faculty Startup Fund from Seoul National University, NRF Mid-Career Research Program [NRF-2021R1A2C2007884] funded by the Korean government, and SNU-NAVER Hyperscale AI Center. The authors thank NAVER Smart Machine Learning (NSML) team for the GPU support. Taesup Moon also thanks the support from Automation and Systems Research Institute (ASRI) at Seoul National University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ss-il: Separated softmax for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihwan</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonsu</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13947</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol?s</forename><surname>Mar?n-Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="233" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling the background for incremental learning in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cermelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9233" to="9242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10486</idno>
		<title level="m">On tiny episodic memories in continual learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Plop: Learning without forgetting for continual semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11390</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Podnet: Pooled outputs distillation for small-tasks incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Ollion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision-ECCV 2020-16th European conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12365</biblScope>
			<biblScope unit="page" from="86" to="102" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XX</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="831" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative region suppression for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangeun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1754" to="1761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heungseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09957</idno>
		<title level="m">Meet the mlaas platform with a real-world case study</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Puzzle mix: Exploiting saliency and local statistics for optimal mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5275" to="5285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Co-mixup: Saliency guided joint mixup with supermodular diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hosan</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimal continual learning has perfect memory and is np-hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremias</forename><surname>Knoblauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Diethe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5327" to="5337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Incremental learning techniques for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="54" to="71" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07734</idno>
		<title level="m">Three scenarios for continual learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large scale incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="374" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency guided self-attention network for weakly and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="14413" to="14423" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno>2017. 88.55 84.42 36.93 84.63 67.40 80.16 76.53 88.01 87.88 34.38 55.55 27.08 69.92 41.33 63.27 80.73 26.89 39.38 26.43 46.10 38.69 59.25</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ss-il: Separated softmax for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihwan</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonsu</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13947</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1209" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling the background for incremental learning in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cermelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9233" to="9242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Plop: Learning without forgetting for continual semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11390</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="831" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Incremental learning techniques for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
