<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object-Centric Stereo Matching for 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
						</author>
						<title level="a" type="main">Object-Centric Stereo Matching for 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Safe autonomous driving requires reliable 3D object detection-determining the 6 DoF pose and dimensions of objects of interest. Using stereo cameras to solve this task is a cost-effective alternative to the widely used LiDAR sensor. The current state-of-the-art for stereo 3D object detection takes the existing PSMNet stereo matching network, with no modifications, and converts the estimated disparities into a 3D point cloud, and feeds this point cloud into a LiDAR-based 3D object detector. The issue with existing stereo matching networks is that they are designed for disparity estimation, not 3D object detection; the shape and accuracy of object point clouds are not the focus. Stereo matching networks commonly suffer from inaccurate depth estimates at object boundaries, which we define as streaking, because background and foreground points are jointly estimated. Existing networks also penalize disparity instead of the estimated position of object point clouds in their loss functions. We propose a novel 2D box association and object-centric stereo matching method that only estimates the disparities of the objects of interest to address these two issues. Our method achieves state-of-the-art results on the KITTI 3D and BEV benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Safe autonomous driving requires determining the six DoF pose and dimensions of objects of interest in a scene, i.e., 3D object detection. Existing methods can be categorized by the sensors they use: LiDAR <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, LiDAR and camera <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, monocular camera <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, and stereo camera setups <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Methods that incorporate LiDAR measurements set the standard for 3D detection performance as LiDAR has the ability to acquire accurate depth information. However, most multibeam LiDAR sensors remain expensive, bulky, and their returns are sparse particularly at long distances. On the other hand, acquiring depth from monocular cameras is ill-posed by nature and thus inaccurate and less reliable. Stereo camera setups are generally less expensive than LiDAR, and they resolve the under-constrained monocular problem through stereopsis. Moreover, given high resolution cameras and a large stereo baseline, stereo methods have the potential for accurate long range perception. Stereo object detection is therefore an important alternative to both monocular and LiDAR/camera methods.</p><p>Prior to our work, the state-of-the-art stereo 3D object detection method on the KITTI benchmark <ref type="bibr" target="#b15">[16]</ref> was Pseudo-LiDAR <ref type="bibr" target="#b12">[13]</ref>. Pseudo-LiDAR uses the existing 3D object detector AVOD <ref type="bibr" target="#b6">[7]</ref> and replaces the LiDAR input with a point cloud derived from the disparity output of the stereo matching network PSMNet <ref type="bibr" target="#b16">[17]</ref>. The performance loss on cars from replacing the LiDAR input is approximately 30% AP. To understand this discrepancy, this work shows that the point clouds derived from PSMNet contain streaking artifacts that warp the piecewise-smooth surfaces in the scene leading to significant classification and localization errors. The cause of streaking originates from the ambiguity of depth values at object edges; it can be hard to discern whether a pixel belongs to the object or the background. For such pixels, deep learning methods are often encouraged to produce depths between these two extremes <ref type="bibr" target="#b17">[18]</ref>. Furthermore, in deep stereo networks, closer objects are often favored during training for two main reasons. First, the inversely proportional relation between depth and disparity causes the same disparity error to have drastically different depth errors depending on the distance of objects. For example, for an object 60 m from the camera in the KITTI dataset, a disparity error of only 0.5 pixels corresponds to a large depth error of 5.1 m, but for a car 10 m away the same disparity error corresponds to a depth error of only 0.1 m. The second reason closer depths are favored during training is that there is a natural imbalance in training data. In a typical driving scene, the image is dominated by foreground pixels.</p><p>This work presents an object-centric stereo matching network, OC Stereo, to address the problems that arise from typical deep stereo matching methods. First, to resolve the streaking issue described above, we propose an objectcentric representation of depth. In 3D object detection, one is primarily concerned with the objects of interest; therefore, we perform stereo matching on only object image crops and mask the ground truth background disparities during training to only penalize errors for object pixels. As a result, we avoid creating streaking artifacts in the object point clouds, and thus capture the true shapes of objects more accurately. Furthermore, as a result of only estimating disparities for the objects of interest, the runtime efficiency is significantly improved-an important aspect for safe self-driving vehicles. Second, to resolve the issue of stereo matching networks favouring closer objects, we introduce a point cloud loss that jointly penalizes the estimated position and shape of the object instances directly, and canonically resize the image crops of objects to balance the number of pixels for close and far objects.</p><p>Our main contributions are as follows: 1) A fast 2D box association algorithm that accurately matches detections between left and right images; 2) A novel object-centric stereo matching architecture that addresses the pixel imbalance problem between near and far objects and suppresses streaking artifacts in the resulting point clouds to improve 3D localization; 3) A point cloud loss within the stereo matching network to help recover object shape and to directly penalize depth errors; 4) State-of-the-art results on the KITTI 3D object detection benchmark <ref type="bibr" target="#b15">[16]</ref> while running 31% faster than the previous state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Stereo Correspondence. Determining stereo correspondences is an active area of research in computer vision. Endto-end networks typically construct correlation layers <ref type="bibr" target="#b18">[19]</ref> or cost volumes <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, which can be processed efficiently on GPUs to produce high quality disparity maps. These methods already achieve less than 2% 3-pixel error on the KITTI 2015 stereo benchmark <ref type="bibr" target="#b22">[23]</ref>. However, due to the inversely proportional relation between disparity and depth, the 3-pixel error metric allows for large inaccuracies in depth especially at far distances, and thus this metric is not as meaningful for 3D object detection performance. We instead focus the stereo network on recovering meaningful object shapes and accurate depth to improve 3D detection performance.</p><p>Streaking Depth. Streaking depths are a common artifact in typical stereo matching networks. Point clouds of foreground objects generated from re-projecting depth maps into 3D space are generally blurred into the background, as it can be ambiguous whether pixels belong to the object or the background. The cause of streaking has been investigated by <ref type="bibr" target="#b17">[18]</ref>, who find that at ambiguous depths common loss functions prefer the mean of the foreground and background depths or do not adequately penalize estimates within this discontinuity. MonoPLiDAR <ref type="bibr" target="#b23">[24]</ref> proposes to eliminate streaking artifacts with instance segmentation masks. Using a depth map estimated from a monocular image, instance segmentation masks are applied to remove background points. While their method removes some streaking, streaking still persists, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, since the instance segmentation masks are not perfect, especially at object edges where the depth ambiguities exist. Also, the full depth map is still predicted, which requires additional computation.</p><p>Stereo 3D Object detection. One of the early stereo 3D detectors, 3DOP <ref type="bibr" target="#b24">[25]</ref>, generates candidate 3D anchors which are scored and regressed to final detections using several handcrafted features. Current state-of-the-art methods are deep learning based. Pseudo-LiDAR <ref type="bibr" target="#b12">[13]</ref> adapt the 3D detectors AVOD <ref type="bibr" target="#b6">[7]</ref> and F-PointNet <ref type="bibr" target="#b5">[6]</ref> to use point clouds from disparity maps predicted by PSMNet <ref type="bibr" target="#b16">[17]</ref>. However, this method results in point clouds with streaking artifacts, and requires additional computation by estimating depths of background areas that are not necessarily relevant for 3D object detection. On the other hand, we save computation and avoid streaking artifacts by using an object-centric approach by only estimating the depths of the objects of interest. Stereo R-CNN <ref type="bibr" target="#b11">[12]</ref> creates 2D anchors that automatically associate left and right bounding boxes. These anchors are used with keypoints to estimate rough 3D bounding boxes that are later refined using photometric alignment on object image crops. TLNet <ref type="bibr" target="#b13">[14]</ref> employ 3D anchors for object correspondence and also use triangulation. However, Stereo R-CNN and TLNet perform 8% AP and 36% AP lower, respectively, than Pseudo-LiDAR on the KITTI moderate car category. This discrepancy suggests that explicit photometric errors and sparse anchor triangulations may be inferior to using disparity cost volumes to learn depth, and that depth estimation is the main area for improvement, which is one of the focuses of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Given a pair of left and right images, I l and I r , our objective is to estimate the 3D pose and dimensions of each object of interest in the scene. The main motivation behind our method is the belief that focusing on the objects of interest will result in better object detection performance. Therefore, instead of performing stereo matching on the full image, we perform stereo matching on Regions of Interest (RoIs), and only for pixels belonging to objects. This approach has three key advantages: 1) we resize the RoIs so there are a similar number of pixels for each object, which reduces class imbalance in depth values, 2) by only comparing RoIs we reduce the possible range of disparity values, and thus have faster runtime because the RoI disparity cost volumes are smaller, 3) we avoid streaking artifacts by ignoring background pixels.</p><p>Overall, the pipeline, shown in <ref type="figure">Fig. 2</ref>, works as follows. First, a 2D detector generates 2D boxes in I l and I r . Next, a box association algorithm matches object detections across both images. Each matched detection pair is passed into the object-centric stereo network, which jointly produces a disparity map and instance segmentation mask for each object. Together, these form a disparity map containing only the objects of interest. Lastly, the disparity map is transformed into a point cloud that can be used by any LiDAR-based 3D object detection network to predict the 3D bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 2D Object Detector and Box Association Algorithm</head><p>Given the stereo image pair input, we identify left and right RoIs, l, r, using a 2D object detector. After applying a 2D detection score threshold t d , we acquire m and n RoIs in the left and right images, respectively. We perform association by computing the Structural SIMilarity index (SSIM) <ref type="bibr" target="#b25">[26]</ref> for each RoI pair combination then matching the highest scores. SSIM is calculated as follows,</p><formula xml:id="formula_0">SSIM (l, r) = (2? l ? r + C 1 )(2? lr + C 2 ) (? 2 l + ? 2 r + C 1 )(? 2 l + ? 2 r + C 2 ) ,<label>(1)</label></formula><p>where ? l , ? l , ? r , ? r , are the left and right RoI pixel intensity mean and variance, ? lr is the correlation of the pixel intensities, and C 1 and C 2 are constants to prevent division by zero. This metric is calculated per image channel and averaged. Our assumption is that objects in the left and right images have similar appearance as SSIM measures the visual similarity between two images emphasizing relations of spatially close pixels. Each RoI is then interpolated to a standard size. The SSIM index is calculated between each left and right RoI. The To improve the robustness of the association, we ensure that the difference between associated 2D bounding box centres are within an adaptive box center threshold. MonoPSR <ref type="bibr" target="#b8">[9]</ref> shows the depth of objects is well correlated with bounding box height. This means that closer objects should have larger disparities while further objects should have smaller disparities. Using the KITTI dataset, we model the relationship between box height and centre disparity using linear regression. Based on an RoI's box height, the data provides the expected centre disparity for its associated box. We therefore constrain the maximum distance between box centers of associated RoIs to be within three standard deviations of the expected disparity. Boxes that do not satisfy these conditions are ignored for the SSIM calculation, further improving the speed and accuracy of the associations. An example of the corresponding RoIs is shown in <ref type="figure">Fig. 2</ref>.</p><p>While our method is reliant on 2D detection quality, we believe using a 2D detector is actually advantageous because 2D detection is a mature field with robust performance. Radosavovic et al. <ref type="bibr" target="#b26">[27]</ref> even claim that the current performance of 2D detectors is accurate enough that detectors can be trained using data it inferences-self-training. In Sec. V we show our 2D detections have higher AP compared to other state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object-Centric Stereo Matching</head><p>Given the associated RoIs, we perform stereo matching to estimate a canonically resized disparity map of dimensions w?h per object. Within the RoIs, disparities are learned only for pixels belonging to the object to remove depth ambiguity and thus depth streaking artifacts.</p><p>Local Disparity Formulation. We estimate the horizontal pixel shift, or disparity, within the aligned left RoI and right RoI. We refer to this disparity estimation as local compared to the global disparity shift between the pixels of the fullsized left right stereo pair. This local formulation leads to positive and negative ground truth disparities. To obtain the ground truth local disparities we first start by forming an array of the local RoI images coordinates i l of the left RoI,</p><formula xml:id="formula_1">i l = ? ? ? 0 . . . w . . . . . . 0 . . . w ? ? ? .<label>(2)</label></formula><p>The global horizontal image coordinates x l of the left RoI is</p><formula xml:id="formula_2">x l = i l + e l ,<label>(3)</label></formula><p>where e l is the horizontal coordinate of the left RoI's left edge. We calculate the disparity map corresponding to the resized RoIs by performing a nearest neighbor resizing of the ground truth global disparity map d g to the canonical size, w ? h. Therefore, the corresponding right global image coordinates are calculated as,</p><formula xml:id="formula_3">x r = x l ? d g .<label>(4)</label></formula><p>These coordinates are normalized to the local coordinate system,</p><formula xml:id="formula_4">i r = (x r ? e r ) w b w,<label>(5)</label></formula><p>where e r is the horizontal coordinate of the right RoI's left edge and w b is the width of the non-resized RoI bounding box. Lastly, the local disparity of the crops can be calculated as</p><formula xml:id="formula_5">d l = i l ? i r .<label>(6)</label></formula><p>During training, we use ground truth instance segmentation masks to only train on disparity values corresponding to the object. This formulation removes depth ambiguity at edges and removes streaking artifacts as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. During inference, we mask background pixels using a predicted instance segmentation mask. From the predicted local disparity map d * l we calculate the global disparity map d * g by reversing the above steps. The corresponding depth map D is calculated from the known horizontal focal length f u and baseline b as,</p><formula xml:id="formula_6">D = f u b d * g .<label>(7)</label></formula><p>Fig. 2: A 2D detector and box association algorithm determine associated RoIs. Our stereo matching network estimates disparities with a 3D CNN and soft argmin operation <ref type="bibr" target="#b16">[17]</ref> for object pixels using the RoIs and instance segmentation. These are converted to a 3D point cloud and can be inputted to any LiDAR-based 3D object detector. X indicates multiplication.</p><p>Lastly, each pixel (u, v) of this depth map is converted into a 3D point as</p><formula xml:id="formula_7">x = (u ? c u )z f u , y = (v ? c v )z f v , z = D(u, v),<label>(8)</label></formula><p>where (c u , c v ) is the camera center and f v is the vertical focal length.</p><p>Object-Centric Stereo Architecture. The described object-centric stereo depth formulation is flexible with most stereo depth estimation networks. In our implementation, we build on PSMNet <ref type="bibr" target="#b16">[17]</ref> with key modifications.</p><p>We use the same feature extractor as <ref type="bibr" target="#b16">[17]</ref>, but use one for the RoIs and another for the full-sized images. Despite only comparing RoIs, we leverage global context by performing RoI Align <ref type="bibr" target="#b27">[28]</ref> on the full-sized image feature extractor outputs. The resulting features from the left image are multiplied with the left crop feature map, and the features from the right image are multiplied with the right crop feature map. To estimate disparity, the left and right feature maps are concatenated to form a 4D disparity cost volume (height ? width ? disparity range ? feature size) as in <ref type="bibr" target="#b16">[17]</ref>. Importantly, however, our input size and disparity range are smaller than what would be used for global disparity estimation because the local disparity range between two RoIs is smaller than the global disparity range between two full-sized images. As a result, we create a set of smaller cost volumes, which results in a faster runtime.</p><p>To predict the instance segmentation map, only the left feature maps are used. The instance segmentation network consists of a simple decoder; the feature map is processed by three repeating bilinear up-sampling and 3 ? 3 convolutional layers resulting in a w ? h instance segmentation mask. For each instance, the predicted segmentation mask is applied to the estimated local disparity map. To deal with overlapping instance masks, each local disparity is converted to a global disparity, resized to the original box size, and placed in farthest to closest depth order in the scene.</p><p>Point Cloud Loss. Similar to <ref type="bibr" target="#b16">[17]</ref>, we use the smooth L1 loss to compare the predicted local disparity d * l and the ground truth local disparity d l . Penalizing the disparities directly, however, is non-ideal because it places less emphasis on far objects due to the inverse relation between disparity and depth. For example, for a car 60 m from the camera in the KITTI dataset, a disparity error of only 0.5 pixels corresponds to a large depth error of 5 meters, but for a car 10 m away the same disparity error corresponds to a depth error of only 0.13 m. An unwanted consequence of computing loss from disparity estimates is that drastically different depth errors can have the same loss value.</p><p>Therefore, we transform the predicted disparities to a point cloud. We then use the smooth L1 loss to compare each object's point cloud p c with its ground truth point cloud p g . Since we are concerned about 3D localization, this loss is more suitable as it directly penalizes predicted 3D point positions and resolves the lack of emphasis on far depths described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 3D Box Regression Network</head><p>One of the benefits of our pipeline is that we can use the estimated point cloud as input to any 3D object detector that processes point clouds. In our implementation we build on the AVOD <ref type="bibr" target="#b6">[7]</ref> architecture and make two modifications. We first note that in AVOD, the RoI cropping operation of the second stage returns identical BEV features regardless of the vertical location of a regressed anchor, or proposal. As well, since our stereo point cloud does not contain ground points, we append the proposal's 3D position information to the feature vector used to regress each 3D proposal. We also check if the final 3D bounding boxes align with the 2D detections in the first stage. If a 3D box projected into the image plane does not overlap with a 2D detection by at least 0.5 IoU, it is removed.</p><p>IV. IMPLEMENTATION 2D Detector and Box Association. We use MS-CNN <ref type="bibr" target="#b28">[29]</ref> as our 2D detector because it has fast runtime speed and high accuracy. The RoIs are cropped and bilinearly resized to 128 ? 128 for association and 224 ? 224 for local stereo matching.</p><p>Object-Centric Stereo Network. During stereo matching, the minimum and maximum local disparities are set as -64 and 90 pixels. This range was found by calculating the range of local disparities for randomly jittered ground truth 2D boxes that maintain a minimum 0.7 IoU with the original box. For faster convergence, the feature extractors are pretrained on full depth maps from the SceneFlow dataset <ref type="bibr" target="#b18">[19]</ref> and depth completed LiDAR scans from the training split of the KITTI object detection dataset. No training is done on the KITTI raw or stereo datasets because these datasets contain overlapping samples with the object detection dataset. The object-centric stereo network, which leverages these feature extractors, is fine-tuned on crops of depth completed LiDAR scans. Depth completion is used for additional training points and faster convergence, and to remove the erroneous depths due to the differing locations of the camera and LiDAR sensor <ref type="bibr" target="#b29">[30]</ref>. The depth completion method used is <ref type="bibr" target="#b30">[31]</ref> because it is structure preserving and does not contain streaking artifacts. The ground truth instance segmentation masks used to mask the background disparity are created by projecting the points within the ground truth 3D boxes into the image. These instance masks exactly correspond to the pixels belonging to the object, but they are not smooth due to the depth completion, so the instance segmentation network is instead trained using masks from <ref type="bibr" target="#b31">[32]</ref>. For optimization, Adam was used with a batch size of 16 and a learning rate of 0.001 for 8 epochs then 0.0001 for 4 more epochs.</p><p>3D Object Detection. For the 3D object detector, we use AVOD <ref type="bibr" target="#b6">[7]</ref> to compare with Pseudo-LiDAR. With a batch size of 1, the Adam optimizer is used with a learning rate of 0.0001 for 50000 steps then decayed to 0.00001 and stopped using early stopping. The data augmentation used was horizontal flipping and PCA jittering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>We compare against the state-of-the-art, perform ablation studies, and provide qualitative results <ref type="figure" target="#fig_1">(Fig. 3)</ref> using the KITTI dataset <ref type="bibr" target="#b15">[16]</ref>. The KITTI dataset contains 7481 training images and 7518 test images, and categorizes objects in three categories: Easy, Moderate, and Hard based on 2D box height, occlusion, and truncation. To compare with the state-of-the-art, we follow the 1:1 training to validation split of <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b5">[6]</ref> and the standard practice of comparing BEV and 3D AP performance using IoUs of 0.5 and 0.7. We also benchmark our results on the online KITTI test server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D AP Comparison with the State-of-the-Art</head><p>As mentioned in Sec. II 3-pixel error is not indicative of 3D object detection performance as it allows large inaccuracies in depth. An alternative is comparing depth map errors. The depth map RMSE for our method and Pseudo-LiDAR is 1.60 m and 1.81 m, respectively, when comparing the same pixels that are predicted in both Pseudo-LiDAR and our depth maps. However, we believe object detection AP is more meaningful than depth map metrics because depth map errors are not as indicative of the shape of each object. We therefore use object detection AP for the remaining comparisons.</p><p>We compare to the state-of-the-art using AP BEV and AP 3D on the validation set in Tab. I and Tab. II. For the car class, we outperform the state-of-the-art in all categories. Most noticeably, we have a 9.2% AP increase in the BEV moderate category at 0.7 IoU, which is used to rank methods on the KITTI online server. For pedestrians and cyclists we surpass Pseudo-LiDAR with F-PointNet (PL-FP) in all but three categories and tie up to rounding error on hard cyclist BEV. We also surpass the performance of Pseudo-LiDAR implemented with AVOD, as shown in the top row, which indicates that much of the performance improvement for PL-FP can be attributed to F-PointNet. We leave using our stereo outputs on different 3D object detectors as future work. Results on the test set show similar performance improvements for our method in Tab. III and Tab. IV.</p><p>In Tab. VII we provide runtime analysis using a Titan Xp GPU. Our method runs faster than the current state-of-the-art, Pseudo-LiDAR, by 160 ms. They run PSMNet (0.410s) and AVOD (0.100s), while our entire pipeline takes 0.350s. Our speed boosts can be attributed to the fact we only estimate disparities for RoIs, and our object-centric formulation builds a set of smaller disparity cost volumes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 2D AP Comparison with Box Association</head><p>We compare our box association method with Stereo-RCNN <ref type="bibr" target="#b11">[12]</ref> using 2D and stereo AP. Stereo AP <ref type="bibr" target="#b11">[12]</ref> is calculated by requiring a minimum 0.7 IoU with the ground truth box for the left and right bounding boxes and for the left and right bounding boxes to belong to the same object. As shown in Tab. V the 2D detector MS-CNN and our box association algorithm outperforms or has comparable results to Stereo R-CNN. In particular, there is a 9.57% AP improvement in the hard category. Moreover, in Tab. V, there is a minimal decrease from our left and right AP to our stereo AP, which demonstrates that minimal performance is lost by performing association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect of Local Stereo Depth Estimation</head><p>In Tab. VI we provide ablation studies. The baseline used is Pseudo-LiDAR <ref type="bibr" target="#b12">[13]</ref>. The third row of the        <ref type="bibr" target="#b12">[13]</ref> as the baseline. Local is our object-centric stereo network.</p><p>that we outperform an additional baseline that only keeps foreground depth pixels from PSMNet using a version of Mask R-CNN <ref type="bibr" target="#b27">[28]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, this is in part because this Mask-RCNN baseline still contains ambiguous depths and is susceptible to streaking artifacts. We note that our object-centric disparity formulation makes our method robust to some erroneous segmentation predictions because our network is trained with only object pixels, so it learns Stage Runtime (s) MS-CNN <ref type="bibr" target="#b28">[29]</ref> 0.080 Box Association 0.009 Stereo Matching 0.161 AVOD <ref type="bibr" target="#b6">[7]</ref> 0.100 Total 0.350 TABLE VII: Runtime Analysis. Runtime for each stage of our method. Our total runtime is faster than the previous state-of-the-art: PSMNet + AVOD (0.410s + 0.100s).</p><p>to set some background pixels to the object depth to help maintain object shape. Tab. VI shows the benefits of our method (Local), pre-training AVOD on depth completed LiDAR, appending anchor information to AVOD's proposal regression, and employing our point cloud loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>3D localization is improved with our object-centric point cloud that avoids streaking artifacts, which occurs with PSMNet even when masked using Mask R-CNN. Ground truth and predictions are shown in red and green, respectively. algorithm determines association by going in order of highest to lowest scoring SSIM indices using the image with fewer boxes. Once a box is associated, it is removed for faster comparison. At the end of the algorithm, unmatched boxes are considered false positives and removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative results on KITTI. Ground truth and predictions are in red and green, respectively. Colored points are predicted by our stereo matching network while LiDAR points are shown in black for visualization purposes only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table shows</head><label>shows</label><figDesc>/ 59.51 45.99 / 43.71 41.92 / 37.99 29.22 / 18.15 21.88 / 14.26 18.83 / 13.72 S-RCNN [12] 87.13 / 85.84 74.11 / 66.28 58.93 / 57.24 68.50 / 54.11 48.30 / 36.69 41.47 / 31.07 PL-FP [13] 89.8 / 89.5 77.6 / 75.5 68.2 / 66.3 72.8 / 59.4 51.8 / 39.8 44.0 / 33.5 PL-AVOD [13] 89.0 / 88.5 77.5 / 76.4 68.7 / 61.2 74.9 / 61.9 56.8 / 45.3 49.0 / 39.0 Ours 90.01 / 89.65 80.63 / 80.03 71.06 / 70.34 77.66 / 64.07 65.95 / 48.34 51.20 / 40.39</figDesc><table><row><cell>Method</cell><cell>Easy</cell><cell>0.5 IoU Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>0.7 IoU Moderate</cell><cell>Hard</cell></row><row><cell>TLNet [14]</cell><cell>62.46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Car Localization and Detection. AP BEV / AP 3D on val. AVOD 36.68 / 27.39 30.08 / 26.00 23.76 / 20.72 36.12 / 35.88 22.99 / 22.78 22.11 / 21.94 PL-FP [13] 41.3 / 33.8 34.9 / 27.4 30.1 / 24.0 47.6 / 41.3 29.9 / 25.2 27.0 / 24.9 Ours 44.00 / 34.80 37.20 / 29.05 30.39 / 28.06 48.20 / 45.59 27.90 / 25.93 26.96 / 24.62</figDesc><table><row><cell>Method</cell><cell>Easy</cell><cell>Pedestrians Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>Cyclists Moderate</cell><cell>Hard</cell></row><row><cell>PSMNet +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Pedestrian and Cyclist Localization and Detection. AP BEV / AP 3D on val. We note that<ref type="bibr" target="#b12">[13]</ref> only provides values up to one decimal place.</figDesc><table><row><cell>Method</cell><cell cols="3">BEV AP Easy Moderate Hard</cell><cell cols="3">3D AP Easy Moderate Hard</cell></row><row><cell>S-RCNN [12]</cell><cell>61.67</cell><cell>43.87</cell><cell>36.44</cell><cell>49.23</cell><cell>34.05</cell><cell>28.39</cell></row><row><cell>PL-FP [13]</cell><cell>55.0</cell><cell>38.7</cell><cell>32.9</cell><cell>39.7</cell><cell>26.7</cell><cell>22.3</cell></row><row><cell>PL-AVOD [13]</cell><cell>66.83</cell><cell>47.20</cell><cell>40.30</cell><cell>55.40</cell><cell>37.17</cell><cell>31.37</cell></row><row><cell>Ours</cell><cell>66.97</cell><cell>54.16</cell><cell>46.70</cell><cell>55.11</cell><cell>38.80</cell><cell>31.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Car Localization and Detection. AP BEV and AP 3D on KITTI test. AVOD [13] 27.5 / 25.2 20.6 / 19.0 19.4 / 15.3 13.5 / 13.3 9.1 / 9.1 9.1 / 9.1 Ours 35.12 / 28.14 23.23 / 21.85 22.56 / 20.92 34.77 / 32.66 22.26 / 21.25 21.36 / 19.77</figDesc><table><row><cell>Method</cell><cell>Easy</cell><cell>Pedestrians Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>Cyclists Moderate</cell><cell>Hard</cell></row><row><cell>PL-FP [13]</cell><cell cols="3">31.3 / 29.8 24.0 / 22.1 21.9 / 18.8</cell><cell>4.1 / 3.7</cell><cell>3.1 / 2.8</cell><cell>2.8 / 2.1</cell></row><row><cell>PL-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Pedestrians and Cyclists Localization and Detection. AP BEV and AP 3D on KITTI test.</figDesc><table><row><cell>Metric</cell><cell cols="7">Left Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard Right Stereo</cell></row><row><cell>S-RCNN [12]</cell><cell>98.73</cell><cell>88.48</cell><cell>71.26 98.71</cell><cell>88.50</cell><cell>71.28 98.53</cell><cell>88.27</cell><cell>71.14</cell></row><row><cell>Ours</cell><cell>97.77</cell><cell>89.93</cell><cell>80.53 98.23</cell><cell>90.09</cell><cell>80.50 97.13</cell><cell>89.63</cell><cell>80.02</cell></row><row><cell cols="2">Ours Adaptive Thresh 98.87</cell><cell>90.53</cell><cell>81.05 98.92</cell><cell>90.50</cell><cell>80.88 98.44</cell><cell>90.38</cell><cell>80.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Stereo 2D AP. 2D detections and stereo box correspondence AP on val.</figDesc><table><row><cell>Version</cell><cell>AP BEV</cell></row><row><cell>Baseline [13]</cell><cell>56.8</cell></row><row><cell>Baseline + Pre-trained weights</cell><cell>57.10</cell></row><row><cell>Baseline + Mask-RCNN [28]</cell><cell>49.20</cell></row><row><cell>Local</cell><cell>64.90</cell></row><row><cell>Local + AVOD mods.</cell><cell>65.40</cell></row><row><cell>Local + AVOD mods. + PC Loss</cell><cell>65.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Ablation Studies. Comparisons of AP BEV at 0.7 IoU using</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lasernet: An efficient probabilistic 3d object detector for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2069" to="2078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09738</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07179</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Triangulation learning network: from monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01193</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth coefficients for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>eeding of IEEE Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6044" to="6053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for end-to-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection with pseudolidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09847</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>3d object proposals for accurate object class detection,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omni-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semi-supervised monocular depth estimation with left-right consistency using deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Loo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07542</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CRV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beat the mturkers: Automatic image labeling from weak 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
