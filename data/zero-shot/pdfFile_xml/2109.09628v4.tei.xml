<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clemson University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clemson University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Self-supervised</term>
					<term>Monocular</term>
					<term>Depth Prediction</term>
					<term>Sparse LiDAR</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised monocular depth prediction provides a cost-effective solution to obtain the 3D location of each pixel. However, the existing approaches usually lead to unsatisfactory accuracy, which is critical for autonomous robots. In this paper, we propose FusionDepth, a novel two-stage network to advance the self-supervised monocular dense depth learning by leveraging low-cost sparse (e.g. 4-beam) LiDAR. Unlike the existing methods that use sparse LiDAR mainly in a manner of time-consuming iterative post-processing, our model fuses monocular image features and sparse LiDAR features to predict initial depth maps. Then, an efficient feed-forward refine network is further designed to correct the errors in these initial depth maps in pseudo-3D space with real-time performance. Extensive experiments show that our proposed model significantly outperforms all the state-of-the-art self-supervised methods, as well as the sparse-LiDAR-based methods on both self-supervised monocular depth prediction and completion tasks. With the accurate dense depth prediction, our model outperforms the state-of-theart sparse-LiDAR-based method (Pseudo-LiDAR++ [1]) by more than 68% for the downstream task monocular 3D object detection on the KITTI Leaderboard. Code is available at https://github.com/AutoAILab/FusionDepth <ref type="figure">Figure 1</ref>: The sparse LiDAR (e.g. 4-beam) provides sparse yet accurate points which cannot be directly used for high-level downstream tasks such as detection due to the sparsity. The monocular depth prediction can be significantly improved by effectively fusing the features of the sparse Li-DAR via self-supervised learning. With our highquality predicted depth, the performance of the downstream perception tasks such as monocular 3D object detection can be remarkably improved. did not utilize the visual context information, and is too slow (1 to 2 FPS) for real-time applications like autonomous robots. Pursing an accurate and real-time self-supervised monocular depth prediction, we propose a two-stage network for depth prediction by fully utilizing the sparse LiDAR points and monocular images in both the feature and the prediction levels. Our framework can learn the complementary information from the two distinct types of features for the dense depth prediction tasks. To overcome the sparsity issue of the sparse Li-DAR, we transform the sparse LiDAR points into pseudo dense representations, which are more suitable for networks to extract features, and then fuse the features with the image features to predict the initial depth. To further improve the quality of initial depth, we train a Re-fineNet to efficiently correct the high-order errors in the 3D space to obtain high-quality dense depth maps.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Obtaining the 3D location of objects is an essential task for autonomous robots. However, accurate dense depth perception with LiDAR is normally expensive, thus limit it for mass production. The depth prediction from monocular images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> is cost-effective and attracting more and more attention from both research and industry communities.</p><p>Many methods have been proposed and remarkable progress has been achieved in recent years <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Unlike other computer vision tasks, it is impractical to obtain large-scale dense depth labels. Therefore, self-supervised monocular depth prediction has been a promising solution. Typically networks are trained to predict both the depth and ego-motion of the camera, while the re-projection photo-metric loss is calculated as an intermediary constraint to optimize the networks. However, these methods usually suffer from multiple challenges due to the loss function design. The most significant one is that the re-projection constraint assumes the scene is static and without occlusions between the neighboring frames. In fact, most of the vital objects (e.g., vehicles, pedestrians, and cyclists in the driving scenario) are dynamic, and the occlusions are almost inevitable.</p><p>To handle these challenges, a potential direction is to perform the monocular depth prediction with other low-cost sensors like sparse (e.g. 4-beam) LiDAR. Compared to the 64-beam Li-DAR, the cost of 4-beam LiDAR is at least two orders lower while providing very sparse yet accurate depth. It is too sparse to be directly used for high-level perception tasks but potentially can be used with images to guide the network for better dense depth prediction Pseudo-LiDAR++ <ref type="bibr" target="#b0">[1]</ref> employed the sparse LiDAR in the post-processing using a graph-based depth correction (GDC) module to improve the performance of stereo 3D detection. However, this approach has two significant limitations: 1) the quality of its predicted dense depth is non-optimal since the sparse LiDAR data is not utilized in the depth prediction network; 2) the GDC post-processing The ultimate goal of depth prediction is to provide 3D information for downstream tasks such as monocular 3D detection and re-construction. However, the relation between the performance of depth prediction and the high-level downstream tasks has not yet been explored. To thoroughly evaluate our proposed method, we report the performance for both low-level tasks, including selfsupervised dense depth prediction and completion, and a downstream high-level perception task: monocular 3D object detection. On all these tasks, our proposed model FusionDepth significantly outperforms the state-of-the-art methods that rely on or do not rely on sparse LiDAR. To summarize, our key contributions are as follows:</p><p>? We propose a novel two-stage self-supervised network to predict and refine dense depth maps by fusing the features of 2D monocular images and 3D sparse LiDAR points. Our experiments demonstrates that our model achieves state-of-the-art performance in the depth prediction and completion tasks on the KITTI dataset.</p><p>? To overcome the sparsity issue of the sparse LiDAR, we propose to transform the sparse points into a novel pseudo dense representation (PDR) which can be more effectively fused with monocular image features.</p><p>? With the improved predicted depth maps, the performance of the downstream task monocular 3D object detection is significantly improved. Our model outperforms the state-of-theart sparse-LiDAR-based 3D detection model (Pseudo-LiDAR++ <ref type="bibr" target="#b0">[1]</ref>) by more than 68% on the KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Self-supervised Monocular Depth Prediction: Early work for depth prediction is usually supervised methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12]</ref>. Pixel-level labeled dense depth is rarely available, in recent years, self-supervised methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16</ref>] became more and more popular. They have achieved great success but still suffer from dynamic objects and scale ambiguity. In contrast, we fuse the feature from sparse LiDAR points to help our method predict a more accurate depth for each pixel.</p><p>Depth Prediction with Sparse LiDAR: Recently, many researchers proposed to use few-beam LiDAR for better depth prediction <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1]</ref>. The Pseudo LiDAR++ <ref type="bibr" target="#b0">[1]</ref> achieved excellent performance by a GDC post-processing module to optimize the predicted depth with 4-beam LiDAR data. However, the potential of the sparse LiDAR is not fully discovered since the sparse LiDAR points are unused in the initial depth prediction stage. To effectively utilize the sparse LiDAR and monocular images, we fuse them in both feature and prediction level for more accurate depth predictions.  <ref type="figure">Figure 2</ref>: Overview of our framework: Our proposed two-stage self-supervised FusionDepth model takes a monocular image and the corresponding sparse LiDAR points as input, and predicts a dense depth map for each monocular image. At inference time, only the modules inside the gray dashed rectangle are needed.</p><p>Depth Completion: The depth completion is a task to generate per-pixel dense depth maps from the relative sparse depths. Most depth completion models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> are trained with labeled data which require intensive human labors. To utilize the massive unlabeled data, self-supervised depth completion methods were developed in recent years <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> to generate depth maps from 64-beams dense LiDAR points. Our proposed method is designed to predict depth maps from 4-beams sparse LiDAR points, however, with the generalizability, our method is also applicable for the depth completion task.</p><p>Monocular 3D Object Detection: Monocular 3D object detection is to directly predict the 3D coordinates of objects from monocular images. There are mainly two types of methods: RGB imagebased and pseudo-LiDAR based. Former employ detection networks like CenterNet <ref type="bibr" target="#b31">[32]</ref> to predict the bounding boxes <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> directly from images. The latter perform the detection over the pseudo-LiDAR representation, which is lifted from the dense depth prediction <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. Benefited from the 2D-to-3D mapping, the pseudo-LiDAR-based methods achieved much better performance <ref type="bibr" target="#b38">[39]</ref>. Our experiments demonstrate that the performance of the monocular 3D object detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39]</ref> can be significantly improved using our depth prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head><p>The overview of our proposed self-supervised framework is shown in <ref type="figure">Fig. 2</ref>. Our framework predicts a dense depth map for each monocular image by taking two types of data as input: monocular image and its corresponding sparse LiDAR points. Our framework consists of two steps: initial depth prediction based on the fused multi-scale features from both the monocular image and sparse LiDAR points, and depth refinement to correct the high-order errors in the initial depth maps. The details of each component are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Initial Depth Generation</head><p>For each monocular image I H?W ?3 , the corresponding sparse LiDAR points are P N ?3 captured by few-beam LiDAR (e.g. <ref type="bibr">4-beam)</ref> where N is the number of the points. Each point p i consists of three values X, Y, Z, representing the location in the 3D space. The network predicts an initial depth map for each image based on the fusion of features from image I and the corresponding sparse LiDAR points P .</p><p>The main challenge here is to effectively fuse the features from a 2D image and the features from a set of unordered LiDAR points. When projecting all the sparse LiDAR points into the image plane, only 1.4% of the pixels have corresponding depth values. We observe that simply concatenating the image data and the projected sparse depth map can only negligibly improve the performance due to the sparsity of the representation. To resolve the sparsity issue, we transform the sparse LiDAR points into pseudo dense representations (PDRs), which can be effectively encoded and fused with the monocular image features by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo Dense Representations Generation:</head><p>For each image I H?W ?3 , all its corresponding sparse LiDAR points P N ?3 are transformed into two-channel PDRs with a size of H ? W ? 2, including a depth channel to present the absolute depth values of each pixel, and a confidence channel presents the reliability of the corresponding depth. Based on the assumption that the depth values should be similar for most neighboring pixels, we generate the depth channel by first projecting each sparse LiDAR point p i into the image plane at the position of (u i , v i ) and then dilate it into a circular area with a radius of R. The depth channel (D) is generated by setting the depth value of pixel (x, y) within this circular area as Z i :</p><formula xml:id="formula_0">r(x, y) = (u i ? x) 2 + (v i ? y) 2 (1) D(x, y) = Z i , if r(x, y) &lt; R 0, otherwise .<label>(2)</label></formula><p>Although the depth channel transfers the sparse LiDAR points into dense representation, it inevitably introduces noises. To mitigate the impact of these noises, we further generate the confidence channel to indicate the reliability of depth for each pixel. The confidence of each pixel is inversely proportional to the distance to its circular center:</p><formula xml:id="formula_1">C(x, y) = 1 r (x, y), if r(x, y) &lt; R 0, otherwise ,<label>(3)</label></formula><p>If more than one sparse data point is close to an PDR pixel, the confidence and depth scores generated from these multiple data points will be averaged. These two channels jointly provide alternative dense representations that can be encoded by the convolutional neural networks more effectively.</p><p>DepthNet for Initial Depth Map Prediction Based on Fused Features. After transforming the sparse LiDAR points into the two-channel PDRs, the features of PDRs and monocular images can be fused together for the initial depth prediction. To enable our network to thoroughly learn the complementary information from two distinct features, we adopt the intermediate fusion to combine the multi-scale deep features layer by layer. As shown in <ref type="figure">Fig. 2</ref>, there are two feature encoders: PDR feature encoder for extracting features that explicitly encodes depth information from the sparse LiDAR pseudo dense representation, and monocular image feature encoder to extract feature which implicitly encodes semantic information from images. The decoder network takes the two types of multi-scale features and concatenates them together to predict the initial depth map at multiple scales. With the effective feature fusion, our DepthNet can predict more accurate dense depths.</p><p>PoseNet for Ego-Motion Prediction. The PoseNet is essential for self-supervised monocular depth prediction since its predicted ego-motion makes the cross-frame projection possible, which is used as geometry constraints to train the network. It is directly related to the quality of the pixel correspondence across frames. To predict accurate ego-motions, as shown in <ref type="figure">Fig. 2</ref>, the PoseNet is also designed to take the complementary monocular images and the PDRs as the input. The ego-motion is formulated as 6 degrees of freedom, consisting the camera rotation and translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Depth Refinement</head><p>Our network produces relatively accurate initial depth predictions by fusing the features of monocular images and sparse LiDAR points. However, the network still makes various errors, such as inconsistent depth prediction for different parts of the same object and the systematic depth bias.</p><p>To further resolve the inconsistent depth prediction and improve the correction computing efficiency, we propose a RefineNet to correct the initial depth errors in the pseudo 3D space <ref type="bibr" target="#b38">[39]</ref>. Our RefineNet is a multi-scale fully convolutional network, takes the PDR feature, image feature, and initial depth as input, outputs the refined final depth prediction. To leverage the 3D information predicted from DepthNet, each initial depth map is first transformed into a 3-channel x-y-z map representation. For a pixel at location (u, v) with depth d at the initial depth map, the transformation is based on the camera intrinsics f x , f y , C x , C y :</p><formula xml:id="formula_2">x = (u ? C x ) ? d/f x , y = (v ? C y ) ? d/f y , z = d,<label>(4)</label></formula><p>Our RefineNet is trained by distilling knowledge from a offline depth correction module in the way of self-training. As shown in <ref type="figure">Fig. 2</ref>, for each initial depth map, a depth correction module is applied to produce a more accurate depth named 'Enhanced Depth' with the guidance of sparse LiDAR points. By training with pairs of the pseudo 3D initial depth, PDR and image features, and the enhanced depth, the RefineNet can improve the initial depth quality by correcting its errors. Compared to the depth correction module GDC <ref type="bibr" target="#b0">[1]</ref> we used as teacher module, our network is very computationally efficient for real-time systems. Moreover, the conventional depth correction module can still be further applied as post-processing after our RefineNet to improve the accuracy by sacrificing the real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Functions</head><p>Our model is jointly trained with two loss functions: the re-projection loss L re to utilize the interframe geometric constraints and the scale-invariant loss L si to distill the knowledge from the enhanced depth maps generated by the offline depth correction model.</p><p>The re-projection loss L re is a linear combination of two parts: the photo-metric loss L p with a filtering mask ?, and the smoothness loss L smooth . The photo-metric loss L p is to evaluate the pixel-level similarity between the re-projected fake image I t?t with the real image I t at adjacent time frames, based on the photo-metric reconstruction error pe which consists of the SSIM and L1 distance to penalize the errors of the re-projected image. We choose to use the frames I t+1 and I t?1 as I t . The proj() projects current frame dense depth D t to frame t with camera intrinsic matrix K and the camera ego-motion estimated by the PoseNet, is the sampling operator.</p><formula xml:id="formula_3">L re = ?L p + L smooth ,<label>(5)</label></formula><formula xml:id="formula_4">L p = t pe(I t , I t?t ),<label>(6)</label></formula><formula xml:id="formula_5">I t?t = I t proj(D t , T t?t , K) ,<label>(7)</label></formula><formula xml:id="formula_6">pe(I a , I b ) = ? 2 (1?SSIM(I a , I b ))+(1??) I a ? I b 1<label>(8)</label></formula><p>To eliminate the shrinking of the depth map, we further adopt the edge-aware metric from <ref type="bibr" target="#b40">[41]</ref> into our smoothness loss function L smooth , which is formulated as:</p><formula xml:id="formula_7">L smooth = |? x d * t | e ?|?xIt| + |? y d * t | e ?|?yIt| ,<label>(9)</label></formula><p>while d * t = d t /d t is the mean-normalized inverse depth. This normalization makes the smoothness loss invariant to output scale.</p><p>For the photo-metric loss L p , we follow <ref type="bibr" target="#b14">[15]</ref> to apply a filtering mask ? to filter out the occlusion and stationary pixels, and then interpolate the depth predictions at each scales to the input resolution before computing our re-projection loss to eliminate the 'holes' at the low-texture area. The filtering mask ? is formulated as:</p><formula xml:id="formula_8">? = min t pe(I t , I t ?t ) &lt; min t pe(I t , I t ) .<label>(10)</label></formula><p>To distill the knowledge from the offline correction model, the scale-invariant loss is employed for optimization, which is formulated as:</p><formula xml:id="formula_9">L si = ? * ?Si,<label>(11)</label></formula><formula xml:id="formula_10">Si = 1 n 2 i,j (log y i ? log y j ) ? (log y * i ? log y * j ) 2 ,<label>(12)</label></formula><p>where y and y * indicate the predicted depth and enhanced depth respectively, n is the number of pixels. The Si loss penalizes the relative depth-differences between each pixel pairs.</p><p>Our framework is trained with a linear combination of the re-projection loss L re and the scaleinvariant loss L si :</p><formula xml:id="formula_11">L total = ?L re + ?L si ,<label>(13)</label></formula><p>while ? and ? are the weights for re-projection loss L re and scale-invariant loss L si respectively. The implementation details can be found in the supplementary materials.    <ref type="bibr" target="#b0">[1]</ref> is stereo, which can greatly improve the detection result. For fair comparison, we replace the it with Monodepth2 <ref type="bibr" target="#b14">[15]</ref>, and refer as MonoPL++ <ref type="bibr" target="#b0">[1]</ref> 4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Depth Prediction</head><p>Dataset. Following the state-of-the-art methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13]</ref>, we evaluate our FusionDepth performance of dense depth prediction on the Eigen split <ref type="bibr" target="#b54">[55]</ref> of the KITTI original dataset. We did not evaluate on the KITTI testing benchmark which is for vision-only methods. The 4-beams data are sampled from original 64-beams LiDAR data same as Pseudo LiDAR++ <ref type="bibr" target="#b0">[1]</ref>.</p><p>Results. To extensively evaluate the performance, we compare with four types of methods including: (1) self-supervised monocular-based methods (M) <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13]</ref>, (2) selfsupervised stereo-based methods (S) <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>, (3) supervised methods (Sup) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, and (4) methods that use LiDAR signal as guidance (L) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45]</ref>. The performance comparison with the state-of-the-art methods for these groups is shown in <ref type="table" target="#tab_9">Table 7</ref>. Note that the initial depth module in Pseudo LiDAR++ <ref type="bibr" target="#b0">[1]</ref> is supervised and stereo. For fair comparison, we replace it with the state-of-the-art unsupervised monocular depth module Monodepth2 <ref type="bibr" target="#b14">[15]</ref>. We will refer this model as MonoPL++.</p><p>Due to the limitations of the re-project photo-metric loss, the self-supervised monocular (M) <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13]</ref> and stereo-based methods (S) <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>   is already better than all these methods and even outperforms the supervised methods (Sup) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. With our RefineNet, our performance is further improved and outperforms all the sparse-LiDARbased methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45]</ref>. Using the GDC for post-processing, our final results significantly outperform all other methods, including the above mentioned most recent work MonoPL++ <ref type="bibr" target="#b0">[1]</ref> which has access to the same amount of the sparse LiDAR points and same post-processing. The results indicate an effective fusion of sparse LiDAR points and monocular images achieves more accurate dense depth predictions. More quantitative and qualitative comparison and error analysis can be found in the supplementary materials.</p><p>To extensively compare with the state-of-the-art methods under the same settings, as shown in <ref type="table" target="#tab_2">Table 2</ref>, we compare with methods that were originally designed to use sparse LiDAR for depth prediction, including full-MAE <ref type="bibr" target="#b16">[17]</ref>, Parse-a-Line <ref type="bibr" target="#b17">[18]</ref>, Sparse-to-Dense <ref type="bibr" target="#b18">[19]</ref>, and MonoPL++ <ref type="bibr" target="#b0">[1]</ref>. For each group of experiments, the same amount of sparse LiDAR points is used for a fair comparison. Under the same settings, our proposed model consistently significantly outperforms all the state-of-the-art methods. These results further confirm the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Monocular 3D Object Detection</head><p>The ultimate goal of the monocular depth prediction is to provide 3D representation for downstream tasks. To demonstrate the impact of our improvement on the depth metrics to downstream tasks, we evaluate the performance of the monocular 3D object detection task with our predicted depth maps as input on the KITTI detection dataset. Following the state-of-the-art methods <ref type="bibr" target="#b38">[39]</ref>, we report 3D Average Precision (AP) as the evaluation metrics. <ref type="table" target="#tab_5">Table 4</ref> shows the performance comparison for the monocular 3D object detection task. The same detection model is employed for all the experiments while the only difference is the input depth. The monodepth2 <ref type="bibr" target="#b14">[15]</ref> is used as baseline model. Our model use the same CNN backbone (ResNet-18 <ref type="bibr" target="#b62">[63]</ref>) as Monodepth2 <ref type="bibr" target="#b14">[15]</ref>. By effectively using the sparse LiDAR, the detection performance can be improved by more than 40% compared to the baseline which only uses monocular images. Compared to the recently proposed MonoPL++ <ref type="bibr" target="#b0">[1]</ref>, our model significantly outperforms it by more than 98.2% in terms of the mAP over all the three categories. Without the time-consuming iterative refinement module, our model is 50 times faster than the MonoPL++ <ref type="bibr" target="#b0">[1]</ref>.</p><p>Furthermore, our RefineNet obtains a significantly increased performance on the detection metrics than the depth metrics. The improvement of applying RefineNet on the depth score is only around 3.4% in terms of the relative error, while the improvement is more than 23% on the detection score. We observe that our improvement in the dense depth prediction can yield a even more significant improvement in the downstream task. This indicates the importance of using the downstream tasks to evaluate the quality of the learned dense depth. <ref type="table" target="#tab_3">Table 3</ref> further shows the comparison of our method with other state-of-the-art for monocular 3D detection tasks on the car category. As an unsupervised learning method, our method significantly outperforms all the state-of-the-art methods by a large margin, including the sparse-LiDAR-based MonoPL++ <ref type="bibr" target="#b0">[1]</ref>. Note that the performance of our model can be further improved by extending it to the supervised setting. More qualitative analysis can be found in the supplementary materials.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera-LiDAR Fusion in Initial Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Depth Completion</head><p>We evaluate our FusionDepth on the KITTI depth completion task. Most (over 95%) of the methods in the KITTI depth completion benchmark is under supervised training. Following other state-ofthe-art self-supervised methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, we test our model on the KITTI validation set, as shown in <ref type="table">Table 6</ref>. By effectively utilizing the LiDAR features, our model outperforms all other self-supervised methods by a large gap with all the metrics demonstrating the generalizability of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Method RMSE iRMSE iMAE KISS-GP <ref type="bibr" target="#b30">[31]</ref> 1593. <ref type="bibr" target="#b36">37</ref> 27.98 2.36 Sparse to dense <ref type="bibr" target="#b27">[28]</ref> 1342.33 4.28 1.64 DDP <ref type="bibr" target="#b29">[30]</ref> 1310.03 --VOICED <ref type="bibr" target="#b28">[29]</ref> 1230.85 3.84 1.29 SelfDeco <ref type="bibr" target="#b26">[27]</ref> 1212.89 3.54 1.29 FusionDepth(Ours) 1193.92 3.385 1.28 <ref type="table">Table 6</ref>: Self-supervised depth completion:</p><p>We evaluate our FusionDepth on the KITTI depth completion task validation set, comparing it to the state-of-the-art self-supervised methods. All metrics are the lower, the better. The best results are in bold.</p><p>To evaluate the effectiveness of each component of our FusionDepth, we perform three groups of experiments to evaluate the impact of pseudo dense representation, camera-LiDAR fusion, and the refinement on the dense depth prediction task. The results of the ablation studies are shown in <ref type="table" target="#tab_7">Table 5</ref>. The impact of the LiDAR sparsity and the error analysis by semantic categories can be found in the supplementary materials.</p><p>Pseudo Dense Representation. When the sparse LiDAR are directly used as input, the performance for depth prediction is only slightly improved by around 6%. However, by directly using our proposed pseudo dense presentation as input, the improvement is doubled (12%), confirming its importance.</p><p>PDR Feature and Image Feature Fusion. We conduct three types of feature fusions, including input level, feature level, and output level fusion. The best performance is achieved by performing the feature level fusion of the PDR features and image features from our proposed encoders.</p><p>Refinement. The third group shows that the depth prediction performance can be improved by our RefineNet evenly either with or without GDC. Although the RefineNet can only slightly improve the performance of dense depth prediction, it can significantly improve the monocular 3D object detection task (+23%), demonstrating the effectiveness of the RefineNet. More qualitative and efficiency analysis of the RefineNet can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a two-stage self-supervised framework FusionDepth that effectively fuses features from monocular camera images and sparse LiDAR data. Our method outperforms all the state-ofthe-art methods on both depth prediction and completion tasks. Also shows a remarkable advantage for downstream tasks like monocular 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>1 More Results for Depth Prediction Depth Prediction on KITTI Dataset: Due to the space limitation, we only compared with part of the sate-of-the-art methods for dense depth prediction task in the main paper. Here, <ref type="table" target="#tab_9">Table 7</ref> shows the complete comparison with the state-of-the-art methods on KITTI <ref type="bibr" target="#b47">[48]</ref> dataset. By effectively using low-cost sparse LiDAR points, our method achieves more accurate dense depth predictions than all state-of-the-art counterparts including the sparse-LiDAR based methods.</p><p>Statistics by Semantic Categories: <ref type="figure">Fig. 3</ref> shows the comparison of depth prediction error by different semantic categories in the KITTI <ref type="bibr" target="#b47">[48]</ref> dataset, while <ref type="figure">Fig 4</ref> shows the average number of pixels per image by different semantic categories. Our proposed model consistently and significantly improves the depth quality for all the semantic categories.</p><p>Depth Error Qualitative Analysis: <ref type="figure" target="#fig_3">Fig. 7</ref> shows the complete qualitative comparison of the depth errors of our method and the image-based depth prediction model monodepth2 <ref type="bibr" target="#b14">[15]</ref>. Leveraging low-cost sparse LiDAR information, our method produces much better results on all of the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of RefineNet:</head><p>To better understand the effectiveness of our RefineNet, we show the qualitative comparison between the initial depth and the refined depth in <ref type="figure">Fig. 8</ref>. These results show that our proposed RefineNet can significantly reduce the depth error on all these objects.</p><p>Computational Efficiency of RefineNet: The existing depth correction / refinement methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b63">64]</ref> conducts iterative optimization on the testing data, and normally they have higher accuracy but is extremely slow (1-2 FPS). <ref type="table" target="#tab_10">Table 8</ref> shows the comparison between our RefineNet and the existing method including GDC <ref type="bibr" target="#b0">[1]</ref> and PnP-Depth <ref type="bibr" target="#b63">[64]</ref>. The comparison shows that our RefineNet is more efficient, achieving real-time speed (139 FPS) on single Nvidia RTX-2080Ti GPU.</p><p>LiDAR Sparsity. As shown in <ref type="figure" target="#fig_1">Fig. 5</ref>, our proposed method can consistently improve the depth prediction even when only one beam of sparse LiDAR points is used. And our method significantly outperforms other methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18]</ref> when the same amount of sparse LiDAR points are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">More Results for Monocular 3D Object Detection</head><p>Comparison With the State-of-the-Art: In addition to the concise quantitative comparison for 3D detection in the main paper, here we show a more comprehensive quantitative evaluation of how our advanced depth prediction improves downstream tasks. We employ the PatchNet <ref type="bibr" target="#b38">[39]</ref> to perform the 3D monocular object detection on the KITTI <ref type="bibr" target="#b47">[48]</ref> dataset using the depth maps generated from our model. <ref type="table">Table. 9</ref> shows the full comparison between our method and state-of-the-art methods on the KITTI testing set, using the KITTI official testing server. Our method significantly outperforms all counterparts, including the Pseudo LiDAR++ <ref type="bibr" target="#b0">[1]</ref> that also uses the sparse LiDAR points.</p><p>Qualitative Comparison: <ref type="figure" target="#fig_2">Fig. 6</ref> shows the qualitative comparison between our method and the state-of-the-art monocular depth prediction model Monodepth2 <ref type="bibr" target="#b14">[15]</ref> on the KITTI validation set. The PatchNet <ref type="bibr" target="#b38">[39]</ref> is employed for detection which takes the depth maps generated by our model and the Monodepth2 as inputs respectively. Note that the Monodepth2 also needs 4-beams LiDAR points to retrieve the absolute metric scale of the depth map before detection. With more accurate depth predictions, our method leads to much better detection results than the Monodepth2.      <ref type="table">Table 9</ref>: 3D detection performance evaluation for the Car category on the testing set of KITTI dataset <ref type="bibr" target="#b47">[48]</ref>. IoU threshold is set to 0.7. For fair comparison, we replace the supervised stereo depth module of Pseudo LiDAR++ with Monodepth2 <ref type="bibr" target="#b14">[15]</ref>. Our method significantly outperforms all counterparts, including the Pseudo LiDAR++ <ref type="bibr" target="#b0">[1]</ref> that also uses the sparse LiDAR points.    <ref type="bibr" target="#b14">[15]</ref>, and the predictions by our method respectively. The Red, yellow, and green indicate the depth error from high to low (best viewed in color). By fusing the low-cost sparse LiDAR information, our method generates much better results than the baseline which only rely on image features for all these objects. <ref type="figure">Figure 8</ref>: Effectiveness of RefineNet: The depth absolute error. The first to third rows are: the input RGB image, our initial depth prediction, and our refined depth prediction. The red, yellow, and green indicate the depth error from high to low (best viewed in color). These results show that our proposed RefineNet can significantly reduce the depth error on all these objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>r o a d s id e w a lk b u il d in g w a ll fe n c e p o le t r a ff ic li g h t t r a ff ic s ig n v e g e t a t io n t e r r a in s k y p e r s o n r id e r c a r t r u c k b u s t r a in m o t o r c y c le b ic y c le 0Depth Error by Semantic Categories: The depth absolute relative error analysis by different semantic categories in the KITTI test set. Our proposed model consistently improves the depth quality for all the semantic categories. a learning rate starting at 1e ? 3 and reduced by 90% every 40 epochs. The entire optimization is done with 100 epochs, and it takes around 10 hours on a single NVIDIA Tesla V100 GPU.r o a d s id e w a lk b u il d in g w a ll f e n c e p o le t r a f f ic li g h t t r a f f ic s ig n v e g e t a t io n t e r r a in s k y p e r s o n r id e r c a r t r u c k b u s t r a in m o t o r c y c le b Number of Pixels by Semantic Categories: The average number of pixels per image by different semantic categories in the KITTI test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Depth Error With Different LiDAR Sparsity: The depth absolute relative errors on the KITTI depth prediction dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Monocular 3D Object Detection: The qualitative comparison of monocular 3D detection by PatchNet<ref type="bibr" target="#b38">[39]</ref> based on the depth from our model and the Monodepth2<ref type="bibr" target="#b14">[15]</ref>. With the accurate dense depth prediction, our method produces much better detection results than the Monodepth2. Green boxes are the ground truth boxes while red boxes are the detection results. The LiDAR points in this figure are only used for visualization purpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Depth Error Qualitative Analysis: The depth absolute error. The first to third rows are: the input RGB image, the prediction of Monodepth2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Depth prediction on KITTI original dataset: Methods are ranked by absolute relative error. The best results are in bold. All methods are using a resolution of 640x192 pixels. Due to the exceptional time-consuming (around 1-2 FPS), we rank methods with and without iterative refinement separately. M , S, and L respectively indicates Monocular, Stereo, and Sparse LiDAR data, with Sup and ? respectively indicating supervised training and iterative correction in testing phase. * Only use LiDAR data in training phase, but tested on the KITTI improved dataset, which usually has a much lower error value.</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>Train</cell><cell cols="4">The lower the better Abs Rel Sq Rel RMSE RMSE log</cell><cell>The higher the better ?1 ?2 ?3</cell></row><row><cell></cell><cell>LEGO [42]</cell><cell></cell><cell></cell><cell></cell><cell>M</cell><cell></cell><cell>0.162</cell><cell>1.352</cell><cell>6.276</cell><cell>0.252</cell><cell>0.783 0.921 0.969</cell></row><row><cell></cell><cell cols="2">PackNet-SfM [16]</cell><cell></cell><cell></cell><cell>M</cell><cell></cell><cell>0.111</cell><cell>0.785</cell><cell>4.601</cell><cell>0.189</cell><cell>0.878 0.960 0.982</cell></row><row><cell></cell><cell cols="2">MonoDepth [14]</cell><cell></cell><cell></cell><cell>S</cell><cell></cell><cell>0.133</cell><cell>1.142</cell><cell>5.533</cell><cell>0.230</cell><cell>0.830 0.936 0.970</cell></row><row><cell></cell><cell cols="2">MonoDepth2 [15]</cell><cell></cell><cell></cell><cell>M+S</cell><cell></cell><cell>0.106</cell><cell>0.818</cell><cell>4.750</cell><cell>0.196</cell><cell>0.874 0.957 0.979</cell></row><row><cell></cell><cell>Dorn [43]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">M+Sup</cell><cell>0.099</cell><cell>0.593</cell><cell>3.714</cell><cell>0.161</cell><cell>0.897 0.966 0.986</cell></row><row><cell></cell><cell>BTS [44]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">M+Sup</cell><cell>0.091</cell><cell>0.555</cell><cell>4.033</cell><cell>0.174</cell><cell>0.904 0.967 0.984</cell></row><row><cell></cell><cell cols="2">Guizilini et al. [45]*</cell><cell></cell><cell></cell><cell>M+L</cell><cell></cell><cell>0.082</cell><cell>0.424</cell><cell>3.73</cell><cell>0.131</cell><cell>0.917</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">FusionDepth (Initial Depth)</cell><cell></cell><cell>M+L</cell><cell></cell><cell>0.078</cell><cell>0.515</cell><cell>3.67</cell><cell>0.154</cell><cell>0.935 0.973 0.986</cell></row><row><cell></cell><cell cols="3">FusionDepth (Refined Depth)</cell><cell></cell><cell>M+L</cell><cell></cell><cell>0.074</cell><cell>0.423</cell><cell>3.61</cell><cell>0.150</cell><cell>0.936 0.973 0.986</cell></row><row><cell></cell><cell cols="2">Struct2Depth [46]</cell><cell></cell><cell></cell><cell>M  ?</cell><cell></cell><cell>0.109</cell><cell>0.825</cell><cell>4.750</cell><cell>0.187</cell><cell>0.874 0.958 0.983</cell></row><row><cell></cell><cell>GLNet [47]</cell><cell></cell><cell></cell><cell></cell><cell>M  ?</cell><cell></cell><cell>0.099</cell><cell>0.796</cell><cell>4.743</cell><cell>0.186</cell><cell>0.884 0.955 0.979</cell></row><row><cell></cell><cell cols="2">MonoPL++ [1]</cell><cell></cell><cell></cell><cell>M+L  ?</cell><cell></cell><cell>0.098</cell><cell>0.714</cell><cell>4.30</cell><cell>0.176</cell><cell>0.899 0.967 0.984</cell></row><row><cell></cell><cell cols="4">FusionDepth (Initial Depth + GDC)</cell><cell>M+L  ?</cell><cell></cell><cell>0.067</cell><cell>0.423</cell><cell>3.42</cell><cell>0.144</cell><cell>0.941 0.977 0.988</cell></row><row><cell></cell><cell cols="5">FusionDepth (Refined Depth + GDC) M+L  ?</cell><cell></cell><cell>0.063</cell><cell>0.364</cell><cell>3.291</cell><cell>0.139</cell><cell>0.945 0.978 0.988</cell></row><row><cell>Method</cell><cell>Samples</cell><cell cols="6">The lower the better The higher the better Abs Rel RMSE ?1 ?2 ?3</cell><cell></cell></row><row><cell>full-MAE [17]</cell><cell>?650</cell><cell>0.179</cell><cell>7.14</cell><cell cols="2">70.9 88.8</cell><cell>95.6</cell><cell></cell><cell></cell></row><row><cell>Liao et al. [18]</cell><cell>225</cell><cell>0.113</cell><cell>4.50</cell><cell cols="2">87.4 96.0</cell><cell>98.4</cell><cell></cell><cell></cell></row><row><cell>Sparse2Dense [19]</cell><cell>100</cell><cell>0.095</cell><cell>4.303</cell><cell cols="2">90.0 96.3</cell><cell>98.3</cell><cell></cell><cell></cell></row><row><cell>FusionDepth</cell><cell>100</cell><cell>0.074</cell><cell>4.11</cell><cell cols="2">93.0 97.0</cell><cell>98.3</cell><cell></cell><cell></cell></row><row><cell>FusionDepth + GDC</cell><cell>100</cell><cell>0.073</cell><cell>4.11</cell><cell cols="2">93.0 97.0</cell><cell>98.3</cell><cell></cell><cell></cell></row><row><cell>Sparse2Dense [19]</cell><cell>200</cell><cell>0.083</cell><cell>3.851</cell><cell cols="2">91.9 97.0</cell><cell>98.6</cell><cell></cell><cell></cell></row><row><cell>FusionDepth</cell><cell>200</cell><cell>0.069</cell><cell>3.92</cell><cell cols="2">93.7 97.0</cell><cell>98.3</cell><cell></cell><cell></cell></row><row><cell>FusionDepth + GDC</cell><cell>200</cell><cell>0.066</cell><cell>3.92</cell><cell cols="2">93.7 97.1</cell><cell>98.4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Supervised Depth</cell><cell cols="3">KITTI Testing (AP |40) Easy Mod. Hard</cell></row><row><cell>Pseudo LiDAR++* [1]</cell><cell></cell><cell>68.5</cell><cell>54.7</cell><cell>51.5</cell></row><row><cell>Decoupled-3D [49]</cell><cell></cell><cell>11.08</cell><cell>7.02</cell><cell>5.63</cell></row><row><cell>MonoPSR [50]</cell><cell>-</cell><cell>10.76</cell><cell>7.25</cell><cell>5.85</cell></row><row><cell>MonoPL [38]</cell><cell></cell><cell>10.76</cell><cell>7.50</cell><cell>6.10</cell></row><row><cell>SS3D [51]</cell><cell>-</cell><cell>10.78</cell><cell>7.68</cell><cell>6.51</cell></row><row><cell>MonoDIS [52]</cell><cell>-</cell><cell>10.37</cell><cell>7.94</cell><cell>6.40</cell></row><row><cell>M3D-RPN [53]</cell><cell>-</cell><cell>14.76</cell><cell>9.71</cell><cell>7.42</cell></row><row><cell>AM3D [54]</cell><cell></cell><cell>16.50</cell><cell>10.74</cell><cell>9.52</cell></row><row><cell>PatchNet [39]</cell><cell></cell><cell>15.68</cell><cell>11.12</cell><cell>10.17</cell></row><row><cell>MonoPL++ [1]</cell><cell></cell><cell>14.93</cell><cell>10.85</cell><cell>9.50</cell></row><row><cell>FusionDepth(Ours)</cell><cell></cell><cell cols="3">25.21 +68.9% +75.0% +74.0% 18.99 16.53</cell></row></table><note>Depth prediction with random- sampled LiDAR points: Comparison of per- formances on the KITTI dataset [48] with meth- ods that also rely on sparse LiDAR points. The input point clouds are randomly sampled from 64-beam LiDAR points. Our FusionDepth out- performs all other methods with a large gap even without refinement.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>3D detection performance evaluation for the Car category on the KITTI dataset testing set. AP 3d @0.7.</figDesc><table /><note>*The depth module of Pseudo LiDAR++</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>usually have Abs Rel over 0.1. With the advantage of using the sparse LiDAR, the performance of our initial depth maps</figDesc><table><row><cell>Method</cell><cell>FPS</cell><cell>mAP Mod.</cell><cell>Easy</cell><cell>Car Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Pedestrian Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Cyclist Mod.</cell><cell>Hard</cell><cell>Depth Abs.Rel</cell></row><row><cell>Monodepth2 [15]</cell><cell></cell><cell>6.45</cell><cell>17.35</cell><cell>12.86</cell><cell>11.45</cell><cell>5.24</cell><cell>4.94</cell><cell>4.54</cell><cell>2.29</cell><cell>1.55</cell><cell>1.55</cell><cell>10.27</cell></row><row><cell>FusionDepth(Initial Depth) FusionDepth(Refined Depth)</cell><cell>&gt;100</cell><cell>8.18 9.02</cell><cell>22.20 22.29</cell><cell>15.37 15.42</cell><cell>14.55 14.76</cell><cell>6.97 6.68</cell><cell>6.49 6.50</cell><cell>5.71 6.00</cell><cell>4.20 7.27</cell><cell>2.69 5.16</cell><cell>2.59 5.14</cell><cell>7.51 7.25</cell></row><row><cell>Delta (%)</cell><cell></cell><cell>+40%</cell><cell cols="3">+28% +20% +29%</cell><cell>+33%</cell><cell>+32%</cell><cell>+32%</cell><cell cols="3">+217% +233% +206%</cell><cell>29%</cell></row><row><cell>MonoPL++ (GDC) [1]</cell><cell></cell><cell>10.56</cell><cell>33.75</cell><cell>22.38</cell><cell>20.45</cell><cell>6.41</cell><cell>5.30</cell><cell>5.14</cell><cell>5.89</cell><cell>4.00</cell><cell>4.07</cell><cell>8.41</cell></row><row><cell>FusionDepth(Initial Depth+GDC) FusionDepth(Refined Depth+GDC)</cell><cell>1-2</cell><cell>16.94 20.93</cell><cell>41.53 44.55</cell><cell>29.49 33.59</cell><cell>24.29 28.87</cell><cell>14.81 18.28</cell><cell>11.97 14.46</cell><cell>10.53 12.32</cell><cell>15.24 23.28</cell><cell>9.35 14.75</cell><cell>8.91 13.38</cell><cell>6.39 6.17</cell></row><row><cell>Delta (%)</cell><cell></cell><cell>+98%</cell><cell cols="3">+32% +50% +41%</cell><cell cols="3">+185% +172% +140%</cell><cell cols="3">+295% +268% +229%</cell><cell>26%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Monocular 3D object detection result with PatchNet<ref type="bibr" target="#b38">[39]</ref> on KITTI dataset, AP @0.7 for cars, AP @0.5 for pedestrians and cyclists. Our FusionDepth can greatly improve the performance both with or without GDC.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>RefinementThe Lower the Better Pseudo Dense Representation Input Level Output Level Feature Level RefineNet GDC Abs Rel Sq Rel RMSE RMSElog</figDesc><table><row><cell>Evaluating Pseudo Dense Representation (PDR)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.115</cell><cell>0.882</cell><cell>4.701</cell><cell>0.190</cell></row><row><cell>0.108</cell><cell>0.814</cell><cell>4.588</cell><cell>0.184</cell></row><row><cell>0.101</cell><cell>0.726</cell><cell>4.364</cell><cell>0.178</cell></row><row><cell>Evaluating Camera-LiDAR Fusion in Initial Prediction</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.101</cell><cell>0.726</cell><cell>4.364</cell><cell>0.178</cell></row><row><cell>0.115</cell><cell>0.907</cell><cell>4.847</cell><cell>0.192</cell></row><row><cell>0.102</cell><cell>0.734</cell><cell>4.369</cell><cell>0.177</cell></row><row><cell>0.078</cell><cell>0.515</cell><cell>3.678</cell><cell>0.154</cell></row><row><cell>Evaluating Refinement</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.078</cell><cell>0.515</cell><cell>3.678</cell><cell>0.154</cell></row><row><cell>0.074</cell><cell>0.433</cell><cell>3.610</cell><cell>0.150</cell></row><row><cell>0.067</cell><cell>0.425</cell><cell>3.420</cell><cell>0.144</cell></row><row><cell>0.063</cell><cell>0.346</cell><cell>3.291</cell><cell>0.139</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation Study: Results on the KITTI depth prediction dataset Eigen<ref type="bibr" target="#b54">[55]</ref> split. We evaluate the effectiveness of PDR, Camera-LiDAR fusion, and RefineNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Depth prediction on KITTI original dataset: Methods are ranked by absolute relative error. The best results are in bold. All methods are using a resolution of 640x192 pixels. Due to the exceptional time-consume (around 1-2 FPS), we rank methods with and without iterative refinement separately. M , S, and L respectively indicates Monocular, Stereo, and Sparse LiDAR data, with Sup and ? respectively indicating supervised training and iterative correction in testing phase. * Only use LiDAR data in training phase, but tested on the KITTI improved dataset, which usually has a much lower error value. ** For a fair comparison, we replace the supervised stereo depth module with monodepth2<ref type="bibr" target="#b14">[15]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Iterative Abs Rel Speed (FPS)</cell></row><row><cell>Without Refinement</cell><cell>-</cell><cell>0.078</cell><cell>-</cell></row><row><cell>FusionDepth (Refine Net + GDC)</cell><cell>Yes</cell><cell>0.064</cell><cell>2.00</cell></row><row><cell>GDC [1]</cell><cell>Yes</cell><cell>0.067</cell><cell>2.01</cell></row><row><cell>PnP Depth [64]</cell><cell>Yes</cell><cell>0.077</cell><cell>15.2</cell></row><row><cell>FusionDepth (Refine Net)</cell><cell>No</cell><cell>0.074</cell><cell>139.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Speed comparison between our RefineNet and other conventional iterative refinement methods: The other methods use LiDAR point cloud to iteratively refine the predictions and it usually results with higher accuracy but super low speed (around 1-2 FPS). As its replacement, our newly designed RefineNet is an efficient feed-forward network that achieves real-time performance (139 FPS) on single Nvidia RTX-2080Ti GPU. Our framework is trained with an Adam optimizer<ref type="bibr" target="#b74">[75]</ref> with a learning rate starting at 1e ? 4 and reduced by 90% every 8 epochs. Our model takes images of resolution 1216 ? 352 as input and outputs predictions of the same resolution. All the models are trained with a batch size of 4 on a single NVIDIA Tesla V100 GPU for 15 epochs, and the training takes around 20 hours.</figDesc><table><row><cell>Depth Completion:</cell></row></table><note>Monocular 3D Object Detection: For monocular 3D object detection, the most recent state-of-the- art model PatchNet [39] is employed as detector to evaluate the performance based on our predicted depth. The PatchNet is trained on the KITTI detection dataset with pseudo-LiDAR patches as input, which is lifted from our predicted depth. The model is optimized with an Adam optimizer [75] with</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenReview.net</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04123</idno>
		<title level="m">Deep learning based monocular depth prediction: Datasets, methods and applications</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep hierarchical guidance and regularization learning for end-to-end depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive hard-mining network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Monocular depth estimation with affinity, vertical pooling, and label enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning depth from single images with deep neural network embedding focal length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploiting temporal consistency for real-time video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Feature-metric loss for self-supervised learning of depth and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Packnet-sfm: 3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02693</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-modal auto-encoders as joint estimators for robotics scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Parse geometry from a line: Monocular depth estimation with partial laser observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">Sparsity invariant cnns</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">CSPN++: learning context and resource aware convolutional spatial propagation networks for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Non-local spatial propagation network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10042</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Depth completion via deep basis fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Depth completion from sparse lidar data with depth-normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Selfdeco: Self-supervised monocular depth completion in challenging indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04977</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised depth completion from visual inertial odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsuei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dense depth posterior (DDP) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Balanced depth completion between dense depth inference and sparse range measurements via kiss-gp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05158</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03343</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Distance-normalized unified representation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection with pseudo-lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rethinking pseudo-lidar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">End-to-end pseudo-lidar for image-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">LEGO: learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Robust semi-supervised monocular depth estimation with reprojected distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with decoupled structured polygon estimation and height-guided depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10478" to="10485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection and box fitting trained end-to-end using intersection-over-union loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>J?rgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">M3D-RPN: monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Unsupervised high-resolution depth learning from videos with dual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Superdepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction. Computer Vision Foundation / IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning monocular depth estimation infusing traditional stereo knowledge. Computer Vision Foundation / IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation. Computer Vision Foundation / IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Undeepvo: Monocular visual odometry through unsupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Plug-and-play: Improve depth prediction via sparse data propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06125</idno>
		<title level="m">Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Signet: Semantic instance aided unsupervised 3d geometry perception. Computer Vision Foundation / IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sunarjo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bharadia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Self-supervised monocular depth hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<publisher>BMVA Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Deep fitting degree scoring network for monocular 3d object detection. Computer Vision Foundation / IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">ROI-10D: monocular lifting of 2d detection to 6d pose and metric shape. Computer Vision Foundation / IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">GS3D: an efficient 3d object detection framework for autonomous driving. Computer Vision Foundation / IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Shift r-cnn: Deep monocular 3d object detection with closed-form geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Naiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Paunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
