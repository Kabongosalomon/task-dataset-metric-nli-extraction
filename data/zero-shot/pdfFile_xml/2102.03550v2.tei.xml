<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UniFuse: Unidirectional Fusion for 360 ? Panorama Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hualie</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Sheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
						</author>
						<title level="a" type="main">UniFuse: Unidirectional Fusion for 360 ? Panorama Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning depth from spherical panoramas is becoming a popular research topic because a panorama has a full field-of-view of the environment and provides a relatively complete description of a scene. However, applying well-studied CNNs for perspective images to the standard representation of spherical panoramas, i.e., the equirectangular projection, is suboptimal, as it becomes distorted towards the poles. Another representation is the cubemap projection, which is distortionfree but discontinued on edges and limited in the field-ofview. This paper introduces a new framework to fuse features from the two projections, unidirectionally feeding the cubemap features to the equirectangular features only at the decoding stage. Unlike the recent bidirectional fusion approach operating at both the encoding and decoding stages, our fusion scheme is much more efficient. Besides, we also designed a more effective fusion module for our fusion scheme. Experiments verify the effectiveness of our proposed fusion strategy and module, and our model achieves state-of-the-art performance on four popular datasets. Additional experiments show that our model also has the advantages of model complexity and generalization capability. The code is available at https://github.com/ alibaba/UniFuse-Unidirectional-Fusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Depth estimation is a fundamental step in 3D reconstruction, having many applications, such as robot navigation and virtual/augmented reality. A spherical (or 360 ? , omnidirectional) panoramic image has a full field-of-view of the environment, thus has the potential to produce a more accurate, complete, and scale-consistent reconstruction of scenes. This paper presents our work on better predicting depth from a single spherical panoramic image.</p><p>The 360 ? panorama is usually represented as the equirectangular projection (ERP) or cubemap projection (CMP) <ref type="bibr" target="#b0">[1]</ref>. Both of them are different from the perspective image and have their respective advantages and disadvantages. EPR provides a complete view of the scene but contains distortion that becomes severer towards the poles. In contrast, CMP is distortion-free but discontinued on face sides and limited in the field-of-view. Applying deep CNNs to panoramic images for accurate depth estimation is thus challenging.</p><p>Recently, BiFuse <ref type="bibr" target="#b1">[2]</ref> combines the two above projections for depth estimation, which builds bidirectional fusion of the two branches at both the encoding and decoding stages and finally uses a refinement network to fuse the estimated depth maps from both branches. To alleviate the discontinuity of CMP, BiFuse also adopts the spherical padding among cube faces. However, with too many modules added, BiFuse becomes over-complicated, as discussed in detail in Sec. IV-B.3. We argue that feeding the ERP features to the CMP branch is unnecessary, as the ultimate goal is to output an equirectangular depth map. Optimizing the cube map depth may cause the training to lose focus on the equirectangular depth. Furthermore, performing the fusion at the encoding stage may disturb the learning of the encoder, as it is usually initialized with ImageNet <ref type="bibr" target="#b2">[3]</ref> pretrained parameters.</p><p>To address the above limitations, we propose a new fusion framework, which unidirectionally feeds the features extracted from CMP to the ERP branch only at the decoding stage to better support the ERP prediction, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The fusion scheme uses the simple U-Net <ref type="bibr" target="#b3">[4]</ref> and performs the fusion at the skip connections so that the fusion has minimum coupling with the backbones. Besides, we design a fusion module for our fusion framework, aiming at using Cubemap to Enhance the Equirectangular features (noted as CEE). We first adopt a residual modulation of the cubemap features to mitigate its discontinuity. Because the concatenation of modulated cubemap features and equirectangular features doubles the feature map channels, to better model the channel-wise importance, we introduce the Squeeze-and-Excitation (SE) <ref type="bibr" target="#b4">[5]</ref> block to the CEE module. Our CEE module works better for our fusion scheme than the simple concatenation or the Bi-Projection <ref type="bibr" target="#b1">[2]</ref> module does.</p><p>Our contributions are summarized as follows: <ref type="bibr" target="#b0">(1)</ref> we propose a new fusion framework of equirectangular and cubemap features for single spherical panorama depth estimation, (2) we design a better fusion module for our unidirectional fusion framework than existing modules, and (3) we perform experiments to show our approach's effectiveness, and the final model establishes the state-of-the-art performance and has advantages on the complexity and generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Make3D <ref type="bibr" target="#b5">[6]</ref> is a seminal work on a single perspective image depth estimation, which uses the traditional graphical model. With the development of deep learning, convolutional neural networks were applied to this task <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. This task is either treated as a dense regression problem <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> or a classification problem <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> by discretizing the depth. The experiments are usually performed on datasets with ground truth depth obtained with physical sensors. To avoid the direct usage of ground truth depth, some work tried to utilize other data source for training, e.g., stereo images <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, monocular videos <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, where the training objective is to minimize the between-view reconstruction arXiv:2102.03550v2 [cs.CV] 27 May 2021 error. However, the performance of these unsupervised approaches is inferior to the supervised ones.</p><p>The spherical panorama has a full field-of-view of a scene, which can extract more accurate and scale-consistent depth than the perspective image. Zioulis et al. <ref type="bibr" target="#b19">[20]</ref> first performed depth estimation on panoramas and proposed to replace the first two layers of the network with a set of rectangle filters <ref type="bibr" target="#b20">[21]</ref> to handle distortion. They constructed the 3D60 dataset rendered from several datasets. But this dataset is relatively easy due to a problem of rendering, as pointed out in Sec. IV-B.1. Later, they constructed both vertical and horizontal stereo panoramas to perform unsupervised 360 ? depth learning <ref type="bibr" target="#b21">[22]</ref>. Similarly, Wang et al. <ref type="bibr" target="#b23">[23]</ref> composed a purely virtual panorama dataset PonoSUNCG with panorama video frames to perform unsupervised depth learning like SfMLearner <ref type="bibr" target="#b16">[17]</ref>. Pano Popups <ref type="bibr" target="#b24">[24]</ref> jointly learns depth with surface normals and boundaries to improve depth estimation. More recently, ODE-CNN <ref type="bibr" target="#b25">[25]</ref> reduces the 360 ? depth estimation problem as an extension problem from the front face depth. Both their experiments are still performed on virtual datasets only.</p><p>However, virtual datasets tend to be too easy, and the models trained on them are probably hard to transfer to real applications. Tateno et al. <ref type="bibr" target="#b26">[26]</ref> first experimented on the real dataset Stanford2D3D <ref type="bibr" target="#b27">[27]</ref>. They proposed to train on the common sub perspective views and then transfer to the full panorama images by applying a distortion-aware deformation on the convolutional kernels of the trained model. Though such a method has the potential to utilize more available RGBD datasets to learn a panorama depth estimation model, it fails to take advantage of the large receptive field-of-view of the panorama. More recent work <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b1">[2]</ref> tends to build a complex model on this task. Jin et al. <ref type="bibr" target="#b28">[28]</ref> leveraged the layout elements as both the prior and regularizer for depth estimation, resulting in a model with three encoders and seven decoders. In comparison, our UniFuse contains only two encoders and one decoder. Wang et al. <ref type="bibr" target="#b1">[2]</ref> first proposed to utilize both EPR and CMP for 360 ? depth estimation. Their model BiFuse is composed of two networks, i.e., the equirectangular and cubemap branches, between which are bidirectional fusion modules. There is also a refinement network to refine the predicted depth maps of the two branches. However, its complex structures may hinder the concentration on the learning of equirectangular features, which is critical to the final equirectangular depth map. In contrast, both our unidirectional fusion framework and the CEE fusion module are designed to use the cubemap to enhance the equirectangular feature learning.</p><p>There are some existing elaborate convolutions for handling distortion of EPR and special padding techniques for discontinuity of both EPR and CMP. The convolutions include the Spherical Convolution (SC) <ref type="bibr" target="#b20">[21]</ref> which is a set of rectangle filters and is used at the front layers of the network, and the Distortion-aware Convolution (DaC) <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref> which samples the features for convolution from a regular grid on the tangent plane instead of EPR. The padding methods include the Circular Padding (CirP) <ref type="bibr" target="#b31">[31]</ref> for EPR and Cube Padding (CuP) <ref type="bibr" target="#b32">[32]</ref> and Spherical Padding (SP) <ref type="bibr" target="#b1">[2]</ref> for CMP. Readers could refer to their original papers for more technical details. We do not adopt these special convolutions and paddings in our models. Experiments in Sec. IV-B.3 show that using these methods in UniFuse does not improve the performance but usually adds the complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY A. Preliminaries</head><p>In this section, we introduce the two common projections for the spherical image, i.e., the equirectangular projection and cubemap projection, and their mutual conversion.</p><p>Equirectangular Projection is the representation of a spherical surface by uniformly sampling it in longitudinal and latitudinal angles. The sampling grid is rectangular, in which the width is twice of the height. Suppose that the longitude and latitude are ? and ? respectively, and we have </p><p>Cubemap Projection is the projection of a spherical surface to the 6 faces of its inscribed cube. The 6 faces cat 1x1 conv  are specific perspective images, whose size is r ? r and focal length r/2. The 6 faces can be denoted as f i , i ? B, D, F, L, R, U , corresponding to the looking directions, ?z(back), ?y(down), z(front), x(left), ?x(right) and y(up). The front face has the identical coordinate system with the spherical surface, while others have either 90 ? or 180 ? rotations around one axis. Let us denote the rotation matrix from the system of the spherical surface to one of the i-th face as R fi . Then we can project the pixel</p><formula xml:id="formula_1">C2E 2 (a) Concatenation cat C2E 2 (b) Bi-Projection cat C2E 2 (c) CEE 3x3 conv ReLU 3x3 conv ? 2 ? 1x1 conv Sigmoid 1x1 conv BN ? cat ? 1x1 conv SE Global pooling 1? 1 ? 2 FC 1? 1 ? 2 / ReLU FC 1? 1 ? 2 / 1? 1 ? 2 1? 1 ? 2</formula><formula xml:id="formula_2">P c = (p x c , p y c , p z c ) in f i by, P s = s ? R fi P c ,<label>(2)</label></formula><p>where, p x c , p y c ? [0, r], p z c = r/2, and the factor s = r/|p c |. C2E is the reprojection of the contents (raw RGB values or features) in cubemaps to the equirectangular grid. C2E is usually performed as an inverse wrapping, where we have to compute the corresponding point with the angular position (?, ?). Specifically, we first use Equ. (1) to reproject the angular position to the spherical surface. Then we determine the projected face by finding the minimum angular distance between it and the looking directions of cubemaps. Finally, we compute the corresponding position in the cube face by using the inverse process of Equ. (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Unidirectional Fusion Network</head><p>Our unidirectional fusion network of ERP and CMP is illustrated in <ref type="figure" target="#fig_0">Fig 1.</ref> The reason to perform fusion in a unidirectional manner is that the ultimate goal of 360 ? depth estimation is to produce an equirectangular depth map, and to feed distortion-free cubemap features to the fullview equirectangular features as a supporting component is a natural choice. We do not perform fusion in a reverse direction, as CMP is limited in the field-of-view and the spherical padding <ref type="bibr" target="#b1">[2]</ref> for the discontinuity of CMP is timeconsuming. Additionally, the decoder for the CMP branch to predict cube depth maps increases the complexity, and optimizing the cube depth maps would distract the learning of equirectangular depth. Therefore, we do not adopt a decoder for the cubemaps, and the network contains only two encoders and one decoder. To avoid disturbing the learning of the backbone, we choose to perform fusion only at the decoding stage, and it is better to fuse the well-encoded features. To this end, we adopt a U-Net <ref type="bibr" target="#b3">[4]</ref> as a baseline network and perform fusion within skip-connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Fusion Modules</head><p>In this section, we introduce our proposed fusion module for the UniFuse framework, as well as two baseline fusion methods, concatenation, and the Bi-Projection <ref type="bibr" target="#b1">[2]</ref>, as illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. The dimensions of common features of these 3 modules are,</p><formula xml:id="formula_3">? F equi /F equi , F c2e /F c2e , F res /F res , F f used : H ?W ?C ? F cube : 6 ? H/2 ? H/2 ? C ? F cat /F cat : H ? W ? 2C</formula><p>where H, W and C are height, width, and channel of the equirectangular features, and cubemap features have the same channels, but the face size is just H/2.</p><p>Concatenation module first casts the CMP features to EPR by C2E, then concatenates them with F equi and finally uses a 1 ? 1 conv. module to reduce the channel from 2C to C. The number of parameters is 2C 2 .</p><p>Bi-Projection <ref type="bibr" target="#b1">[2]</ref> aims at generating a masked feature map from one branch and add it to another one. In (b) of <ref type="figure" target="#fig_2">Fig. 2</ref>, we omit the E2C path, as it is not necessary in our unidirectional fusion framework. The M ask is H ? W ? 1.</p><p>To generate the mask, the Bi-Projection first uses two 3 ? 3 conv. modules to encode F equi and F c2e as F equi and F c2e , then reduce the concatenated feature map's channel to 1, and finally apply a Sigmoid function to scale the M ask between 0 and 1. Such masked modification in Bi-Projection may be useful in gradually improving the feature learning in two branches in BiFuse, but seems not effective enough for our unidirectional fusion framework at the decoding stage. The number of parameters is 18C 2 + 4C + 1.</p><p>CEE is a more elaborate concatenation that better facilitates the fusion process. It aims at using the distortion-free cubmap to enhance the equirectangular features. Because the cubemap features are probably inconsistent in cubemap boundaries, we first generate a residual feature map F res to be added to F c2e to reduce such an effect. To generate F res , we design a residual block inspired by ResNet <ref type="bibr" target="#b33">[33]</ref> to the concatenation of F equi and F c2e . The residual block contains a 1?1 conv. module to squeeze the channels and a 3?3 conv. module to generate the residual feature map. <ref type="figure" target="#fig_3">Fig. 3</ref> shows the feature map of F c2e and F c2e at the 1/2 resolution stage. Cracks appear between cube faces in F c2e but they disappear in F c2e , which indicates that the residual modulation has filled them. An intuitive explanation is that the continuous F equi helps the residual block localize inconsistent boundaries of F c2e and the supervision from continuous ground truth helps learn to generate values for filling boundaries. The remained part is similar to Concatenation. As we have dual channels of features from both branches, before the final 1?1 conv. module, we add a Squeeze-and-Excitation (SE) <ref type="bibr" target="#b4">[5]</ref> block, which can adaptively recalibrates channel-wise feature responses, and thus masking the fusion better. We set r in the SE block as 16, so the total number of parameters is 13.5C 2 + 4C. Therefore the number of parameters in our CEE is considerably smaller (75%) than that of the Bi-Projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Experimental Settings 1) Datasets: Our experiments are conducted on four datasets, Matterport3D <ref type="bibr" target="#b34">[34]</ref>, Stanford2D3D <ref type="bibr" target="#b27">[27]</ref>, 3D60 <ref type="bibr" target="#b19">[20]</ref>, and PanoSUNCG <ref type="bibr" target="#b23">[23]</ref>. Matterport3D and Stanford2D3D are real-world datasets collected by Matterport's Pro 3D Camera. While Matterport3D provides the raw depth, Stanford2D3D constructs the depth maps from reconstructed 3D models. Thus, the bottom and top depth is missed in Matterport3D, and some depth in Stanford2D3D is inaccurate, as shown in <ref type="figure">Fig. 4</ref>. 3D60 is a 360 ? depth dataset provided by Omnidepth <ref type="bibr" target="#b19">[20]</ref>, and it is rendered from 3D models of two realistic datasets, Matterport3D and Stanford2D3D, and two synthetic datasets, SceneNet <ref type="bibr" target="#b35">[35]</ref> and SunCG <ref type="bibr" target="#b36">[36]</ref>. In contrast, PanoSUNCG is a purely virtual dataset rendered from SunCG <ref type="bibr" target="#b36">[36]</ref>. The statistics of the 4 datasets are listed in Tab. I, and real datasets are smaller than virtual ones.</p><p>2) Implementation Details: We implement the proposed approach using Pytorch <ref type="bibr" target="#b37">[37]</ref>. The ResNet18 <ref type="bibr" target="#b33">[33]</ref> pretrained on ImageNet <ref type="bibr" target="#b2">[3]</ref> is used as backbone for most experiments, except for some using other backbones in Sec. IV-B.3. We use Adam <ref type="bibr" target="#b38">[38]</ref> with default parameters as the optimizer and a constant learning rate of 0.0001. Besides the common data augmentation techniques, random color adjustment, and leftright-flipping, we also use the random yaw rotation, as the <ref type="table" target="#tab_8">Dataset  Matterport3D Stanford2D3D  3D60  PanoSUNCG  #train  7829  1040  35979  21025  #validation  947  ---#test  2014  373  1298  3944   TABLE I: The Statistics of the Datasets.</ref> ERP property is invariant under such transformation. We use the popular BerHu loss <ref type="bibr" target="#b8">[9]</ref> as the regression objective in training. During training, we randomly select 40 and 800 samples from the training set of Stanford2D3D and 3D60 for validation, and we use the last five scenes (1091 samples) from 80 training scenes of PanoSUNCG for validation. We train the real datasets for 100 epochs, and the virtual datasets for 30 epochs, as the virtual datasets are quite large, while BiFuse <ref type="bibr" target="#b1">[2]</ref> trains all datasets for 100 epochs. Following BiFuse, we set the input size for real and virtual datasets to 512 ? 1024 and 256 ? 512. We train most models on an NVIDIA 2080Ti GPU, the batch size of virtual datasets is 8, but the batch size of real datasets is just 6 due to the limited GPU memory. For some models in Sec. IV-B.3, we have to use two GPUs, each with a batch size of 3. These models include UniFuse with CuP <ref type="bibr" target="#b32">[32]</ref>, SP <ref type="bibr" target="#b1">[2]</ref> and CirP <ref type="bibr" target="#b31">[31]</ref>, and the equirectangular baseline with DaC <ref type="bibr" target="#b29">[29]</ref>.</p><p>3) Evaluation Metrics: We use some standard metrics for evaluation, including four error metrics, mean absolute error (MAE), absolute relative error (Abs Rel), root mean square error (RMSE) and the root mean square error in log space (RMSElog), and three accuracy metrics, i.e., the percentages of pixels where the ratio (?) between the estimated depth and ground truth depth is smaller than 1.25, 1.25 2 , and 1.25 3 . Note that, while most papers on depth estimation use log e in RMSElog, the latest BiFuse adopts log 10 . As BiFuse is the state-of-the-art method with which we mainly compare our UniFuse model, we also adopt log 10 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Performance Comparison:</head><p>The quantitative comparison among the start-of-the-art methods of spherical depth estimation, our equirectangular baseline, and UniFuse model on the four datasets are shown in Tab. II. We directly take the results from related papers for comparison. Our UniFuse model has established new state-of-the-art performance on all of the four datasets, especially on the biggest realistic dataset, Matterport3D, by a significant margin. To be specific, UniFuse outperforms BiFuse <ref type="bibr" target="#b1">[2]</ref> by reducing the error Abs Rel from 0.2048 to 0.1063 and improving accuracy metric of ? &lt; 1.25 by 4.45%. In terms of fusion effectiveness, our UniFuse framework reduces the error metrics by over 10% in average from our equirectangular baseline, while BiFuse only reduces by about 4% from its equirectangular baseline. Although UniFuse's improvement on the loosest accuracy metric of ? &lt; 1.25 3 is slightly smaller than BiFuse's improvement, UniFuse performs much better than BiFuse on enhancing the other two tighter accuracy metrics, especially over 3 times on the accuracy of ? &lt; 1.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error   <ref type="bibr" target="#b1">[2]</ref> reduce the errors on Stanford2D3D more largely than Matterport3D, perhaps because the former is much less complex. Similarly, UniFuse outperforms BiFuse to a bigger extent on the other four error metrics. For the accuracy metrics, our UniFuse still has a better improvement on the tightest one than BiFuse.</p><note type="other">metric</note><p>For 3D60, our UniFuse still has a much better improvement on MAE, Abs Rel, RMSElog, and ? &lt; 1.25 and slightly less improvement on other three metrics than BiFuse <ref type="bibr" target="#b1">[2]</ref>. Overall, the performance on 3D60 is much higher than the two realistic datasets, and thus the effectiveness of the fusion is inferior. Our UniFuse model significantly outperforms BiFuse and performs approximately to the model by Cheng et al. <ref type="bibr" target="#b25">[25]</ref>. However, their model takes the depth map of the front face and extends it to the entire 360 ? space. This requires an extra depth camera and careful calibration between the depth camera and panorama camera. The virtual PanoSUNCG is also very easy, and our final model still outperforms BiFuse at most metrics expect the RMSE.</p><p>We also provide qualitative results of our equirectangular baseline and UniFuse model in <ref type="figure">Fig. 4</ref>. Two examples from the test set of each dataset are shown here and the dark region of the ground truth depth maps indicates unavailable depth. It can be observed that the UniFuse model produces accurate depth maps with fewer artifacts than the equirectangular baseline, which further verifies the effectiveness of our proposed approach. The two examples of the 3D60 dataset are rendered from the 3D models of Matterport3D, and it appears that the farther it gets, the darker the scene is. But in the realistic dataset, it is not the case. We believe that the problematic rendering makes 3D60 easy, as the network probably uses the brightness as a cue for predicting depth. However, sometimes such a cue may cause problems. For instance, the regions within a blue rectangle on the two examples contain dark areas, and our equirectangular baseline tends to predict the regions farther.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Ablation Study:</head><p>We compare the effectiveness of Ima-geNet <ref type="bibr" target="#b2">[3]</ref> pretraining (pt) and the three different fusion modules in our proposed fusion framework on the Matterport3D dataset in Tab. III. The pretraining is useful for both the baseline and UniFuse. For the baseline, disabling pretraining makes the Abs Rel error increase by 8.36% and the ? &lt;   a bigger performance degradation. Therefore, pretraining is also useful for ERP. There is an explanation for this effect in <ref type="bibr" target="#b28">[28]</ref>, which adopts a pretrained U-Net for ERP. The reason is that the high-level parameters from perspective images can be more easily fine-tuned to the equirectangular ones.</p><p>The simple concatenation of equirectangular features and cubemap features produces a good performance gain upon the equirectangular baseline. Specifically, the Abs Rel error is reduced by 5.14%, and the ? &lt; 1.25 accuracy increases 1.3%. This indicates that our unidirectional fusion strategy is effective, although simple. Our fusion strategy is also compatible with the Bi-Projection module in BiFuse <ref type="bibr" target="#b1">[2]</ref>. Bi-Projection roughly doubles the performance gain of the naive concatenation. By looking backwards to Tab. II, it is easily to find that the performance gain of our simple unidirectional fusion scheme with Bi-Projection significantly surpasses that of the complex bidirectional scheme of BiFuse. This further verified that our simplified fusion scheme is  3) Complexity Analysis: We compare the complexity among the models of this work and the models in BiFuse <ref type="bibr" target="#b1">[2]</ref> as well as some of their performance on Matterport3D to understand how the performance is boosted by adding complexity. We also examine the efficiency and effectiveness of some existing methods handling discontinuity and distortion for panoramas in our models, which are listed at the end of Sec II. The complexity metrics include the number of neural model parameters, the GPU memory, and computation time when the model infers an image with a size of 512 ? 1024. The experiment is performed on an NVIDIA Titan Xp GPU, and the computation time is averaged over 1000 images. The results are listed in Tab IV. R50 and R18 indicates that the backbone are ResNet-50 and ResNet-18 <ref type="bibr" target="#b33">[33]</ref>, respectively, while MV2 represents MobileNetV2 <ref type="bibr" target="#b39">[39]</ref>.</p><p>We obtained the complexity of BiFuse from its open inference code. The models of BiFuse are much more complicated than ours. Its baseline is much more complex than our baseline. It is an FCRN <ref type="bibr" target="#b8">[9]</ref> with the R50 backbone whose first layer is replaced with SC <ref type="bibr" target="#b20">[21]</ref>. Ours is just a simpler U-Net using a pretrained R18 backbone. However, our baseline still performs slightly better. BiFuse's fusion scheme significantly increases the complexity. It almost quadruples the number of parameters. One fold of parameter comes from the cubemap branch. The Bi-Projection modules on both encoding and decoding stages and the final refinement module to fuse the depth maps from both the equirectangular and cubemap branches increase the neural parameters further. BiFuse also increases the inference time to about 10 times. We believe that only part of it comes from the Bi-Projection modules and the final refinement module. Most of it probably can be attributed to SP <ref type="bibr" target="#b1">[2]</ref> on the cubemap branch, which will be explained later when using SP and CuP <ref type="bibr" target="#b32">[32]</ref> in UniFuse.</p><p>In contrast, our UniFuse on ResNet-18 only doubles the complexity of parameters and time upon our baseline, but the performance boosts, and it is still real-time (&gt; 30f ps).</p><p>To prove that the performance of our UniFuse is not simply obtained by adding complexity, we also experiment with our baseline with ResNet-50, whose complexity is similar to our UniFuse on ResNet-18 but the performance is far behind. To explore the possibility to apply our models on mobile devices, we also experiment on the MobileNetV2 <ref type="bibr" target="#b39">[39]</ref>. The results show that our UniFuse can still boost performance, even to a smaller extent than using ResNet-18. The inference time is still real-time, and the GPU memory and parameters decrease a great deal, which indicates our models have potential to be applied to mobile robots or AR/VR devices.</p><p>The remaining experiments are about using special padding and convolution methods in our models, and the results indicate it is unnecessary to adopt them in UniFuse. We find that both SP and CuP are implemented in BiFuse's code, so we introduce them in UniFuse. The results show the inference time increases over 10 times for CuP and over 20 times for SP. They have similar implementations. For each of 6 cube faces, it has 4 sides on the feature map to be sampled from adjacent cube faces, so there are 24 loops for each padding. The padding should be performed on convolutions with kernel size bigger than 1. Therefore, the inference time greatly increases. SP uses interpolation in each loop, so its time increases further. However, they do not improve the performance of UniFuse. Although BiFuse's paper has an ablation study where they enhance the predictions of the cubemap branch, we hypothesize that they are less useful for fusion, especially for UniFuse, which uses CMP as a supplement. A similar observation can be seen in the experiments of CirP <ref type="bibr" target="#b31">[31]</ref>. CirP almost does not improve the performance of UniFuse but is effective for our baseline.</p><p>There is a small technical difference for DaC among different studies <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>. We implement the version by Coors et al. <ref type="bibr" target="#b29">[29]</ref> in our equirectangular baseline. DaC improves the performance of our baseline but is far inferior to UniFuse. However, as interpolation has to be performed densely in convolution, the resulted space and  time complexity is much higher, which is in accordance with the experiments of CFL <ref type="bibr" target="#b30">[30]</ref>. We replace the first layer of our models with BiFuse's implementation of SC <ref type="bibr" target="#b20">[21]</ref>. The resulted complexity is almost the same, since only the first layer is changed. SC slightly improves the performance of our baseline but reduces the performance of UniFuse. Therefore, SC is not effective for our UniFuse. 4) Generalization Capability: We examine the generalization capability between BiFuse <ref type="bibr" target="#b1">[2]</ref> and our UniFuse in Tab V. We can perform such examination as BiFuse provides a pretrained model on the entire Matterport3D dataset. We also trained our UniFuse on the whole Matterport3D dataset. Next, we evaluate both the BiFuse and our UniFuse on the test set of Matterport3D to see the effectiveness of fitting, as the test set has also been trained on. Finally, we evaluate both models on the test set of Stanford2D3D. As the sensing depth of Matterport3D does not cover the top and bottom of the panorama, to make a fair comparison, we do not count the topmost and lowest 68 pixels following BiFuse's code when evaluating on Stanford2D3D. These two datasets have a certain domain gap, as Matterport2D3D is about various household scenes, while Stanford2D3D is about the office scenes of a university. Thus, this transfer can examine the generalization capability of different models.</p><p>From Tab V, BiFuse's Abs Rel is 3 times of our UniFuse's and UniFuse has 8.0% higher on the accuracy ? &lt; 1.25 than BiFuse. This indicates UniFuse fits Matterport3D much better than BiFuse. But the better fitting is not overfitting, as UniFuse also transfers to Stanford2D3D much better. The visualization results on <ref type="figure">Fig 5 also verify</ref> this fact, and one extra merit of UniFuse is that even there is no ground truth depth on the top and bottom, it can produce plausible depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we have shown our UniFuse model for single spherical panorama depth estimation. Our UniFuse model is a simple yet effective framework that utilizes both equirectangular projection and cubemap projection for 360 ? depth estimation. We have also designed the new CEE fusion module for our framework to enhance the equirectangular features better. Experiments have verified that both our framework and module are effective. The final UniFuse model makes significant progress over the state-ofthe-art methods on four popular 360 ? panoramic datasets, especially on the biggest realistic dataset, Matterport3D. Furthermore, we have shown that our model has much lower model complexity and higher generalization capability than previous works, indicating the potential to apply it in realworld applications. We are exploring the possibility to apply our model to practical fields, such as mobile robots.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Our Proposed Unidirectional Fusion Framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(?, ?) ? [0, 2?] ? [0, ?]. The angular position (?, ?) can be converted into the coordinate P s = (p x s , p y s , p z s ) in the standard spherical surface with radius r by, p x s = r sin(?) cos(?), p y s = r sin(?), p z s = r cos(?) cos(?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>The Fusion Modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>The Visualization of F c2e and F c2e .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>1.25 accuracy drop over 2%. Disabling pretraining in both equirectangular and cubemap branches of UniFuse results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Quantitative Comparison on Four Datasets. ?The RMSElog e of our UniFuse on 3D60 is 0.0725. For the Stanford2D3D dataset, UniFuse outperforms Bi-Fuse and another recent method by Jin et al. [28]. UniFuse performs the best on most metrics except being sightly inferior to the model by Jin et al. [28] on ? &lt; 1.25 2 . Note that, Jin et al. only experimented on a small portion of the Stanford2D3D dataset that satisfies the Manhattan structure (404 samples of the training set and 113 of the test set), as they proposed joint learning of 360 ? depth and Manhattan layout. In contrast, UniFuse is not limited to such a specific structure, and if the model by Jin et al. were evaluated on the entire test set of Stanford2D3D, its performance might degrade largely. In terms of fusion effectiveness, both UniFuse and BiFuse</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Qualitative Comparison between Our Equirectangular Baseline and UniFuse Model. Best viewed in color.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Ground Truth</cell><cell>Our Equi.</cell><cell>Our UniFuse</cell></row><row><cell>Fig. 4: Method</cell><cell cols="2">MAE ? Abs Rel ?</cell><cell cols="2">RMSE ? ? &lt; 1.25 ?</cell></row><row><cell>Equi. w/o pt</cell><cell>0.3548</cell><cell>0.1413</cell><cell>0.5946</cell><cell>81.53</cell></row><row><cell>Equi.</cell><cell>0.3267</cell><cell>0.1304</cell><cell>0.5460</cell><cell>83.70</cell></row><row><cell>Concat.</cell><cell>0.3162</cell><cell>0.1237</cell><cell>0.5340</cell><cell>85.00</cell></row><row><cell>Bi-Proj.</cell><cell>0.3096</cell><cell>0.1188</cell><cell>0.5283</cell><cell>85.94</cell></row><row><cell>CEE w/o SE</cell><cell>0.3046</cell><cell>0.1161</cell><cell>0.5217</cell><cell>86.53</cell></row><row><cell>UniFuse w/o pt</cell><cell>0.3164</cell><cell>0.1195</cell><cell>0.5440</cell><cell>85.53</cell></row><row><cell>UniFuse</cell><cell>0.2814</cell><cell>0.1063</cell><cell>0.4941</cell><cell>88.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Ablation Study.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Model Complexity Comparison.</figDesc><table><row><cell>even more effective. Furthermore, our proposed CEE fusion</cell></row><row><cell>module produces a large performance gap over Bi-Projection.</cell></row><row><cell>To be specific, our CEE module has an Abs Rel 10.52%</cell></row><row><cell>less than Bi-Projection, and the accuracy ? &lt; 1.25 is 3.03%</cell></row><row><cell>higher. When turning off the SE block of our CEE module,</cell></row><row><cell>the performance is still markedly better than Bi-Projection.</cell></row><row><cell>Thus, our proposed CEE module is much more effective</cell></row><row><cell>in fusing the equirectangular features and cubemap features</cell></row><row><cell>than Bi-Projection under our unidirectional fusion scheme.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Generalization Comparison.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Shenzhen Institute of Artificial Intelligence and Robotics for Society, The Chinese University of Hong Kong, Shenzhen. 2 Alibaba Cloud A.I. Lab. This work was mainly done when Hualie Jiang interned at Alibaba Cloud A.I. Lab.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Standardization status of 360 degree video coding and delivery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skupin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Hannuksela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Visual Communications and Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bifuse: Monocular 360 depth estimation via bi-projection fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High quality monocular depth estimation via a multi-scale network and a detail-preserving objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with hierarchical fusion of dilated cnns and soft-weighted-sum inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical binary classification for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1975" to="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth and ego-motion using multiple masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dipe: Deeper into photometric errors for unsupervised learning of depth and ego-motion from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Omnidepth: Dense depth estimation for indoors spherical panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zioulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karakottas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zarpalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spherical convolution for fast features from 360 imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spherical view synthesis for self-supervised 360 ? depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zioulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karakottas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zarpalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">Qualitative Comparison between BiFuse and Our UniFuse Model. Best viewed in color</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-supervised learning of depth and camera motion from 360 ? videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="53" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pano popups: Indoor 3d reconstruction with a plane-aware network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ode-cnn: Omnidirectional depth extension networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="589" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distortion-aware convolutional filters for dense prediction in panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geometric structure based and regularized depth estimation from 360 indoor imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spherenet: Learning spherical representations for detection and classification in omnidirectional images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Condurache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="518" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Corners for layout: End-to-end layout recovery from 360 images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Labrador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Facil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perez-Yus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Demonceaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1255" to="1262" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Omnidirectional cnn for visual place recognition and navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2341" to="2348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cube padding for weakly-supervised saliency prediction in 360 videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matterport3D: Learning from RGB-D data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scenenet: An annotated model generator for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>P?tr?ucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
