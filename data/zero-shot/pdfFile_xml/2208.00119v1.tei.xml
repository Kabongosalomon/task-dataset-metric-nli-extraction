<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAS: Densely-Anchored Sampling for Deep Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">PengCheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangxin</forename><surname>Hunag</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangwei</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Yang</surname></persName>
							<email>msyangran@mail.scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
							<email>mingkuitan@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Information Technology R&amp;D Innovation Center of Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
							<email>wangyw@pcl.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">PengCheng Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DAS: Densely-Anchored Sampling for Deep Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep Metric Learning ? Missing Embedding ? Embedding Space Exploitation ? Densely-Anchored Sampling</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Metric Learning (DML) serves to learn an embedding function to project semantically similar data into nearby embedding space and plays a vital role in many applications, such as image retrieval and face recognition. However, the performance of DML methods often highly depends on sampling methods to choose effective data from the embedding space in the training. In practice, the embeddings in the embedding space are obtained by some deep models, where the embedding space is often with barren area due to the absence of training points, resulting in so called "missing embedding" issue. This issue may impair the sample quality, which leads to degenerated DML performance. In this work, we investigate how to alleviate the "missing embedding" issue to improve the sampling quality and achieve effective DML. To this end, we propose a Densely-Anchored Sampling (DAS) scheme that considers the embedding with corresponding data point as "anchor" and exploits the anchor's nearby embedding space to densely produce embeddings without data points. Specifically, we propose to exploit the embedding space around single anchor with Discriminative Feature Scaling (DFS) and multiple anchors with Memorized Transformation Shifting (MTS). In this way, by combing the embeddings with and without data points, we are able to provide more embeddings to facilitate the sampling process thus boosting the performance of DML. Our method is effortlessly integrated into existing DML frameworks and improves them without bells and whistles. Extensive experiments on three benchmark datasets demonstrate the superiority of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Deep Metric Learning (DML) serves to learn an embedding function to project semantically similar data into nearby embedding space and plays a vital role in many applications, such as image retrieval and face recognition. However, the performance of DML methods often highly depends on sampling methods to choose effective data from the embedding space in the training. In practice, the embeddings in the embedding space are obtained by some deep models, where the embedding space is often with barren area due to the absence of training points, resulting in so called "missing embedding" issue. This issue may impair the sample quality, which leads to degenerated DML performance. In this work, we investigate how to alleviate the "missing embedding" issue to improve the sampling quality and achieve effective DML. To this end, we propose a Densely-Anchored Sampling (DAS) scheme that considers the embedding with corresponding data point as "anchor" and exploits the anchor's nearby embedding space to densely produce embeddings without data points. Specifically, we propose to exploit the embedding space around single anchor with Discriminative Feature Scaling (DFS) and multiple anchors with Memorized Transformation Shifting (MTS). In this way, by combing the embeddings with and without data points, we are able to provide more embeddings to facilitate the sampling process thus boosting the performance of DML. Our method is effortlessly integrated into existing DML frameworks and improves them without bells and whistles. Extensive experiments on three benchmark datasets demonstrate the superiority of our method.</p><p>Keywords: Deep Metric Learning ? Missing Embedding ? Embedding Space Exploitation ? Densely-Anchored Sampling <ref type="figure">Fig. 1</ref>. Illustration of the "missing embedding" issue. The data points of similar semantics are mapped into the nearby embedding space that is often with barren area due to the absence of data points, resulting in the "missing embedding" issue image classification <ref type="bibr">[12]</ref>, few-shot learning <ref type="bibr">[32]</ref>, video representation learning <ref type="bibr">[5]</ref> and sound generation <ref type="bibr">[6]</ref> etc. Since it was introduced, it has sparked considerable interest in the community, where academics have offered a variety of methods <ref type="bibr">[18,</ref><ref type="bibr">36,</ref><ref type="bibr">38,</ref><ref type="bibr">42,</ref><ref type="bibr">45,</ref><ref type="bibr" target="#b18">53,</ref><ref type="bibr" target="#b21">56]</ref> and have made substantial progress <ref type="bibr">[37,</ref><ref type="bibr">41]</ref>. The goal of DML is to learn a deep model that is capable of mapping semantically similar data points to similar embeddings in the embedding space. To accomplish this, most existing approaches <ref type="bibr">[7,</ref><ref type="bibr">18,</ref><ref type="bibr">42,</ref><ref type="bibr">48,</ref><ref type="bibr" target="#b18">53,</ref><ref type="bibr" target="#b21">56]</ref> train the deep model with loss functions that bring the embeddings from semantically similar data points close to each other and vice versa. However, some embeddings may have limited contribution or bring no improvement to train the deep model <ref type="bibr" target="#b21">[56]</ref>, or even lead to bad local minima early on in training (such as a collapsed model) <ref type="bibr">[42]</ref>. Thus, sampling informative and stable embeddings is very important to facilitate the training of deep model <ref type="bibr" target="#b21">[56]</ref>. As a result, improving the sample quality is of significance to achieve effective DML. There are two commonly used measures for this goal: designing more effective sampling methods or providing more embeddings.</p><p>Pioneering efforts have made substantial progress toward the design of effective sampling methods upon embedding pairs <ref type="bibr">[41,</ref><ref type="bibr">42,</ref><ref type="bibr" target="#b21">56]</ref> or a full batch of embeddings <ref type="bibr">[36,</ref><ref type="bibr">38]</ref>. These methods typically perform sampling on a batch of embeddings, which often leads to inaccurate sampling results due to the following reasons. First, the batch size is typically constrained by the memory of a single GPU as the sampling process typically cannot cross different GPU devices <ref type="bibr">[41]</ref>. Second, even with GPU that has sufficient memory to support a larger batch size, the embedding space that contains the embeddings embedded by deep models may still with barren area due to the absence of data points, resulting in a "missing embedding" issue (as shown in <ref type="figure">Fig. 1</ref>). Thus, the limited amount of embeddings may impair the sample quality and the performance of DML. Based on the above analyses, we ask: "Can we overcome the inaccurate sampling issue brought by the absence of data points?" Very recently, a few attempts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">28,</ref><ref type="bibr" target="#b19">54,</ref><ref type="bibr" target="#b29">64]</ref> have been committed to answering this question by pseudo embedding generation. Hard example generation approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">64]</ref> generate hard embeddings from easy embeddings with an additional generative adversarial network or auto-encoder. Embedding expansion <ref type="bibr">[28]</ref> performs interpolation between embeddings to achieve augmentation in embedding space. Cross batch memory <ref type="bibr" target="#b19">[54]</ref> maintains embeddings from previous iterations and considers them are still informative in the current batch in terms of sampling. However, these approaches either leverage additional sub-network <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">64]</ref>, which introduce extra training cost, or need further modification to the sampling and loss computation process <ref type="bibr">[28,</ref><ref type="bibr" target="#b19">54]</ref> in standard DML <ref type="bibr">[18,</ref><ref type="bibr">20,</ref><ref type="bibr">42,</ref><ref type="bibr">45,</ref><ref type="bibr" target="#b18">53]</ref>, which may limit their applicability to other tasks.</p><p>In this paper, we seek to densely produce embeddings without data points to alleviate the "missing embedding" issue. In this way, with the combination of embeddings with and without data points, we are able to provide more embeddings for sampling to improve the sample quality and achieve effective DML. Our motivation stems from a fundamental hypothesis of metric learning: the embeddings that are close to each other in the embedding space have similar semantics. Unfortunately, how to exploit the embedding space to produce effective embeddings without data points remains an unsolved problem. To this end, we propose a Densely-Anchored Sampling (DAS) scheme to consider the embedding with data point as "anchor" and densely exploit the anchor's nearby embedding space to produce embeddings that have no corresponding data points. The proposed DAS is consist of two modules, namely, Discriminative Feature Scaling (DFS) and Memorized Transformation Shifting (MTS), which exploit the embedding space around single and multiple embeddings respectively to produce effective embeddings with no corresponding data points. To be specific, based on observations that effective semantics for one embedding are highly activated features <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b13">14]</ref>, DFS identifies these features and applies random scaling on them. In this sense, we are able to exploit the embedding space around a single embedding by enhancing or weakening its effective semantics to produce embeddings. Based on the fact that semantic differences (i.e., transformations) of intra-class embeddings can be added to other embeddings to generate effective embeddings [31], we assume that they can be added in a way like word embeddings [34]: Queen = Woman + (King ? Man). Thus, our Memorized Transformation Shifting module exploits the embedding space among multiple embeddings by adding (i.e., shift) intra-class embeddings' semantic differences to other embeddings of the same class to produce effective embeddings.</p><p>Our main contributions are summarized as follows. First, we propose a novel and plug-and-play Densely-Anchored Sampling (DAS) scheme that exploits embeddings' nearby embedding space and densely produces embeddings without data points to improve the sampling quality and performance of DML. Second, we propose two modules, namely Discriminative Feature Scaling (DFS) and Memorized Transformation Shifting (MTS) to exploit embedding space around a single and multiple embeddings to produce embeddings. Last, extensive experiments demonstrate the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sampling Methods in DML. Sampling informative and stable embeddings is vital to train the deep model in DML <ref type="bibr">[41,</ref><ref type="bibr">42,</ref><ref type="bibr" target="#b21">56]</ref>. Thus, various sampling approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">41,</ref><ref type="bibr">42,</ref><ref type="bibr" target="#b18">53,</ref><ref type="bibr" target="#b21">56]</ref> have been tailored to effectively sample the em-beddings to train the deep model. To further improve sampling efficiency, some researchers propose to leverage a whole batch of embeddings <ref type="bibr">[20,</ref><ref type="bibr">25,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b18">53,</ref><ref type="bibr" target="#b32">67]</ref>. Even though sophisticated sampling methods improve DML, due to the absence of data points, sampling embeddings that are often with "missing embedding" leads to inaccurate sampling, thereby degenerating the final performance. In this paper, we propose a DAS scheme to produce embeddings with no data points by exploiting embeddings' nearby embedding space to achieve effective DML.</p><p>Loss Functions for DML. Studies on DML losses can be grouped into two categories: pair-based and proxy-based. The pair-based losses <ref type="bibr">[18, 22, 26, 42, 45, 51-53, 56, 58]</ref> are constructed upon the pairwise distance between embeddings. Although pair-based approaches mine the rich information among vast embedding pairs, they typically encounter the sampling effective embedding pairs issues. Instead, proxy-based <ref type="bibr">[1,</ref><ref type="bibr">10,</ref><ref type="bibr">25,</ref><ref type="bibr">33,</ref><ref type="bibr">36,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b25">60]</ref> losses introduce the concept of "proxy" as a class representation and avoid the sampling issue by optimizing the embedding close to its proxy. However, proxy-based methods are very difficult to train when the number of classes is extremely large <ref type="bibr">[38]</ref>, limiting their applicability to real-life scenarios. Thus, in this paper, we focus on developing an effective technique to alleviates the general data sampling issue for pair-based methods that have wider applicability and delivers boosted performance for them.</p><p>Pseudo Embedding Generation. Methods on synthesizing pseudo embedding have been recently shown as an important technique to improve DML <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">28,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b19">54,</ref><ref type="bibr" target="#b27">62,</ref><ref type="bibr" target="#b29">64]</ref>. DAML <ref type="bibr" target="#b12">[13]</ref> uses an additional generative adversarial network to generates only hard negatives to improve the model training. HDML <ref type="bibr" target="#b27">[62]</ref> leverages the inter-class information for embedding generation on the sampled embeddings. DVML [31] apply extra generator and decoder to model the class centers that may have inaccurate distance estimation to the real embeddings and employs them to generate embeddings. Embedding expansion [28] linearly interpolates the embeddings to obtain more embeddings. Cross batch memory <ref type="bibr" target="#b19">[54]</ref> stores embeddings from previous batches and considers them beneficial for the upcoming sampling process. However these approaches <ref type="bibr">[28,</ref><ref type="bibr" target="#b19">54]</ref> suffer from the following limitations: First, the generated embeddings can only be considered as negative during sampling, which may limit the power of them. Second, the sampling or loss computation process need to be modified to dock with them, limiting their applicability. Third, an additional sub-network is introduced in order to generate embeddings, which brings heavy computation cost. Last, information source that may have inaccurate distance measurement to real embeddings such as class centers and inter-class differences are considered to produce embeddings. Different from them, DAS is a light-weight module, which produces embeddings by densely sampling around the "anchor" and serves as a plug-and-play component to facilitate the sampling process in standard DML.</p><p>Data Augmentation. Augmentation in data space such as image <ref type="bibr">[44]</ref> has been widely studied and is considered an important technique to avoid overfitting. Recently, many efforts have been made to design effective augmentation methods <ref type="bibr">[8,</ref><ref type="bibr">11,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b20">55,</ref><ref type="bibr" target="#b22">57]</ref> in feature space, aiming to provide more features for training when the source data (e.g., images) is scarce. These methods often in-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Images</head><p>Discriminative Feature Scaling  <ref type="figure">Fig. 2</ref>. Illustration of our Densely-Anchored Sampling (DAS) scheme, which leverages two modules to exploit anchors' (i.e., embeddings with data points) nearby embedding space to densely produce embeddings without data points: DFS performs random scaling on the discriminative features to produce embeddings around a single embedding; MTS exploits the embedding space among multiple embeddings by adding the intraclass semantic differences to embeddings of the same class to produce embeddings. With DAS, we alleviate the "missing embedding" issue by providing more embeddings for sampling, thereby, achieving effective DML troduce complicated training processes <ref type="bibr">[8,</ref><ref type="bibr">49]</ref>. Wang et al. <ref type="bibr" target="#b20">[55]</ref> propose ISDA that estimates semantic differences with covariance matrices and develops an improved version of cross-entropy loss. The computation and memory consumption of covariance matrices are much heavier than DAS. Moreover, ISDA can not be trivially extended to pair-based DML loss. Yin et al. <ref type="bibr" target="#b22">[57]</ref> introduce FTL that requires extra networks (e.g., decoder and feature transfer module) and a carefully designed bi-stage training strategy to achieve feature translation, which is less efficient and general than DAS. Unlike these methods, we view the feature space augmentation as an approach to fill the "missing embedding" in the embedding space and propose a simpler solution under the context of DML.</p><formula xml:id="formula_0">? = ? + Anchor</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Densely-Anchored Sampling</head><p>In this paper, we seek to improve DML by alleviating the "missing embedding" issue incurred in sampling. The overall process of DAS scheme is shown in <ref type="figure">Fig. 2</ref>.</p><formula xml:id="formula_1">Notation. Let v = f ? (I) ? R d be an embedding with data point I, where f ? (?)</formula><p>is a deep model (e.g., CNNs) with learnable parameters ?. Let y v be the label of the embedding v. Let v ? denote embedding without data point. Following previous methods [42], we normalize both v and v ? to the d-dimensional hypersphere (i.e., ?v? 2 , ?v ? ? 2 = 1). We omit the normalization process for brevity. Let C denote the number of training classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition and Motivation</head><p>DML seeks to learn a deep model that keeps similar data points close, and vice versa. Formally, we define the distance between two embeddings as follows <ref type="bibr" target="#b21">[56]</ref>:</p><formula xml:id="formula_2">D ij = ?f ? (I i ) ? f ? (I j )?,<label>(1)</label></formula><p>where ? ? ? denotes the ? 2 norm. For any positive pair of embeddings (y i = y j ), the distance should be small; Whilst for negative pair (y i ? = y j ), it should be large. In practice, limited by computing resources, it is infeasible to optimize every element in D ij . Therefore, it is necessary to sample effective embedding pair for the objective construction (take the contrastive loss as an example):</p><formula xml:id="formula_3">L = (i,j)?Q I{y i = y j } D ij + I{y i ? = y j } [? ? D ij ] + ,<label>(2)</label></formula><p>where I{?} is the indication function, ? is a margin, Q = S(D) indicates indexes of the sampled embedding pairs and S denotes some sampling function. Without causing ambiguity, we denote the points on the embedding space as "anchor points", based on which we will conduct sampling to train the deep model. Note that each data point shall have an anchor point on the embedding space. However, due to the absence of training data, the embedding space may have a lot of "barren area". Thus, it is very difficult to provide sufficient anchor points for sampling and learn a deep model with good performance. To this end, we propose to densely produce anchor points with no data points to facilitate the sampling, thereby improving the training in DML.</p><p>Specifically, we first propose to exploit the embedding space around a single embedding by enhancing or weakening its semantics. We term this process as semantic scaling. Second, we propose to add (i.e., shift) intra-class differences (i.e., transformations) to embeddings to exploit the embedding space among multiple embeddings. We term this process as semantic shifting. Based on the above analyses, the formulation of DAS scheme is formulated as</p><formula xml:id="formula_4">v ? = DAS(v; s, b) = s ? v scaling + b shifting ,<label>(3)</label></formula><p>where v and v ? are embeddings with and without data points, respectively. ? denotes the Hadamard product. s, b ? R d are semantic scaling and semantic shifting factors, respectively. Moreover, given a set of semantic scaling and shifting factor pairs {(s t , b t )}, we are able to produce a set of embeddings by</p><formula xml:id="formula_5">v ? t = DAS(v; s t , b t ).<label>(4)</label></formula><p>Therefore, the semantic scaling and shifting factors is essential to the quality of the produced embeddings and we propose DFS and MTS to acquire them:</p><formula xml:id="formula_6">s = DFS({v | y v = c}),<label>(5)</label></formula><formula xml:id="formula_7">b = MTS({v | y v = c}),<label>(6)</label></formula><p>where DFS and MTS take embeddings from the same class as input and produce the semantic scaling and shifting factors, respectively.  <ref type="figure">Fig. 3</ref>. Illustration of the proposed Discriminative Feature Scaling (DFS) module, which identifies the discriminative features (e.g., channels) and applies different random scaling to them to produce embeddings around a single embedding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MTS Embeddings</head><p>Semantic Shifted Embeddings <ref type="figure">Fig. 4</ref>. Illustration of the proposed Memorized Transformation Shifting (MTS) module. MTS adds the intra-class transformations to embeddings of the same class to produce embeddings among multiple embeddings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discriminative Feature Scaling</head><p>We seek to obtain effective semantic scaling factors to produce anchor points around a single anchor. Thus, as shown in <ref type="figure">Fig. 3</ref>, our Discriminative Feature Scaling (DFS) module carries out the semantic scaling mechanism by finding the discriminative features in an embedding and applying random scaling on them. The feasibility of DFS comes from two aspects. First, visual attributes can be predicted reliably using a sparse number of neurons from CNNs <ref type="bibr" target="#b13">[14]</ref>. Second, in CNNs, neurons that match a diverse set of object concepts are highly activated <ref type="bibr">[2,</ref><ref type="bibr">3]</ref>. In practice, the embedding typically occupies a high dimensional embedding space, which contains semantics for all training classes and semantics for each class are diverse. Thus, the semantics for one class are more likely to be noise for another. In this sense, it is very important to find out the effective e.g., discriminative features for each class in order to perform semantic scaling. To this end, we propose to identify the effective semantics by counting the number of occurrences of the highly-activated neurons for each class.</p><p>To be specific, we first initialize a Frequency Recorder Matrix (FRM) P ? R C?d as a zero matrix. Then, given a set of embeddings {v | y v = c} from class c, we update P as follows:</p><formula xml:id="formula_8">P[c, k] = P[c, k] + 1, if v[k] ? Top(v, K), P[c, k], otherwise,<label>(7)</label></formula><p>where Top(v, K) is an operator to select the top-K elements from the vector v. During training, we constantly update the FRM by recording the position of highly-activated neurons from the embeddings of the same class. In this way, FRM 1 serves as a stable, accurate and effective semantics identifier for training classes. Given the FRM, we compute the class-wise binary channel mask M ? R C?d by</p><formula xml:id="formula_9">M[c, k] = 1, if P[c, k] ? Top(P[c], K), 0, otherwise.<label>(8)</label></formula><p>Given an embedding v, we attain the semantic scaling factor by</p><formula xml:id="formula_10">s = ? ? M[y v ] + 1 d ? (1 ? M[y v ]),<label>(9)</label></formula><p>where ? ? R d and ? ? Uniform[1 ? r s , 1 + r s ] d and r s ? (0, 1) is a hyperparameter to be set. Note that we only randomly scale the discriminative features while leaving the indiscriminative ones intact. To produce more than one scaling factor, we repeatedly sample ? from the uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Memorized Transformation Shifting</head><p>To produce anchors without data points among multiple anchors, we propose a module to provide effective semantic shifting factors. As shown in <ref type="figure">Fig. 4</ref>, our Memorized Transformation Shifting (MTS) module exploit intra-class embeddings' nearby embedding space by leveraging the differences between embeddings and adding them to other embeddings of the same class. On the basis of that semantic differences of embeddings can be added to other embedding to generate effective embeddings [31], the motivation of our MTS comes from the semantic relations of word embedding [34]: Woman + (King ? Man) = Queen, where a "woman" pluses "royal" semantics (e.g., transformation) becomes a "Queen". The transformations from both inter-class and intra-class embeddings are candidates for our design choices. However, the majority of the inter-class transformations typically are not transferable due to large inter-class differences. Thus, we only consider intra-class embeddings to attain the transformations. As suggested by the latest research [41], sampling only two images for each class in a batch consistently achieves good performance, which leads to very limited transformations we can obtain in one batch i.e., two transformations. To address this issue, we construct a bank to memorize the transformations from previous iterations to ensure the diversity of the transformations. Specifically, we construct a transformation bank B ? R C?Z?d , where Z is the bank capacity for each class. Then, during training, once we obtain a set of embeddings V c = {v | y v = c} from the class c, we calculate the transformations between them by</p><formula xml:id="formula_11">t z = v i ? v j , v i , v j ? V c , i ? = j.<label>(10)</label></formula><p>Then, the transformations are en-queued into B according to the FIFO principle to ensure that the transformations in the bank are in a relatively fresh state:</p><formula xml:id="formula_12">B[y v , z] = t z , z ? {1, 2, . . . , Z}.<label>(11)</label></formula><p>Note that z is reset to 1 when it reaches Z. Finally, with the assistance of the bank B, given an embedding v, we retrieve the semantic shifting factor as follow</p><formula xml:id="formula_13">b = r b t, t ? {B[yv, z] | z=1, 2, . . . , Z}.<label>(12)</label></formula><p>r b is a hyper-parameter. Multiple shifting factors are formed by repeat sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training method of DAS-based DML</head><formula xml:id="formula_14">Require: Training image-label pairs S = {(Ii, yi)} N i=1</formula><p>; the embedding function f ? ; number of embeddings to produce T ; number of training classes C; transformation bank capacity Z; learning rate ?. Ensure: Optimized embedding function f * ? . 1: Initialize ? from ImageNet pretrained model. 2: Initialize the frequency recorder matrix P ? R C?d = 0. 3: Initialize the transformation bank B ? R C?Z?d = 0. 4: while not converge do 5:</p><p>Obtain a batch image-label pairs {(Ii, yi)} B i=1 from S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Compute embeddings vi ? f ? (Ii), i = 1, 2, ? ? ? , B. 7:</p><p>// perform semantic scaling by Discriminative Feature Scaling 8:</p><p>Update the frequency recorder matrix P by Eqn. <ref type="bibr">(7)</ref>. 9:</p><p>Acquire semantic scaling factors {sj} B?T j=1 by Eqn. (9). 10:</p><p>// perform semantic shifting by Memorized Transformation Shifting 11:</p><p>Obtain intra-class transformations {t} by Eqn. (10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Update the transformation bank B by Eqn. (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Attain semantic shifting factors {bj} B?T j=1 by Eqn. (12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>// perform densely-anchored sampling 15:</p><p>Produce embeddings {v ? j } B?T j=1 by Eqn. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Sample positive and negative embedding sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>Compute the training loss LDAS-DML by Eqn. <ref type="bibr" target="#b12">(13)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18:</head><p>Update the parameters ? by ? ? ? ? ?? ? LDAS-DML. 19: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DML with Densely-Anchored Sampling</head><p>The overall algorithm of integrating DAS into DML is detailed in Algorithm 1. Given anchor-label pairs {(v, y v )}, we produce embedding-label pairs {(v ? , y ? v )} with no data points by DAS scheme, where y ? v = y v since the class semantic is preserved. Then, embedding-label pairs with or without data points are fed into the sampling module to obtain the positive and negative embedding sets (e.g., pairs, triplets, etc. specified by the sampling and loss functions):</p><formula xml:id="formula_15">{(P, N )} = Sample({(v, y v )} ? {(v ? , y ? v )})</formula><p>. Last, given a DML loss function L DML 2 , DAS-based DML objective function is formulated as:</p><formula xml:id="formula_16">L DAS-DML = L DML ({(P, N )}).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We use three popular benchmarks: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with State-of-the-arts</head><p>In this section, we compare our method with state-of-the-art competitors to investigate the effectiveness of DAS. The results are shown in <ref type="table" target="#tab_6">Table 4</ref>. For fair comparisons, the results of the closely related baseline EE are from the reimplementation by <ref type="bibr">[4]</ref> using the stronger IBN 512 backbone (G 512 in the original paper). Our approach is based on MS loss and achieves superior performance on all datasets and evaluation metrics. First, when combining the R 512 backbone and DAS, we are able to boost the R@1 metrics by 3.49% on CUB and 1.74% on CARS, which shows that DAS is able to deliver more accurate image retrieval results even with higher embedding dimension (i.e., 512). Second, for our closely relative opponent, EE, its improvements on MS are marginal, showing that simply performing interpolation to generate embeddings is inferior to DAS. Last, even for a strong baseline, ProxyAnchor that leverages the advanced training techniques, and sophisticated loss, we still outperform it considerably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness of DAS on Pair-based Loss</head><p>Quantitative results. To investigate the efficacy of DAS, we conduct experiments with R 128 backbone on the widely used pair-based losses. We reimplement all baselines under the same settings for a fair comparison. The results are presented in <ref type="table" target="#tab_2">Table 2</ref>. For considered pair-based losses, DAS is able to improve their performance on both image retrieval and clustering metrics. Notably, for approaches such as GenLifted and N-Pair that leverage the whole batch of embeddings for loss computation, DAS still improves their performance, showing its effectiveness. Last, even for a very strong baseline, MS, that considers different kinds of relationships among embedding pairs and designs sophisticated weighting mechanism, DAS is able to greatly improve it without bells and whistles. Convergence analyses. We provide the results of training loss and test set R@1 in <ref type="figure" target="#fig_1">Fig. 5</ref> to analyze the training behaviors of DAS. <ref type="bibr">5</ref> The loss curve with DAS decreases smoother than that without DAS, showing that DAS provides more embeddings to facilitate sampling, thereby, stabilizing the training. Moreover, with DAS, the training loss is higher than the baseline, one possible reason is that DAS is able to act as a regularizer to avoid overfitting, which is consistent with the result that DAS achieves a higher test set R@1. Similar phenomenon is also observed in other embeddings generation methods [28].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of DAS on Sampling Method</head><p>In this section, we investigate the effectiveness of DAS by evaluating it with different sampling approaches. We choose two popular loss functions: triplet and  contrastive losses that are sensitive to the sampling methods. Therefore, various sampling approaches are tailored for them. The experiment results are presented in <ref type="table" target="#tab_3">Table 3</ref>. For two loss functions, despite the choice of sampling approaches, DAS is able to improve them considerably on both image retrieval and clustering metrics. Notably, when applying DAS, triplet loss with random sampling outperform the one with the semi-hard sampling. This indicates that producing more embeddings for sampling is as important as the sampling approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results</head><p>To better understand our method, we compare contrastive loss (with distance weighted sampling) w/ or w/o DAS and visualize image retrieval results on both CARS ( <ref type="figure">Fig. 6</ref>) and SOP <ref type="figure">(Fig. 6)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>Effect of DFS and MTS. In this section, we perform ablation studies to evaluate the performance gain by each module in DAS. The loss function and sampling method are triplet loss and distance weighted sampling, respectively. The results are in <ref type="table" target="#tab_5">Table 5</ref>. First, DFS, alone, boosts R@1 by +2.42%, verifying that producing embeddings around a single embedding is able to force the model to focus on real semantics and achieve better image retrieval results. Second, with MTS only, the clustering metrics are greatly improved, indicating that producing embeddings among multiple embeddings are beneficial to the image clustering task. Last, with DFS and DAS, all metrics are further improved, suggesting that DFS and MTS reinforce and complement each other. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Further Discussions</head><p>More discussions on DFS. One may question whether the proposed DFS requires the linear assumption on high dimensional feature space. In fact, we do not make this assumption and our method is built on a very basic hypothesis of metric learning: the embeddings that are close to each other in the embedding space have similar semantics. More critically, DFS does not rely on the linearity assumption. Instead, DFS is based on the observations that effective semantics for one embedding are highly activated features <ref type="bibr">[2,</ref><ref type="bibr">3]</ref>.</p><p>More discussions on MTS. Li et al.</p><p>[30] also apply a memory module is leveraged to store abundant features and conduct neighborhood search upon them to enhance the discriminative power of a general CNN feature on the image search and few-shot learning tasks. Unlike them, DAS constructs a memory bank with the intra-class embedding transformations, which allows us to access intraclass transformations from current and previous iterations, and thus improves the diversity of produced embeddings for DML.</p><p>Comparisons with more related works. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a Densely-Anchored Sampling (DAS) scheme to alleviate the "missing embedding" issue incurred during DML sampling. To this end, we propose to produce embeddings with no data points by exploiting the embeddings' nearby embedding space. Specifically, we propose a DFS module that identifies an embedding's discriminative features and performs random scaling on them to exploit the embedding space around it. Moreover, we propose a MTS module to exploit embedding space among multiple embedding by adding the intra-class semantic differences to embeddings of the same class. By combining the embeddings with and without data points, DAS provides more embeddings for sampling to improve the sampling quality and achieve effective DML. Extensive experiments with various loss functions and sampling methods on three public available benchmarks show that DAS is effective. In the future, we plan to apply DAS to other areas such as self-supervised learning that require sampling. We organize our supplementary materials as follows. In Section A, we provide the detailed formulations of both pair-based and proxy-based DML loss functions. In Section B, we detail the formulations of DML sampling methods. In Section C, we provide more implementation details of DAS. In Section D, we analyze the overhead of DAS, In Section E, we provide experiment results of DAS on widely used proxy-based losses. In Section F, we provide results on DAS w/o image augmentation. In Section G, we study the effect of batch size on DAS. In Section H, we study the effect of embedding dimension on DAS. In Section I, we visualize and analyze the frequency recorder matrix. In Section J, we provide the evolution of training process w.r.t. more DML losses. In Section K, we investigate the effect of hyper-parameters r s , r b , T . In Section L, we provide qualitative results w.r.t. DFS and MTS. In Section M, we provide more qualitative results on different loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements. This work was partially supported by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Formulations of Loss Function in DML</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Pair-based Loss Function</head><p>Contrastive Loss <ref type="bibr">[2]</ref>. The goal of contrastive loss is simply pulling the embeddings of the same class as close as possible and separating the embeddings of different classes at least of a given margin. Specifically, contrastive loss requires the index set of the sampled embedding pairs P = {(i, j)} and the pair-wise euclidean distance is calculated as D ij = ?v i ? v j ?. Then the formulation of contrastive loss is as follows</p><formula xml:id="formula_17">L Contrastive = (i,j)?P I{y i = y j } D ij + I{y i ? = y j } [? ? D ij ] + ,<label>(I)</label></formula><p>where I{?} is the indicator function, ? (set to 1.0 in this paper) is the margin.</p><p>Triplet Loss <ref type="bibr">[8]</ref>. Triplet loss extends the contrastive loss by converting the absolute distance relationship between embeddings into a relative distance relationship (i.e., ranking): the distance between embeddings of different classes should be farther away than any embeddings of the same class. Specifically, triplet loss requires sampling a set of embedding triplets T = {(a, p, n)}, where y a = y p ? = y n and a, p, n are the index of the anchor, positive and negative, respectively. The formulation of triplet loss is as follows</p><formula xml:id="formula_18">L Triplet = (a,p,n)?T [D ap ? D an + ?] + ,<label>(II)</label></formula><p>where ? (set to 0.2 in this paper) is the margin.</p><p>Margin Loss <ref type="bibr">[11]</ref>. Margin loss introduces a more flexible optimization paradigm into the triplet loss. Specifically, a adjustable and learnable margin ? ? R C is proposed to replace the fixed margin (i.e., 0) between embedding of different classes, which converts the triplet ranking problem into a relative ordering of pairs. The formulation of margin loss is as follows</p><formula xml:id="formula_19">L Margin = (i,j)?P I{y i = y j } [? + D ij ? ? yi ] + + I{y i ? = y j } [? + ? yi ? D ij ] + ,</formula><p>(III) where ? (set to 0.2 in this paper) is the margin in the triplet loss and ? yi is the learnable margin for class y i . Each element in ? is initialized with 1.2 and the learning rate for ? is set to 5e ?4 .</p><p>Generalized Lifted Structure Loss <ref type="bibr">[3]</ref>. Generalized lifted structure loss extends the standard lifted structure loss <ref type="bibr">[6]</ref> by considering all embeddings from the same class w.r.t. the anchor during intra-class distance minimization. Generalized lifted structure loss pulls embeddings of the same class w.r.t. the anchor close while pushing embeddings of different classes apart. To save computation cost, each embedding in a batch is used as the anchor once. To be specific, the index set of the sampled embeddings is P = {(a, Q, R)}, where a / ? Q, R, and y a = y q ? = y r , q ? Q, r ? R. Then the formulation of generalized lifted structure loss is as follows</p><formula xml:id="formula_20">L GenLifted = (a,Q,R)?P ? ? log q?Q exp (D aq ) + log r?R exp (? ? D ar ) ? ? + + ? ?v a ? 2 ,</formula><p>(IV) where ? (set to 1.0 in this paper) is the margin to avoid pushing the embeddings of different classes too large and ? (set to 5e ?3 in this paper) regularizes the embeddings. Note that, in this loss, embeddings for distance computation and producing embeddings with no data points are not normalized.</p><p>N-Pair Loss <ref type="bibr">[9]</ref>. N-Pair loss extends the triplet loss by considering all embeddings of different classes during inter-class distance maximization. Specifically, the index set of the sampled embeddings is P = {(a, p, R)}, where y a = y p ? = y r , r ? R, and the pair-wise distance is calculated as D ij = v T i v j . Then the formulation of N-Pair loss is as follows</p><formula xml:id="formula_21">L N-Pair = (a,p,R)?P log 1 + r?R exp (D ar ? D ap ) + ? ?v a ? 2 ,<label>(V)</label></formula><p>where ? (set to 5e ?3 in this paper) controls the optimization strength on the embedding regularization. Note that, in this loss, embeddings for distance computation and producing embeddings with no data points are also not normalized.</p><p>Multi-similarity Loss <ref type="bibr">[10]</ref>. Apart from considering simple anchor-positive, anchor-negative relationships, multi-similarity loss better leverages all embeddings in a batch by additionally considering positive-positive and negative-negative relationship. Also, to save computation cost, each embedding in a batch will only be used as anchor once. For a anchor a, let P a and N a denote its corresponding positive and negative embedding index sets, respectively. Given the pair-wise distance computed by D ij = v T i v j , the sampled embedding index set is constructed as P = {(a, Q, R)}, where a / ? Q, R, Q = {q | y q = y a , D aq &gt; min i?Pa (D ai ? ?)} and R = {r | y r ? = y a , D ar &lt; max j?Na (D aj + ?)}. Then the formulation of multi-similarity loss is as follows</p><formula xml:id="formula_22">L MS = (a,Q,R)?P 1 ? log ? ? 1 + q?Q exp (?? (D aq ? ?)) ? ? + 1 ? log 1 + r?R exp (? (D ar ? ?)) ,<label>(VI)</label></formula><p>where ?, ?, ?, ? are hyper-parameters to be set. In this paper, we set ? = 2, ? = 40, ? = 5e ?1 , ? = 1e ?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proxy-based Loss Function</head><p>Softmax Loss <ref type="bibr">[12]</ref>. Different from the pair-based loss function, softmax loss1 introduces a proxy i.e., classifier for each class and optimizes the embedding by pulling it close to its proxy. The formulation of softmax loss is as follows:</p><formula xml:id="formula_23">L Softmax = ? i log exp W T yi v i / T c?C exp (W T c v i / T ) ,<label>(VII)</label></formula><p>where W ? R C?d is the classifier weight for all training classes. Since the embedding v i is normalized, a temperature T (set to 5e ?2 in this paper) is used to boost the gradient. Moreover, the learning rate of W is set to 1e ?5 for CARS and CUB, 2e ?3 for SOP.</p><p>ArcFace Loss <ref type="bibr">[1]</ref>. ArcFace loss improves the vanilla softmax by adding an angular margin into embedding and its corresponding proxy to achieve more compact intra-class representation. The formulation of ArcFace loss is as follows</p><formula xml:id="formula_24">L ArcFace = ? i log exp s ? cos W T yi v i + ? exp s ? cos W T yi v i + ? + c? =yi exp (s ? cos (W T c v i ))</formula><p>, (VIII) where W ? R C?d is the classifier weight for all training classes and ?, s are hyperparameters to be set. In this paper, we set ? = 5e ?1 and s = 16. Moreover, the learning rate of W is set to 5e ? 4 for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed Formulations of Sampling Method for DML</head><p>Random Sampling <ref type="bibr">[4]</ref>. Random sampling simply selects the index of positive pair or negative pair in a most trivial way i.e., randomly selecting. To be specific, given an embedding v i , its index of positive is randomly draw from {j | y i = y j , i ? = j} and its index of negative is randomly draw from {k | y i ? = y k , i ? = k}.</p><p>Semi-hard Sampling <ref type="bibr">[8]</ref>. Semi-hard sampling is proposed to effectively sample embedding triplets that grows cubically to batch size. In the training process, most of the triplets satisfy the objective function and they provide limited (or no) training signal to train the model, thereby impeding the model learning <ref type="bibr">[8]</ref>. Thus, given an anchor v a and its positive v p (randomly sampled), semi-hard sampling carefully choose negative embedding's index as follows</p><formula xml:id="formula_25">n ? {i | y i ? = y a , ?v a ? v p ? 2 &lt; ?v a ? v i ? 2 }. (IX)</formula><p>Soft-hard Sampling <ref type="bibr">[7]</ref> To avoid selecting "hard" embeddings that impedes model training, semi-hard sampling chooses embeddings that are relatively close to the anchor. Soft-hard triplet sampling shows that a probabilistic (soft) selection of potentially hard embeddings is actually beneficial. Given an anchor embedding v a , soft-hard sampling attain the indexes of positive and negative embedding as follows</p><formula xml:id="formula_26">p ? {i | y i = y a , ?v a ? v i ? 2 &gt; arg min q?Qa ?v a ? v q ? 2 }, (X) n ? {j | y j ? = y a , ?v a ? v j ? 2 &lt; arg max r?Ra ?v a ? v r ? 2 },<label>(XI)</label></formula><p>where R a = {r | y r ? = y a }, Q a = {q | y q ? = y a } are the positive and negative index sets w.r.t. the anchor a, respectively. In this way, soft-hard sampling explores more triplets than semi-hard sampling to improve the model training.</p><p>Distance-weighted Sampling <ref type="bibr">[11]</ref> . Different from other sampling strategy that considers a certain distance range of embeddings, distance-weighted sampling considers a wide range of embeddings in a probabilistic way. Since the embedding space is typically a d-dimensional hypersphere S d?1 , the analytical distribution of pairwise distance on a hypersphere obeys</p><formula xml:id="formula_27">q (D ij ) ? D d?2 ij [1 ? 1 4 D ij ] d?3 2 ,<label>(XII)</label></formula><p>and</p><formula xml:id="formula_28">D ij = ?v i ? v j ? for any embedding pairs v i , v j ? S d?1 .</formula><p>To obtain a wide range of negative embeddings that are able to improve the embedding diversity as well as model training, distance-weighted sampling acquires the index of negative embedding based on the inversed distance distribution</p><formula xml:id="formula_29">P (n | a) ? min ?, q ?1 (D an ) .<label>(XIII)</label></formula><p>In this paper, we set ? = 5e ?1 and the largest distance to 1.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Implementation Details</head><p>In this section, we provide more implementation details. As for image augmentation process, random crop (image size 224?224) with random horizontal flip (p = 0.5) is applied during training and single center crop (image size 256?256) is used for testing. In terms of training strategy, the number of training epochs is 300. We use Adam <ref type="bibr">[5]</ref> as the optimizer. The initial learning rate is 1e ?5 , which is reduced by a factor of 0.3 in 200 th and 250 th epoch, respectively. The weight decay is 4e ?4 . For batch preparation, SPC-2 construction <ref type="bibr">[7]</ref> is used (2 samples per category). The batch size is set to 112. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Efficiency and Overhead Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Effectiveness of DAS on Proxy-based Loss</head><p>Although DAS is developed for pair-based loss, we perform experiments to evaluate the generalization ability of DAS on classic and widely used proxy-based losses i.e., Softmax and ArcFace. The results are presented in <ref type="table" target="#tab_8">Table I</ref>. The improvements are still observed when equipped with DAS for Softmax and ArcFace across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F DAS w/o Image Augmentation</head><p>We further perform experiments without image augmentation using triplet loss and distance weighted sampling on CARS. The results are shown in <ref type="table" target="#tab_8">Table II</ref>. DAS boosts all metrics considerably, showing that DAS is complementary to image augmentation technique.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Effect of Batch Size</head><p>In this section, we investigate the effect of batch size on the proposed DAS. The results are presented in <ref type="figure">Fig. I</ref>. The loss function and sampling method are margin loss <ref type="bibr">[11]</ref> and distance-weighted sampling <ref type="bibr">[11]</ref>, respectively. From <ref type="figure">Fig. I</ref>, we have the following observations: First, under various batch size and image retrieval evaluation metrics, when equipped with DAS, the model is able to consistently obtain better results than the one trained without DAS. Second, we observe that the model trained with DAS and batch size = 32 outperforms the one trained without DAS and batch size = 224 in terms of R@1. It shows that producing effective embeddings without datapoints by DAS is as equally important as providing more data points in a batch to achieve improved performance. These results well prove the rationality of our motivation and the efficacy of DAS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Effect of Embedding Dimension</head><p>In this section, we evaluate the proposed method on different embedding dimensions. The results are shown in <ref type="figure">Fig. II</ref>. We use the margin loss <ref type="bibr">[11]</ref> as the loss function while leveraging the distance-weighted sampling as the sampling method <ref type="bibr">[11]</ref>. From <ref type="figure">Fig. II</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Visualization Results on Frequency Recorder Matrix</head><p>In this section, we visualize the Frequency Recorder Matrix (FRM) P introduced in the DFS module. The FRM serves as a stable and effective identifier for semantic scaling by considering the top activated features for one class as the effective semantics instead of noises. The loss function and sampling method we used here are triplet loss <ref type="bibr">[8]</ref> and distance-weighted sampling <ref type="bibr">[11]</ref>, respectively. We perform experiments on all three datasets (i.e., CARS, CUB and SOP). The results are depicted in <ref type="figure">Fig. III</ref>, from which, we have the following observations: First, for different training stages (i.e., epoch = 1, 150, 300), the number of top activated features for embeddings of the same classes are limited (i.e.,around 4 ? 8) across all datasets. Second, as the training process proceeds, more features are likely to be the top activated features. Third, for the large scale dataset SOP, more features are likely to be the top activated ones due to the rich semantics covered by adequate data points. In this sense, with the proposed FRM, we are able to figure out channels with more discriminative power to achieve effective semantic scaling. These results demonstrate the rationality of the proposed FRM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t. Different Losses</head><p>In this section, we provide the evolution of training loss and test set R@1 w.r.t. different losses in the training process. The results are depicted in <ref type="figure">Fig. IV</ref>. We have the following observations: First, when training with DAS, the training losses are generally higher and decrease smoother than the baseline, which demonstrates that producing more embeddings by DAS is able to consistently provide training signal to train the model. Second, when equipped with DAS, the test set R@1s are higher than the baseline. Third, for some loss functions that face severe overfitting problems such as contrastive loss and generalized lifted structure loss, DAS is able to ease the overfitting problem. These results verify the effectiveness of the proposed method across different loss functions and sampling methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Ablation Studies on Hyper-parameters</head><p>In this section, we investigate the effect of the hyper-parameters r s , r b , T in DAS. The loss function and sampling method are margin loss <ref type="bibr">[11]</ref> and distanceweighted sampling <ref type="bibr">[11]</ref>, respectively. The default hyper-parameters' settings are (r s , r b , T ) = (1e ?2 , 1e ?2 , 3). Random scale in DFS (r s ). The results on different random scale in DFS are shown in <ref type="table" target="#tab_8">Table III</ref> (a). Our method is insensitive to a wide range of random scale, showing that scaling the discriminative features is able to provide effective semantics of different strength. Semantic shifting scale in MTS (r b ). r b is to provide the flexibility of controlling the strength of adding intra-class semantic differences. The results on different semantic shifting scales in MTS are shown in <ref type="table" target="#tab_8">Table III</ref> (b). Our method obtains similar results under different r b and reaches the best results when r b = 1, which suggests that larger r b is able to cover more barren area in the embedding space to improve the model training.</p><p>Number of the produced embeddings T . The results on different numbers of the produced embeddings are shown in <ref type="table" target="#tab_8">Table III</ref> (c). As T increases from 1 to 5, the proposed DAS achieves better results and reaches the best result at T = 5. When T = 10, the performance is worse than T = 5, which indicates that too many produced embeddings with no data points will dominate the optimization direction and impair the learning of embeddings with data points. Thus, the semantic scaling is implemented as randomly scaling the top K features in an embedding; Whilst we perform semantic shifting by adding the transformation (obtained from another two embeddings of the same class) to the embedding. The loss function and sampling method we used here are contrastive loss <ref type="bibr">[2]</ref> and distance-weighted sampling <ref type="bibr">[11]</ref>, respectively. The results for semantic scaling and shifting are shown in <ref type="figure" target="#fig_6">Fig. V and Fig. VI, respectively</ref>. We have the following observations: 1) When we apply different semantic scaling to the query, the model trained with DAS consistently retrieves correct results, which is not the case for the baseline. 2) The model trained with DAS is able to retrieve expected results even with the semantic shifted embedding while the baseline fails to do so. These results show that DAS is able to produce embeddings with effective semantics to train the model, which is insensitive to the semantic differences and consistently achieves good generalization ability after training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M More Qualitative Results</head><p>In this section, we provide qualitative results on different losses w/ or w/o DAS. The results for CARS and SOP are in <ref type="figure" target="#fig_6">Fig. VII and Fig. VIII, respectively</ref>. From those results, we can see that the proposed DAS can enforce the model to focus on real semantics despite the background noises and other semantics' interference such as car's colors, drastic viewpoint changes etc. These results show the generalization ability and robustness of the proposed DAS.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>The training loss and test set R@1 on CARS. The sampling method and loss function are triplet loss and distance weighted sampling, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>DAS takes extra cost only in the training stage. Specifically, w/ and w/o DAS, the training time cost for[11]  are 1.15s vs. 0.70s per batch, which includes the cost of DAS and using more embeddings for sampling and loss computation. Moreover, DAS only consumes 13% of the total time, which is efficient compared to the whole training procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Visualization of the frequency recorder matrix at epoch = 150 Visualization of the frequency recorder matrix at epoch = 300 Fig. III: From left to right, the visualized FRM on CARS, CUB and SOP, respectively. Each element in P is normalized (i.e., divided by the maximum value in its row). Only the first 48 classes are presented due to page limit J Evolution of Training Process w.r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Experiments on MS w/ or w/o DAS Fig. IV: The training loss and test set R@1 on CARS with different losses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Top 6 retrieved results with different scales on CARS. The expected and unexpected results are framed by green and red rectangles, respectively scores: [1, 0, 1] score: 1 | scores: [0, 0, 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. VI :</head><label>VI</label><figDesc>Top 3 retrieved results with MTS on CARS. The expected and unexpected results are framed by green and red rectangles, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Top 3  retrieved results using the model trained with MS[10]    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. VII :</head><label>VII</label><figDesc>Top 3 retrieved results using the model trained by different loss functions that are equipped w/ or w/o on CARS. The expected and unexpected results are framed by green and red rectangles, respectively w/ DAS w/o DAS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. VIII :</head><label>VIII</label><figDesc>Top 3 retrieved results using the model trained by different loss functions that are equipped w/ or w/o on SOP. The expected and unexpected results are framed by green and red rectangles, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Space with Anchor Points Embedding Space with Dense Anchor Points</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Sampling Functions</cell><cell>? metric</cell></row><row><cell>Points</cell><cell>Generated Anchor Points</cell><cell>Classification Boundary</cell><cell>Semantic Difference</cell></row></table><note>? Memorized Transformation Shifting? Embedding</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>1) CUB2011-200 (CUB) [50], a fine-grained bird dataset with the first 100 categories for training and another 100 categories for testing. 2) CARS196 (CARS) [29], a fine-grained vehicle dataset with the first 98 classes for training and another 98 classes for testing. 3) Stanford Online Products (SOP) [38], a large-scale online products dataset with the train and test partitions as 11,318 classes and another 11,316 classes, respectively. Comparisons with SoTA methods on CUB, CARS and SOP. The best results are in bold. * indicates the reimplementation by [4]. ? denotes our reimplementation 74.40 83.10 90.00 79.60 86.50 91.90 95.10 72.70 86.20 93.80 98.00 HDC [59] G 384 53.60 65.70 77.00 85.60 73.70 83.20 89.50 93.80 69.50 84.40 92.80 97.70 A-BIER [39] G 384 57.50 68.70 78.30 86.20 82.00 89.00 93.20 96.10 74.20 86.90 94.00 97.80 ABE [27] G 512 60.60 71.50 79.80 87.40 85.20 90.50 94.00 96.10 76.30 88.40 94.80 98.20 HTL [15] IBN 512 57.10 68.80 78.70 86.50 81.40 88.00 92.70 95.70 74.80 88.30 94.80 98.40 RLL-H [52] IBN 512 57.40 69.70 79.20 86.90 74.00 83.60 90.10 94.10 76.10 89.10 95.40 N/A SoftTriple [40] IBN 512 65.40 76.40 84.50 90.40 84.50 90.70 94.50 96.90 78.30 90.30 95.90 N/A MS [53] IBN 512 65.70 77.00 86.30 91.20 84.10 90.40 94.00 96.50 78.20 90.50 96.00 98.70 ProxyGML [67] IBN 512 66.60 77.60 86.40 N/A 85.50 91.80 95.30 N/A 78.00 90.60 96.20 N/A ProxyAnchor [25] IBN 512 68.40 79.20 86.80 91.60 86.10 91.70 95.00 97.30 79.10 90.80 96.20 98.70 Contrastive + XBM [54] IBN 512 65.80 75.90 84.00 89.90 82.00 88.70 93.10 96.10 79.50 90.80 96.10 98.70 MS * [53] IBN 512 64.50 76.20 84.60 90.50 82.10 88.80 93.20 96.10 76.30 89.70 96.00 98.80 MS + EE * [28] IBN 512 65.10 76.80 86.10 91.00 82.70 89.20 93.80 96.40 77.00 89.50 96.00 98.80 ProxyAnchor + MemVir [4] IBN 512 69.00 79.20 86.80 91.60 86.70 92.00 95.20 97.40 79.70 91.00 96.30 98.60 MS ? [53] IBN 512 65.72 77.19 85.74 91.56 83.86 90.41 94.64 96.99 76.89 89.58 95.59 98.60 MS + DAS (K = 8) (Ours) IBN 512 67.07 78.11 86.43 91.88 85.66 91.60 95.27 97.37 78.16 90.26 95.99 98.76 MS ? [53] R 512 66.46 77.28 85.85 91.69 83.99 90.39 94.51 96.80 79.53 91.06 96.30 98.83 MS + DAS (K = 8) (Ours) R 512 69.19 79.25 87.09 92.62 87.84 93.15 95.99 97.85 80.59 91.80 96.68 98.95 Implementation details 3 We leverage two popular backbones: ResNet50 [19] (R d ) and Inception BN [23] (IBN d ), where their parameters are initialized from ImageNet [9] pre-trained models and d denotes the embedding dimension. Note that some approaches also consider GoogleNet [46] (G d ) as the backbone. Here, we mainly consider the settings of d = 128, 512. R 128 is used as the default backbone. The embedding layer is randomly initialized. Regarding evaluation metrics, Recall at k (R@k) [24], Normalized Mutual Information (NMI) [43] and F1 score (F1) [45] are used, where R@k measures the image retrieval performance while F1 and NMI measure the image clustering performance. For hyper-parameters in DAS, we set (T, K, Z, r s , r b ) = (3, 4, 10, 1e ?2 , 1e ?2 ) by default.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>CUB R@1 R@2 R@4 R@8 R@1 R@2 R@4 R@8 R@1 R@10 R@100 R@1000 CARS SOP</cell></row><row><cell>Margin [56]</cell><cell>R 128</cell><cell>63.60</cell></row></table><note>4 Our source code is publicly available at https://github.com/lizhaoliu-Lec/DAS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparisons with pair-based methods on CUB, CARS and SOP. [S] and [D] denote semi-hard and distance weighted sampling, respectively 32.82 64.64 74.64 31.98 63.22 73.51 33.47 89.33 Triplet [S] + DAS 60.82 33.86 65.67 77.21 33.88 64.84 73.99 33.91 89.42 Triplet [D] [56] 62.68 36.39 67.03 78.86 35.80 65.85 77.54 37.10 90.05 Triplet [D] + DAS 64.28 38.16 68.06 82.63 39.14 68.12 77.95 37.64 90.18 Contrastive [D] [56] 61.65 35.23 66.58 76.03 32.77 64.09 73.13 35.60 89.78 Contrastive [D] + DAS 63.67 36.25 67.15 80.74 36.07 65.93 74.80 36.21 89.89 Margin [56] 62.61 37.33 67.58 80.10 37.85 67.15 78.69 39.20 90.50 Margin + DAS 64.50 37.86 68.04 82.29 38.22 67.94 79.14 39.52 90.56 GenLifted [20] 58.81 34.64 65.50 72.45 32.43 64.00 76.18 37.26 90.13 GenLifted + DAS 59.94 35.09 66.07 73.55 32.85 64.11 76.92 37.64 90.21 N-Pair [45] 60.55 36.94 67.19 77.35 36.26 66.74 77.71 37.13 90.15 N-Pair + DAS 62.81 38.37 68.43 79.93 38.06 68.20 77.98 37.82 90.28 MS [53] 62.63 38.88 68.19 82.04 40.85 69.45 78.89 37.53 90.12 MS + DAS 64.13 39.18 69.08 83.31 42.78 70.77 79.44 38.77 90.40</figDesc><table><row><cell>Method</cell><cell>CUB R@1 F1 NMI R@1 F1 NMI R@1 F1 NMI CARS SOP</cell></row><row><cell>Triplet [S] [42]</cell><cell>60.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparisons with various sampling methods on CARS IBN 512 85.73 91.96 95.51 97.54 MS + DAS (Ours) IBN 512 85.66 91.60 95.27 97.37 MS + SEC + DAS (Ours) IBN 512 87.80 93.16 96.18 98.01</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Table 4. Comparisons with more related</cell></row><row><cell></cell><cell></cell><cell></cell><cell>works on CARS</cell><cell></cell></row><row><cell>Method</cell><cell>Sampling</cell><cell>DAS R@1 F1 NMI</cell><cell>Method</cell><cell>Backbone R@1 R@2 R@4 R@8</cell></row><row><cell>Triplet Contrastive</cell><cell>Random Semi-hard [42] Soft-hard [41] Distance [56] Random Distance [56]</cell><cell>74.21 33.41 64.28 ? 76.79 35.21 65.49 74.64 31.98 63.22 ? 77.10 33.82 65.03 79.20 35.55 66.08 ? 80.54 37.52 66.77 78.86 35.80 65.85 ? 81.34 37.27 67.21 42.44 15.83 48.87 ? 50.79 19.40 52.71 76.03 32.77 64.09 ? 80.70 35.47 66.01</cell><cell>N-Pair + HDML [63] N-Pair + HDML-A [63] N-Pair + DAS (Ours) MS + SEC [61] Margin + DiVA [35] Margin + DAS (Ours) ProxyNCA++ [47] Margin + DiVA Margin + DRML [66] Margin + DCML [65] Margin + DAS (Ours)</cell><cell>G 512 G 512 G 512 IBN 512 83.10 90.00 N/A N/A 68.90 78.90 85.80 90.90 81.10 88.80 93.70 96.70 83.70 90.33 94.47 96.77 IBN 512 84.85 90.32 93.99 96.40 R 512 86.50 92.50 95.70 97.70 R 512 82.20 89.00 N/A N/A R 512 73.30 83.00 89.80 94.40 R 512 85.20 91.80 96.00 98.00 R 512 88.34 93.21 95.92 97.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>datasets.6  On CARS, the model performs better with DAS despite the background noises or the interference from the car's color, demonstrating that DAS enforces the model to focus on real semantics. On SOP, the model with DAS is insensitive to drastic viewpoint changes. These results verify the generalization ability and robustness of DAS under various scenes. Top 3 retrieved results from the R 128 trained w/ or w/o DAS. green and red rectangles indicate desired and undesired results, respectively</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CARS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SOP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Query</cell><cell>Top 1</cell><cell>Top 2</cell><cell>Top 3</cell><cell>Top 1</cell><cell>Top 2</cell><cell>Top 3</cell><cell>Query</cell><cell>Top 1</cell><cell>Top 2</cell><cell>Top 3</cell><cell>Top 1</cell><cell>Top 2</cell><cell>Top 3</cell></row><row><cell></cell><cell></cell><cell>w/ DAS</cell><cell></cell><cell></cell><cell>w/o DAS</cell><cell></cell><cell></cell><cell></cell><cell>w/ DAS</cell><cell></cell><cell></cell><cell>w/o DAS</cell><cell></cell></row><row><cell>Fig. 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation studies on CARS +2.42) 36.22 (+0.42) 66.84 (+0.99) ? 81.83 (+2.97) 38.26 (+2.46) 67.81 (+1.96) , 32, we obtain results R@1 = 78.76, 78.86, 79.14, 78.09, 77.94, 77.96, where substantial improvements are observed when K = {1, 2, 4} and larger K (i.e., K &gt; 4) leads to worse results. Effect of Z in MTS. The larger bank capacity (Z) allows us to access intra-class transformations from current and previous iterations, which improves the diversity of the produced embeddings. We conduct experiments on CARS with margin loss by setting Z = 1, 2, 3, 4, 5 and obtain R@1 = 82.35, 82.38, 82.42, 82.75, 82.33, showing that history transformations (Z &gt; 2) bring slight improvements.</figDesc><table><row><cell cols="2">DFS MTS R@1</cell><cell>F1</cell><cell>NMI</cell></row><row><cell>?</cell><cell>78.86 81.28 (</cell><cell>35.80</cell><cell>65.85</cell></row></table><note>? ? 82.63 (+3.77) 39.14 (+3.34) 68.12 (+2.27) Effect of K in DFS. Multi-dimensional embeddings have a diverse set of se- mantic features [2, 3]. The top-K mask is effective to discover the discriminative features. We perform experiments on SOP, a large-scale and diverse dataset with margin loss to support our claim. With K = 1, 2, 4, 8, 16</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>, we compare DAS with HDML [63], SEC [61], DiVA [35], ProxyNCA++ [47], DRML [66], DCML [65] on CARS under the same settings. We can see that DAS outperforms HDML, DiVA, DRML and DCML, and achieves comparable performance to SEC. Note that DAS can be also incorporated into SEC. As a result, using MS loss, SEC + DAS outperforms SEC by 2.07% in R@1. These results further verify the applicability of DAS on some regularization techniques in DML.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>. Hadsell, R., Chopra, S., LeCun, Y.: Dimensionality reduction by learning an invariant mapping. In: IEEE Computer Society Conference on Computer Vision and Pattern Recognition. vol. 2, pp. 1735-1742. IEEE (2006) 2, 3, 4 19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Kim, S., Kim, D., Cho, M., Kwak, S.: Proxy anchor loss for deep metric learning. Milbich, T., Roth, K., Bharadhwaj, H., Sinha, S., Bengio, Y., Ommer, B., Cohen, J.P.: Diva: Diverse visual feature aggregation for deep metric learning. In: European Conference on Computer Vision. pp. 590-607. Springer (2020) 12, 14 36. Movshovitz-Attias, Y., Toshev, A., Leung, T.K., Ioffe, S., Singh, S.: No fuss distance metric learning using proxies. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 360-368 (2017) 2, 4 37. Musgrave, K., Belongie, S., Lim, S.N.: A metric learning reality check.</figDesc><table><row><cell>pp. 770-778 (2016) 10 20. Hermans, A., Beyer, L., Leibe, B.: In defense of the triplet loss for person re-35. In: European</cell></row><row><cell>identification. arXiv preprint arXiv:1703.07737 (2017) 3, 4, 11 Conference on Computer Vision. pp. 681-699. Springer (2020) 2 21. Hoi, S.C., Liu, W., Chang, S.F.: Semi-supervised distance metric learning for col-38. Oh Song, H., Xiang, Y., Jegelka, S., Savarese, S.: Deep metric learning via lifted laborative image retrieval and clustering. ACM Transactions on Multimedia Com-structured feature embedding. In: Proceedings of the IEEE Conference on Com-puting, Communications, and Applications 6(3), 1-26 (2010) 1 puter Vision and Pattern Recognition. pp. 4004-4012 (2016) 2, 4, 9 22. Hu, J., Lu, J., Tan, Y.P.: Discriminative deep metric learning for face verification in 39. Opitz, M., Waltner, G., Possegger, H., Bischof, H.: Deep metric learning with the wild. In: Proceedings of the IEEE Conference on Computer Vision and Pattern bier: Boosting independent embeddings robustly. IEEE Transactions on Pattern Recognition. pp. 1875-1882 (2014) 4 Analysis and Machine Intelligence 42(2), 276-290 (2018) 10 23. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by 40. Qian, Q., Shang, L., Sun, B., Hu, J., Li, H., Jin, R.: Softtriple loss: Deep metric reducing internal covariate shift. In: International Conference on Machine Learning. learning without triplet sampling. In: Proceedings of the IEEE/CVF International pp. 448-456. PMLR (2015) 10 Conference on Computer Vision. pp. 6450-6458 (2019) 4, 10 24. Jegou, H., Douze, M., Schmid, C.: Product quantization for nearest neighbor 41. Roth, K., Milbich, T., Sinha, S., Gupta, P., Ommer, B., Cohen, J.P.: Revisiting search. IEEE Transactions on Pattern Analysis and Machine Intelligence 33(1), training strategies and generalization performance in deep metric learning. In: 117-128 (2010) 10 International Conference on Machine Learning. pp. 8242-8252. PMLR (2020) 2, 3, 8, 12 42. Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face recognition and clustering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 815-823 (2015) 1, 2, 3, 4, 5, 11, 12 43. Sch?tze, H., Manning, C.D., Raghavan, P.: Introduction to Information Retrieval, 25. 2288-2297 (2019) 4 vol. 39. Cambridge University Press Cambridge (2008) 10 27. Kim, W., Goyal, B., Chawla, K., Lee, J., Kwon, K.: Attention-based ensemble for 44. Shorten, C., Khoshgoftaar, T.M.: A survey on image data augmentation for deep deep metric learning. In: Proceedings of the European Conference on Computer learning. Journal of Big Data 6(1), 1-48 (2019) 4 Vision. pp. 736-751 (2018) 10 45. Sohn, K.: Improved deep metric learning with multi-class n-pair loss objective. In: 28. Ko, B., Gu, G.: Embedding expansion: Augmentation in embedding space for deep Proceedings of the 30th International Conference on Neural Information Processing metric learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision Systems. pp. 1857-1865 (2016) 2, 3, 4, 10, 11 and Pattern Recognition. pp. 7255-7264 (2020) 2, 3, 4, 10, 11 46. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., 29. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for fine-Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings grained categorization. In: Proceedings of the IEEE International Conference on of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1-9 Computer Vision Workshops. pp. 554-561 (2013) 9 (2015) 10 30. Li, S., Chen, D., Liu, B., Yu, N., Zhao, R.: Memory-based neighbourhood em-47. Teh, E.W., DeVries, T., Taylor, G.W.: Proxynca++: Revisiting and revitalizing bedding for visual recognition. In: Proceedings of the IEEE/CVF International proxy neighborhood component analysis. In: European Conference on Computer Conference on Computer Vision. pp. 6102-6111 (2019) 14 Vision. pp. 448-464. Springer (2020) 12, 14 31. Lin, X., Duan, Y., Dong, Q., Lu, J., Zhou, J.: Deep variational metric learning. In: 48. Ustinova, E., Lempitsky, V.: Learning deep embeddings with histogram loss. In: Proceedings of the European Conference on Computer Vision. pp. 689-704 (2018) Proceedings of the International Conference on Neural Information Processing Sys-3, 4, 8 32. Liu, L., Cao, J., Liu, M., Guo, Y., Chen, Q., Tan, M.: Dynamic extension nets for tems. pp. 4177-4185 (2016) 2</cell></row><row><cell>few-shot semantic segmentation. In: Proceedings of the 28th ACM International 49. Volpi, R., Morerio, P., Savarese, S., Murino, V.: Adversarial feature augmentation</cell></row><row><cell>Conference on Multimedia. pp. 1441-1449 (2020) 2 for unsupervised domain adaptation. In: Proceedings of the IEEE Conference on</cell></row><row><cell>33. Liu, W., Wen, Y., Yu, Z., Li, M., Raj, B., Song, L.: Sphereface: Deep hypersphere Computer Vision and Pattern Recognition. pp. 5495-5504 (2018) 4, 5</cell></row><row><cell>embedding for face recognition. In: Proceedings of the IEEE Conference on Com-50. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The caltech-ucsd</cell></row><row><cell>puter Vision and Pattern Recognition. pp. 212-220 (2017) 4 birds-200-2011 dataset (2011) 9 34. Mikolov, T., Yih, W.t., Zweig, G.: Linguistic regularities in continuous space word Peng Cheng Labo-51. Wang, J., Song, Y., Leung, T., Rosenberg, C., Wang, J., Philbin, J., Chen, B., Wu, ratory Research Project No. PCL2021A07, National Natural Science Foundation representations. In: Proceedings of the 2013 Conference of the North American Y.: Learning fine-grained image similarity with deep ranking. In: Proceedings of</cell></row><row><cell>of China (NSFC) 62072190, Program for Guangdong Introducing Innovative and Chapter of the Association for Computational Linguistics: Human Language Tech-the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1386-1393</cell></row><row><cell>Enterpreneurial Teams 2017ZT07X183. nologies. pp. 746-751 (2013) 3, 8 (2014) 4</cell></row></table><note>18In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3238-3247 (2020) 4, 10 26. Kim, S., Seo, M., Laptev, I., Cho, M., Kwak, S.: Deep metric learning beyond binary supervision. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table I :</head><label>I</label><figDesc>Comparisons with proxy-based approaches on various datasets 36.12 66.73 79.07 37.11 67.01 77.92 37.20 90.05 Softmax + DAS 62.02 36.24 67.42 81.23 39.95 68.91 79.36 38.72 90.40 ArcFace [1] 61.56 35.73 66.83 79.50 37.75 67.82 78.08 37.79 90.18 ArcFace + DAS 62.80 37.63 67.80 82.22 40.82 69.82 78.12 38.08 90.26</figDesc><table><row><cell>Method</cell><cell>CUB R@1 F1 NMI R@1 F1 NMI R@1 F1 NMI CARS SOP</cell></row><row><cell>Softmax [12]</cell><cell>61.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table II :</head><label>II</label><figDesc>Comparisons without image augmentation on CARS</figDesc><table><row><cell>DAS R@1</cell><cell>F1</cell><cell>NMI</cell></row><row><cell cols="3">61.47 ? 65.13 (+3.66) 23.77 (+1.71) 55.89 (+2.01) 22.06 53.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>, we obtain the following results: First, under different embedding dimension, DAS consistently reaches the best performance for all image retrieval metrics. Second, the model that trained with DAS and embedding dimension = 64 obtains a comparable result like the one trained without DAS and embedding dimension = 128 regarding R@1. It shows that the produced embeddings by DAS are able to force the model to better leverage the model capacity. And covering the barren area in embedding space is important to get improved performance when model capacity is low. These results demonstrate the effectiveness of DAS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell>94</cell><cell></cell><cell></cell><cell></cell><cell>97</cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell>92</cell><cell></cell><cell></cell><cell></cell><cell>96</cell><cell></cell><cell></cell></row><row><cell>R@1</cell><cell>75</cell><cell></cell><cell></cell><cell>R@2</cell><cell>84 86</cell><cell></cell><cell></cell><cell>R@4</cell><cell>90</cell><cell></cell><cell></cell><cell>R@8</cell><cell>94 95</cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell cols="2">w/o DAS w/ DAS (Ours) Margin</cell><cell>80 82</cell><cell></cell><cell cols="2">w/o DAS w/ DAS (Ours) Margin</cell><cell>88</cell><cell></cell><cell cols="2">w/ DAS (Ours) w/o DAS Margin</cell><cell>92 93</cell><cell></cell><cell cols="2">w/ DAS (Ours) w/o DAS Margin</cell></row><row><cell></cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>512</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>512</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>512</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>512</cell></row><row><cell></cell><cell></cell><cell cols="2">Embedding Dim</cell><cell></cell><cell></cell><cell cols="2">Embedding Dim</cell><cell></cell><cell></cell><cell cols="2">Embedding Dim</cell><cell></cell><cell></cell><cell cols="2">Embedding Dim</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">(a) Experiments using different embedding dim.</cell><cell></cell><cell></cell></row><row><cell cols="17">Fig. II: Test set R@{1, 2, 4, 8} on CARS with different embedding dimension d</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table III :</head><label>III</label><figDesc>Experiments on different hyper-parameters on CARS (a) Effect of the random scale rs in DFS rs 1e ?2 1e ?1 2e ?1 5e ?1 R@1 82.29 82.25 82.30 82.43 (b) Effect of the scale r b in MTS r b 1e ?3 1e ?2 1e ?1 1 R@1 82.19 82.29 82.07 82.55 (c) Effect of the number of produced embedding (T ) in DAS R@1 80.78 82.29 83.40 81.75 L Qualitative Results of DFS and MTS In this section, we investigate the effectiveness of the proposed DFS and MTS. Specifically, we apply DFS and MTS in the test phase and compare results from the model trained with or without them. Since the training and test classes are different, the DFS and MTS modules used for training are unavailable here.</figDesc><table><row><cell>T</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Visualization of the frequency recorder matrix is in the supplementary.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See supplementary for detailed DML sampling methods and loss functions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">See supplementary for more details.4  Experiments on hyper-parameters T, rs, r b are in the supplementary.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Results on more pair-based losses are in the supplementary.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">More qualitative results are in the supplementary.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Lizhao Liu 1,2 , Shangxin Hunag 1 , Zhuangwei Zhuang 1 , Ran Yang 1 , Mingkui Tan 1,3 ? , and Yaowei Wang 2 ? 1 South China University of Technology 2 PengCheng Laboratory 3 Key Laboratory of Big Data and Intelligent Robot, Ministry of Education {selizhaoliu,sevtars,z.zhuangwei,msyangran}@mail.scut.edu.cn, mingkuitan@scut.edu.cn, wangyw@pcl.ac.cn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? Corresponding authors.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ensemble deep manifold similarity learning using hard proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aziere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the role of individual units in a deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning with memory-based virtual classes for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Byungsoo Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rspnet: Relative speed perception for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating visually aligned sound from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8292" to="8302" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05538</idno>
		<title level="m">Dataset augmentation in feature space</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust transfer metric learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="660" to="670" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep adversarial metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the relationship between visual attributes and convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Proxy synthesis: Learning with synthetic classes for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1460" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ranked list loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>3, 4</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-batch memory for embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Implicit semantic data augmentation for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature transfer learning for face recognition with under-represented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep metric learning with tuplet margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Classification is a strong baseline for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep metric learning with spherical embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An adversarial approach to hard triplet generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hardness-aware deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hardness-aware deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep compositional metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep relational metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fewer is more: A deep graph metric learning perspective using fewer proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person reidentification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Revisiting training strategies and generalization performance in deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Classification is a strong baseline for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
