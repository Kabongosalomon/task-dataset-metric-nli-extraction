<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AiATrack: Attention in Attention for Transformer Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenyuan</forename><surname>Gao</surname></persName>
							<email>shenyuangao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Wormpex AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<email>xgwang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<email>jsyuan@buffalo.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AiATrack: Attention in Attention for Transformer Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Tracking</term>
					<term>Attention Mechanism</term>
					<term>Vision Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer trackers have achieved impressive advancements recently, where the attention mechanism plays an important role. However, the independent correlation computation in the attention mechanism could result in noisy and ambiguous attention weights, which inhibits further performance improvement. To address this issue, we propose an attention in attention (AiA) module, which enhances appropriate correlations and suppresses erroneous ones by seeking consensus among all correlation vectors. Our AiA module can be readily applied to both self-attention blocks and cross-attention blocks to facilitate feature aggregation and information propagation for visual tracking. Moreover, we propose a streamlined Transformer tracking framework, dubbed AiA-Track, by introducing efficient feature reuse and target-background embeddings to make full use of temporal references. Experiments show that our tracker achieves state-of-the-art performance on six tracking benchmarks while running at a real-time speed. Code and models are publicly available at https://github.com/Little-Podi/AiATrack.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual tracking is one of the fundamental tasks in computer vision. It has gained increasing attention because of its wide range of applications <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b17">18]</ref>. Given a target with bounding box annotation in the initial frame of a video, the objective of visual tracking is to localize the target in successive frames. Over the past few years, Siamese trackers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b66">67]</ref>, which regards the visual tracking task as a one-shot matching problem, have gained enormous popularity. Recently, several trackers <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b56">57]</ref> have explored the application of the Transformer <ref type="bibr" target="#b51">[52]</ref> architecture and achieved promising performance.</p><p>The crucial components in a typical Transformer tracking framework <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b60">61</ref>] are the attention blocks. As shown in <ref type="figure">Fig. 1</ref>, the feature representations of the reference frame and search frame are enhanced via self-attention blocks, and the correlations between them are bridged via cross-attention blocks for target prediction in the search frame. The Transformer attention <ref type="bibr" target="#b51">[52]</ref>   <ref type="figure">Fig. 1</ref>. Motivation of the proposed method. The left part of the figure shows a typical Transformer tracking framework. On the right, the nodes denote features at different positions in a feature map. These nodes serve as queries and keys for a self-attention block. The links between nodes represent the correlations between queries and keys in the attention mechanism. Some correlations of the green node is erroneous since it is linked to the nodes at irrelevant positions. By applying the proposed module to the raw correlations, we can seek consensus from the correlations of other nodes (e.g. the brown node) that can provide supporting cues for the appropriate correlations. By this means, the quality of the correlations can be refined. a set of key-value pairs as input and outputs linear combinations of values with weights determined by the correlations between queries and the corresponding keys. The correlation map is computed by the scaled dot products between queries and keys. However, the correlation of each query-key pair is computed independently, which ignores the correlations of other query-key pairs. This could introduce erroneous correlations due to imperfect feature representations or the existence of distracting image patches in a background clutter scene, resulting in noisy and ambiguous attention weights as visualized in <ref type="figure">Fig. 4</ref>.</p><p>To address the aforementioned issue, we propose a novel attention in attention (AiA) module, which extends the conventional attention <ref type="bibr" target="#b51">[52]</ref> by inserting an inner attention module. The introduced inner attention module is designed to refine the correlations by seeking consensus among all correlation vectors. The motivation of the AiA module is illustrated in <ref type="figure">Fig. 1</ref>. Usually, if a key has a high correlation with a query, some of its neighboring keys will also have relatively high correlations with that query. Otherwise, the correlation might be noise. Motivated by this, we introduce the inner attention module to utilize these informative cues. Specifically, the inner attention module takes the raw correlations as queries, keys, and values and adjusts them to enhance the appropriate correlations of relevant query-key pairs and suppress the erroneous correlations of irrelevant query-key pairs. We show that the proposed AiA module can be readily inserted into the self-attention blocks to enhance feature aggregation and into the cross-attention block to facilitate information propagation, both of which are very important in a Transformer tracking framework. As a result, the overall tracking performance can be improved.</p><p>How to introduce the long-term and short-term references is still an open problem for visual tracking. With the proposed AiA module, we present AiA-Track, a streamlined Transformer framework for visual tracking. Unlike previous practices <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58]</ref>, which need an extra computational cost to process the selected reference frame during the model update, we directly reuse the cached features which are encoded before. An IoU prediction head is introduced for selecting high-quality short-term references. Moreover, we introduce learnable target-background embeddings to distinguish the target from the background while preserving the contextual information. With these designs, the proposed AiATrack can efficiently update short-term references and effectively exploit the long-term and short-term references for visual tracking.</p><p>We verify the effectiveness of our method by conducting comprehensive experiments on six prevailing benchmarks covering various kinds of tracking scenarios. Without bells and whistles, the proposed AiATrack sets new state-of-the-art results on these benchmarks with a real-time speed of 38 frames per second (fps).</p><p>In summary, the main contributions of our work are three-fold:</p><p>? We propose a novel attention in attention (AiA) module, which can mitigate noise and ambiguity in the conventional attention mechanism <ref type="bibr" target="#b51">[52]</ref> and improve tracking performance by a notable margin. ? We present a neat Transformer tracking framework with the reuse of encoded features and the introduction of target-background embeddings to efficiently and effectively leverage temporal references. ? We perform extensive experiments and analyses to validate the effectiveness of our designs. The proposed AiATrack achieves state-of-the-art performance on six widely used benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Tracking</head><p>Recently, Transformer <ref type="bibr" target="#b51">[52]</ref> has shown impressive performance in computer vision <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b13">14]</ref>. It aggregates information from sequential inputs to capture global context by an attention mechanism. Some efforts <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19]</ref> have been made to introduce the attention structure to visual tracking. Recently, several works <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b56">57]</ref> apply Transformer architecture to visual tracking. Despite their impressive performance, the potential of Transformer trackers is still limited by the conventional attention mechanism. To this end, we propose a novel attention module, namely, attention in attention (AiA), to further unveil the power of Transformer trackers.</p><p>How to adapt the model to the appearance change during tracking has also been investigated by previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58]</ref>. A straightforward solution is to update the reference features by generation <ref type="bibr" target="#b62">[63]</ref> or ensemble <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58]</ref>. However, most of these methods need to resize the reference frame and re-encode the reference features, which may sacrifice computational efficiency. Following discriminative correlation filter (DCF) method <ref type="bibr" target="#b23">[24]</ref>, another family of approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3]</ref> optimize the network parameters during the inference. However, they need sophisticated optimization strategies with a sparse update to meet real-time requirements. In contrast, we present a new framework that can efficiently reuse the encoded features. Moreover, a target-background embedding assignment mechanism is also introduced. Different from <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b32">33]</ref>, our target-background embeddings are directly introduced to distinguish the target and background regions and provide rich contextual cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Mechanism</head><p>Represented by non-local operation <ref type="bibr" target="#b53">[54]</ref> and Transformer attention <ref type="bibr" target="#b51">[52]</ref>, attention mechanism has rapidly received great popularity over the past few years. Recently, Transformer attention has been introduced to computer vision as a competitive architecture <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b13">14]</ref>. In vision tasks, it usually acts as a dynamic information aggregator in spatial and temporal domains. There are some works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> that focus on solving existing issues in the conventional attention mechanism. Unlike these, in this paper, we try to address the noise and ambiguity issue in conventional attention mechanism by seeking consensus among correlations with a global receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Correlation as Feature</head><p>Treating correlations as features has been explored by several previous works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b3">4]</ref>. In this paper, we use correlations to refer to the matching results of the pixels or regions. They can be obtained by squared difference, cosine similarity, inner product, etc. Several efforts have been made to recalibrate the raw correlations by processing them as features through hand-crafted algorithms <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b4">5]</ref> or learnable blocks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b8">9]</ref>. To our best knowledge, we introduce this insight to the attention mechanism for the first time, making it a unified block for feature aggregation and information propagation in Transformer visual tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention in Attention</head><p>To present our attention in attention module, we first briefly revisit the conventional attention block in vision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref>. As illustrated in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>, it takes a query and a set of key-value pairs as input and produces an output which is a weighted sum of the values. The weights assigned to the values are computed by taking the softmax of the scaled dot products between the query and the corresponding keys. Denote queries, keys and values by Q, K, V ? R HW ?C respectively. The conventional attention can be formulated as </p><formula xml:id="formula_0">ConvenAttn(Q, K, V) = (Softmax QK T ? C V )W o<label>(1)</label></formula><formula xml:id="formula_1">whereQ = QW q ,K = KW k ,V = VW v are different linear transformations.</formula><p>Here, W q , W k , W v and W o denote the linear transform weights for queries, keys, values, and outputs, respectively.</p><p>However, in the conventional attention block, the correlation of each querykey pair in the correlation map M =QK T ? C ? R HW ?HW is computed independently, which ignores the correlations of other query-key pairs. This correlation computation procedure may introduce erroneous correlations due to imperfect feature representations or the existence of distracting image patches in a background clutter scene. These erroneous correlations could result in noisy and ambiguous attentions as visualized in <ref type="figure">Fig. 4</ref>. They may unfavorably affect the feature aggregation in self-attention and the information propagation in crossattention, leading to sub-optimal performance for a Transformer tracker.</p><p>To address the aforementioned problem, we propose a novel attention in attention (AiA) module to improve the quality of the correlation map M. Usually, if a key has a high correlation with a query, some of its neighboring keys will also have relatively high correlations with that query. Otherwise, the correlation might be a noise. Motivated by this, we introduce the AiA module to utilize the informative cues among the correlations in M. The proposed AiA module seeks the correlation consistency around each key to enhance the appropriate correlations of relevant query-key pairs and suppress the erroneous correlations of irrelevant query-key pairs. Specifically, we introduce another attention module to refine the correlation map M before the softmax operation as illustrated in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. As the newly introduced attention module is inserted into the conventional attention block, we call it an inner attention module, forming an attention in attention structure. The inner attention module itself is a variant of the conventional attention. We consider columns in M as a sequence of correlation vectors which are taken as queries Q ? , keys K ? and values V ? by the inner attention module to output a residual correlation map.</p><p>Given the input Q ? , K ? and V ? , we first generate transformed queriesQ ? and keysK ? as illustrated in the right block of <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. To be specific, a linear transformation is first applied to reduce the dimensions of Q ? and K ? to HW ? D (D ? HW ) for computational efficiency. After normalization <ref type="bibr" target="#b0">[1]</ref>, we add 2-dimensional sinusoidal encoding <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref> to provide positional cues. Then,Q ? andK ? are generated by two different linear transformations. We also normalize V ? to generate the normalized correlation vectorsV ? , i.e.V ? = LayerNorm(V ? ). WithQ ? ,K ? andV ? , the inner attention module generates a residual correlation map by</p><formula xml:id="formula_2">InnerAttn(M) = (Softmax Q ?K?T ? D V ? )(1 + W ? o )<label>(2)</label></formula><p>where W ? o denotes linear transform weights for adjusting the aggregated correlations together with an identical connection.</p><p>Essentially, for each correlation vector in the correlation map M, the AiA module generates its residual correlation vector by aggregating the raw correlation vectors. It can be seen as seeking consensus among the correlations with a global receptive field. With the residual correlation map, our attention block with AiA module can be formulated as</p><formula xml:id="formula_3">AttninAttn(Q, K, V) = (Softmax(M + InnerAttn(M))V)W o<label>(3)</label></formula><p>For a multi-head attention block, we share the parameters of the AiA module between the parallel attention heads. It is worth noting that our AiA module can be readily inserted into both self-attenion and cross-attention blocks in a Transformer tracking framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Framework</head><p>With the proposed AiA module, we design a simple yet effective Transformer framework for visual tracking, dubbed AiATrack. Our tracker is comprised of a network backbone, a Transformer architecture, and two prediction heads as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Given the search frame, the initial frame is taken as a long-term reference and an ensemble of several intermediate frames are taken as short-term references. The features of the long-term and short-term references and the search frame are extracted by the network backbone and then reinforced by the Transformer encoder. We also introduce learnable target-background embeddings to distinguish the target from background regions. The Transformer decoder propagates the reference features as well as the target-background embedding maps to the search frame. The output of the Transformer is then fed to a target prediction head and an IoU prediction head for target localization and short-term reference update, respectively. Transformer Architecture. The Transformer encoder is adopted to reinforce the features extracted by the convolutional backbone. For the search frame, we flatten its features to obtain a sequence of feature vectors and add sinusoidal positional encoding as in <ref type="bibr" target="#b6">[7]</ref>. The sequence of feature vectors is then taken by the Transformer encoder as its input. The Transformer encoder consists of several layer stacks, each of which is made up of a multi-head self-attention block and a feed-forward network. The self-attention block serves to capture the dependencies among all feature vectors to enhance the original features, and is equipped with the proposed AiA module. Similarly, this procedure is applied independently to the features of the reference frames using the same encoder.</p><p>The Transformer decoder propagates the reference information from the longterm and short-term references to the search frame. Different from the classical Transformer decoder <ref type="bibr" target="#b51">[52]</ref>, we remove the self-attention block for simplicity and introduce a two-branch cross-attention design as shown in <ref type="figure" target="#fig_2">Fig. 3</ref> to retrieve the target-background information from long-term and short-term references. The long-term branch is responsible for retrieving reference information from the initial frame. Since the initial frame has the most reliable annotation of the tracking target, it is crucial for robust visual tracking. However, as the appearance of the target and the background change through the video, the reference information from the long-term branch may not be up-to-date. This could cause tracker drift in some scenes. To address this problem, we introduce the short-term branch to utilize the information from the frames that are closer to the current frame. The cross-attention blocks of the two branches have the identical structure following the query-key-value design in the vanilla transformer <ref type="bibr" target="#b51">[52]</ref>. We take the features of the search frame as queries and the features of the reference frames as keys. The values are generated by combining the reference features with target-background embedding maps, which will be described below. We also insert our AiA module into cross-attention for better reference information propagation.</p><p>Target-Background Embeddings. To indicate the target and background regions while preserving the contextual information, we introduce a target embedding E tgt ? R C and a background embedding E bg ? R C , both of which are learnable. With E tgt and E bg , we generate target-background embedding maps E ? R HW ?C for the reference frames with a negligible computational cost. Let's consider a location p in a H ? W grid, the embedding assignment is formulated as</p><formula xml:id="formula_4">E(p) = E tgt if p falls in the target region E bg otherwise<label>(4)</label></formula><p>Afterward, we attach the target-background embedding maps to the reference features and feed them to cross-attention blocks as values. The targetbackground embedding maps enrich the reused appearance features by providing contextual cues. Prediction Heads. As described above, our tracker has two prediction heads. The target prediction head is adopted from <ref type="bibr" target="#b57">[58]</ref>. Specifically, the decoded features are fed into a two-branch fully-convolutional network which outputs two probability maps for the top-left and the bottom-right corners of the target bounding box. The predicted box coordinates are then obtained by computing the expectations of the probability distributions of the two corners.</p><p>To adapt the model to the appearance change during tracking, the tracker needs to keep the short-term references up-to-date by selecting reliable references which contain the target. Moreover, considering our embedding assignment mechanism in Eq. 4, the bounding box of the selected reference frame should be as accurate as possible. Inspired by IoU-Net <ref type="bibr" target="#b28">[29]</ref> and ATOM <ref type="bibr" target="#b10">[11]</ref>, for each predicted bounding box, we estimate its IoU with the ground truth via an IoU prediction head. The features inside the predicted bounding box are passed to a Precise RoI Pooling layer whose output is taken by a fully connected network to produce an IoU prediction. The predicted IoU is then used to determine whether to include the search frame as a new short-term reference.</p><p>We train the two prediction heads jointly. The loss of target prediction is defined by the combination of GIoU loss <ref type="bibr" target="#b45">[46]</ref> and L1 loss between the predicted bounding box and the ground truth. The training examples of the IoU prediction head are generated by sampling bounding boxes around the ground truths. The loss of IoU prediction is defined by mean squared error. We refer readers to the supplementary material for more details about training.  <ref type="figure">Fig. 4</ref>. Visualization of the effect of the proposed AiA module. We visualize several representative correlation vectors before and after the refinement by the AiA module. The visualized correlation vectors are reshaped according to the spatial positions of queries. We select the correlation vectors of keys corresponding to the target object regions in the first column. It can be observed that the erroneous correlations are effectively suppressed and the appropriate ones are enhanced with the AiA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tracking with AiATrack</head><p>Given the initial frame with ground truth annotation, we initialize the tracker by cropping the initial frame as long-term and short-term references and precomputing their features and target-background embedding maps. For each subsequent frame, we estimate the IoU score of the bounding box predicted by target prediction head for model update. The update procedure is more efficient than the previous practices <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58]</ref>, as we directly reuse the encoded features. Specifically, if the estimated IoU score of the predicted bounding box is higher than the pre-defined threshold, we generate the target-background embedding map for the current search frame and store the embedding map in a memory cache together with its encoded features. For each new-coming frame, we uniformly sample several short-term reference frames and concatenate their features and embedding maps from the memory cache to update the short-term reference ensemble. The latest reference frame in the memory cache is always sampled as it is closest to the current search frame. The oldest reference frame in the memory cache will be popped out if the maximum cache size is reached.  <ref type="table">Table 1</ref>. State-of-the-art comparison on LaSOT, TrackingNet, and GOT-10k. The best two results are shown in red and blue, respectively. All the trackers listed above adopt ResNet-50 pre-trained on ImageNet-1k as network backbone and the results on GOT-10k are obtained without additional training data for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Our experiments are conducted with NVIDIA GeForce RTX 2080 Ti. We adopt ResNet-50 <ref type="bibr" target="#b21">[22]</ref> as network backbone which is initialized by the parameters pretrained on ImageNet-1k <ref type="bibr" target="#b12">[13]</ref>. We crop a search patch which is 5 2 times of the target box area from the search frame and resize it to a resolution of 320 ? 320 pixels. The same cropping procedure is also applied to the reference frames. The cropped patches are then down-sampled by the network backbone with a stride of 16. The Transformer encoder consists of 3 layer stacks and the Transformer decoder consists of only 1 layer. The multi-head attention blocks in our tracker have 4 heads with channel width of 256. The inner AiA module reduces the channel dimension of queries and keys to 64. The FFN blocks have 1024 hidden units. Each branch of the target prediction head is comprised of 5 Conv-BN-ReLU layers. The IoU prediction head consists of 3 Conv-BN-ReLU layers, a PrPool <ref type="bibr" target="#b28">[29]</ref> layer with pooling size of 3 ? 3 and 2 fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Comparisons</head><p>We compare our tracker with several state-of-the-art trackers on three prevailing large-scale benchmarks (LaSOT <ref type="bibr" target="#b16">[17]</ref>, TrackingNet and <ref type="bibr" target="#b44">[45]</ref> and GOT-10k <ref type="bibr" target="#b24">[25]</ref>) and three commonly used small-scale datasets (NfS30 <ref type="bibr" target="#b29">[30]</ref>, OTB100 <ref type="bibr" target="#b54">[55]</ref> and UAV123 <ref type="bibr" target="#b43">[44]</ref>  <ref type="table">Table 2</ref>. State-of-the-art comparison on commonly used small-scale datasets in terms of AUC score. The best two results are shown in red and blue. <ref type="figure">Fig. 5</ref>. Attribute-based evaluation on La-SOT in terms of AUC score. Our tracker achieves the best performance on all attribute splits while making a significant improvement in various kinds of scenarios such as background clutter, camera motion, and deformation. Axes of each attribute have been normalized.</p><p>previous best tracker KeepTrack <ref type="bibr" target="#b40">[41]</ref> by 1.9% in area-under-the-curve (AUC) and 3.6% in precision while running much faster (see Tab. 2). We also provide an attribute-based evaluation in <ref type="figure">Fig. 5</ref> for further analysis. Our method achieves the best performance on all attribute splits. The results demonstrate the promising potential of our approach for long-term visual tracking. TrackingNet. TrackingNet <ref type="bibr" target="#b44">[45]</ref> is a large-scale short-term tracking benchmark. It provides 511 testing video sequences without publicly available ground truths. Our performance reported in Tab. 1 is obtained from the online evaluation server. Our approach achieve 82.7% in AUC score and 87.8% in normalized precision score, surpassing all previously published trackers. It demonstrates that our approach is also very competitive for short-term tracking scenarios. GOT-10k. To ensure zero overlaps of object classes between training and testing, we follow the one-shot protocol of GOT-10k <ref type="bibr" target="#b24">[25]</ref> and only train our model with the specified subset. The testing ground truths are also withheld and our result is evaluated by the official server. As demonstrated in Tab. 1, our tracker improves all metrics by a large margin, e.g. 2.3% in success rate compared with STARK <ref type="bibr" target="#b57">[58]</ref> and TrDiMP <ref type="bibr" target="#b52">[53]</ref>, which indicates that our tracker also has a good generalization ability to the objects of unseen classes. NfS30. Need for Speed (NfS) <ref type="bibr" target="#b29">[30]</ref> is a dataset that contains 100 videos with fast-moving objects. We evaluate the proposed tracker on its commonly used version NfS30. As reported in Tab. 2, our tracker improves the AUC score by 2.7% over STARK <ref type="bibr" target="#b57">[58]</ref> and performs the best among the benchmarked trackers. OTB100. Object Tracking Benchmark (OTB) <ref type="bibr" target="#b54">[55]</ref> is a pioneering benchmark for evaluating visual tracking algorithms. However, in recent years, it has been noted that this benchmark has become highly saturated <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b40">41]</ref>. Still, the results in Tab. 2 show that our method can achieve comparable performance with state-of-the-art trackers.</p><p>UAV123. Finally, we report our results on UAV123 <ref type="bibr" target="#b43">[44]</ref> which includes 123 video sequences captured from a low-altitude unmanned aerial vehicle perspective. As shown in Tab. 2, our tracker outperforms KeepTrack <ref type="bibr" target="#b40">[41]</ref> by 0.9% and is suitable for UAV tracking scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>To validate the importance of the proposed components in our tracker, we conduct ablation studies on LaSOT testing set and its new extension set <ref type="bibr" target="#b15">[16]</ref>, totaling 430 diverse videos. We summarize the results in Tab. 3, Tab. 4 and Tab. 5.</p><p>Target-Background Embeddings. In our tracking framework, the reference frames not only contain features from target regions but also include a large proportion of features from background regions. We implement three variants of our method to demonstrate the necessity of keeping the context and the importance of the proposed target-background embeddings. As shown in the 1st part of Tab. 3, we start from the variant (a), which is the implementation of the proposed tracking framework with both the target-background embeddings and the AiA module removed. Based on the variant (a), the variant (b) further discards the reference features of background regions with a mask. The variant (c) attaches the target-background embeddings to the reference features. Compared with the variant (a), the performance of the variant (b) drops drastically, which suggests that context is helpful for visual tracking. With the proposed target-background embeddings, the variant (c) can consistently improve the performance over the variant (a) in all metrics. This is because the proposed target-background embeddings further provide cues for distinguishing the target and background regions while preserving the contextual information.</p><p>Long-Term and Short-Term Branch. As discussed in Sec. 3.2, it is important to utilize an independent short-term reference branch to deal with the appearance change during tracking. To validate this, we implement a variant (d) by removing the short-term branch from the variant (c). We also implement a variant (e) by adopting a single cross-attention branch instead of the proposed two-branch design for the variant (c). Note that we keep the IoU prediction head for these two variants during training to eliminate the possible effect of IoU prediction on feature representation learning. From the 2nd part of Tab. 3, we can observe that the performance of variant (d) is worse than variant (c), which suggests the necessity of using short-term references. Meanwhile, compared with variant (c), the performance of variant (e) also drops, which validates the necessity to use two separate branches for the long-term and short-term references. This is because the relatively unreliable short-term references may disturb the robust long-term reference and therefore degrade its contribution.</p><p>Effectiveness of the AiA Module. We explore several ways of applying the proposed AiA module to the proposed Transformer tracking framework. The variant (f) inserts the AiA module into self-attention blocks in the Transformer encoder. Compared with the variant (c), the performance can be greatly improved on the two subsets of LaSOT. The variant (g) inserts the AiA module into the cross-attention blocks in the Transformer decoder, which also brings a   <ref type="table">Table 3</ref>. Ablative experiments about different components in the proposed tracker. We use ? to denote the basic framework and ? to denote our final model with AiA. The best results in each part of the table are marked in bold. consistent improvement. These two variants demonstrate that the AiA module generalizes well to both self-attention blocks and cross-attention blocks. When we apply the AiA module to both self-attention blocks and cross-attention blocks, i.e. the final model (i), the performance on the two subsets of LaSOT can be improved by 1.7?2.7% in all metrics compared with the basic framework (c).</p><p>Recall that we introduce positional encoding to the proposed AiA module (see <ref type="figure" target="#fig_1">Fig. 2</ref>). To verify its importance, we implement a variant (h) by removing positional encoding from the variant (i). We can observe that the performance drops accordingly. This validates the necessity of positional encoding, as it provides spatial cues for consensus seeking in the AiA module. More analysis about the components of the AiA module are provided in the supplementary material. Superiority of the AiA Module. One may concern that the performance gain of the AiA module is brought by purely adding extra parameters. Thus, we design two other variants to demonstrate the superiority of the proposed module.</p><p>First, we implement a variant of our basic framework where each Attention-Add-Norm block is replaced by two cascaded ones. From the comparison of the first two rows in Tab. 4, we can observe that simply increasing the number of attention blocks in our tracking framework does not help much, which demonstrates that our AiA module can further unveil the potential of the tracker.</p><p>We also implement a variant of our final model by replacing the proposed inner attention with a convolutional bottleneck <ref type="bibr" target="#b21">[22]</ref>, which is designed to have a similar computational cost. From the comparison of the last two rows in Tab. 4, we can observe that inserting a convolutional bottleneck can also bring positive effects, which suggests the necessity of correlation refinement. However, the convolutional bottleneck can only perform a fixed aggregation in each local neighborhood, while our AiA module has a global receptive field with dynamic weights determined by the interaction among correlation vectors. As a result, our AiA module can seek consensus more flexibly and further boost the performance.  <ref type="table">Table 5</ref>. Impact of ensemble size in terms of AUC score and the running speed. All of our ablative experiments are conducted with ensemble size as 3 by default.</p><p>Visualization Perspective. In <ref type="figure">Fig. 4</ref>, we visualize correlation maps from the perspective of keys. This is because we consider the correlations of one key with queries as a correlation vector. Thus, the AiA module performs refinement by seeking consensus among the correlation vectors of keys. Actually, refining the correlations from the perspective of queries also works well, achieving 68.5% in AUC score on LaSOT. Short-Term Reference Ensemble. We also study the impact of the ensemble size in the short-term branch. Tab. 5 shows that by increasing the ensemble size from 1 to 3, the performance can be stably improved. Further increasing the ensemble size does not help much and has little impact on the running speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present an attention in attention (AiA) module to improve the attention mechanism for Transformer visual tracking. The proposed AiA module can effectively enhance appropriate correlations and suppress erroneous ones by seeking consensus among all correlation vectors. Moreover, we present a streamlined Transformer tracking framework, dubbed AiATrack, by introducing efficient feature reuse and embedding assignment mechanisms to fully utilize temporal references. Extensive experiments demonstrate the superiority of the proposed method. We believe that the proposed AiA module could also be beneficial in other related tasks where the Transformer architecture can be applied to perform feature aggregation and information propagation, such as video object segmentation <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref>, video object detection <ref type="bibr" target="#b22">[23]</ref> and multi-object tracking <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b63">64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AiATrack: Attention in Attention for Transformer Visual Tracking (Supplementary Material)</head><p>The supplementary material provides additional details about the experiments and analyses of the proposed method.</p><p>6 Additional Experiment Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Target Prediction</head><p>To make the tracking procedure in an end-to-end manner without tedious postprocessing, we adopt the anchor-free prediction head proposed in <ref type="bibr" target="#b57">[58]</ref>, which outputs the probability maps P tl (x, y) and P br (x, y) for the top-left and the bottom-right bounding box corners. The coordinates x tl , y tl , x br , y br of the predicted bounding box are then obtained by</p><formula xml:id="formula_5">x tl = H y=0 W x=0 x ? P tl (x, y), y tl = H y=0 W x=0 y ? P tl (x, y) (5) x br = H y=0 W x=0 x ? P br (x, y), y br = H y=0 W x=0 y ? P br (x, y)<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Training Objective</head><p>With the predicted bounding box b and predicted IoU i, the whole network is jointly trained by minimizing prediction errors. The bounding box prediction loss is defined as the combination of GIoU loss <ref type="bibr" target="#b45">[46]</ref> and L1 loss. Together with the IoU prediction loss, the loss function can be written as</p><formula xml:id="formula_6">L = ? giou L giou (b, b) + ? l1 ?b ? b? 1 + ? mse (i ? i) 2<label>(7)</label></formula><p>where b and i represent the ground truths of bounding box and IoU respectively and ? giou , ? l1 , ? mse are the trade-off weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Training Strategy</head><p>Similar to previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58]</ref>, we utilize the training splits of LaSOT <ref type="bibr" target="#b16">[17]</ref>, TrackingNet <ref type="bibr" target="#b44">[45]</ref>, GOT-10k <ref type="bibr" target="#b24">[25]</ref>, and COCO <ref type="bibr" target="#b36">[37]</ref> for offline training. As for the COCO image dataset, we apply data augmentation to generate synthetic video clips of diverse classes. During training, we randomly sample the search frame and reference frames such that the index of the search frame is larger than the indexes of reference frames. For training efficiency, we only sample one frame as the short-term reference. We also apply random affine transformations to jitter the sizes and locations of the short-term reference frame and search frame to simulate real tracking scenarios and avoid the influence of absolute   <ref type="table">Table 6</ref>. Study about the different structures of the AiA module. LN denotes applying layer normalization to the value. LT denotes applying linear transformation to the value. IC denotes using identical connection after the correlation aggregation.</p><p>position bias caused by padding <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b65">66]</ref>. The network is trained with the AdamW optimizer <ref type="bibr" target="#b37">[38]</ref>. The learning rate is 1e-5 for the network backbone and 1e-4 for the other components. It decays by a factor of 10 during training. The parameters of the first convolutional layer and the first stage in the ResNet-50 <ref type="bibr" target="#b21">[22]</ref> backbone are fixed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Different Structures of the AiA Module</head><p>Besides variant (h) in the paper, we also explore other structures of the AiA module, where the following components are studied: (1) Layer normalization applied to the value. (2) Linear transformation applied to the value. (3) Identical connection after the correlation aggregation. To evaluate their effect, we design two other structures of the AiA module, i.e. AiAv2 and AiAv3. The differences between these structures are shown in <ref type="figure">Fig. 6</ref>. Note that AiAv1 is the structure we implement in AiATrack and AiAv3 is a typical self-attention structure in the vanilla Transformer <ref type="bibr" target="#b51">[52]</ref>.  <ref type="table">Table 7</ref>. State-of-the-art comparison on VOT2020.</p><p>From the results in Tab. 6, we can observe that the layer normalization and the identical connection are not key components in our AiA module. Applying linear transformation to the value can further improve the performance, but we remove it for the trade-off between performance and computational cost. Besides the observations above, all the experimental results validate the effectiveness of correlation refinement in the conventional attention mechanism with an extra attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Results on VOT</head><p>Different from previous reset-based evaluation protocol <ref type="bibr" target="#b31">[32]</ref>, VOT2020 <ref type="bibr" target="#b30">[31]</ref> proposes a new anchor-based evaluation protocol which is more reasonable. The same as STARK <ref type="bibr" target="#b57">[58]</ref> and DualTFR <ref type="bibr" target="#b55">[56]</ref>, we use Alpha-Refine <ref type="bibr" target="#b58">[59]</ref> to generate masks for evaluation since the ground truths of VOT2020 are annotated by the segmentation masks. The overall performance is ranked by the Expected Average Overlap (EAO). As shown in Tab. 7, our tracker exhibits very competitive performance, outperforming STARK with a margin of 5% in terms of EAO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Additional Visualization Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Attribute Analysis</head><p>We also provide detailed attribute analysis on LaSOT <ref type="bibr" target="#b16">[17]</ref>. <ref type="figure" target="#fig_6">Fig. 7</ref> shows that our tracker has an encouraging performance in various kinds of scenarios like background clutter, camera motion, and deformation. The results suggest the great potential of the proposed method when dealing with challenging scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Qualitative Comparisons</head><p>To qualitatively compare our tracker with the state-of-the-art trackers, we visualize our tracking results with two recent representative trackers: KeepTrack <ref type="bibr" target="#b40">[41]</ref> and STARK <ref type="bibr" target="#b57">[58]</ref>. <ref type="figure" target="#fig_7">Fig. 8</ref> shows the tracking outputs for these trackers on some challenging video examples.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>takes queries and arXiv:2207.09603v2 [cs.CV] 22 Jul 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Structures of conventional attention and the proposed attention in attention (AiA) module. denotes matrix multiplication and denotes element-wise addition. The numbers beside arrows are feature dimensions which do not include the batch size. Matrix transpose operations are omitted for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of the proposed Transformer tracking framework. The self-attention and cross-attention blocks are all equipped with the proposed AiA module. Note that only the components on the light green background need to be computed during the inference phase as described in Sec. 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>AiA in self-attn 68.6 78.7 72.9 46.2 54.4 53.4 (g) AiA in cross-attn 67.5 77.9 71.8 46.2 54.2 53.3 (h) w/o pos in both 68.0 78.2 72.7 46.2 54.0 53.0 (i) AiA in both ? 68.7 79.3 73.7 46.8 54.4 54.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig</head><label></label><figDesc>Fig. 6. Detailed illustration of the differences between different structures of the AiA module. denotes matrix multiplication and denotes element-wise addition. The numbers beside arrows are feature dimensions which do not include the batch size. Matrix transpose operations are omitted for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Attribute analysis on LaSOT. AUC scores are showed in the legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative comparisons with two representative state-of-the-art trackers on 8 challenging sequences: bird-17, goldfish-8, sepia-13, shark-2, sheep-3, squirrel-8, tiger-4, turtle-8. Frame indexes are given on the top-left of each figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>79.4 73.8 82.7 87.8 80.4 69.6 63.2 80.0 STARK-ST50 [58] ICCV2021 66.4 76.3 71.2 81.3 86.1 78.1 68.0 62.3 77.7</figDesc><table><row><cell>Tracker</cell><cell>Source</cell><cell cols="6">LaSOT [17] TrackingNet [45] GOT-10k [25] AUC PNorm P AUC PNorm P AO SR0.75 SR0.5</cell></row><row><cell cols="5">AiATrack 69.0 KeepTrack [41] ICCV2021 67.1 77.2 70.2 -Ours</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DTT [61]</cell><cell cols="2">ICCV2021 60.1</cell><cell>-</cell><cell cols="4">-79.6 85.0 78.9 63.4 51.4 74.9</cell></row><row><cell>TransT [8]</cell><cell cols="7">CVPR2021 64.9 73.8 69.0 81.4 86.7 80.3 67.1 60.9 76.8</cell></row><row><cell>TrDiMP [53]</cell><cell cols="2">CVPR2021 63.9</cell><cell>-</cell><cell cols="4">61.4 78.4 83.3 73.1 67.1 58.3 77.7</cell></row><row><cell>TrSiam [53]</cell><cell cols="2">CVPR2021 62.4</cell><cell>-</cell><cell cols="4">60.0 78.1 82.9 72.7 66.0 57.1 76.6</cell></row><row><cell>KYS [4]</cell><cell cols="3">ECCV2020 55.4 63.3</cell><cell cols="4">-74.0 80.0 68.8 63.6 51.5 75.1</cell></row><row><cell cols="5">Ocean-online [67] ECCV2020 56.0 65.1 56.6 -</cell><cell>-</cell><cell cols="2">-61.1 47.3 72.1</cell></row><row><cell cols="3">Ocean-offline [67] ECCV2020 52.6</cell><cell>-</cell><cell>52.6 -</cell><cell>-</cell><cell cols="2">-59.2</cell><cell>-</cell><cell>69.5</cell></row><row><cell cols="8">PrDiMP50 [12] CVPR2020 59.8 68.8 60.8 75.8 81.6 70.4 63.4 54.3 73.8</cell></row><row><cell cols="4">SiamAttn [62] CVPR2020 56.0 64.8</cell><cell cols="2">-75.2 81.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DiMP50 [3]</cell><cell cols="7">ICCV2019 56.9 65.0 56.7 74.0 80.1 68.7 61.1 49.2 71.7</cell></row><row><cell cols="8">SiamRPN++ [34] CVPR2019 49.6 56.9 49.1 73.3 80.0 69.4 51.7 32.5 61.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). The results are summarized in Tab. 1 and Tab. 2. LaSOT. LaSOT<ref type="bibr" target="#b16">[17]</ref> is a densely annotated large-scale dataset, containing 1400 long-term video sequences. As shown in Tab. 1, our approach outperforms the</figDesc><table><row><cell>Tracker</cell><cell cols="6">SiamRPN++ PrDiMP50 TransT STARK-ST50 KeepTrack AiATrack [34] [12] [8] [58] [41] (Ours)</cell></row><row><cell>NfS30 [30]</cell><cell>50.2</cell><cell>63.5</cell><cell>65.7</cell><cell>65.2</cell><cell>66.4</cell><cell>67.9</cell></row><row><cell>OTB100 [55]</cell><cell>69.6</cell><cell>69.6</cell><cell>69.4</cell><cell>68.5</cell><cell>70.9</cell><cell>69.6</cell></row><row><cell>UAV123 [44]</cell><cell>61.3</cell><cell>68.0</cell><cell>69.1</cell><cell>69.1</cell><cell>69.7</cell><cell>70.6</cell></row><row><cell>Speed (fps)</cell><cell>35</cell><cell>30</cell><cell>50</cell><cell>42</cell><cell>18</cell><cell>38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>AiA cascade 67.1 77.0 71.7 44.6 52.9 51.6 40 conv in both ? 67.9 78.2 72.8 46.0 53.4 52.8 39 AiA in both ? 68.7 79.3 73.7 46.8 54.4 54.2 38 Superiority comparison with the tracking performance and the running speed.</figDesc><table><row><cell>Modification</cell><cell cols="2">Correlation Refinement AUC PNorm P AUC PNorm P (fps) LaSOT [17] LaSOTExt [16] Speed</cell></row><row><cell cols="2">w/o AiA  ? w/o Ensemble Size 1 ?</cell><cell>67.0 77.0 71.3 44.7 52.7 51.5 44 2 3 4 5 6 10</cell></row><row><cell cols="3">LaSOT [17] 66.8 68.1 68.7 69.0 68.2 68.6 68.9</cell></row><row><cell cols="3">LaSOTExt [16] 44.9 46.3 46.8 46.2 47.4 47.7 47.1</cell></row><row><cell cols="3">Speed (fps) 39 39 38</cell><cell>38 38 38 34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. 6. Detailed illustration of the differences between different structures of the AiA module.denotes matrix multiplication and denotes element-wise addition. The numbers beside arrows are feature dimensions which do not include the batch size. Matrix transpose operations are omitted for brevity.</figDesc><table><row><cell>Structure</cell><cell cols="2">Modification LN LT IC AUC PNorm P AUC PNorm P LaSOT [17] LaSOTExt [16]</cell></row><row><cell cols="2">AiAv1 ?</cell><cell>? 68.7 79.3 73.7 46.8 54.4 54.2</cell></row><row><cell>AiAv2</cell><cell></cell><cell>68.8 79.3 73.6 46.7 54.5 53.8</cell></row><row><cell cols="2">AiAv3 ? ?</cell><cell>69.2 79.6 74.3 48.4 56.6 56.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Know your surroundings: Exploiting scene information for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gms: Grid-based motion statistics for fast, ultra-robust feature correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hift: Hierarchical feature transformer for aerial tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cats: Cost aggregation transformers for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-performance longterm tracking with meta-updater</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sstvos: Sparse spatiotemporal transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality large-scale single object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Handcrafted and deep trackers: Recent visual object tracking approaches and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stmtrack: Template-free visual tracking with space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video object segmentation using global and instance embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph attention tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end video object detection with spatial-temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08248</idno>
		<title level="m">How much position information do convolutional neural networks encode? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiani</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The eighth visual object tracking vot2020 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>K?m?r?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">?</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luke?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Drbohlav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The seventh visual object tracking vot2019 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>?cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Drbohlav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Siamese network with interactive transformer for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13983</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Correspondence networks with adaptive neighbourhood consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Costain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Howard-Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint inductive and transductive learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning for visual tracking: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Marvasti-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ghanei-Yakhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning target candidate association to keep track of what not to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02702</idno>
		<title level="m">Trackformer: Multiobject tracking with transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Convolutional hough matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Trackingnet: A largescale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient neighbourhood consensus networks via submanifold sparse convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scramsac: Improving ransac&apos;s efficiency with a spatial consistency filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Matching local self-similarities across images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
		<title level="m">Transtrack: Multiple object tracking with transformer</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transformer meets tracker: Exploiting temporal context for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning tracking representations via dual-branch fully transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Siamese transformer pyramid networks for real-time uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evangeliou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsoukalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tzes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal transformer for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Alpha-refine: Boosting tracking performance by precise bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Associating objects with transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">High-performance discriminative tracking with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deformable siamese attention networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning the model update for siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06864</idno>
		<title level="m">Bytetrack: Multi-object tracking by associating every detection box</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Toward accurate pixelwise object tracking via attention retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ocean: Object-aware anchor-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
