<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Realistic Evaluation of Transductive Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Veilleux</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Boudiaf</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Piantanida</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><forename type="middle">Ben</forename><surname>Ayed</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ts</forename><surname>Montreal</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">?TS Montreal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">?TS Montreal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">L2S, CentraleSup?lec CNRS Universit? Paris-Saclay</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Realistic Evaluation of Transductive Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transductive inference is widely used in few-shot learning, as it leverages the statistics of the unlabeled query set of a few-shot task, typically yielding substantially better performances than its inductive counterpart. The current few-shot benchmarks use perfectly class-balanced tasks at inference. We argue that such an artificial regularity is unrealistic, as it assumes that the marginal label probability of the testing samples is known and fixed to the uniform distribution. In fact, in realistic scenarios, the unlabeled query sets come with arbitrary and unknown label marginals. We introduce and study the effect of arbitrary class distributions within the query sets of few-shot tasks at inference, removing the class-balance artefact. Specifically, we model the marginal probabilities of the classes as Dirichlet-distributed random variables, which yields a principled and realistic sampling within the simplex. This leverages the current few-shot benchmarks, building testing tasks with arbitrary class distributions. We evaluate experimentally state-of-the-art transductive methods over 3 widely used data sets, and observe, surprisingly, substantial performance drops, even below inductive methods in some cases. Furthermore, we propose a generalization of the mutual-information loss, based on ?-divergences, which can handle effectively class-distribution variations. Empirically, we show that our transductive ?-divergence optimization outperforms state-of-the-art methods across several data sets, models and few-shot settings. Our code is publicly available at https://github.com/oveilleux/Realistic_Transductive_Few_Shot.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning models are widely dominating the field. However, their outstanding performances are often built upon training on large-scale labeled data sets, and the models are seriously challenged when dealing with novel classes that were not seen during training. Few-shot learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> tackles this challenge, and has recently triggered substantial interest within the community. In standard few-shot settings, a model is initially trained on large-scale data containing labeled examples from a set of base classes. Then, supervision for a new set of classes, which are different from those seen in the base training, is restricted to just one or a few labeled samples per class. Model generalization is evaluated over few-shot tasks. Each task includes a query set containing unlabeled samples for evaluation, and is supervised by a support set containing a few labeled samples per new class.</p><p>The recent few-shot classification literature is abundant and widely dominated by convoluted metalearning and episodic-training strategies. To simulate generalization challenges at test times, such strategies build sequences of artificially balanced few-shot tasks (or episodes) during base training, each containing both query and support samples. Widely adopted methods within this paradigm include: Prototypical networks <ref type="bibr" target="#b3">[4]</ref>, which optimizes the log-posteriors of the query points within each base-training episode; Matching networks <ref type="bibr" target="#b2">[3]</ref>, which expresses the predictions of query points as linear functions of the support labels, while deploying episodic training and memory architectures; MAML (Model-Agnostic Meta-Learning) <ref type="bibr" target="#b4">[5]</ref>, which encourages a model to be "easy" to fine-tune; and the meta-learner in <ref type="bibr" target="#b5">[6]</ref>, which prescribes optimization as a model for few-shot learning. These popular methods have recently triggered a large body of few-shot learning literature, for instance, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, to list a few.</p><p>Recently, a large body of works investigated transductive inference for few-shot tasks, e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, among many others, showing substantial improvements in performances over inductive inference 2 . Also, as discussed in <ref type="bibr" target="#b23">[24]</ref>, most meta-learning approches rely critically on transductive batch normalization (TBN) to achieve competitive performances, for instance, the methods in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, among others. Adopted initially in the widely used MAML <ref type="bibr" target="#b4">[5]</ref>, TBN performs normalization using the statistics of the query set of a given few-shot task, and yields significant increases in performances <ref type="bibr" target="#b23">[24]</ref>. Therefore, due to the popularity of MAML, several meta-learning techniques have used TBN. The transductive setting is appealing for few-shot learning, and the outstanding performances observed recently resonate well with a well-known fact in classical transductive inference <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>: On small labeled data sets, transductive inference outperforms its inductive counterpart. In few-shot learning, transductive inference has access to exactly the same training and testing data as its inductive counterpart <ref type="bibr" target="#b2">3</ref> . The difference is that it classifies all the unlabeled query samples of each single few-shot task jointly, rather than one sample at a time.</p><p>The current few-shot benchmarks use perfectly class-balanced tasks at inference: For each task used at testing, all the classes have exactly the same number of samples, i.e., the marginal probability of the classes is assumed to be known and fixed to the uniform distribution across all tasks. This may not reflect realistic scenarios, in which testing tasks might come with arbitrary class proportions. For instance, the unlabeled query set of a task could be highly imbalanced. In fact, using perfectly balanced query sets for benchmarking the models assumes exact knowledge of the marginal distributions of the true labels of the testing points, but such labels are unknown. This is, undeniably, an unrealistic assumption and an important limitation of the current few-shot classification benchmarks and datasets. Furthermore, this suggests that the recent progress in performances might be, in part, due to class-balancing priors (or biases) that are encoded in state-of-the-art transductive models. Such priors might be implicit, e.g., through carefully designed episodic-training schemes and specialized architectures, or explicit, e.g., in the design of transductive loss functions and constraints. For instance, the best performing methods in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> use explicit label-marginal terms or constraints, which strongly enforce perfect class balance within the query set of each task. In practice, those class-balance priors and assumptions may limit the applicability of the existing few-shot benchmarks and methods. In fact, our experiments show that, over few-shot tasks with random class balance, the performances of state-of-the-art methods may decrease by margins. This motivates re-considering the existing benchmarks and re-thinking the relevance of class-balance biases in state-of-the-art methods.</p><p>Contributions We introduce and study the effect of arbitrary class distributions within the query sets of few-shot tasks at inference. Specifically, we relax the assumption of perfectly balanced query sets and model the marginal probabilities of the classes as Dirichlet-distributed random variables. We devise a principled procedure for sampling simplex vectors from the Dirichlet distribution, which is widely used in Bayesian statistics for modeling categorical events. This leverages the current few-shot benchmarks by generating testing tasks with arbitrary class distributions, thereby reflecting realistic scenarios. We evaluate experimentally state-of-the-art transductive few-shot methods over 3 widely used datasets, and observe that the performances decrease by important margins, albeit at various degrees, when dealing with arbitrary class distributions. In some cases, the performances drop even below the inductive baselines, which are not affected by class-distribution variations (as they do not use the query-set statistics). Furthermore, we propose a generalization of the transductive mutualinformation loss, based on ?-divergences, which can handle effectively class-distribution variations. Empirically, we show that our transductive ?-divergence optimization outperforms state-of-the-art few-shot methods across different data sets, models and few-shot settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Standard few-shot settings</head><p>Base training Assume that we have access to a fully labelled base dataset</p><formula xml:id="formula_0">D base = {x i , y i } N base i=1</formula><p>, where x i ? X base are data points in an input space X base , y i ? {0, 1} |Y base | the one-hot labels, and Y base the set of base classes. Base training learns a feature extractor f ? : X ? Z, with ? its learnable parameters and Z a (lower-dimensional) feature space. The vast majority of the literature adopts episodic training at this stage, which consists in formatting D base as a series of tasks (=episodes) in order to mimic the testing stage, and train a meta-learner to produce, through a differentiable process, predictions for the query set. However, it has been repeatedly demonstrated over the last couple years that a standard supervised training followed by standard transfer learning strategies actually outperforms most meta-learning based approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>. Therefore, we adopt a standard cross-entropy training in this work.</p><p>Testing The model is evaluated on a set of few-shot tasks, each formed with samples from</p><formula xml:id="formula_1">D test = {x i , y i } Ntest i=1 , where y i ? {0, 1} |Ytest| such that Y base ? Y test = ?.</formula><p>Each task is composed of a labelled support set S = {x i , y i } i?I S and an unlabelled query set Q = {x i } i?I Q , both containing instances only from K distinct classes randomly sampled from Y test , with K &lt; |Y test |. Leveraging a feature extractor f ? pre-trained on the base data, the objective is to learn, for each few-shot task, a classifier f W : Z ? ? K , with W the learnable parameters and ? K = {y ? [0, 1] K / k y k = 1} the (K ? 1)-simplex. To simplify the equations for the rest of the paper, we use the following notations for the posterior predictions of each i ? I S ? I Q and for the class marginals within Q:</p><formula xml:id="formula_2">p ik = f W (f ? (x i )) k = P(Y = k|X = x i ; W , ?) and p k = 1 |I Q | i?I Q p ik = P(Y Q = k; W , ?),</formula><p>where X and Y are the random variables associated with the raw features and labels, respectively; X Q and Y Q means restriction of the random variable to set Q. The end goal is to predict the classes of the unlabeled samples in Q for each few-shot task, independently of the other tasks. A large body of works followed a transductive-prediction setting, e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, among many others. Transductive inference performs a joint prediction for all the unlabeled query samples of each single few-shot task, thereby leveraging the query-set statistics. On the current benchmarks, tranductive inference often outperforms substantially its inductive counterpart (i.e., classifying one sample at a time for a given task). Note that our method is agnostic to the specific choice of classifierf W , whose parameters are learned at inference. In the experimental evaluations of our method, similarly to <ref type="bibr" target="#b22">[23]</ref>, we used p ik ? exp(? ? 2 w k ? z i 2 ), with W := (w 1 , . . . , w K ),</p><formula xml:id="formula_3">z i = f ? (xi) f ? (xi) 2</formula><p>, ? is a temperature parameter and base-training parameters ? are fixed 4 .</p><p>Perfectly balanced vs imbalanced tasks In standard K-way few-shot settings, the support and query sets of each task T are formed using the following procedure: (i) Randomly sample K classes Y T ? Y test ; (ii) For each class k ? Y T , randomly sample n S k support examples, such that n S k = |S|/K; and (iii) For each class k ? Y T , randomly sample n Q k query examples, such that n Q k = |Q|/K. Such a setting is undeniably artificial as we assume S and Q have the same perfectly balanced class distribution. Several recent works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> studied class imbalance exclusively on the support set S. This makes sense as, in realistic scenarios, some classes might have more labelled samples than others. However, even these works rely on the assumption that query set Q is perfectly balanced. We argue that such an assumption is not realistic, as one typically has even less control over the class distribution of Q than it has over that of S. For the labeled support S, the class distribution is at least fully known and standard strategies from imbalanced supervised learning could be applied <ref type="bibr" target="#b37">[38]</ref>. This does not hold for Q, for which we need to make class predictions at testing time and whose class distribution is unknown. In fact, generating perfectly balanced tasks at test times for benchmarking the models assumes that one has access to the unknown class distributions of the query points, which requires access to their unknown labels. More importantly, artificial balancing of Q is implicitly or explicitly encoded in several transductive methods, which use the query set statistics to make class predictions, as will be discussed in section 4. 3 Dirichlet-distributed class marginals for few-shot query sets</p><p>Standard few-shot settings assume that p k , the proportion of the query samples belonging to a class k within a few-shot task, is deterministic (fixed) and known priori: p k = n Q k /|Q| = 1/K, for all k and all few-shot tasks. We propose to relax this unrealistic assumption, and to use the Dirichlet distribution to model the proportions (or marginal probabilities) of the classes in few-shot query sets as random variables. Dirichlet distributions are widely used in Bayesian statistics to model K-way categorical events <ref type="bibr" target="#b4">5</ref> . The domain of the Dirichlet distribution is the set of K-dimensional discrete distributions, i.e., the set of vectors in</p><formula xml:id="formula_4">(K ? 1)-simplex ? K = {p ? [0, 1] K | k p k = 1}.</formula><p>Let P k denotes a random variable associated with class probability p k , and P the random simplex vector given by P = (P 1 , . . . , P K ). We assume that P follows a Dirichlet distribution with parameter vector a = (a 1 , . . . , a K ) ? R K : P ? Dir(a). The Dirichlet distribution has the following density function:</p><formula xml:id="formula_5">f Dir (p; a) = 1 B(a) K k=1 p a k ?1 k</formula><p>for p = (p 1 , . . . , p K ) ? ? K , with B denoting the multivariate Beta function, which could be expressed with the Gamma function <ref type="bibr" target="#b5">6</ref> </p><formula xml:id="formula_6">: B(a) = K k=1 ?(a k ) ?( K k=1 ? k )</formula><p>. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the Dirichlet density for K = 3, with a 2-simplex support represented with an equilateral triangle, whose vertices are probability vectors (1, 0, 0), (0, 1, 0) and (0, 0, 1). We show the density for a = a1 K , with 1 K the K-dimensional vector whose all components are equal to 1 and concentration parameter a equal to 0.5, 2, 5 and 50. Note that the limiting case a ? +? corresponds to the standard settings with perfectly balanced tasks, where only uniform distribution, i.e., the point in the middle of the simplex, could occur as marginal distribution of the classes.</p><p>The following result, well-known in the literature of random variate generation <ref type="bibr" target="#b38">[39]</ref>, suggests that one could generate samples from the multivariate Dirichlet distribution via simple and standard univariate Gamma generators. Theorem 3.1. ([39, p. 594]) Let N 1 , . . . , N K be K independent Gamma-distributed random variables with parameters a k : N k ? Gamma(a k ), i.e., the probability density of N k is univariate Gamma 7 , with shape parameter a k . Let P k = N k K k=1 N k , k = 1, . . . , K. Then, P = (P 1 , . . . , P K ) is Dirichlet distributed: P ? Dir(a), with a = (a 1 , . . . , a K ).</p><p>A proof based on the Jacobian of random-variable transformations P k = N k K k=1 N k , k = 1, . . . , K, could be found in <ref type="bibr" target="#b38">[39]</ref>, p. 594. This result prescribes the following simple procedure for sampling random simplex vectors (p 1 , . . . , p K ) from the multivariate Dirichlet distribution with parameters a = (a 1 , . . . , a K ): First, we draw K independent random samples (n 1 , . . . , n K ) from Gamma distributions, with each n k drawn from univariate density f Gamma (n; a k ); To do so, one could use standard random generators for the univariate Gamma density; see Chapter 9 in <ref type="bibr" target="#b38">[39]</ref>. Then, we set p k = n k K k=1 n k . This enables to generate randomly n Q k , the number of samples of class k within query set Q, as follows: n Q k is the closest integer to p k |Q| such that k n Q k = |Q|. <ref type="bibr" target="#b4">5</ref> Note that the Dirichlet distribution is the conjugate prior of the categorical and multinomial distributions. <ref type="bibr" target="#b5">6</ref> The Gamma function is given by: ?(a) = ? 0 t a?1 exp(?t)dt for a &gt; 0. Note that ?(a) = (a ? 1)! when a is a strictly positive integer. <ref type="bibr" target="#b6">7</ref> Univariate Gamma density is given by: fGamma(n; a k ) = n a k ?1 exp(?n)</p><formula xml:id="formula_7">?(a k )</formula><p>, n ? R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">On the class-balance bias of the best-performing few-shot methods</head><p>As briefly evoked in section 2, the strict balancing of the classes in both S and Q represents a strong inductive bias, which few-shot methods can either meta-learn during training or leverage at inference. In this section, we explicitly show how such a class-balance prior is encoded in the two best-performing transductive methods in the literature <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>, one based on mutual-information maximization <ref type="bibr" target="#b22">[23]</ref> and the other on optimal transport <ref type="bibr" target="#b30">[31]</ref>.</p><p>Class-balance bias of optimal transport Recently, the transductive method in <ref type="bibr" target="#b30">[31]</ref>, referred to as PT-MAP, achieved the best performances reported in the literature on several popular benchmarks, to the best of our knowledge. However, the method explicitly embeds a class-balance prior. Formally, the objective is to find, for each few-shot task, an optimal mapping matrix M ? R |Q|?K + , which could be viewed as a joint probability distribution over X Q ? Y Q . At inference, a hard constraint M ? {M : M 1 K = r, 1 |Q| M = c} for some r and c is enforced through the use of the Sinkhorn-Knopp algorithm. In other words, the columns and rows of M are constrained to sum to pre-defined vectors r ? R |Q| and c ? R K . Setting c = 1 K 1 K as done in <ref type="bibr" target="#b30">[31]</ref> ensures that M defines a valid joint distribution, but also crucially encodes the strong prior that all the classes within the query sets are equally likely. Such a hard constraint is detrimental to the performance in more realistic scenarios where the class distributions of the query sets could be arbitrary, and not necessarily uniform. Unsurprisingly, PT-MAP undergoes a substantial performance drop in the realistic scenario with Dirichlet-distributed class proportions, with a consistent decrease in accuracy between 18 and 20 % on all benchmarks, in the 5-ways case.</p><p>Class-balance bias of transductive mutual-information maximization Let us now have a closer look at the mutual-information maximization in <ref type="bibr" target="#b22">[23]</ref>. Following the notations introduced in section 2, the transductive loss minimized in <ref type="bibr" target="#b22">[23]</ref> for a given few-shot task reads:</p><formula xml:id="formula_8">L TIM = CE ? I(X Q ; Y Q ) = CE ? 1 |I Q | i?Q K k=1 p ik log(p ik ) H(Y Q |X Q ) +? K k=1 p k log p k ?H(Y Q ) ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_9">I(X Q ; Y Q ) = ?H(Y Q |X Q ) + ?H(Y Q )</formula><p>is a weighted mutual information between the query samples and their unknown labels (the mutual information corresponds to ? = 1), and CE := ? 1 |I S | i?S K k=1 y ik log(p ik ) is a supervised cross-entropy term defined over the support samples. Let us now focus our attention on the label-marginal entropy term, H(Y Q ). As mentioned in <ref type="bibr" target="#b22">[23]</ref>, this term is of significant importance as it prevents trivial, single-class solutions stemming from minimizing only conditional entropy H(Y Q |X Q ). However, we argue that this term also encourages class-balanced solutions. In fact, we can write it as an explicit KL divergence, which penalizes deviation of the label marginals within a query set from the uniform distribution:</p><formula xml:id="formula_10">H(Y Q ) = ? K k=1 p k log ( p k ) = log(K) ? D KL ( p u K ).<label>(2)</label></formula><p>Therefore, minimizing marginal entropy H(Y Q ) is equivalent to minimizing the KL divergence between the predicted marginal distribution p = ( p 1 , . . . , p K ) and uniform distribution u K = 1</p><formula xml:id="formula_11">K 1 K .</formula><p>This KL penalty could harm the performances whenever the class distribution of the few-shot task is no longer uniform. In line with this analysis, and unsurprisingly, we observe in section 6 that the original model in <ref type="bibr" target="#b22">[23]</ref> also undergoes a dramatic performance drop, up to 20%. While naively removing this marginal-entropy term leads to even worse performances, we observe that simply down-weighting it, i.e., decreasing ? in Eq. (1), can drastically alleviate the problem, in contrast to the case of optimal transport where the class-balance constraint is enforced in a hard manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Generalizing mutual information with ?-divergences</head><p>In this section, we propose a non-trivial, but simple generalization of the mutual-information loss in (1), based on ?-divergences, which can tolerate more effectively class-distribution variations. We identified in section 4 a class-balance bias encoded in the marginal Shannon entropy term. Ideally, we would like to extend this Shannon-entropy term in a way that allows for more flexibility: Our purpose is to control how far the predicted label-marginal distribution, p, could depart from the uniform distribution without being heavily penalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Background</head><p>We argue that such a flexibility could be controlled through the use of ?-divergences <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, which generalize the well-known and widely used KL divergence. ?-divergences form a whole family of divergences, which encompasses Tsallis and Renyi ?-divergences, among others.</p><p>In this work, we focus on Tsallis's <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref> formulation of ?-divergence. Let us first introduce the generalized logarithm <ref type="bibr" target="#b43">[44]</ref>:</p><formula xml:id="formula_12">log ? (x) = 1 1?? x 1?? ? 1 .</formula><p>Using the latter, Tsallis ?-divergence naturally extends KL. For two discrete distributions p = (p k ) K k=1 and q = (q k ) K k=1 , we have:</p><formula xml:id="formula_13">D ? (p q) = ? K k=1 p k log ? q k p k = 1 1 ? ? 1 ? K k=1 p ? k q 1?? k .<label>(3)</label></formula><p>Note that the Shannon entropy in Eq. (2) elegantly generalizes to Tsallis ?-entropy:</p><formula xml:id="formula_14">H ? (p) = log ? (K) ? K 1?? D ? (p u K ) = 1 ? ? 1 1 ? k p ? k .<label>(4)</label></formula><p>The derivation of Eq. (4) is provided in appendix. Also, lim ??1 log ? (x) = log(x), which implies:</p><formula xml:id="formula_15">lim ??1 D ? (p q) = D KL (p q) and lim ??1 H ? (p) = H(p) = ? K k=1 p k log ( p k ) .</formula><p>Note that ?-divergence D ? (p q) inherits the nice properties of the KL divergence, including but not limited to convexity with respect to both p and q and strict positivity D ? (p q) ? 0 with equality if p = q. Furthermore, beyond its link to the forward KL divergence D KL (p q), ?-divergence smoothly connects several well-known divergences, including the reverse KL divergence D KL (q p) (? ? 0), the Hellinger (? = 0.5) and the Pearson Chi-square (? = 2) distances <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of the gradients</head><p>As observed from Eq. (4), ?-entropy is, just like Shannon Entropy, intrinsically biased toward the uniform distribution. Therefore, we still have not properly answered the question: why would ?-entropy be better suited to imbalanced situations? We argue the that learning dynamics subtly but crucially differ. To illustrate this point, let us consider a simple toy logistic-regression example. Let l ? R denotes a logit, and p = ?(l) the corresponding probability, where ? stands for the usual sigmoid function. The resulting probability distribution simply reads p = {p, 1 ? p}. In <ref type="figure" target="#fig_1">Figure 2</ref>, we plot both the ?-entropy H ? (left) and its gradients ?H ? /?l (right) as functions of p. The advantage of ?-divergence now becomes clearer: as ? increases, H ? (p) accepts more and more deviation from the uniform distribution (p = 0.5 on <ref type="figure" target="#fig_1">Figure 2</ref>), while still providing a barrier preventing trivial solutions (i.e., p = 0 or p = 1, which corresponds to all the samples predicted as 0 or 1). Intuitively, such a behavior makes ?-entropy with ? &gt; 1 better suited to class imbalance than Shannon entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Proposed formulation</head><p>In light of the previous discussions, we advocate a new ?-mutual information loss, a simple but very effective extension of the Shannon mutual information in Eq. <ref type="formula" target="#formula_8">(1)</ref>:</p><formula xml:id="formula_16">I ? (X Q ; Y Q ) = H ? (Y Q ) ? H ? (Y Q |X Q ) = 1 ? ? 1 ? ? 1 |I Q | i?I Q K k=1 p ? ik ? K k=1 p ? k ? ?<label>(5)</label></formula><p>with H ? the ?-entropy as defined in Eq. <ref type="bibr" target="#b3">(4)</ref>. Note that our generalization in Eq. <ref type="bibr" target="#b4">(5)</ref> has no link to the ?-mutual information derived in <ref type="bibr" target="#b44">[45]</ref>. Finally, our loss for transductive few-shot inference reads:</p><formula xml:id="formula_17">L ?-TIM = CE ? I ? (X Q ; Y Q ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we thoroughly evaluate the most recent few-shot transductive methods using our imbalanced setting. Except for SIB <ref type="bibr" target="#b15">[16]</ref> and LR-ICI <ref type="bibr" target="#b16">[17]</ref> all the methods have been reproduced in our common framework. All the experiments have been executed on a single GTX 1080 Ti GPU.</p><p>Datasets We use three standard benchmarks for few-shot classification: mini-Imagenet <ref type="bibr" target="#b45">[46]</ref>, tiered-Imagenet <ref type="bibr" target="#b29">[30]</ref> and Caltech-UCSD Birds 200 (CUB) <ref type="bibr" target="#b46">[47]</ref>. The mini-Imagenet benchmark is a subset of the ILSVRC-12 dataset <ref type="bibr" target="#b45">[46]</ref>, composed of 60,000 color images of size 84 x 84 pixels <ref type="bibr" target="#b2">[3]</ref>. It includes 100 classes, each having 600 images. In all experiments, we used the standard split of 64 base-training, 16 validation and 20 test classes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref>. The tiered-Imagenet benchmark is a larger subset of ILSVRC-12, with 608 classes and 779,165 color images of size 84 ? 84 pixels. We used a standard split of 351 base-training, 97 validation and 160 test classes. The Caltech-UCSD Birds 200 (CUB) benchmark also contains images of size 84 ? 84 pixels, with 200 classes. For CUB, we used a standard split of 100 base-training, 50 validation and 50 test classes, as in <ref type="bibr" target="#b31">[32]</ref>. It is important to note that for all the splits and data-sets, the base-training, validation and test classes are all different.</p><p>Task sampling We evaluate all the methods in the 1-shot 5-way, 5-shot 5-way, 10-shot 5-way and 20-shot 5-way scenarios, with the classes of the query sets randomly distributed following Dirichlet's distribution, as described in section 3. Note that the total amount of query samples |Q| remains fixed to 75. All the methods are evaluated by the average accuracy over 10,000 tasks, following <ref type="bibr" target="#b32">[33]</ref>. We used different Dirichlet's concentration parameter a for validation and testing. The validation-task generation is based on a random sampling within the simplex (i.e Dirichlet with a = 1 K ). Testingtask generation uses a = 2 ? 1 K to reflect the fact that extremely imbalanced tasks (i.e., only one class is present in the task) are unlikely to happen in practical scenarios; see <ref type="figure" target="#fig_0">Figure 1</ref> for visualization.</p><p>Hyper-parameters Unless identified as directly linked to a class-balance bias, all the hyperparameters are kept similar to the ones prescribed in the original papers of the reproduced methods. For instance, the marginal entropy in TIM <ref type="bibr" target="#b22">[23]</ref> was identified in section 4 as a penalty that encourages class balance. Therefore, the weight controlling the relative importance of this term is tuned. For all methods, hyper-parameter tuning is performed on the validation set of each dataset, using the validation sampling described in the previous paragraph.</p><p>Base-training procedure All non-episodic methods use the same feature extractors, which are trained using the same procedure as in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20]</ref>, via a standard cross-entropy minimization on the base classes with label smoothing. The feature extractors are trained for 90 epochs, using a learning rate initialized to 0.1 and divided by 10 at epochs 45 and 66. We use a batch size of 256 for ResNet-18 and of 128 for WRN28-10. During training, color jitter, random croping and random horizontal flipping augmentations are applied. For episodic/meta-learning methods, given that each requires a specific training, we use the pre-trained models provided with the GitHub repository of each method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Main results</head><p>The main results are reported in <ref type="table" target="#tab_0">Table 1</ref>. As baselines, we also report the performances of state-ofthe-art inductive methods that do not use the statistics of the query set at adaptation and are, therefore, Accuracy is averaged over 10,000 tasks. A red arrow (?) indicates a performance drop between the artificially-balanced setting and our testing procedure, and a blue arrow (?) an improvement. Arrows are not displayed for the inductive methods as, for these, there is no significant change in performance between both settings (expected). '-' signifies the result was computationally intractable to obtain.  unaffected by class imbalance. In the 1-shot scenario, all the transductive methods, without exception, undergo a significant drop in performances as compared to the balanced setting. Even though the best-performing transductive methods still outperforms the inductive ones, we observe that more than half of the transductive methods evaluated perform overall worse than inductive baselines in our realistic setting. Such a surprising finding highlights that the standard benchmarks, initially developed for the inductive setting, are not well suited to evaluate transductive methods. In particular, when evaluated with our protocol, the current state-of-the-art holder PT-MAP averages more than 18% performance drop across datasets and backbones, Entropy-Min around 7%, and TIM around 4%. Our proposed ?-TIM method outperforms transductive methods across almost all task formats, datasets and backbones, and is the only method that consistently inductive baselines in fair setting. While stronger inductive baselines have been proposed in the literature <ref type="bibr" target="#b47">[48]</ref>, we show in the supplementary material that ?-TIM keeps a consistent relative improvement when evaluated under the same setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>mini-ImageNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation studies</head><p>In-depth comparison of TIM and ?-TIM While not included in the main <ref type="table" target="#tab_0">Table 1</ref>, keeping the same hyper-parameters for TIM as prescribed in the original paper <ref type="bibr" target="#b22">[23]</ref> would result in a drastic drop of about 18% across the benchmarks. As briefly mentioned in section 4 and implemented for tuning <ref type="bibr" target="#b22">[23]</ref> in <ref type="table" target="#tab_0">Table 1</ref>, adjusting marginal-entropy weight ? in Eq. (1) strongly helps in imbalanced scenarios, reducing the drop from 18% to only 4%. However, we argue that such a strategy is sub-optimal in comparison to using ?-divergences, where the only hyper-parameter controlling the flexibility of the marginal-distribution term becomes ?. First, as seen from <ref type="table" target="#tab_0">Table 1</ref>, our ?-TIM achieves consistently better performances with the same budget of hyper-parameter optimization as the standard TIM. In fact, in higher-shots scenarios (5 or higher), the performances of ?-TIM are substantially better that the standard mutual information (i.e. TIM). Even more crucially, we show in <ref type="figure" target="#fig_2">Figure 3</ref> that ?-TIM does not fail drastically when ? is chosen sub-optimally, as opposed to the case of weighting parameter ? for the TIM formulation. We argue that such a robustness makes of ?-divergences a particularly interesting choice for more practical applications, where such a tuning might be intractable. Our results points to the high potential of ?-divergences as loss functions for leveraging unlabelled data, beyond the few-shot scenario, e.g., in semi-supervised or unsupervised domain adaptation problems.</p><p>Varying imbalance severity While our main experiments used a fixed value a = 2?1 K , we wonder whether our conclusions generalize to different levels of imbalance. Controlling for Dirichlet's parameter a naturally allows us to vary the imbalance severity. In <ref type="figure">Figure 4</ref>, we display the results obtained by varying a, while keeping the same hyper-parameters obtained through our validation protocol. Generally, most methods follow the expected trend: as a decreases and tasks become more severely imbalanced, performances decrease, with sharpest losses for TIM <ref type="bibr" target="#b22">[23]</ref> and PT-MAP <ref type="bibr" target="#b30">[31]</ref>. In fact, past a certain imbalance severity, the inductive baseline in <ref type="bibr" target="#b32">[33]</ref> becomes more competitive than most transductive methods. Interestingly, both LaplacianShot <ref type="bibr" target="#b19">[20]</ref> and our proposed ?-TIM are able to cope with extreme imbalance, while still conserving good performances on balanced tasks.  <ref type="figure">Figure 4</ref>: 5-shot test accuracy of transductive methods versus imbalance level (lower a corresponds to more severe imbalance, as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>On the use of transductive BN In the case of imbalanced query sets, we note that transductive batch normalization (e.g as done in the popular MAML <ref type="bibr" target="#b48">[49]</ref>) hurts the performances. This aligns with recent observations from the concurrent work in <ref type="bibr" target="#b49">[50]</ref>, where a shift in the marginal label distribution is clearly identified as a failure case of statistic alignment via batch normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We make the unfortunate observation that recent transductive few-shot methods claiming large gains over inductive ones may perform worse when evaluated with our realistic setting. The artificial balance of the query sets in the vanilla setting makes it easy for transductive methods to implicitly encode this strong prior at meta-training stage, or even explicitly at inference. When rendering such a property obsolete at test-time, the current top-performing method becomes noncompetitive, and all the transductive methods undergo performance drops. Future works could study the mixed effect of imbalance on both support and query sets. We hope that our observations encourage the community to rethink the current transductive literature, and build upon our work to provide fairer grounds of comparison between inductive and transductive methods.</p><p>A On the performance of ?-TIM on the standard balanced setting</p><p>In the main tables of the paper, we did not include the performances of ?-TIM in the standard balanced setting. Here, we emphasize that ?-TIM is a generalization of TIM <ref type="bibr" target="#b22">[23]</ref> as when ? ? 1 (i.e., the ?-entropies tend to the Shannon entropies), ?-TIM tends to TIM. Therefore, in the standard setting, where optimal hyper-parameter ? is obtained over validation tasks that are balanced (as in the standard validation tasks of the original TIM and the other existing methods), the performance of ?-TIM is the same as TIM. When ? is tuned on balanced validation tasks, we obtain an optimal value of ? very close to 1, and our ?-mutual information approaches the standard mutual information.</p><p>When the validation tasks are uniformly random, as in our new setting and in the validation plots we provided in the main figure, one can see that the performance of ?-TIM remains competitive when we tend to balanced testing tasks (i.e., when a is increasing), but is significantly better than TIM when we tend to uniformly-random testing tasks (a = 1). These results illustrate the flexibility of ?-divergences, and are in line with the technical analysis provided in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Comparison with DeepEMD</head><p>The recent method <ref type="bibr" target="#b47">[48]</ref> achieves impressive results in the inductive setting. As conveyed in the main paper, inductive methods tend to be unaffected by class imbalance on the query set, which legitimately questions whether strong inductive methods should be preferred over transductive ones, including our proposed ?-TIM. In the case of DeepEMD, we expand below on the levers used to obtain such results, and argue those are orthogonal to the loss function, and therefore to our proposed ?-TIM method. More specifically:</p><p>1. DeepEMD uses richer feature representations: While all the methods we reproduce use the standard global average pooling to obtain a single feature vector per image, DeepEMD-FCN leverages dense feature maps (i.e without the pooling layer). This results in a richer, much higher-dimensional embeddings. For instance, the standard RN-18 yields a 512-D vector per image, while the FCN RN-12 used by DeepEMD yields a 5x5x640-D feature map (i.e 31x larger). As for DeepEMD-Grid and DeepEMD-Sampling, they build feature maps by concatenating feature extracted from N different patches taken from the original image (which requires as many forward passes through the backbone). Also, note that prototypes optimized for during inference have the same dimension as the feature maps. Therefore, taking richer and larger feature representations also means increasing the number of trainable parameters at inference by the same ratio. 2. DeepEMD uses a more sophisticated notion of distance (namely the Earth Moving Distance), introducing an EMD layer, different from the standard classification layer. While all methods we reproduced are based on simple and easy-to-compute distances between each feature and the prototypes (e.g Euclidian, dot-product, cosine distance), the flow-based distance used by DeepEMD captures more complex patterns than the usual Euclidian distance, but is also much more demanding computationally (as it requires solving an LP program). Now, we want to emphasize that the model differences mentioned above can be straightforwardly applied to our ?-TIM (and likely the other methods) in order to boost the results at the cost of a significant increase of compute requirement. To demonstrate this point, we implemented our ?-TIM in with the three ResNet-12 based architectures proposed in DeepEMD (cf <ref type="table" target="#tab_3">Table 3</ref>) using our imbalanced tasks, and consistently observed +3 to +4 without changing any optimization hyperparameter from their setting, and using the pre-trained models the authors have provided. This figure matches the improvement observed w.r.t to SimpleShot with the standard models (RN-18 and WRN). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Relation between ?-entropy and ?-divergence</head><p>We provide the derivation of Eq. (4) in the main paper, which links ?-entropy H ? (p) to the ?divergence:</p><formula xml:id="formula_18">log ? (K) ? K 1?? D ? (p||u K ) = 1 1 ? ? K 1?? ? 1 ? K 1?? ? ? 1 K k=1 p ? k 1 K 1?? ? 1 = 1 1 ? ? K 1?? ? 1 1 ? ? ? 1 ? ? 1 K k=1 p ? k + K 1?? ? ? 1 = 1 ? ? 1 1 ? K k=1 p ? k<label>(7)</label></formula><p>D Comparison with other types of imbalance</p><p>The study in <ref type="bibr" target="#b37">[38]</ref> examined the effect of class imbalance on the support set after defining several processes to generate class-imbalanced support sets. In particular, the authors proposed linear and step imbalance. In a 5-way setting, a typical linearly imbalanced few-shot support would look like {1, 3, 5, 7, 9} (keeping the total number of support samples equivalent to standard 5-ways 5-shot tasks), while a step imbalance task could be {1, <ref type="bibr">9, 9, 9}</ref>. To provide intuition as to how these two types of imbalance related to our proposed Dirichlet-based sampling scheme, we super-impose Dirichlet's density on all valid linear and step imbalanced distributions for 3-ways tasks in <ref type="figure" target="#fig_4">Figure 5</ref>. Combined, linear and step imbalanced valid distributions allow to cover a fair part of the simplex, but Dirichlet sampling allows to sample more diverse and arbitrary class ratios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Influence of each term in TIM and ?-TIM</head><p>We report a comprehensive ablation study, evaluating the benefits of using the ?-entropy instead of the Shannon entropy (both conditional and marginal terms), as well as the effect of the marginal-entropy terms in the loss functions of TIM and ?-TIM. The results are reported in <ref type="table" target="#tab_4">Table 4</ref>. ?-TIM yields better performances in all settings.</p><p>On the ?-conditional entropy: The results of ?-TIM obtained by optimizing the conditional entropy alone (without the marginal term) are 4.5 to 7.2% higher in 1-shot, 0.8 to 3.5% higher in 5-shot and 0.1 to 1.3% higher in 10-shot scenarios, in comparison to its Shannon-entropy counterpart in TIM. Note that, for the conditional-probability term, the ?-entropy formulation has a stronger effect in lower-shot scenarios (1-shot and 5-shot). We hypothesize that this is due to the shapes of the ?-entropy functions and their gradient dynamics (see <ref type="figure" target="#fig_1">Fig. 2</ref> in the main paper), which, during training, assigns more weight to confident predictions near the vertices of the simplex (p = 1 or p = 0) and less weight to uncertain predictions at the middle of the simplex (p = 0.5). This discourages propagation of errors during training (i.e., learning from uncertain predictions), which are more likely to happen in lower-shot regimes.</p><p>Flexibility of the ?-marginal entropy: An important observation is that the marginal-entropy term does even hurt the performances of TIM in the higher shot scenarios (10-shot), even though the results correspond to the best ? over the validation set. We hypothesize that this is due to the strong classbalance bias in the Shannon marginal entropy. Again, due to the shapes of the ?-entropy functions and their gradient dynamics, ?-TIM tolerates better class imbalance. In the 10-shot scenarios, the performances of TIM decrease by 1.8 to 3.2% when including the marginal entropy, whereas the performance of ?-TIM remains approximately the same (with or without the marginal-entropy term). These performances demonstrate the flexibility of ?-TIM. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Dirichlet density function for K = 3, with different choices of parameter vector a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(Left) ?-entropy as a function of p = ?(l). (Right) Gradient of ?-entropy w.r.t to the logit l ? R as a function of p = ?(l). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Validation and Test accuracy versus ? for TIM<ref type="bibr" target="#b22">[23]</ref> and ? for our proposed ?-TIM, using our protocol. Results are obtained with a RN-18. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of Dirichlet sampling with linear and step imbalance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of state-of-the-art methods in our realistic setting on mini-ImageNet, tiered-ImageNet and CUB. Query sets are sampled following a Dirichlet distribution with a = 2 ? 1 K .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparaisons of state-of-the-art methods in our realistic setting on CUB. Query sets are sampled following a Dirichlet distribution with a = 2 ? 1 K . Accuracy is averaged over 10,000 tasks.A red arrow (?) indicates a performance drop between the artificially-balanced setting and our testing procedure, and a blue arrow (?) an improvement. Arrows are not displayed for the inductive methods as, for these, there is no significant change in performance between both settings (expected). '-' signifies the result was computationally intractable to obtain.</figDesc><table><row><cell></cell><cell>CUB</cell><cell></cell><cell></cell></row><row><cell>1-shot</cell><cell>5-shot</cell><cell>10-shot</cell><cell>20-shot</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with DeepEMD [48]. Input: W=Whole images are used as input ; P = Multiples patches of the whole image are used as input. Embeddings: G=Global averaged features are used (i.e 1 feature vector per image) ; L = Local features are used (i.e 1 feature map per image ).</figDesc><table><row><cell>Method</cell><cell cols="6">Distance RN-18 (W/G) WRN (W/G) FCN RN-12 (W/L) Grid RN-12 (P/L) Sampling RN-12 (P/L)</cell></row><row><cell cols="2">SimpleShot [33] Euclidian</cell><cell>63.0</cell><cell>66.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>?-TIM</cell><cell>Euclidian</cell><cell>67.4</cell><cell>69.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepEMD [48]</cell><cell>EMD</cell><cell>-</cell><cell>-</cell><cell>65.9</cell><cell>67.8</cell><cell>68.8</cell></row><row><cell>?-TIM</cell><cell>EMD</cell><cell>-</cell><cell>-</cell><cell>68.9</cell><cell>72.0</cell><cell>72.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>An ablation study evaluating the benefits of using the ?-entropy instead of the Shannon entropy (both conditional and marginal terms), as well as the effect of the marginal-entropy terms in the loss functions of TIM and ?-TIM.CE + H(Y Q |X Q ) ? H(Y Q )</figDesc><table><row><cell>Loss</cell><cell>Dataset</cell><cell cols="5">Network Method 1-shot 5-shot 10-shot</cell></row><row><cell></cell><cell>mini-Imagenet</cell><cell>RN-18</cell><cell>TIM ?-TIM</cell><cell>42.2 48.4</cell><cell>79.5 82.4</cell><cell>85.5 86.0</cell></row><row><cell></cell><cell></cell><cell>WRN</cell><cell>TIM ?-TIM</cell><cell>52.8 57.3</cell><cell>82.7 84.6</cell><cell>87.5 88.0</cell></row><row><cell>CE + H(Y Q |X Q )</cell><cell>tiered-Imagenet</cell><cell>RN-18</cell><cell>TIM ?-TIM</cell><cell>52.4 59.0</cell><cell>83.7 86.3</cell><cell>88.4 89.2</cell></row><row><cell></cell><cell></cell><cell>WRN</cell><cell>TIM ?-TIM</cell><cell>49.6 56.8</cell><cell>84.1 87.6</cell><cell>89.1 90.4</cell></row><row><cell></cell><cell>CUB</cell><cell>RN-18</cell><cell>TIM ?-TIM</cell><cell>56.4 63.2</cell><cell>89.0 89.8</cell><cell>92.2 92.3</cell></row><row><cell></cell><cell>mini-Imagenet</cell><cell>RN-18</cell><cell>TIM ?-TIM</cell><cell>67.3 67.4</cell><cell>79.8 82.5</cell><cell>82.3 85.9</cell></row><row><cell></cell><cell></cell><cell>WRN</cell><cell>TIM ?-TIM</cell><cell>69.8 69.8</cell><cell>82.3 84.8</cell><cell>84.5 87.9</cell></row><row><cell></cell><cell>tiered-Imagenet</cell><cell>RN-18</cell><cell>TIM ?-TIM</cell><cell>74.1 74.4</cell><cell>84.1 86.6</cell><cell>86.0 89.3</cell></row><row><cell></cell><cell></cell><cell>WRN</cell><cell>TIM ?-TIM</cell><cell>75.8 76.0</cell><cell>85.4 87.8</cell><cell>87.3 90.4</cell></row><row><cell></cell><cell>CUB</cell><cell>RN-18</cell><cell>TIM ?-TIM</cell><cell>74.8 75.7</cell><cell>86.9 89.8</cell><cell>89.5 92.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The best-performing state-of-the-art few-shot methods in the transductive-inference setting have achieved performances that are up to 10% higher than their inductive counterparts; see<ref type="bibr" target="#b22">[23]</ref>, for instance.<ref type="bibr" target="#b2">3</ref> Each single few-shot task is treated independently of the other tasks in the transductive-inference setting. Hence, the setting does not use additional unlabeled data, unlike semi-supervised few-shot learning<ref type="bibr" target="#b29">[30]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">? is either fixed, e.g.,<ref type="bibr" target="#b22">[23]</ref>, or fine-tuned during inference, e.g.,<ref type="bibr" target="#b14">[15]</ref>. There is, however, evidence in the literature that freezing ? yields better performances<ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33]</ref>, while reducing the inference time.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project was supported by the Natural Sciences and Engineering Research Council of Canada (Discovery Grant RGPIN 2019-05954). This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sk?odowska-Curie grant agreement ?792464.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 6</ref><p>: Validation and Test accuracy versus ? for TIM <ref type="bibr" target="#b22">[23]</ref> and versus ? for ?-TIM, using our task-generation protocol. Results are obtained with a RN-18. Best viewed in color. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Code -Implementation of our framework</head><p>As mentioned in our main experimental section, all the methods have been reproduced in our common framework, except for SIB 8 <ref type="bibr" target="#b15">[16]</ref> and LR-ICI 9 <ref type="bibr" target="#b16">[17]</ref>, for which we used the official public implementations of the works. <ref type="bibr" target="#b7">8</ref>  tiered -Imagenet <ref type="figure">Figure 9</ref>: Validation and Test accuracy versus ? for TIM <ref type="bibr" target="#b22">[23]</ref> and versus ? for ?-TIM on 10-shot and 20-shot tasks, using our task-generation protocol. Results are obtained with a WRN. Best viewed in color.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning from one example through shared densities on transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yanbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Few-shot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Empirical bayes transductive meta-learning with synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Instance credibility inference for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attentive weights generation for few shot learning via information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dpgn: Distribution propagation graph network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Laplacian regularized few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prototype rectification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An ensemble of epoch-wise empirical bayes for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transductive information maximization for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tasknorm: Rethinking batch normalization for meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-learning probabilistic inference for prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to learn kernels with variational random features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An overview of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks (TNN)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="988" to="999" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dengyong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Leveraging the feature distribution in transfer-based few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno>arXiv preprint:1911.04623</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<title level="m">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv preprint:1903.03096</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning to balance: Bayesian meta-learning for imbalanced and out-of-distribution tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to stop while learning to predict</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Few-shot learning with class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ochal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv preprint:2101.02523</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Non-Uniform Random Variate Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chernoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="507" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">?-divergence is unique, belonging to both f -divergence and bregman divergence classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4925" to="4931" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Quantification method of classification processes -concept of structural a-entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Havrda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Charv?t</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kybernetika</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="35" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Possible generalization of boltzmann-gibbs statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tsallis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical physics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="479" to="487" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Families of alpha-beta-and gamma-divergences: Flexible and robust measures of similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1532" to="1568" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Information measures and capacity of order ? for discrete memoryless channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topics in information theory</title>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Limitations of post-hoc feature alignment for robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
