<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lossy Compression for Lossless Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dubois</surname></persName>
							<email>yanndubois@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bloem-Reddy</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Ullrich</surname></persName>
							<email>karenu@fb.com</email>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Vector Institute</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lossy Compression for Lossless Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most data is automatically collected and only ever "seen" by algorithms. Yet, data compressors preserve perceptual fidelity rather than just the information needed by algorithms performing downstream tasks. In this paper, we characterize the bit-rate required to ensure high performance on all predictive tasks that are invariant under a set of transformations, such as data augmentations. Based on our theory, we design unsupervised objectives for training neural compressors. Using these objectives, we train a generic image compressor that achieves substantial rate savings (more than 1000? on ImageNet) compared to JPEG on 8 datasets, without decreasing downstream classification performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Progress in important areas requires processing huge amounts of data. For climate prediction, models are still data-limited <ref type="bibr" target="#b0">[1]</ref>, despite the Natl. Center for Computational Sciences storing 32 million gigabytes (GB) of climate data <ref type="bibr" target="#b2">[2]</ref>. For autonomous driving, capturing a realistic range of rare events with current methods requires around 3 trillion GB of data. <ref type="bibr" target="#b0">1</ref> At these scales, data are only processed by task-specific algorithms, and storing data in human-readable formats can be prohibitive. We need compressors that retain only the information needed for algorithmic execution of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Standard rec. Our rec. Existing lossy compressors are not up to the challenge, because they aim to reconstruct the data for human perception <ref type="bibr" target="#b5">[5]</ref><ref type="bibr" target="#b6">[6]</ref><ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref>. However, much of perceptual information is not needed to perform the tasks that we care about. Consider classifying images, which can require about 1 MB to store. Classification is typically invariant under small image transformations, such as rescalings or rotations, and could instead be performed using a representation that discards such information (see <ref type="figure" target="#fig_0">Fig. 1</ref>). The amount of unnecessary perceptual information is likely substantial, as illustrated by the fact that typical image classification can be performed using a detailed caption, which requires only about 1 kB to store (1000? fewer bits).</p><p>Our goal is to quantify the bit-rate needed to ensure high performance on a collection of prediction tasks. In the simple case of a single supervised task, the minimum bit-rate is achieved by compressing predicted labels, and essentially corresponds to the Information Bottleneck (IB; <ref type="bibr" target="#b12">[11]</ref>). Our challenge, instead, is to ensure good performance on any future tasks of interest, which will rarely be completely known at compression time, or might be too large to enumerate.</p><p>We overcome this challenge by focusing on sets of tasks that are invariant under user-defined transformations (e.g., translation, brightness, cropping), as is the case for many tasks of interest to humans <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b14">13]</ref>. This structure allows us to characterize a worst-case invariant task, which bounds the relative predictive performance on all invariant tasks. As a result, the bit-rate required to perform well on all invariant tasks is exactly the rate to compress the worst-case labels. At a high level, the worst-case task is to recognize which examples are transformed versions of one another, and rate savings come from discarding information from those transformations.</p><p>We also provide two unsupervised neural compressors to target the optimal rates. One is similar to a variational autoencoder <ref type="bibr" target="#b15">[14]</ref> that reconstructs canonical examples ( <ref type="figure" target="#fig_0">Fig. 1)</ref>. Our second is a simple modification of contrastive self-supervised learning (SSL; <ref type="bibr" target="#b16">[15]</ref>), which allows us to convert pre-trained SSL models into powerful, generic compressors. Our contributions are:</p><p>? We formalize the notion of compression for downstream predictive tasks.</p><p>? We characterize the bits needed for high performance on any task invariant to augmentations.</p><p>? We provide unsupervised objectives to train compressors that approximate the optimal rates. ? We show that our compressor outperforms JPEG by orders of magnitude on 8 datasets on which it was never trained (i.e., zero-shot). E.g., on ImageNet <ref type="bibr" target="#b17">[16]</ref>, it decreases the bit-rate by 1000?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Rate-distortion theory background</head><p>The goal of lossy compression theory is to find the number of bits (bit-rate) required to store outcomes x of a random variable (r.v.) X, so that it can be reconstructed within a certain tolerance. This is accomplished in Shannon's <ref type="bibr" target="#b18">[17]</ref> rate-distortion (RD) theory by mapping X into a r.v. Z with low mutual information I[X; Z]. Specifically, given a distortion measure D[X, Z], the RD theory characterizes the minimal achievable bit-rate for a distortion threshold ? by </p><p>In lossy compression, Z is usually a reconstruction of X, i.e., it aims to faithfully approximate X. As a result, typical distortions, e.g., the mean squared error (MSE), assume that the sample spaces X , Z of both r.v.s are the same. This assumption is not required. Indeed, any distortion d : X ? Z ? R ?0 of the form D[X, Z] = E p(X,Z) [d(X, Z)], where there exists a z ? Z such that D[X, z] is finite, is a valid choice <ref type="bibr" target="#b19">[18]</ref>. This shows that RD theory can be used outside of reconstructions. In the following we refer to Z as a compressed representation of X to distinguish it from a reconstruction.</p><p>3 Minimal bit-rate for high predictive performance</p><p>In this section, we characterize the bit-rate needed to represent X to ensure high performance on downstream tasks. Our argument has three high-level steps: (i) define a distortion that controls downstream performance when predicting from Z instead of X; (ii) simplify and validate this distortion when desired tasks satisfy an invariance condition; (iii) apply RD theory with the valid distortion. For simplicity, our presentation is relatively informal; formal proofs are in Apps. A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A distortion for worst-case predictive performance</head><p>Suppose X is an image. Potential downstream tasks might include Y dog , whether the image displays a dog; or Y hd , whether the image is hand-drawn. Formally, these and other downstream tasks are expressed as T = {Y dog , Y hd , . . .}, a set of random variables that are jointly distributed with X. Let R[Y | X] denote the Bayes (best possible) risk when predicting Y from X. For ease of presentation in the main paper, we consider only classification tasks T and Bayes risk of the standard log loss R[Y | X] := inf q E p(X,Y ) [? log q(Y |X)]. We deal with MSE and regression in Appx. B.6.</p><p>In this setting, a meaningful distortion D T [X, Z] quantifies the difference between predicting any Y ? T from the compressed Z, as opposed to using X. This is the worst-case excess risk, If D T [X, Z] = 0, it is possible to achieve lossless prediction: performing as well using Z as using X. More generally, bounding D T by ? ensures that R[Y | Z] ? R[Y | X] ? ? for all tasks in T . However, there are two issues that need to be addressed before Eq. (2) can be used. First, it is not clear whether D T is a valid distortion for RD theory. Second, the worst excess-risk D T assumes access to all downstream tasks of interest T during compression, which is unrealistic in general.</p><formula xml:id="formula_1">D T [X, Z] := sup Y ?T R[Y | Z] ? R[Y | X] .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Invariant tasks</head><p>The tasks that we care about are not arbitrary, and often share structure. One such structure is invariance to certain pre-specified transformations of input data. For example, computer vision tasks are often invariant to mild transformations such as brightness changes. Such invariance structure is common in realistic tasks, as seen by the wide-spread use of data augmentations <ref type="bibr" target="#b14">[13]</ref> in machine learning (ML), which encourage predictions to be the same for an unaugmented x and an augmented x + . Motivated by this we focus on sets of invariant tasks T .</p><p>We consider a general notion of invariance, namely invariance specified by an equivalence relation ? on X . <ref type="bibr" target="#b2">2</ref> The equivalence induces a partition of X into disjoint equivalence classes, and we are interested in tasks whose conditional distributions are constant within these classes.</p><p>Definition 1. The set of invariant tasks of interest with respect to an equivalence (X , ?), denoted T ? , is all random variables Y such that x ? x + =? p(Y | x) = p(Y | x + ) for any x, x + ? X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Rate-distortion theory for invariant task prediction</head><p>The key to simplifying D T? is the existence of a (non-unique) worst-case invariant task, denoted M (X). Such task contains all and only information to which tasks Y ? T ? are not invariant; we call them maximal invariants. A maximal invariant M ( ? ) with respect to ? is any function satisfying 3</p><p>x ? x + ?? M (x) = M (x + ) for any x, x + ? X .</p><p>A maximal invariant removes all information that tasks are invariant to, as it maps equivalent inputs to the same output, i.e., M (x) = M (x + ). Yet, it retains the minimal information needed to perform invariant tasks, by mapping non-equivalent inputs x ? x ? to different outputs M (x) = M (x ? ).</p><p>In other words, M (x) indexes the equivalence classes. For example, the Euclidean norm is a maximal invariant for rotation invariance, as all vectors that are rotated versions of one another can be characterized by their radial coordinate. For data augmentations, the canonical (unaugmented) version of the input is a maximal invariant. Other examples are shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>We prove in Appx. B.2 that under weak regularity conditions, maximal invariant tasks exist in T ? , and that they achieve the supremum in Eq. <ref type="bibr" target="#b2">(2)</ref>. This allows us to show that D T? reduces to the Bayes risk of predicting M (X) from Z and that it is a valid distortion measure. Crucially, this allows us to quantify downstream performance without enumerating invariant tasks. </p><p>Here we used R[M (X) | X] = 0, as M is a deterministic function. Also, note that the countable requirement holds when tasks are invariant to some rounding of the input, as is typically the case due to floating-point storage. We accommodate the uncountable case for squared-error loss in Appx. B.6.</p><p>With a valid distortion in hand, we invoke the RD theorem with D T? to obtain our "Rate-Invariance" (RI) theorem. The RI theorem characterizes the bit-rate needed to store X while ensuring small log-loss on invariant tasks. We obtain analogous results for squared-error loss.  To ensure lossless prediction, i.e., Rate(0), our theorem states that we require a bit-rate of H[M (X)]. Intuitively, this is because M (X) contains the minimal information needed to predict losslessly any Y ? T ? . <ref type="bibr" target="#b4">4</ref> Furthermore, the theorem relates compression and prediction by showing that allowing a ? decrease in log-loss performance on all tasks can save exactly ? bits. Intuitively, this is a linear relationship, because expected log-loss is measured in bits. On the right of Eq. (5) we further decompose H[M (X)] into two terms to provide another interpretation: (i) H[X], which, for discrete X, is the bit-rate required to losslessly compress X, and (ii) H[X | M (X)], which quantifies the information removed due to the invariance of desired tasks. Importantly, removing this information does not impact the best possible predictive performance. See <ref type="figure" target="#fig_4">Fig. 3</ref>.</p><p>The bit-rate gains can be substantial, depending on the invariances. Consider compressing a sequence of n i.i.d. fair coin flips. Suppose one is only interested in predicting permutation invariant labels. Then instead of compressing the entire sequence in H[X n ] = n H[X] = n bits, one could compress the number of heads, which is a maximal invariant for permutation invariance, in O(log n) bits. <ref type="bibr" target="#b5">5</ref> As more interesting examples, we recover in Appx. B.4 results from (i) unlabeled graph compression <ref type="bibr" target="#b21">[20]</ref>; (ii) multiset compression <ref type="bibr" target="#b22">[21]</ref>; (iii) single task compression (IB; <ref type="bibr" target="#b12">[11]</ref>). The equivalence ? can be induced by any transformations, such as transforming an image to its caption. We use this idea in Sec. 5.3 to obtain &gt;1000? compression on ImageNet without sacrificing predictive performance.  <ref type="figure" target="#fig_8">Figure 4</ref>: Our unsupervised objectives for invariant image compression under data augmentation use the same encoder, but differ in their approximation to the invariance distortion. Both models encode the augmented data, pass the representation through an entropy bottleneck which ensures that they are compressed, and use a distortion to retain the information about the identity of the original data. The models differ in how they retain that information: (VIC) by reconstructing unaugmented inputs; (BINCE) by recognizing which inputs come from the same original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Unsupervised training of invariant neural compressors</head><p>In this section, we design practical, invariant neural compressors that bound optimal rates. Derivations are in Appx. C. In particular, we are interested in the arg min encoders p(Z|X) of the RD function (Eq. (1)) under the invariance distortion D T? . To accomplish this, we can optimize the following equivalent (i.e., it induces the same RI function) Lagrangian, where ? takes the role of ?, 6 arg min</p><formula xml:id="formula_4">p(Z|X) I[X; Z] + ? ? R[M (X) | Z] .<label>(6)</label></formula><p>In ML, the maximal invariant M is often not available. Instead, invariances are implicitly specified by sampling a random augmentation from A, applying it to a datapoint X, and asking that the model's prediction be invariant between X and A(X). For example, invariance to cropping can be enforced by randomly cropping images while retaining the original label. We show in Appx. C, that in such case, we can treat the augmented A(X) as the new source, Z as the representation of A(X), and the unaugmented X as the maximal invariant task M (A(X)). Indeed, R[M (A(X)) | Z] is equal to R[X | Z] up to a constant, so we can rewrite Eq. (6) as the following equivalent objective, arg min p(Z|A(X))</p><formula xml:id="formula_5">I[A(X); Z] + ? ? R[X | Z] .<label>(7)</label></formula><p>Such reformulation is possible if random augmentations retain the invariance structure X ? A(X) but "erase" enough information about equivalent inputs, specifically, if X? ?A(X) | M (X). We discuss the second requirement in Appx. C but note that it will likely not be a practical issue if the dataset is small compared to the support |D| |X |. With this, we have an objective whose r.v.s. are easy to sample from. However, both terms in Eq. <ref type="bibr" target="#b7">(7)</ref> are still challenging to estimate.</p><p>In the following, we develop two practical variational bounds to Eq. <ref type="bibr" target="#b7">(7)</ref>, which can be optimized by stochastic gradient descent <ref type="bibr" target="#b24">[23]</ref> over the encoder's parameters. Both approximations use the standard lossy neural compression bound I[Z;</p><formula xml:id="formula_6">A(X)] ? H[Z] ? min ? E p(Z) [? log q ? (Z)] where q ? (Z)</formula><p>is called an entropy model (or a prior) <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b26">25]</ref>. This has the advantage that the learned q ? can be used for entropy coding Z <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b28">27]</ref>. See Ball? et al. <ref type="bibr" target="#b29">[28]</ref> for possible entropy models. Our two approximations differ in how they upper bound R[X | Z]. The first uses a reconstruction loss, which attempts to reconstruct the unaugmented input x ? D from A(x). The second uses a discrimination loss, which attempts to recognize which examples are augmented versions of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Variational Invariant Compressor (VIC)</head><p>Our first model is a modified neural compressor in which inputs are augmented but target reconstructions are not. We refer to it as a variational invariant compressor (VIC). See <ref type="figure" target="#fig_8">Fig. 4</ref> for an illustration. VIC has an encoder p ? (Z|A(X)), an entropy model q ? (Z), and a decoder q ? (X|Z). Given a data sample x ? D, we apply a random augmentation A(x), and encode it to get a representation Z. The decoder then attempts to reconstruct the unaugmented x from Z. This leads to the objective,</p><formula xml:id="formula_7">L VIC (?, ?, ?) := ? x?D E p(A)p?(Z|A(x)) [log q ? (Z) + ? ? log q ? (x | Z)] .<label>(8)</label></formula><p>The term log q ? (Z) is an entropy bottleneck, which bounds the rate I[A(X); Z] and ensures that unnecessary information is removed. The term log</p><formula xml:id="formula_8">q ? (x | Z) bounds the distortion R[X | Z] ? E p(X,Z) [? log q ? (X | Z)]</formula><p>and ensures that VIC preserves the information needed for invariant tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bottleneck InfoNCE (BINCE)</head><p>Our second compressor retains all predictive information without reconstructing the data. It has two components: an entropy bottleneck and an InfoNCE <ref type="bibr" target="#b16">[15]</ref> objective, which is the standard in contrastive SSL. We refer to this as the bottleneck InfoNCE (BINCE), see <ref type="figure" target="#fig_8">Fig. 4</ref>. BINCE has an advantage over VIC in that it avoids the problem of reconstructing possibly high dimensional data.</p><formula xml:id="formula_9">Algorithm 1 BINCE's forward pass for x Require: p?, q ? , f ? , D, A, ?, n, x 1:x ? sample(A(x)) Augment 2: z ? sample(p?(Z|x)) Encode 3: rate_loss ? ? log q ? (z) 4: {x ? i } n i=1 ? select(D \{x}) n times 5:x ? sample([A(x), A(x ? 1 ), . . . , A(x ? n )]) 6: z ? sample(p?(Z|x)) 7: z + ? z[0] 8: softmax ? exp f ? (z + ,z) ( z ?z exp f ? (z ,z))</formula><p>9: distortion_loss ? ? log(softmax) 10: return rate_loss + ? ? distortion_loss Algorithm 1 shows how to train BINCE, where each call to A returns an independent augmentation of its input. As with VIC, for every datapoint x ? D, we obtain a representation Z by applying an augmentation A(x) and passing it through the encoder p ? (Z | A(X)). We then sample a "positive" example Z + by encoding a different augmented version of the same underlying datapoint x. Finally, we sample n "negative" examples Z ? i by encoding aug-</p><formula xml:id="formula_10">mentations A(x ? i ) of datapoints x ? i ? D that are different from x. This results in a sequence Z = (Z + , Z ? 1 , . . . , Z ? n ).</formula><p>For conciseness we will denote the above sampling procedure as p ? (Z, Z | A, D, x). The final loss uses a discriminator f ? that is optimized to score the equivalence of two representation,</p><formula xml:id="formula_11">L BINCE (?, ?, ?) := ? x?D E p(A)p?(Z,Z|A,D,x) log q ? (Z) + ? ? log exp f ? (Z + , Z) Z ?Z exp f ? (Z , Z) .<label>(9)</label></formula><p>BINCE retains the necessary information by classifying (as seen by the softmax) which Z is associated with an equivalent example X. Both VIC and BINCE give rise to efficient compressors by passing X through p ? (Z|X) and entropy coding using q ? (Z). In theory they can both recover the optimal rate for lossless predictions, i.e., H[M (X)], in the limit of infinite samples (|D|,n) and unconstrained variational families. In practice, BINCE has the advantage over VIC of (i) not requiring a high dimensional decoder; and (ii) giving (for suitable f ? ) representations that are approximately linearly separable <ref type="bibr" target="#b30">[29]</ref><ref type="bibr" target="#b31">[30]</ref><ref type="bibr" target="#b32">[31]</ref> and thus easy to predict from <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b33">32]</ref>. The disadvantages of BINCE are that it (i) does not provide to reconstructions diminishes interpretability; and (ii) has a high bias, unless the number of negative samples n is large <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b35">34]</ref>, which is computationally intensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluated our framework focusing on two questions: (i) What compression rates can our framework achieve at what cost? (ii) Can we train a general purpose predictive image compressor? For all experiments, we train the compressors, freeze them, train the downstream predictors, and finally evaluate both on a test set. For classical compressors, standard neural compressors (VC) and our VIC, we used either reconstructionsX as inputs to the predictors or representations Z. As BINCE does not provide reconstructions, we predicted from the compressed Z using a multi-layer perceptron (MLP). We used ResNet18 <ref type="bibr" target="#b36">[35]</ref> for encoders and image predictors. For entropy models we used Ball? et al.'s <ref type="bibr" target="#b29">[28]</ref> hyperprior, which uses uniform quantization. We optimized hyper-parameters on validation using random search. For classification tasks, we report classification error instead of log-loss. The former is more standard and gave similar results (see Appx. F.2). For experimental details see Appx. E. For additional results see Appx. F. Code is at github.com/YannDubs/lossyless.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Building intuition with toy experiments</head><p>To build an visual intuition, we compressed samples from a 2D banana source distribution <ref type="bibr" target="#b37">[36]</ref>, assuming rotation invariant tasks, e.g., classifying whether points are in the unit circle. We also compressed MNIST digits as in <ref type="figure" target="#fig_0">Fig. 1</ref>  Can we recover the optimal bit-rate? We investigated whether our losses can achieve the optimal bit-rate for lossy prediction by using supervised augmentations, i.e., A(x) randomly samples a train example x + that has the same label. For MNIST the single-task optimal bit-rate is H[Y ] = log(10) ? 3.3 bits. VIC and BINCE respectively achieve 5.7 and 5.9 bits, which shows that our losses are relatively good despite practical approximations. Details in Appx. F.2.</p><p>What is the impact of the choice of augmentations? The choice of augmentation A implicitly defines the desired task-set T , i.e., T is the set of all tasks for which A does not remove information.</p><p>As a result Theorem 2 can be rewritten as Rate(?) = I[X; A(X)] ? ?, so the rate decreases when A removes more information from X. To illustrate this we trained our VIC using three augmentation sets on MNIST, all of which keep the true label invariant but progressively discard more X information. VIC respectively achieves a rate of 185.3, 79.0, and 5.7 bits, which shows the importance of using augmentations that remove X information. Details and BINCE results are in Appx. F.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluating our methods with controlled experiments</head><p>To investigate our methods, we compressed the STL10 dataset <ref type="bibr" target="#b38">[37]</ref>. We augment (flipping, color jittering, cropping) the train and test set, to ensure that the task invariance assumptions are satisfied. We focus on more realistic settings in the next section. In each experiment, we sampled 100 combinations of hyper-parameters to ensure equal computational budget across models and baselines. Should we predict from representations Z or reconstructions X? In <ref type="table" target="#tab_2">Table 1</ref> we analyzed the impact of predicting from Z instead ofX for VIC and see that this increases accuracy by 9%. In contrast, predicting from Z for VC decreases performance by 12% (see Appx. F.3). This suggests that invariant reconstructionsX might not be easy to predict from with standard image predictors.</p><p>Are we learning invariant compressors? Invariant compressors should provide RD curves that are robust to test distribution shift in the desired augmentations. We thus trained our VIC by applying the augmentations 50% of the time but varying that probability p at test time. In Appx. F.3 we show that this distribution shift have negligible influence on RD curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3</head><p>A zero-shot compressor using pre-trained self-supervised models BINCE includes a standard contrastive SSL loss. So, we investigated whether existing pre-trained SSL models <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b41">41]</ref> can be used to build generic compressors. In particular, we investigated whether CLIP <ref type="bibr" target="#b41">[41]</ref> could be quickly turned into a powerful task-centric compressor for computer vision. In the introduction, we motivated large compression gains by noting that typical image classification tasks can be predicted from detailed captions instead of images (around 1000? more bits). CLIP is a vision transformer <ref type="bibr" target="#b42">[42]</ref> pre-trained on 400M pairs of images and text (x image , x + text ) using a contrastive loss. The "augmentation" A is then a function that maps x image to its associated x + text and vis-versa. This will partition the images and texts into sets, each of which are associated directly or by transitivity in CLIP's dataset. This suggests that CLIP is retaining the image information that corresponds to a detailed caption, and may be turned into a generic compressor for image classification.</p><p>CLIP can essentially be seen as a BINCE model with an image-to-text augmentation, but without an entropy bottleneck. (For details about the CLIP-BINCE relation see Appx. C.5.) We thus constructed an approximation of our desired image-to-text BINCE compressor by two simple steps. First, we downloaded and froze CLIP's parameters. Second, we trained, on the small MSCOCO dataset <ref type="bibr" target="#b44">[43]</ref>, an entropy bottleneck to compress CLIP's representation. The latter step can be done by training any lossy compressor on CLIP's representations, we did so using Ball? et al.'s <ref type="bibr" target="#b29">[28]</ref> hyperprior entropy model with a learned rounded precision. We then evaluated our resulting compressor on 8 datasets (various classification tasks and image shapes) that were never seen during training (zero-shot), by training an MLP for downstream predictions on each dataset. One can see this as a multi-task setting (each dataset is a distinct task). We investigate the case of multiple labels per images in Appx. F.5.</p><p>Can we use pretrained SSL to obtain a generic compressor? <ref type="table" target="#tab_3">Table 2</ref> shows that we can exploit existing state-of-the-art (SOTA) SSL models to get a powerful image compressor, which achieves 1000? bit-rate gains on ImageNet compared to JPEG (at the quality level used for storing ImageNet). The bit-rate gains (1 st row) are significant across all zero-shot datasets, even for biological tissues (PCam; <ref type="bibr" target="#b45">[44]</ref>). Importantly, these gains come at little cost in test performance. Indeed, the test accuracies of MLPs from our representations (2 nd row) is similar to a near SOTA model trained on the uncompressed images (3 rd row is from Radford et al. <ref type="bibr" target="#b41">[41]</ref>). These results are not surprising as JPEG is optimized to retain perceptual rather than classification information. Note that the large variance in rate gains come from JPEG rates due to different images shapes (see <ref type="table" target="#tab_4">Table 3</ref>).</p><p>Our CLIP compressor retains all the information needed to get 0 error for those tasks. <ref type="table" target="#tab_3">Table 2</ref> provides the test performance for MLPs, while our theory discusses Bayes risk, which is independent of specific predictors and generalization. We estimated the excess Bayes risk for our datasets by counting the images (in train and test) that get compressed to the same Z but have different labels.</p><p>We found that we are in the lossless prediction regime for those datasets. What is the effect of the entropy bottleneck? In <ref type="table" target="#tab_4">Table 3</ref> we compare the pretrained CLIP, to our CLIP compressor with an entropy bottleneck (EB) trained at different values for ?. When trained with a high ?, our EB improves bit-rates by an average of 6? without impacting predictions. For our compressor from Table 2 (CLIP+EB ?) the gains increase to 11? with little predictive impact. The sacrifice in predictions is more clear for 16? bit-rate gains (low ?). This shows that CLIP's raw representations retain unnecessary information as it not explicitly trained to discard information.</p><p>How would end-to-end BINCE compare to staggered training? Compression gains can likely be larger by end-to-end training of BINCE, which would require access to CLIP's original dataset. <ref type="bibr" target="#b7">7</ref> To get an idea of potential gains we compared end-to-end and staggered BINCE on augmented MNIST in Appx. F.2. We found significant rate improvements (358 to 131 bits) for similar test accuracy.</p><p>Our CLIP compressor is simple to use. In Appx. E.7, we provide a minimal script (150 lines) to train a generic compressor in less than five minutes on a single GPU. The script contains an efficient entropy coder for our model (200 images/second), which shows its practicality. As usual in SSL, the compressed representations are also more computationally efficient to work with than standard compressors. In our minimal script we achieve the desired performance (98.7% on STL) using a linear model that is trained in one second, which is 1000? faster than the baseline in <ref type="table" target="#tab_3">Table 2</ref>. This shows that our pipeline can improve computational efficiency in addition to storage efficiency.</p><p>What augmentations to use for SSL compression? <ref type="table" target="#tab_5">Table 4</ref> compares two ResNet50 pretrained with contrastive learning using invariance to text-image (CLIP) or standard image augmentations (SimCLR <ref type="bibr" target="#b33">[32]</ref>) such as cropping or flipping. We see that CLIP's augmentation usually give better compression and downstream performance, which shows the importance of the choice of augmentations. This also supports our motivation of using text-image augmentations, which are likely label-preserving for a vast amount of tasks but discard large amounts of unnecessary information. In Appx. D we discuss more related work, including invariances in compression and the link to SSL.</p><p>Task-centric compression. To our knowledge, our paper is the first to formalize compression only for predictions. IB <ref type="bibr" target="#b12">[11]</ref> uses a task-centric distortion, but is not used for compression as it requires supervised training, so there are no advantages compared to compressing predicted labels. Some authors used heuristics to bypass the supervised issue, e.g., focusing on low frequencies for classification <ref type="bibr" target="#b46">[45]</ref> or high frequencies for segmentation <ref type="bibr" target="#b47">[46]</ref>. Other authors have incorporated predictive errors to perceptual distortions <ref type="bibr" target="#b48">[47,</ref><ref type="bibr" target="#b50">48]</ref>, but cannot compress without the perceptual distortion for the same reason as IB. One exception is Weber et al.'s <ref type="bibr" target="#b51">[49]</ref> compressor, which (when removing their perceptual distortion) minimizes MSE in the hidden layers of a pretrained classifier. Even more related is Singh et al.'s <ref type="bibr" target="#b52">[50]</ref> work on compressing pretrained features for transfer learning, which is practice is similar to our compression of SSL features. Their work do not provide theoretical justifications, and are constrained to tasks that are similar to those used for pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Outlook</head><p>Given the ever increasing amount of data that is processed by task-specific algorithms, it is necessary to rethink the current task-agnostic compression paradigm. We formalized the first compression framework for retaining only the information necessary for high performance on desired tasks. Using our theory, we provide two unsupervised objectives for training neural compressors. Experimentally, we show that these compressors can achieve bit-rates that are orders of magnitude (1000? on ImageNet) smaller than standard image compressors without losing predictive performance.</p><p>There are a number of caveats that should be addressed. First, to achieve better rates, our theory requires an irrecoverable loss of information. This can be an issue if the set of desired tasks changes. For example, if one uses text-image invariances then it may be impossible to perform image segmentation from the compressed representations. One solution would be to keep an original copy and use invariant compression for duplicated data, e.g., for the thousands copies of ImageNet. A second issue is the interpretability of the compressed representations. This can be partially addressed by reconstructing prototypical data as in <ref type="figure" target="#fig_0">Fig. 1</ref> (post-hoc decoders could be trained for BINCE).</p><p>A third caveat is that the compressed representations may be harder to learn from, e.g., neural networks may struggle to predict from representations even if the information is retained. Although our experiments actually showed the opposite, this should be addressed theoretically, e.g., using decodable information <ref type="bibr" target="#b53">[51,</ref><ref type="bibr" target="#b54">52]</ref>. Finally, successful use of our framework requires access to labelpreserving augmentations A that discard significant information about X. Finding such an A may be challenging for some tasks. Given that augmentations are ubiquitous in ML, the community will hopefully continue developing task-specific augmentations which we could take advantage of.</p><p>Nevertheless, we achieved orders of magnitude improvements in compression for predictions, and we believe that our improvements are just the beginning. For example, many tasks can be answered by referencing a detailed natural language description of the data. In these cases, the improvements can be very large, potentially 1M? for videos. <ref type="bibr" target="#b8">8</ref> In the long-term, we hope that abandoning perceptual reconstructions will enable individuals to process data at scales that are currently only possible at large institutions, and our society to take advantage of large data sources in a more sustainable way. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table of Contents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Notation</head><p>Probability We assume a background standard probability space (?, H, P) that is rich enough to support all random variables used. Letters that are upper-case X represent a random variable, while realizations are denoted with the associated lower case x. The sample space of a random variable will be written using a calligraphic X , and we will say that X takes value in (t.v.i) X . We denote the probability distribution of X as P (X) and the probability density function, if it exists, as p(X). X d ? N (0, 1) denotes that X has a certain distribution (here, Gaussian). Expectations are written as:</p><formula xml:id="formula_12">E P (X) [X], or E p(X) [X]</formula><p>when the density exists. Independence between two random variables X and Y is denoted with X? ?Y . To denote conditional independence between two random variables X and Y given Z we either use X? ?Y | Z or say that X ? Z ? Y forms a Markov Chain. f ? g denotes a composition of two functions f and g, but in the case of random variable we also use the shorthand f (X) := f ? X.</p><p>Information theory For notational convenience (see Assumption 5 below), when dealing with log loss we will always assume the existence of probability densities, in which case the KL divergence between two probability distributions on X , P and Q, is</p><formula xml:id="formula_13">D KL [p(X) q(X)] := p(X) log p(X) q(X) dx.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The mutual information between random variables X and</head><formula xml:id="formula_14">Z is I[X; Z] := D KL [p(X, Z) p(X)p(Z)]. The (differential or discrete) entropy of a random variable is H[X] := E P (X) [? log p(X)], while the conditional (differential) entropy is H[X | Z] := E P (X,Z) [? log p(X|Z)].</formula><p>Equivalence x ? x denotes that x and x are equivalent with respect to (w.r.t.) an equivalence relation on X (the exact relation being implicit). The equivalence class of x under ? consist of all elements that are equivalent to x, i.e. [x] := {x ? X | x ? x}. The set of all equivalence classes (the quotient set) will be denoted as</p><formula xml:id="formula_15">X / ? := {[x] | x ? X }, while the canonical projection is denoted as ? ? : x ? [x].</formula><p>Risk minimization We will often use variational optimization. When the variational family is not made explicit it means that the optimization is over all functions with the correct domain and codomain, e.g. min q(Y | X) means that that the optimization is done over the collection of all conditional probability densities on Y given the random variable X.</p><p>For a fixed "action" or "decision" space A, a loss function is defined as L :</p><formula xml:id="formula_16">Y ? A ? R. The (expected) risk of a predictor h : X ? A is E P (X,Y ) [L(Y, h(X))].</formula><p>The Bayes (best achievable) risk when predicting Y from X using some (unspecified) loss is denoted as R[Y | X]. When the loss L is specified, we denote the Bayes risk as</p><formula xml:id="formula_17">R L [Y | X] := inf h : X ?A E P (X,Y ) [L(Y, h(X))].</formula><p>For the case of log loss (always assumed in the main text) <ref type="bibr" target="#b2">2</ref> . Letters X, Z, and Y refer to the input, representation and target of a predictive task, respectively.</p><formula xml:id="formula_18">we have R log [Y | X] := inf q E P (X,Y ) [? log q(Y |X)]. For MSE loss we have R mse [Y | X] := inf f : X ?Y E P (X,Y ) Y ? f (X)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Assumptions</head><p>In this section, we discuss the assumptions that we make throughout our paper. Specifically, we discuss why we make those assumption and why such assumptions should hold in practice. All our assumptions should hold in most practical scenarios. The following assumptions will be implicit in the rest of our work.</p><p>Assumption 1 (Finite risk). We restrict ourselves to tasks Y , such that | R[Y | ?] | &lt; ? for any finite constant ? in the domain of the predictor f . Similarly we restrict ourselves to X with R[X | ?] &lt; ?, and to equivalences relations on X such if there exists a maximal invariant then there exists some</p><formula xml:id="formula_19">maximal invariant M (X) with R[M (X) | ?] &lt; ? for any finite constant ?.</formula><p>Assumption 1 ensures that we can take differences of Bayes risks as in Definition 7. For the case of log loss our assumption is equivalent to requiring finite (differential or discrete) entropy of H[X], H[M (X)] and H[Y ]. For MSE loss, this is equivalent to finite variance for X, Y and M (X). Specifically, we will restrict ourselves to random variables Y and M (X) that are bounded in L 2 (?, H, P). (See Assumption 6 in Appx. B.6.) Note that R[M (X) | ?] &lt; ? comes directly from R[X | ?] &lt; ? for the two main losses that we consider. Indeed, for log loss this comes directly from the data processing inequality. For MSE such M (X) can easily be constructed by mapping any x to a value in [x] that is smaller than the expected value over the equivalence class</p><formula xml:id="formula_20">E[X|X ? [x]].</formula><p>Assumption 2 (Existence of regular conditional probabilities). We restrict ourselves to standard Borel measurable spaces, so that the existence of regular conditional probability distributions is ensured. This is necessary to ensure the existence of probability kernel in Lemma 6. This technical assumption essentially holds for all practical purposes. Unless stated otherwise, we denote B(Y) the Borel ?-algebra of a set Y.</p><p>Assumption 3 (Measurability of functions). We assume that all functions introduced in the following sections are measurable with respect to the "natural" measurable spaces of the functions' domain and codomain. (A few special functions will be shown to be measurable.) In particular, we require (i) the measurability of M (?) which implies that M (X) is a random variable; and (ii) the measurability of the projection ? ? : X ? X / ?, which implies that there always exists a maximal invariant in the form of the projection ? ? . This technical assumption holds for essentially all practical purposes. Assumptions 1 to 3 are used throughout our work. Two further assumptions are needed for log loss, which we remove in Appx. B.6 when we obtain results for MSE.</p><p>Assumption 4 (Countably many equivalence classes). For the log loss risk (Appx. B.3) we restrict our discussion to equivalences such that the quotient set X / ? is countable. This ensures that M (X) is a discrete random variable thereby ensuring that our invariance distortion R[M (X) | Z] is independent of the choice of maximal invariant M as the conditional entropy is invariant to bijections.</p><p>Note that Assumption 4 holds when X is countable which always happens in practice due to floating point arithmetic, i.e. every real number has to be rounded to the closest 64 bits number. Another perspective is to say that X is actually uncountable, but that all tasks we care about are always invariant to rounding to the nearest 64 bits number due to floating point arithmetic. As a result, the maximal invariant is the usual maximal invariant rounded to the closest floating point. For example, if X is a 2D Gaussian we cannot work directly with translations on the y-axis (which gives uncountably many [x], one for each real number on the x-axis), but can work with y-axis invariance combined with invariance to rounding on the x-axis (e.g. closest 64 bits number).</p><p>Assumption 5 (Convenience assumption: Existence of densities). In sections Apps. B.2 and B.3, where we work with log loss, we restrict ourselves to cases where the (conditional) probability mass/density function exist, i.e., to probability distributions that are absolutely continuous w.r.t. to some (shared) underlying measure. This assumption is not needed but it simplifies the notation, and ensures that the differential entropy of random variables is well defined. Such assumption could be removed by using the general definition of mutual information as a supremum over partitions and by defining continuous entropy as H[X] = I[X; X] <ref type="bibr" target="#b55">[53,</ref><ref type="bibr" target="#b56">54]</ref>, also known as Jaynes's <ref type="bibr" target="#b57">[55]</ref> limiting density of discrete points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Definitions</head><p>In the main paper we were relatively informal in our definitions, here we restate our main definitions more formally.</p><p>Definition 2 (Maximal invariant). Let ? denote an equivalence relation on X . We say that a measurable function M :</p><formula xml:id="formula_21">X ? M is a maximal invariant w.r.t. (X , ?) if ?x, x ? X x ? x ?? M (x) = M (x )<label>(10)</label></formula><p>Note that our notion of maximal invariants generalizes the notion of maximal invariants in probabilistic group theory <ref type="bibr" target="#b20">[19]</ref>. We refer the reader to Lehmann and Romano <ref type="bibr" target="#b58">[56]</ref> for many examples in the group case. As in the group case, a maximal invariant typically is not unique.</p><p>The invariance structure that we want our tasks to have is based on their conditional distributions given X, defined as follows.</p><formula xml:id="formula_22">Definition 3 (Conditional invariance). We say that Y is conditionally invariant w.r.t. (X , ?), if the regular conditional distribution x ? P (Y | x) is invariant w.r.t. ?, i.e. ?x, x ? X we have x ? x =? P (Y | x) = P (Y | x )<label>(11)</label></formula><p>Definition 4 (Invariant tasks of interest). The set of all invariant tasks of interest T ? w.r.t. to a loss and an equivalence (X , ?) is the set of all random variables Y that are conditionally invariant w.r.t. (X , ?) and that satisfy Assumption 1 (finite risk).</p><p>First we require the notion of a valid distortion <ref type="bibr" target="#b19">[18]</ref>, which ensures that we can apply the rate distortion theorem.</p><p>Definition 5 (Valid distortion). Let X and Z be two random variables that take values in X and Z, respectively. Then</p><formula xml:id="formula_23">an (expected) distortion D is valid w.r.t. X, Z if there exists a point-wise distortion d : X ? Z ? R ?0 such that E p(X,Z) [d(X, z)] ? ? for some z ? Z and D[X, Z] := E p(X,Z) [d(X, Z)] .<label>(12)</label></formula><p>In the context of the current work, a representation Z which arises by encoding X using p(Z | X) should not depend on any particular task Y .</p><p>Definition 6 (Representation for a task set). Let X, Z be two random variables and T be a set of random variables. Z is a representation of X for T if for all Y ? T such that Y and Z are not almost surely equal, we have the pairwise conditional independence Y ? ?Z | X.</p><p>Note that if Z / ? T then it is not almost surely equal to any Y ? T . The condition allows for the possibility that Z ? T , but it must be conditionally independent, given X, of all other random variables in T .</p><p>We now recall the excess risk distortion D T . Definition 7 (Excess risk distortion). Let X and Z be two random variables. Let T be a set of random variables such that under a loss L, the Bayes risks in (13) below are well defined for each Y ? T . The excess risk distortion D T is defined as:</p><formula xml:id="formula_24">D T [X, Z] := sup Y ?T R[Y | Z] ? R[Y | X]<label>(13)</label></formula><p>B Proofs: optimal bit-rate</p><p>In this section we prove all results from Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Basic properties of equivalence relations and maximal invariants</head><p>To begin, we collect some basic properties of equivalence relations and maximal invariants. As these a general result that might be of interest beyond our work (especially Lemma 6) we will prove them without assuming the existence of densities, i.e., without Assumption 5. Recall that ? ? : X ? X / ? is the projection from X onto its quotient by ?, denoted X / ?. <ref type="bibr" target="#b59">[57]</ref>, <ref type="bibr">Theorem 19)</ref>. Given an equivalence relation ? on X , let f : X ? S be any function such that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3 (Mac Lane and Birkhoff</head><formula xml:id="formula_25">x ? x ? f (x) = f (x ).</formula><p>Then there is exactly one function g : </p><formula xml:id="formula_26">X / ?? S for which f = g ? ? ? . If f is a surjection and f (x) = f (x ) ? x ? x , then g is a bijection.</formula><formula xml:id="formula_27">= f ? M with f := g ? g ?1 which is indeed bijective: f ?1 := g ? g ?1 . Lemma 5. Let M be any maximal invariant w.r.t. (X , ?). Then a measurable function f : X ? S is invariant with respect to (X , ?) if and only if there exists a measurable function h : M ? S such that f (x) = (h ? M )(x) for all x ? X , in which case f is measurable with respect to the ?-algebra generated by M . Proof. Clearly f = h ? M is (X , ?)-invariant because M is,</formula><p>and measurability of f follows from measurability of M and h.</p><formula xml:id="formula_28">From Lemma 3, if f is (X , ?)-invariant then there is a function s : X / ? ? X such that f = s ? ? ? .</formula><p>Since ? ? and f are measurable, so too is s. Again by Lemma 3, there exists a bijective mapping g :</p><formula xml:id="formula_29">X / ? ? M such that M = g ? ? ? . We thus conclude that f = h ? M , for h := s ? g ?1 .</formula><p>The measurability of h follows from the measurability of s and of g ?1 .</p><p>Finally, we establish a key conditional independence relationship, which shows that for invariant tasks, Y ? M (X) ? X forms a Markov Chain. This is a generalization of an probabilistic group theoretical results (Theorem 4.4 in Eaton <ref type="bibr" target="#b20">[19]</ref>, Theorem 7 in Bloem-Reddy and Teh <ref type="bibr" target="#b60">[58]</ref>), to any equivalences (rather than only group orbits) and without making the assumption of (marginal) invariance of P (X) to ?. Lemma 6. Let X and Y be two random variables, and M :</p><formula xml:id="formula_30">X ? M be a maximal invariant w.r.t. (X , ?) as in Definition 2. Then Y is conditionally invariant w.r.t. (X , ?) as in Definition 3 if and only if Y ? ?X | M (X). Proof. Let Y be a Y-valued random variable that is conditionally invariant w.r.t. (X , ?). Recall that B(Y) is the Borel ?-algebra of Y. By Assumption 2, there exists a probability kernel ? Y (A, x) from (X , B(X )) into (Y, B(Y)), such that for each set A ? B(Y), x ? ? Y (A, x) is a measurable function mapping X ? R ?0 . Conditional invariance means that x ? x =? ? Y (A, x) = ? Y (A, x ) for each A ? B(Y). That is, as a function of x, ? Y (A, ? ) is invariant w.r.t. (X , ?). By Lemma 5, x ? ? Y (A, x) = ? Y (A, M (x)), where ? Y is a probability kernel from (M, B(M)) into (Y, B(Y)). Therefore, for any A ? B(Y), B ? B(X ), E X,Y [1 B (X)1 A (Y )] = E X [1 B (X)? Y (A, X)] = E X [1 B (X)? Y (A, M (X))]</formula><p>, which can be extended to arbitrary measurable functions on X ? Y by a standard (monotone class) argument. This in turn implies that P Y |M (X) is a version of P Y |X , i.e., they are equal almost surely P(X), and therefore Y ? ?X | M (X).</p><p>Finally, we prove that in realistic settings, there exists at least one M (X) ? T ? . Lemma 7. Let T ? be the invariant tasks of interest w.r.t. (X , ?) and any loss function. Then there exists at least one maximal invariant that belongs to T ? .</p><p>Proof. First, we have to prove by construction that a maximal invariant always exists. By definition equivalent elements have the same equivalence class and so x ? x ?? ? ? (x) = ? ? (x ). The projection map is measurable by assumption (Assumption 3), so it is a maximal invariant.</p><p>Second, due to the existence of at least one maximal invariant M = ? ? we have by Assumption 1 that that there exists at least one M s.t.</p><formula xml:id="formula_31">R[M (X) | ?]. This M is therefore in T ? .</formula><p>We close this section by establishing some properties of Bayes risk in this context. The following lemma, a data-processing inequality, appears in Xu and Raginsky <ref type="bibr" target="#b61">[59]</ref>; we include it here for completeness, and provide a slightly more detailed proof.</p><p>Lemma 8 (Data-processing inequality for Bayes risk). Let Z ? X ? Y be a Markov chain of random variables. For any loss function L,</p><formula xml:id="formula_32">R[Y | X] ? R[Y | Z] .<label>(14)</label></formula><p>Proof</p><formula xml:id="formula_33">. Recall that one characterization of conditional independence is that Y ? ?Z | X if and only if Z = f (X, U ) almost surely for some measurable function f and U ? Unif(0, 1) with U ? ?(X, Y ) [60, Prop. 6.13].</formula><p>Let ? z be a Bayes decision rule for predicting Y from Z, and likewise for ? x . By definition,</p><formula xml:id="formula_34">R[Y | Z] = E Z,Y [L(Y, ? z (Z))] = E X,Y,U [L(Y, ? z (f (X, U )))] . For any u ? (0, 1), ? z (f ( ? , u)) is a valid decision rule for predicting Y from X with risk at least as great as ? x . Therefore, R[Y | X] ? R[Y | Z].</formula><p>Corollary 9. Let T ? be the invariant tasks of interest with respect to (X , ?) and any loss function, and M any maximal invariant. For any Y ? T ? ,</p><formula xml:id="formula_35">R[Y | M (X)] = R[Y | X] .<label>(15)</label></formula><p>Proof. The result follows from applying Lemma 8 to the trivial conditional independence Y ? ?M (X) | X and the non-trivial conditional independence from Lemma 6 Y ? ?X | M (X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proposition 1: simplifying and validating D T for log loss</head><p>In this section we show that Definition 7 is a valid distortion for log loss, and we prove the equivalence between Definition 7 and R log [M (X) | Z]. That equivalence is the key to prove Theorem 2.</p><p>The main steps in the proof are the following:</p><p>1. Using the strict properness of the log loss, we relate the Bayes risk to the entropy:</p><formula xml:id="formula_36">R log [Y | Z] = H[Y | Z]<label>(16)</label></formula><p>2. We show that the supremum is achieved by M (X):</p><formula xml:id="formula_37">sup Y ?T? R log [Y | Z] ? R log [Y | X] = H[M (X) | Z] ? H[M (X) | X]<label>(17)</label></formula><p>3. Since M is a deterministic function and M (X) is discrete, we have H[M (X) | X] = 0. Therefore,</p><formula xml:id="formula_38">sup Y ?T? R log [Y | Z] ? R log [Y | X] = H[M (X) | Z]<label>(18)</label></formula><p>4. We conclude, as desired, that</p><formula xml:id="formula_39">sup Y ?T? R log [Y | Z] ? R log [Y | X] = R log [M (X) | Z]<label>(19)</label></formula><p>The first step consists of relating the log loss Bayes risk and conditional entropy. This is a simple lemma that directly comes from the fact that the conditional distribution p(Y | Z) is the Bayes predictor.</p><p>Lemma 10. Let Y, X be random variables then the log loss Bayes risk is equal to the conditional (discrete or differential) entropy:</p><formula xml:id="formula_40">R log [Y | X] = H[Y | X]<label>(20)</label></formula><p>Proof.</p><formula xml:id="formula_41">R log [Y | X] = inf q(Y | X) E P (X,Y ) [? log q(Y |X)] Definition (21) = E P (X,Y ) [? log p(Y |X)] Strict Proper. (22) = H[Y | X] Definition<label>(23)</label></formula><p>Where Eq. <ref type="formula" target="#formula_1">(22)</ref> uses the strict properness of the logarithmic scoring function rule <ref type="bibr" target="#b63">[61]</ref>.</p><p>In the rest of the section, we will often be working with H[M (X)] and H[M (X) | Z]. Importantly, we would like our results to be independent of the choice of maximal invariant M . We now prove that this will indeed be the case as all these (conditional) entropy terms are independent of the choice of M . We only prove it for the marginal entropy H[M (X)] but the same proof holds for conditional entropies. Proof. Any M (X) is conditionally invariant due to the Definition 2. From Assumption 1 we know that there exists at least one M (X) with finite entropy, by Lemma 11 they must all have finite entropy. We conclude that all M (X) ? T ? (Definition 4).</p><p>We are now ready to prove the desired proposition. </p><formula xml:id="formula_42">D T? [X, Z] = R log [M (X) | Z]<label>(24)</label></formula><p>Proof. We first prove that D T? [X, Z] = H[M (X) | Z], from which it is straightforward to show that D T? is a valid distortion. Starting from the definition of D T? , we have</p><formula xml:id="formula_43">D T? [X, Z] := sup Y ?T? R log [Y | Z] ? R log [Y | X] Definition 7 (25) = sup Y ?T? H[Y | Z] ? H[Y | X] Lemma 10 (26) = sup Y ?T? H[Y | Z] ? H[Y | X, M (X), Z] Y ? ?(M (X), Z)|X (27) = sup Y ?T? H[Y | Z] ? H[Y | M (X), Z] Y ? ?X|(M (X), Z) (28) = sup Y ?T? I[Y ; M (X)|Z] Def. (29) = sup Y ?T? H[M (X) | Z] ? H[M (X) | Y, Z]</formula><p>Symmetry and def.</p><formula xml:id="formula_44">(30) = H[M (X) | Z] ? inf Y ?T? H[M (X) | Y, Z] (31) = H[M (X) | Z] ? 0 Discrete H and Y = M (X) (32) = R log [M (X) | Z] Lemma 10<label>(33)</label></formula><p>Eq. <ref type="formula" target="#formula_1">(27)</ref>  From Eq. (32) it is easy to see that D T? is valid as</p><formula xml:id="formula_45">D T? [X, Z] = H[M (X) | Z] = E p(X,Z) [d(X, Z)] with d(x, z) := ? log p(M (x) | z) which due to the discreteness of M (X) (Assumption 4)</formula><p>is a function whose codomain is R ?0 as desired. Due to Assumption 1 we know that for all constant</p><formula xml:id="formula_46">z ? Z we have H[M (X) | z] ? ?, so D T? is valid. Note that D T? [X, Z] = R log [M (X) | Z]</formula><p>is very simple to work with if we have access to some M (X). Unfortunately, in practice M (X) might not be known, but often we will have access to some other random variableX which has all the information necessary about M (X). See for example Appx. C.3. We now prove that in such case we can optimize</p><formula xml:id="formula_47">R log X | Z instead of R log [M (X) | Z]. Proposition 13 (Invariant Distortion without M (X)). Let T ? , ?, M, X, Z, D T? [X, Z] be as in Prop. 1. LetX be a random variable such thatX ? X andX? ?X | M (X) almost surely. Then D T? [X, Z] = R log X | Z + c<label>(34)</label></formula><p>where c depends only onX and not on Z.</p><p>Proof. From Definition 6 and the fact that M (X) ? T ? (Lemma 12) we know that Z ? X ? M (X) forms a Markov Chain (MC). Due to our assumptionX ? X a.s. we also have that M (X) = M (X) a.s. so M (X) ?X ? M (X) forms a MC. Putting all together we obtain the MC Z ? X ? M (X) ? X ? M (X), which allows us to derive the following.</p><formula xml:id="formula_48">D T? [X, Z] = R log [M (X) | Z] Prop. 1 (35) = H[M (X)] ? I[M (X); Z] Lemma 10 (36) = H[M (X)] ? I (M (X),X); Z Z ? M (X) ?X (37) = H[M (X)] ? I X ; Z Z ?X ? M (X) (38) = H[M (X)] ? H X + H X | Z (39) = H X | M (X) + R log X | Z Lemma 10<label>(40)</label></formula><p>where the last line uses the Markov Chain M (X) ?X ? M (X) to provide a more interpretable value for the constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Theorem 2: optimal bit-rate under log loss</head><p>Our main theoretical result is to characterize the minimal achievable rate to bound the Bayes risk of any invariant task. Here we provide the proof for the case of log loss risk. The result follows from Shannon's <ref type="bibr" target="#b18">[17]</ref> rate distortion theorem, and the validity of D T? (Proposition 1).</p><p>For convenience, we restate the well known rate distortion theorem. </p><formula xml:id="formula_49">R(?) = min p(Z|X) such that D[X;Z]?? I[X; Z]<label>(41)</label></formula><p>We can now state our rate-invariance theorem.</p><p>Theorem 2 (Rate-invariance for log loss). Let ? ? 0. Let ? be an equivalence relation on X that partitions X into countably many equivalence classes (Assumption 4). Let T ? be the invariant tasks of interest w.r.t. (X ,?) and the log loss, M be any maximal invariant, and Z be a representation of X for T ? . Let Rate(?) denote the minimum achievable bit-rate for transmitting an i</p><formula xml:id="formula_50">.i.d. source of Z such that for any Y ? T ? we have R log [Y | Z] ? ? + R log [Y | X].</formula><p>Then Rate(?) is finite and given by</p><formula xml:id="formula_51">Rate(?) = max(0, H[M (X)] ? ?) (42) = max(0, H[X] ? H[X | M (X)] ? ?)<label>(43)</label></formula><p>Proof. We first prove that Rate(?) ? max(0, H[M (X)] ? ?). We then prove that the rate</p><formula xml:id="formula_52">max(0, H[M (X)] ? ?) is achievable and so Eq. (42) holds. Finally, we prove that H[M (X)] = H[X] ? H[X | M (X)] so Eq. (43) holds which concludes the proof. We want to transmit Z such that ?Y ? T ? we have R log [Y | Z] ? ? + R log [Y | X], in other words we would like sup Y ?T R log [Y | Z] ? R log [Y | X] =: D T? [X, Z] ? ?. As D T? is valid (Proposition 1)</formula><p>we can directly apply the rate distortion theorem ( <ref type="formula" target="#formula_0">Lemma 14)</ref>: </p><formula xml:id="formula_53">Rate(?) = min p(Z|X) s.t. D T? [X,Z]?? I[X; Z] Lemma 14 and Proposition 1 (44) ? min p(Z|X) s.t. D T? [X,Z]?? I[M (X); Z] DPI (45) = min p(Z|X) s.t. D T? [X,Z]?? H[M (X)] ? H[M (X) | Z]<label>(46)</label></formula><formula xml:id="formula_54">H[M (X)] ? ? (48) = H[M (X)] ? ? No Z (49)</formula><p>Where Eq. (45) uses the data processing inequality (DPI). As the rate is always non-negative we have</p><formula xml:id="formula_55">Rate(?) ? max(0, H[M (X)] ? ?).</formula><p>We now prove that max(0, H[M (X)] ? ?) is attainable and so Rate(?) = max(0, H[M (X)] ? ?). Specifically we need to find a representation Z of X such that</p><formula xml:id="formula_56">Rate(?) = 0 If ? ? H[M (X)] H[M (X)] ? ? Else<label>(50)</label></formula><p>The first case is trivial: set Z to be independent of M (X) and . Note that we will never divide by zero as H[M (X)] = 0 would be in the first case of Eq. <ref type="bibr" target="#b52">(50)</ref>. Importantly this Z still satisfies X? ?Z | M (X) as it was constructed solely using M (X) and independent noise.</p><p>We thus proved that max(0, H[M (X)] ? ?) is obtainable and that Rate(?) ? max(0, H[M (X)] ? ?). From which we conclude that the best achievable bit-rate is</p><formula xml:id="formula_57">Rate(?) = max(0, H[M (X)] ? ?). Eq. (43), follows from H[M (X)] = I[M (X); X] = H[X]?H[X | M (X)],</formula><p>which is a valid decomposition as both (differential conditional) entropy term are finite due to Assumption 1. The finiteness of Rate(?) comes from the fact that Rate(?) ? H[M (X)] &lt; ? due to Assumption 1.</p><p>By setting ? = 0 we directly get the best achievable rate for the lossless prediction but lossy compression setting.</p><p>Corollary 15 (Invariant source coding for log loss). Let X,?, T ? , M , Z be as in Theorem 2. Let Rate(0) denote the minimum achievable bit-rate for transmitting an i.i.d. source of Z such that for</p><formula xml:id="formula_58">any Y ? T ? we have R log [Y | Z] = R log [Y | X].</formula><p>Then Rate(0) is finite and given by</p><formula xml:id="formula_59">Rate(0) = H[M (X)] (52) = H[X] ? H[X | M (X)]<label>(53)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Recovering previous results in the literature</head><p>Corollary 15 recovers many previous results in the literature:</p><p>Unlabeled Graphs Let us consider the task of compressing unlabeled graphs, here we consider tasks that are invariant to graph isomorphisms. A possible maximal invariant is the graph canonization and H[M (X)] becomes the well known structural entropy (also called topological information content) <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b67">65]</ref>. If all isomorphic graphs are permissible and equiprobable, Yongwook Choi and Szpankowski <ref type="bibr" target="#b67">[65]</ref> show that the structural entropy is</p><formula xml:id="formula_60">H[S] = H[X] ? E x?p(X) log n! |Aut G (x)|</formula><p>. This is Eq. (53), where the second term corresponds to H[X | M (X)] with a uniform distribution on isomorphic graphs. Multisets Let us derive the best achievable bit-rate for compressing multisets. Let X be any sequence and T ? be invariant to permutations of that sequence. One possible maximal invariant in that case is the empirical measure (also called type), i.e., the counts K 1 , . . . , K n of each of the n elements that are present in the sequence X. Lossless compression of multisets thus requires H[M (X)] = H[K 1 , . . . , K n ], as discussed by Varshney and Goyal <ref type="bibr" target="#b22">[21]</ref>. Using Eq. (53) we can also characterize the bits gains that you obtain by considering the invariance, namely, H[X|M (X)]. This recovers theorem 1 of Varshney and Goyal <ref type="bibr" target="#b22">[21]</ref>, where H[X|M (X)] is called the "order entropy". Note that similarly to our example in the main text about i.i.d. coin flips, the amount of bits needed to losslessly compress the multiset grows as ?(log n) <ref type="bibr" target="#b22">[21]</ref>. Information Bottleneck (IB) Suppose you are interested in predicting a single task</p><formula xml:id="formula_61">Y = t(X),</formula><p>where t is a (deterministic) "target function". The task is invariant to any transformations between examples in the preimage of the labeling. So the maximal invariant is t(?) and the distortion becomes</p><formula xml:id="formula_62">H[t(X) | Z] = H[Y | Z].</formula><p>Then the rate-distortion function (Eq. (41)) becomes the information bottleneck (IB) <ref type="bibr" target="#b12">[11]</ref>. Using Corollary 15 we see that for lossless predictions the optimal rate is Rate</p><formula xml:id="formula_63">(0) = H[Y ] = H[X] ? H[X | Y ] = I[X;</formula><p>Y ] as shown in <ref type="bibr" target="#b68">[66,</ref><ref type="bibr" target="#b69">67]</ref>. From a compression stand point this is nevertheless not very useful as Rate(0) = H[Y ], so IB for deterministic labels tells you to entropy code the labels Y . Lossless Let X be discrete. Every task will always be invariant to the equality "=" equivalent relation.</p><p>In this case the maximal invariant is the identity function, and we recover Shannon B.5 Generalizing Theorem 2: optimal bit rate for lossless prediction and any loss Corollary 15 characterizes the minimal achievable bit rate for the lossless prediciton regime w.r.t. log loss. Here we show that the same result generalizes to essentially all loss function of practical interest.</p><p>The invariant source coding theorem does not hold for any loss, for example if a loss is a constant function then the Bayes risk R L [Y | Z] will not depend on the input Z, and so the best achievable bit rate will trivially be 0 which is different than H[M (X)]. But it essentially holds for all losses that are minimized only by the "correct" predictor. Specifically it holds for all losses that we dub information preserving.</p><p>Definition 8 (Information preserving losses). Let L : Y ? A ? R ?0 be any loss function such that the Bayes risk R L [Y | Z] is well defined for all random variable Z. We say that L is an information preserving loss iff the optimal risk of deterministic targets is achieved only using inputs that have all the information about the output, i.e., iff for any function t : X ? Y and and r.v.s Z, X we have</p><formula xml:id="formula_64">R L [t(X) | X] = R L [t(X) | Z] =? ?h : Z ? Y s.t. t(X) a.s. = h(Z)<label>(54)</label></formula><p>In particular we have that if t(X) is a discrete r.v. then</p><formula xml:id="formula_65">R L [t(X) | X] = R L [t(X) | Z] =? H[t(X) | Z] = 0.</formula><p>Essentially all losses used in practice satisfy Definition 8. For example it holds for the following very general families of losses:</p><p>Strictly proper scoring rules Let L : Y ? P(Y) ? R ?0 be a scoring rule that essentially quantifies with L(y, q(Y ), ) the price/loss incurred by probabilistic prediction q(Y ) when y is observed (lower is better). L is strictly proper <ref type="bibr" target="#b63">[61]</ref> (w.r.t. P(Y)) iff:</p><formula xml:id="formula_66">?p, q ? P(Y) E p(Y ) [L(y, q(Y ))] ? E p(Y ) [L(y, p(Y ))]<label>(55)</label></formula><p>with equality if and only if p = q. Common examples are the log loss <ref type="bibr" target="#b70">[68]</ref>, Brier score <ref type="bibr" target="#b71">[69]</ref>, spherical score <ref type="bibr" target="#b72">[70]</ref>, or the maximum mean discrepancy with characteristic bounded kernels <ref type="bibr" target="#b73">[71,</ref><ref type="bibr" target="#b74">72]</ref>.</p><p>Point-wise loss functions Let L : Y ?? ? R ?0 be a loss function that essentially quantifies with L(y,?) the price/loss incurred by the point prediction? when y is observed (lower is better). As is standard <ref type="bibr" target="#b75">[73]</ref> we assume that :</p><formula xml:id="formula_67">L(y,?) = 0 ??? = y<label>(56)</label></formula><p>This holds for most pointwise losses of interest: mean squared error, mean absolute error, 0-1 loss (accuracy), Huber loss, . . . = t(X) as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now let us consider the case of proper scoring rules. Suppose that</head><formula xml:id="formula_68">Z is such that R L [t(X) | Z] = R L [t(X) | X].</formula><p>As L is a strictly proper scoring rule we have that p(t(X)|Z) a.s.</p><p>= p(t(X)|X). The latter is a delta function, so the former must also. We thus have the existence of h s.t. h(Z) a.s.</p><p>= t(X) as desired.</p><p>We now have all the tools to prove the general invariant source coding theorem. </p><formula xml:id="formula_69">= H[X] ? H[X | M (X)]<label>(57)</label></formula><p>Proof. By Corollary 9 we know that for all</p><formula xml:id="formula_71">Y ? T ? we have R L [Y | X] = R L [Y | M (X)]</formula><p>for any loss function. As a result, the lossless prediction bit rate is at most Rate(0) ? H[M (X)] because by Shannon's <ref type="bibr" target="#b76">[74]</ref> source coding theorem M (X) can be transmitted using H[M (X)] bits as it is discrete (Assumption 4) and its entropy is finite (Assumption 1).</p><p>Let us now show that it is not possible to achieve a lower rate. By Lemma 7 there exists at least one maximal invariant such that M (X) ? T ? . We now prove that there is no Z such that B.6 Generalizing Theorem 2: optimal bit rate under MSE loss</p><p>In Appx. B.3 we proved the rate-invariance theorem for the case of log loss. Log loss is the standard loss function for classification in ML, but in the case of regresssion it is more common to use the MSE loss function. In Theorem 17 we have seen that our results for lossless prediction regime also holds for MSE (and other losses). Due to the importance of MSE in ML, we also provide a full rate-invariance theorem for MSE.</p><p>We assume that Y ? R k for all Y ? T ? , with some k &lt; ?. Tasks taking values in fewer dimensions can always be padded with zeros. In this section || ? || denotes the Euclidean norm, the L 2 -norm of a random variable X is E[||X|| 2 ], and L 2 (?, H, P) is the Hilbert space of all R k -valued random variables with finite L 2 -norm (random variables that are almost surely equal are identified as the same element of L 2 ). Since ? and P remain unchanged but we may consider different ?-algebras, we use, e.g., L 2 (H) for short.</p><p>Importantly, we do not require Assumption 4 (countable X / ?). Instead, we require the following common (known as a finite power constraint in compression <ref type="bibr" target="#b66">[64]</ref>) regularity condition on T ? to ensure that we can attain a relevant supremum.</p><p>Assumption 6 (L 2 -boundedness). We assume that T ? is bounded in L 2 . That is, there is some</p><formula xml:id="formula_72">0 &lt; B 2 &lt; ? such that E[||Y || 2 ] ? B 2 for all Y ? T ? .</formula><p>Note that this is a slightly more stringent version of Assumption 1, as it essentially requires bounded variance of Y rather than only finite variance. The main step in the proof of a rate-invariance theorem for MSE is the following proposition, which shows that the excess risk distortion for MSE is a valid distortion that can be expressed in terms of a maximum over M b . </p><formula xml:id="formula_73">f ?M b R mse [f (M (X)) | Z] .<label>(59)</label></formula><p>Proof. For compactness of notation, we use, for example, E X,Y to denote expectation with respect to P, and E X E Y |X to denote an iterated expectation. Our proof makes use of conditional expectation in L 2 being defined as projection in a Hilbert space. See [e.g., 75, Ch. <ref type="bibr" target="#b23">[22]</ref><ref type="bibr" target="#b24">[23]</ref>.</p><p>Firstly, fix some Y ? T ? . It is well known that</p><formula xml:id="formula_74">E[||Y ? ?(X)|| 2 ] = E[ E[||Y || 2 | X] ? ||E[Y | X]|| 2 ] + E[ ||E[Y |X] ? ?(X)|| 2 ] .</formula><p>Taking the infimum over all measurable ? :</p><formula xml:id="formula_75">X ? R k , we have inf ? : X ?R k E X,Y [||Y ? ?(X)|| 2 ] = E X E Y |X [||Y || 2 ] ? ||E Y |X [Y ]|| 2 , when ?(X) = E[Y | X] P(X)-almost everywhere. Now by the conditional invariance, Y ? ?X | M (X) (Lemma 6), which implies E Y |X [f (Y )] = E Y |M (X) [f (Y )] for any measurable function f . Therefore, inf ? : X ?R k E X,Y [||Y ? ?(X)|| 2 ] = E M (X) E Y |M (X) [||Y || 2 ] ? ||E Y |M (X) [Y ]|| 2 ,<label>(60)</label></formula><formula xml:id="formula_76">when ?(X) := ? (M (X)) = E Y |M (X) [Y ] P(X)-almost everywhere. Similarly, for fixed Z t.v.i Z with Z? ?Y | M (X), E Z,Y [||Y ? ?(Z)|| 2 ] = E M (X) E Y,Z|M (X) [[||Y ? ?(Z)|| 2 ]<label>(61)</label></formula><formula xml:id="formula_77">= E M (X) E Y |M (X) [||Y || 2 ] ? ||E Y |M (X) [Y ]|| 2 + E M (X) ||E Y |M (X) [Y ] ? E Z|M (X) [?(Z)]|| 2 + E Z|M (X) [||?(Z) ? E Z|M (X) [?(Z)]|| 2 ]</formula><p>Observe that for any Y ? T ? , (60) and the first term of (61) will cancel in the excess risk distortion. Therefore,</p><formula xml:id="formula_78">D T? [X, Z] = sup Y ?T? inf ?:Z?R k E M (X) ||E Y |M (X) [Y ] ? E Z|M (X) [?(Z)]|| 2 +E Z|M (X) [||?(Z) ? E Z|M (X) [?(Z)]|| 2 ] .</formula><p>When taking the supremum over Y ? T ? , Y can only affect D T? through its conditional expectation given</p><formula xml:id="formula_79">M (X), E Y |M (X) [Y ]. That conditional expectation is a B(M)-measurable function, so E Y |M (X) [Y ] ? M b for all Y ? T ? . Therefore, {E Y |M (X) [Y ] : Y ? T ? } ? M b ? T ? ,</formula><p>and we can take the supremum over functions</p><formula xml:id="formula_80">f ? M b instead of Y ? T ? , which yields D T? [X, Z] = sup f ?M b inf ?:Z?R k E M (X) ||f (M (X)) ? E Z|M (X) [?(Z)]|| 2 +E Z|M (X) [||?(Z) ? E Z|M (X) [?(Z)]|| 2 ] .</formula><p>Expanding each quadratic and canceling terms involving E M (X) [||E Z|M (X) [?(Z)]|| 2 ], we find</p><formula xml:id="formula_81">D T? [X, Z] = sup f ?M b inf ?:Z?R k E M (X),Z ||f (M (X)) ? ?(Z)|| 2 (62) = sup f ?M b R mse [f (M (X)) | Z]<label>(63)</label></formula><formula xml:id="formula_82">= sup f ?M b E M (X),Z ||f (M (X)) ? E M (X)|Z [f (M (X))]|| 2 .<label>(64)</label></formula><p>Now, since conditional expectation given Z is just projection onto the (Hilbert) subspace L 2 (B(Z)), we have</p><formula xml:id="formula_83">D T? [X, Z] = sup h?M b ?L 2 (B(Z)) ? E X [||h(M (X))|| 2 ] ,</formula><p>where L 2 (B(Z)) ? is the subspace orthogonal to L 2 (B(Z)) in L 2 (H). Since L 2 (B(Z)) ? and L 2 (B(M)) are both closed (sub-)Hilbert spaces, it is straightforward to show that so too is their intersection Note that the last last part of the proof makes it clear that for MSE the invariance distortion is either 0 or B 2 . Intuitively this happens because MSE risk is not invariant to bijections so it possible to make any predictive mistake arbitrarily bad by setting M (X) to be arbitrarily large at this mistaken prediction. This suggests that for the MSE risk (and other loss functions that are not invariant to bijections) the expected excess risk might be better suited than the worst case excess risk that we considered.</p><p>As the invariant distortion under MSE is valid, we can now simply incorporate it into the rate distortion theorem to get the desired theorem. </p><formula xml:id="formula_84">Y ? T ? we have R mse [Y | Z] ? ? + R mse [Y | X].</formula><p>Then Rate(?) is given by</p><formula xml:id="formula_85">Rate(?) = inf P (Z|X) s.t. D T? [X,Z]?? I[X; Z] .<label>(65)</label></formula><p>where</p><formula xml:id="formula_86">D T? [X, Z] := sup f ?M b R mse [f (M (X)) | Z].</formula><p>Proof. The result (65) follows from the fact that D T? is a valid distortion (Prop. 18) and the ratedistortion theorem <ref type="bibr" target="#b15">(14)</ref>.</p><p>As a corollary, we obtain the following lower bound for the rate, which may be useful in practice.</p><p>Corollary 20. Let M be any R k -valued maximal invariant with a probability density with respect to Lebesgue measure. Let g : M ? R k be any homeomorphism of M (including the identity map), and f * any maximum distortion achieving function. Then the following lower bounds hold:</p><formula xml:id="formula_87">Rate(?) ? h(g(M (X))) ? k 2 ln(2?e?/k)<label>(66)</label></formula><p>Rate(?) ? h(f * (M (X))) ? k 2 ln(2?e?/k) . The first inequality is an equality if and only if X? ?Z | M (X); the second if and only if f * is a homeomorphism of M (and therefore is itself a maximal invariant). Second, using the translationinvariance of differential entropy and the fact that conditioning reduces differential entropy,</p><formula xml:id="formula_88">I[M (X); Z] = h(M (X)) ? h(M (X) | Z) (68) ? h(M (X)) ? h(M (X) ? E M (X)|Z [M (X)] | Z) (69) ? h(M (X)) ? h(M (X) ? E M (X)|Z [M (X)]) ,<label>(70)</label></formula><p>Now,</p><formula xml:id="formula_89">E M (X),Z ||M (X) ? E M (X)|Z [M (X)]|| 2 ? E M (X),Z ||f * (M (X)) ? E M (X)|Z [f * (M (X))]|| 2 = D T? [X, Z] .</formula><p>The maximum entropy distribution subject to this second-moment constraint is the k-dimensional Gaussian distribution N (0, K), where K is a diagonal covariance matrix with entries</p><formula xml:id="formula_90">K ii = E M (X),Z (f * (M (X)) ii ? E M (X)|Z [f * (M (X))] ii ) 2 .</formula><p>The differential entropy of that Gaussian distribution is 1 2 log (2?e) k det(K) , and by Jensen's inequality,</p><formula xml:id="formula_91">log det(K) = k i=1 log K ii = k i=1 log E M (X),Z (f * (M (X)) ii ? E M (X)|Z [f * (M (X))] ii ) 2 ? k log k i=1 1 k E M (X),Z (f * (M (X)) ii ? E M (X)|Z [f * (M (X))] ii ) 2 = k log(D T? [X, Z] /k) .</formula><p>Putting this together with the first inequality in <ref type="formula" target="#formula_4">(68)</ref>,</p><formula xml:id="formula_92">I[M (X); Z] ? h(M (X)) ? k 2 log(2?e) ? k 2 log(D T? [X, Z] /k) ? h(M (X)) ? k 2 log(2?e) ? k 2 log(?/k) .</formula><p>The same argument holds for either of g(M (X)) or f * (M (X)), yielding the stated lower bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Variational objectives</head><p>In this section we will derive the variational bounds for estimating the rate and the distortion. In contrast to the proofs of main theoretical results (previous section) derivations will be less formal. Throughout this section we focus on the log loss and implicitly make all assumptions described in Appx. A.2.</p><p>Recall that the optimal bit-rate is simply the Rate Distortion function using our invariance distortion (Rate-Invariance function; Eq. (44) ), so any optimal encoder (for a given ?) can be obtained by using the following arg minimum:</p><formula xml:id="formula_93">arg min p(Z|X) s.t. R log [M (X) | Z]?? I[X; Z]<label>(71)</label></formula><p>As optimization in machine learning is typically unconstrained, we prefer using the following Lagrangian formulation.</p><p>arg min</p><formula xml:id="formula_94">p(Z|X) I[X; Z] + ? ? R log [M (X) | Z]<label>(72)</label></formula><p>Both of these formulations are equivalent in that the set of encoders that minimize Eq. (72) for some ? ? R ?0 is equal to the set of encoders that minimize Eq. (71) for some ? ? R ?0 <ref type="bibr" target="#b78">[76,</ref><ref type="bibr" target="#b79">77]</ref>.</p><p>Note that due to the piece-wise linearity of our RI function ( <ref type="figure" target="#fig_4">Fig. 3</ref>  <ref type="bibr" target="#b23">[22]</ref> shows that this can be easily solved by considering the squared distortion R log [M (X) | Z] 2 , in which case sweeping over ? would be equivalent to sweeping over delta ? in Eq. (71). We did not see any difference in practice so preferred using the more understandable R log [M (X) | Z]).</p><p>Both terms I[X; Z] and R log [M (X) | Z] are hard to estimate from samples, so the rest of the section is devoted to deriving variational upper bounds on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Variational upper bound for the rate term I[Z; X]</head><p>Let us discuss how to approximate the rate term I[X; Z]. The mutual information is well known to be hard to estimate from samples <ref type="bibr" target="#b80">[78,</ref><ref type="bibr" target="#b81">79]</ref>, but fortunately many variational bounds have previously proposed, see Poole et al. <ref type="bibr" target="#b34">[33]</ref> for examples. In the following we denote a family of variational distributions over Z (priors or entropy models) as Q := {q ? P(Z)}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 Mutual information bottleneck</head><p>The first bound that we consider is the standard upper bound on I[X; Z], e.g., in VAE <ref type="bibr" target="#b15">[14]</ref> or VIB <ref type="bibr" target="#b82">[80]</ref>. Specifically:</p><formula xml:id="formula_95">I[Z; X] := H[Z] ? H[Z | X] (73) = E p(Z) [? log p(Z)] ? H[Z | X]<label>(74)</label></formula><formula xml:id="formula_96">= inf q?Q E p(X)p(Z|X) ? log p(Z)q(Z) q(Z) ? H[Z | X]<label>(75)</label></formula><formula xml:id="formula_97">= inf q?Q E p(X)p(Z|X) [? log q(Z)] ? E p(X)p(Z|X) log p(Z) q(Z) ? H[Z | X]<label>(76)</label></formula><formula xml:id="formula_98">= inf q?Q E p(X)p(Z|X) [? log q(Z)] ? D KL [p(Z) q(Z)] ? H[Z | X]<label>(77)</label></formula><formula xml:id="formula_99">? inf q?Q E p(X)p(Z|X) [? log q(Z)] ? H[Z | X]<label>(78)</label></formula><p>The approximation gap is then min q?Q D KL [p(Z) q(Z)]. The bound has the advantage that if p(Z) ? Q then the bound is tight. The major issue with the mutual information bottleneck, is that no efficient compressors can in general achieve the rate given by it <ref type="bibr" target="#b83">[81]</ref>. <ref type="bibr" target="#b9">9</ref> For example, if we decided to entropy code Z using the entropy model q(Z) then we would achieve E p(X)p(Z|X) [? log q(Z)] bits which is H[Z | X] more than what is given by our bound. 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.2 Entropy bottleneck</head><p>One specific case of the mutual information bottleneck which enables efficient compression, is when Z is discrete and arises from a deterministic transformation of X. Indeed, in this case H[Z | X] = 0 so entropy coding (e.g. <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b28">27]</ref>) can reach the rate given by our bound. Using the same derivation as for the mutual information bottleneck, we get,</p><formula xml:id="formula_100">I[Z; X] = H[Z] ? inf q?Q E p(X)p(Z|X) [? log q(Z)] .<label>(79)</label></formula><p>This is the standard bound used in neural compressors <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b26">25]</ref>. The entropy bottleneck bound has the following downsides compared to mutual information bottleneck:</p><p>? It is generally not true that for any ? the optimal rate can be achieved by a discrete and deterministic Z. For the specific case of ? = 0 and with Assumption 4 it is the case, as we can simply set Z = M (X). ? It is not suitable for gradient based optimization w.r.t. to the encoder (due to the discreteness of Z) so we typically have to add noise during training <ref type="bibr" target="#b25">[24]</ref> which can cause a mismatch between training and testing <ref type="bibr" target="#b83">[81]</ref>.</p><p>Despite these issues we will mostly use the entropy bottleneck bound in experiments as we want our method to give rise to practical compressors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Variational upper bound for the distortion term R log [M (X) | Z]</head><p>Let us now consider variational upper-bounds on the distortion R log [M (X) | Z]. For conciseness we will consider the same setting as in the main paper, i.e., log loss risk and countably many equivalence classes (Assumption 4). But it is easy to see that the direct distortion bound generalizes to any loss without Assumption 4. 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 Direct distortion</head><p>The obvious variational bound on the Bayes risk is the Bayes risk constrained to some functional family. Q := {q : Z ? P(M)} denotes a variational family of regular conditional distributions (decoders), then,</p><formula xml:id="formula_101">R log [M (X) | Z] := inf q E p(X)p(Z|X) [? log q(M (X) | Z)]<label>(80)</label></formula><formula xml:id="formula_102">? inf q ?Q E p(X)p(Z|X) [? log q (M (X) | Z)]<label>(81)</label></formula><p>which comes from the fact that we are taking an inf over a subset Q of all possible distribution. A simple derivation shows that the approximation gap is min q ?Q D KL [p(M (X), Z) q (M (X) | Z)p(Z)], so the bound is tight if p(M (X) | Z) ? Q . This direct distortion is simple, but typical variational families will require predicting ("reconstructing") an expected prediction E q (M (X)|Z) [M (X)|Z] which is challenging when M is in high dimension (e.g. unaugmented images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 Contrastive distortion</head><p>We now consider a bound that does not require explicitely predicting M (X), by considering a noise contrastive estimator <ref type="bibr" target="#b87">[85]</ref>. Suppose that for any Z we can sample from a sequence </p><formula xml:id="formula_103">M = (M + , M ? 1 , . . . , M ? n ), where M + d ? p(M (X) | Z) and M ? i n i=1 i.i.d.</formula><formula xml:id="formula_104">? H[M (X)] ? inf f ?F E p(Z)p(M |Z) log n + log exp f (M + , Z) M ?M exp f (M , Z) InfoNCE (85) = inf f ?F E p(Z)p(M |Z) ? log exp f (M + , Z) M ?M exp f (M , Z) + (const)<label>(84)</label></formula><p>Eq. (85) uses InfoNCE <ref type="bibr" target="#b16">[15]</ref>, which is a lower bound on mutual information <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b35">34]</ref>. The last equation removes constants w.r.t. p(Z | X) and F, as these terms do not have to be optimized over. We see that we are only left with a log softmax term 12 that essentially aims to classify which of all the M comes from the p(M (X) | Z). The bound is tight if the variational family F contains the optimal predictor and as the number of negatives tends to infinity. For a detailed discussion about noise contrastive estimation under the log loss, refer to <ref type="bibr" target="#b87">[85]</ref><ref type="bibr" target="#b88">[86]</ref><ref type="bibr" target="#b89">[87]</ref>.</p><p>Note that the contrastive distortion has the advantage of not having to reconstruct high dimensional data (e.g. for images), but it suffers from bias in the case where the number of negatives n is small <ref type="bibr" target="#b34">[33]</ref>.</p><p>One additional derivation which we will need in the following section, is that an upper bound can also be obtained by replacing M (X) by any other r.v. U s.t. U ? M (X) ? Z forms a Markov Chain. Indeed starting from Eq. (84), we have,</p><formula xml:id="formula_106">lhs = H[M (X)] ? I[M (X); Z] (87) ? H[M (X)] ? I[U ; Z] DPI (88) ? inf f ?F E p(Z)p(U |Z) ? log exp f (U + , Z) U ?U exp f (U , Z) + (const)<label>(89)</label></formula><p>The bound can still be tight if in addition we have the following Markov Chain M (X) ? U ? Z, which implies that I[U ; Z] = I[M (X); Z].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Case study: VIC and BINCE under data augmentations</head><p>The derivations in the previous 2 subsections are relatively general and abstract. As a case study, we now discuss the two objectives that we propose in the main paper for the case where we have access to the desired data augmentation and where we use neural functional families. Namely, the variational families for the entropy model p ? (Z), the encoder p ? (Z | X), the decoder p ? (M (X) | Z), and the discriminator f ? (X, Z) are all parametric neural families. Throughout this subsection we will consider that we only have access to p(X) through a dataset D := {x i } i of samples which were independently sampled from p(X).</p><p>Let us formalize what we mean by having access to the correct data augmentations. Let us denote as A the r.v. over a set of augmentations a : X ?X , i.e., a stochastic process. LetX = A(X) be the augmented source. Note that by A(X) we mean the r.v. which arises by sampling an augmentation a from the stochastic process p(A), and then applying it to some samples x from X.</p><p>Assumption 7 (Augmentations). We assume knowledge of a random augmentation generator A that satisfies the following two key properties ? Retain invariance. We require A to retain the invariance structure to (?, X ), specifically, X ? A(X) almost surely. ? Remove information. We require A to remove as much information as possible about the input.</p><p>Specifically, X? ?A(X) | M (X) almost surely.</p><p>The first requirement is simple but clearly not sufficient. For example, the identity function does satisfy such requirement for any equivalence relation (X ? X by definition), yet it does not correspond to what we think as an augmentation because it does not remove any information about the input. The second requirement formalizes exactly what is required, namely that the augmentation must remove all information about the input besides the knowledge about its equivalence class (which is needed for the first requirement).</p><p>The first requirement will typically hold. The second in more stringent. Note that it holds if for all equivalent examples x ? x in X we have p(A(x)) = p(A(x )). Indeed p(A(x)) = p(X|x) = p(X|x, M (x)) and similarly p(A(x )) = p(X|x , M (x )), using M (x) = M (x ) we have p(X|x, M (x)) = p(X|x , M (x)) for all equivalent x, x which implies that X? ?X | M (X) as desired. In practice this only needs to hold for examples in our datasets, i.e., the second requirement holds if for all equivalent x, x ? D in a dataset we have p(A(x)) = p <ref type="figure">(A(x )</ref>). This is likely to hold in practice as the number of examples that are equivalent in a dataset will be small if |X | |D| as is typically the case. In particular, if a dataset does not contain any equivalent examples, i.e., for any x, x ? D we have x ? x then the requirement trivially holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Issue: dealing with unknown M (X)</head><p>One issue which arises in practice is that we generally do not have access to M (X). We will overcome this issue by taking advantage of the fact that we have access to data augmentations A that induce our equivalence relation. Intuitively, we will treat the augmented r.v.X := A(X) as if it were the source, and use the actual source X instead of M (X). Note that by A(X) we mean the r.v. which arises by sampling an augmentation a from the stochastic process p(A), and then applying it to some samples x from X. From now on, let us denote as Z d ? p(Z|X) the representation that arises from the augmented source. Under suitable conditions on A we can replace the previous objective Eq. <ref type="bibr" target="#b74">(72)</ref> with the following equivalent objective, which we denote as L ? RI , arg min p(Z|A(X))</p><formula xml:id="formula_107">I[A(X); Z] + ? ? R log [X | Z] .<label>(90)</label></formula><p>By equivalence of those objectives we mean that for any ? ? R ?0 the set of RD tuples  <ref type="figure">X)</ref>). Indeed, an optimal representation will compress all that information. <ref type="bibr" target="#b14">13</ref> As a result, we can attain the same optimal bit rate by considering any sourceX that is a transformed version of X as long as the transformation does not change the distribution of the maximal invariant, i.e., p(M (X)) = p(M (X)). This is clearly the case forX := A(X) as our augmentation retains invariance (Assumption 7). Now let us consider why and when replacing M (X) by X makes sense. Using Prop. 13 we know that if A is s.t.X ? X andX ? X | M (X) forms a Markov Chain then we can replace (up to constants which are not important for arg min) the distortion term</p><formula xml:id="formula_108">R log M (X) | Z by R log [X | Z].</formula><p>These are exactly our requirements on augmentations (Assumption 7).</p><p>For the rest of this section we will thus be working with L ? RI (Eq. (90)) instead of Eq. <ref type="bibr" target="#b74">(72)</ref>. Note that this means that, in theory, we should always use the augmentedX from now on, i.e., not only at train time but also at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.1 Variational Invariant Compressor (VIC)</head><p>As seen in the main text the VIC loss is essentially a neural compressor where inputs are augmented but not the target reconstructions. We derive it by combining our entropy bottleneck bound (Appx. C.1.2) and our direct distortion (Appx. C.2.2), which gives the following upper bound on L RI ? ,</p><formula xml:id="formula_109">L RI ? (?) := I[A(X); Z] + ? ? R log [X | Z] (91) ? inf ? E p(A)p(X)p?(Z|A(X)) [? log q ? (Z)]</formula><p>Eq. (79) (92)</p><formula xml:id="formula_110">+ ? ? inf ? E p(A)p(X)p?(Z|A(X)) [? log q ? (X | Z)] Eq. (80) (93) = inf ?,? ? E p(A)p(X)p?(Z|A(X)) [log q ? (Z) + ? log q ? (X | Z)]<label>(94)</label></formula><p>Using a Monte Carlo estimate for the expectation over p(X), we get our desired objective,</p><formula xml:id="formula_111">L VIC ? (?, ?, ?) := ? 1 |D| x?D E p(A)p?(Z|A(x)) [log q ? (Z) + ? ? log q ? (x | Z)] .<label>(95)</label></formula><p>In practice, we approximate the expectation over A and Z using a single sample for computational efficiency. A full algorithm is provided in Algorithm 2 and illustrated in <ref type="figure" target="#fig_8">Fig. 4</ref> of the main text.</p><p>Algorithm 2 Variational Invariant Compressor (VIC). Single sample forward pass.</p><p>Require: Encoder p ? (Z|A(X)), Entropy Model q ? (Z), Decoder q ? (X|Z) Require: Dataset D, random augmentation generator A, Lagrange multiplier</p><formula xml:id="formula_112">? 1: x ? select(D) sample 2:x ? sample(A(x)) random augment 3: z ? sample(p ? (Z|x)) encode 4: rate_loss ? ? log q ? (z) Entropy Bottleneck 5: distortion_loss ? ? log q ? (x|z)</formula><p>Direct Distortion 6: return rate_loss + ? ? distortion_loss Note that L VIC ? tends to L RI ? when using unconstrained variational families and as the dataset grows to infinity. This essentially shows that VIC objective (with infinite samples and unconstrained families) will learn the optimal deterministic and discrete Z (as discussed in Appx. C.1.2), in particular, when ? &gt; 1 it will learn an encoder which is optimal for the lossless prediction regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.2 Bottleneck InfoNCE (BINCE)</head><p>VIC for images and data augmentation suffers from the issue that it needs a predictor which reconstructs a high dimensional image (as discussed in Appx. C.2.1). To solve this issue we discuss our BINCE objective, which as seen in the main text, is essentially a standard contrastive self-supervised (SSL) objective with an additional entropy bottleneck. We derive it by combining our entropy bottleneck bound (Appx. C.1.2) and our contrastive distortion (Appx. C.2.2).</p><p>Note that in Appx. C.2.2 for each Z we needed a sequence M of outcomes of M (X) that are sampled either from the conditional p(M (X)|Z) or the marginal p(M (X)). As we will replace M (X) by X ( see Appx. C.4) we now need a sequence of r.v. X := (X + , X ? 1 , . . . , X ? n ) s.t. X + is sampled from the conditional p(X|Z) while each X ? i are independently sampled from the marginal p(X). Furthermore, as is standard in self-supervised learning (e.g. <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b33">32]</ref>) we will actually be using a sequence? of positive and negative representations instead of X. We do so by independently augmenting and encoding each r.v. in X. Using our requirement on the augmentations (Assumption 7) we thus have the following Markov Chain? ? X ? M (X) ? A(X) ? Z. As a result, we can use? instead of X in InfoNCE (see Eq. <ref type="formula" target="#formula_5">(87)</ref>). For conciseness we will denote the above sampling procedure as p ? (Z,? | A, X). We then have the following upper bound on L RI ? ,</p><formula xml:id="formula_113">L RI ? (?) := I[A(X); Z] + ? ? R log [X | Z] (96) ? inf ? E p(A)p(X)p?(Z|A(X)) [? log q ? (Z)] + (const) (97) + ? ? inf ? E p(A)p(X)p?(Z|A(X))p(?|Z) ? log exp f ? (? + , Z) ? ?? exp f ? (? , Z) (98) = inf ?,? ? E p(A)p(X)p?(Z,? | A,X) log q ? (Z) + ? log exp f ? (? + , Z) ? ?? exp f ? (? , Z)<label>(99)</label></formula><p>Using a Monte Carlo estimate for the expectation over p(X), we get our desired objective,</p><formula xml:id="formula_114">L BINCE (?, ?, ?) := ? x?D E p(A)p?(Z,?|A,D,x) log q ? (Z) + ? ? log exp f ? (? + , Z) ? ?? exp f ? (? , Z) .<label>(100)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Batch forward pass for BINCE</head><p>Require: encoder p ? (Z|A(X)), entropy model q ? (Z), discriminator f ? (Z , Z)</p><formula xml:id="formula_115">Require: augmentations A, data D, Lagrangian coefficient ?, batch size b 1: x ? select(D) b times sample 2:x ? sample)(A(x)) Random augment 1 3:x ? sample)(A(x))</formula><p>Random augment 2 4: z, z ? sample(p ? (Z|x)), sample(p ? (Z|x )) Encode 5: zs ? concat(z, z ) 6: rate_loss ? average ? log q ? (z i ) over z i ? zs Entropy Bottleneck 7: distortion_loss ? 0 8: for i ? 1, . . . , b do 9:</p><formula xml:id="formula_116">z + ? z [i]</formula><p>Select positive <ref type="bibr">10:</ref> softmax ? exp f ? (z + , z)/( z ?zs exp f ? (z , z)) Softmax 11:</p><p>distortion_loss ?= 1 b log(softmax) Contrastive Distortion 12: end for return rate_loss + ? ? distortion_loss</p><p>In practice we approximate the expectation over A,? and Z using a single sample for computational efficiency. Just as with VIC we have that L VIC ? tends to L RI ? when using unconstrained variational families and as the dataset and number of negatives n grows to infinity.</p><p>In the main paper we provided a simple algorithm (Algorithm 1) to compute BINCE for a single example x ? D. This is computationally intensive as it requires sampling one sequence of r.v. for each example. In practice, this is nevertheless easily amenable to batch computation. Indeed, negative representations? ? are positive representations? + for a different example. As a result, we can first sample a batch x := (x 1 , . . . , x n ) from D. Then augment it to two different sequencesx,x . And finally represent each sequences to obtain z, z . Then for any z i := z[i] we have that z i := z [i] is a positive example while all other z ? z, z are negatives. We thus only need to sample a single representation per example in the dataset. A full algorithm for batch computations is provided in Algorithm 3 (using only one of the 2 augmented batches for notational convenience).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 CLIP as BINCE's distortion</head><p>One of our main experiment (Sec. 5.3), consists in using a pretrained CLIP to make a powerful image compressor. We are able to do so by realizing that CLIP essentially corresponds to BINCE's distortion (second term in Eq. (9) with the following choices:</p><p>? Augmentation: text to image transformation. CLIP's dataset contains pairs of associated images and detailed text (x img , x txt ). The "augmentation" is then a function that maps x img to its associated x txt and vis versa. This will partition the joint image-text space of X := X img ? X txt into sets, each of which are associated (directly or by transitivity) with a common text description or image. ? Discriminator: a dot product, i.e, f ? (Z , Z) = Z T Z.</p><p>? Encoder: a deterministic function defined by cases. Specifically, sampling from p(Z|X) gives the output of the visual transformer (image encoder) Z = V iT (X) if X is an image and the output of the text transformer (text encoder) Z = transf ormer(X) if X is a sentence.</p><p>The only minor difference is that CLIP performs contrastive learning between text-image and imagetext but never text-text and image-image. BINCE would instead make no distinction between modalities as the equivalence class is on the joint image and text space. Both are nevertheless valid approximations to R[M (X) | Z].</p><p>Although CLIP's augmentation will always give rise to a valid equivalence relation, it would in theory recover the degenerate solution of [x] = X for all x ? X if the dataset was "infinite". Indeed, any image could possible just be described by the text "an image", which would recover the aforementioned degenerate solution. There are different ways of collecting the datasets that could avoid this issue, e.g., ensuring that the description is more precise than that. In practice, this is unlikely to be an issue as the dataset is finite.</p><p>Another possible theoretical issue of CLIP's augmentation/equivalence structure, is that it is likely that very few images have a common associated text in CLIP's dataset (or vis-versa). In theory, this would thus recover the degenerate solution where no points are equivalent to one another, i.e., [x] = {x} for all x ? X . In practice, this issue is probably avoided due to the fact that images will get clustered as long as the the text description is similar enough for the text encoder to provide (essentially) the same text encoding (due to computational/architectural constraints). I.e., the images will actually get partitioned based on the value of the representation of their associated text rather than the text itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Extended related work</head><p>Invariances and symmetries. Invariances are ubiquitous in ML, as seen by the use of data augmentations <ref type="bibr" target="#b14">[13]</ref> and invariant models <ref type="bibr" target="#b60">[58,</ref><ref type="bibr" target="#b90">[88]</ref><ref type="bibr" target="#b91">[89]</ref><ref type="bibr" target="#b92">[90]</ref><ref type="bibr" target="#b93">[91]</ref><ref type="bibr" target="#b94">[92]</ref><ref type="bibr" target="#b95">[93]</ref>. These force models not to rely on nuisances to improve generalization <ref type="bibr" target="#b96">[94]</ref><ref type="bibr" target="#b97">[95]</ref><ref type="bibr" target="#b98">[96]</ref>. We directly discard such nuisances from the data to improve compression. Others have used symmetries in X for lossless compression of multisets <ref type="bibr" target="#b22">[21]</ref>, graphs <ref type="bibr" target="#b99">[97]</ref><ref type="bibr" target="#b100">[98]</ref><ref type="bibr" target="#b101">[99]</ref>, or structured images <ref type="bibr" target="#b102">[100]</ref><ref type="bibr" target="#b103">[101]</ref><ref type="bibr" target="#b104">[102]</ref><ref type="bibr" target="#b105">[103]</ref><ref type="bibr" target="#b106">[104]</ref>. We, instead, use invariance of the tasks Y for lossless prediction.</p><p>Neural lossy compression. Most research in neural compression is either focused on estimation and optimization of the rate term <ref type="bibr" target="#b83">[81,</ref><ref type="bibr" target="#b107">[105]</ref><ref type="bibr" target="#b108">[106]</ref><ref type="bibr" target="#b109">[107]</ref><ref type="bibr" target="#b110">[108]</ref><ref type="bibr" target="#b112">[109]</ref><ref type="bibr" target="#b114">[110]</ref><ref type="bibr" target="#b116">[111]</ref> or on developing perceptually meaningful distortions <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b117">112,</ref><ref type="bibr" target="#b118">113]</ref>. Our paper also develops a new distortion, but does not optimize for perception. Improvements in the rate objectives are orthogonal to our work and can also help our method.</p><p>Self-supervised learning. Our objective (Eq. <ref type="bibr" target="#b7">(7)</ref> in main text) can be seen as contrastive SSL <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b33">32]</ref> with an information bottleneck, a version of <ref type="bibr" target="#b119">[114,</ref><ref type="bibr" target="#b120">115]</ref> with an information instead of a variance bottleneck, a SSL VIB <ref type="bibr" target="#b82">[80]</ref>, or an invariant VAE <ref type="bibr" target="#b15">[14]</ref>. At a higher level our work differs on two key aspects. First, minimizing the information I[A(X); Z] arises from our desire to perform compression rather than to (optimally <ref type="bibr" target="#b54">[52]</ref>) help generalization <ref type="bibr" target="#b121">[116,</ref><ref type="bibr" target="#b123">117]</ref>. Second, we provide the first formalism of a minimal pretext task M (X) that retains all information about any invariant task. This is related to the multi-view literature, where one only needs to retain information which is invariant across views <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b124">118,</ref><ref type="bibr" target="#b126">119]</ref>. The main difference is that we prove the existence of a single pretext task.</p><p>The most similar setting to ours is the recent work of Mitrovic et al. <ref type="bibr" target="#b127">[120]</ref>, which (in Appx. D) analyses contrastive learning using equivalence relations. Specifically, they also consider tasks Y whose conditional distribution are invariant to an equivalence relation. Their Theorem 1 is then similar to our Lemma 6, but only considers the restricted case of deterministic labeling and finite sample space X . Furthermore, they only talk about invariant representations (sufficiency), while we characterize all invariant representations using the maximal invariant (necessity and sufficiency).</p><p>Information theory and predictions. Theorem 2 relates exactly predictive loss and compression rate. Although such results is to our knowledge (surprisingly) new, it fits in a long line of work that relates Bayes predictions and generalized information theory <ref type="bibr" target="#b54">[52,</ref><ref type="bibr" target="#b61">59,</ref><ref type="bibr" target="#b63">61,</ref><ref type="bibr" target="#b128">[121]</ref><ref type="bibr" target="#b129">[122]</ref><ref type="bibr" target="#b130">[123]</ref><ref type="bibr" target="#b131">[124]</ref>.</p><p>Maximal invariants and minimal sufficient statistics. As seen by our coin toss example, if the marginal p(X) is invariant to the equivalence, i.e., X ? T ? , then maximal invariants coincide with minimal sufficient statistics <ref type="bibr" target="#b132">[125,</ref><ref type="bibr" target="#b134">126]</ref>. In our work we are interested in predicting a target Y rather than reconstructing the source X. A sufficient statistic w.r.t. to another r.v. Y is referred to as adequate statistics. 14 Maximal invariants can thus be seen as minimal adequate statistics for the set of all invariant tasks of interest T ? . Using minimal adequate statistics as good representations for performing a task has been well investigated in ML to improving generalization <ref type="bibr" target="#b54">[52,</ref><ref type="bibr" target="#b121">116,</ref><ref type="bibr" target="#b138">[129]</ref><ref type="bibr" target="#b140">[130]</ref><ref type="bibr" target="#b141">[131]</ref><ref type="bibr" target="#b142">[132]</ref>.</p><p>The main difference with our work is that (i) we consider adequacy for a collection of tasks instead of a single task; (ii) minimality arises from a compression perspective rather than for generalization. Although we are not aware of any use of minimal adequacy for compression (even single task), minimal sufficiency is often used for compressing distributions <ref type="bibr" target="#b143">[133,</ref><ref type="bibr" target="#b144">134]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Reproducibility</head><p>In this section we provide further details of the hyperparameters chosen for the various experiments in the main text. The code to reproduce all experiments can be found at github.com/YannDubs/ lossyless. We checkpoint and use the model which achieves the smallest validation loss for evaluation. Unless stated otherwise, all the models are trained for 100 epochs, using Adam <ref type="bibr" target="#b145">[135]</ref> as the optimizer, and a batch-size of 128. The learning rate starts at 1e-3 that decreases exponentially until reaching 1e-6 at the end of training. For all convolutional layers we use Kaiming normal initialization <ref type="bibr" target="#b146">[136]</ref>, for all linear layers we use Kaiming uniform initialization <ref type="bibr" target="#b146">[136]</ref>, while all biases are always initialized at 0. Activation functions are ReLUs while other unspecified parameters are PyTorch <ref type="bibr" target="#b147">[137]</ref> defaults. For our invariant models, instead of optimizing I[Z; X] + ? D T? [X, Z] we optimize ? I[Z; X] + D T? [X, Z] , which is a more standard formulation for VIB, VAE, and neural compressors. In the following sections we will sometimes refer to ? as 1 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Banana</head><p>For the Banana dataset most of the arguments were selected so as to replicate <ref type="figure" target="#fig_0">Fig.1.B</ref>. from <ref type="bibr" target="#b37">[36]</ref>. <ref type="bibr" target="#b16">15</ref> Data.</p><p>The data distribution is obtained by starting from a bivariate Gaussian X ? N (0, diag([3, 0.5])). It is then transformed to a banana distribution using the following transformation: x 2 = x 2 + 0.1x 2 1 ? 9. We then rotate it and shift it: Hyperparameters. For all Banana experiments we use a 2 dimensional representation Z ? R 2 , a learning rate of 1e-3 that decreases exponentially until reaching 1e-6 at the end of training, and a batch size of 8192. The encoder (and decoder if there is one) is always a 2-hidden layer MLP with 1024 hidden neurons, and softplus activation. In all cases we an entropy bottleneck with a factorized prior from <ref type="bibr" target="#b29">[28]</ref>. Experiment: <ref type="figure">Fig. 5</ref>. We train both a standard variational compressor (VC) and our variational invariant compressor (VIC). The downstream performance loss is the MSE when predicting the maximal invariant. In both cases we use ? = 0.07, which was chosen so that the downstream performance is similar for both. For VIC the data is first augmented using rotations, passed through an encoder, then the decoder predicts the maximal invariant M : x ? Rot(225) ? [0, x 2 ] T , i.e., the point with the same radius but positioned at 225 degrees. Note that we use this maximal invariant (instead of the more natural x 2 ) to ensure that the reconstructions (codebooks) can be plotted in in a nice way in the original space X . The choice of maximal invariant does not impact the learned partition of the space.</p><p>Each plot ( <ref type="figure">Fig. 5 right)</ref> is generated by first taking a meshed grid of 500 2 source points in [?5, 5] 2 . Then we quantize every point in the mesh by passing it through our encoder. The partition of the space (delimited with pink contours) corresponds to all points in the mesh that got mapped to the same quantized representation. To obtain the codebook (pink dots), we pass the quantized representations through our learned decoders. Finally, we plot the distribution of our learned entropy model by rescaling the codes so that their area is proportional to the rate assigned by the entropy model, i.e., ? log q ? (z).</p><p>To obtain rate-invariance curves <ref type="figure">(Fig. 5 left)</ref>, we sweep over ? = 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000. For each point in <ref type="figure">Fig. 5a</ref> we plotted the average over 5 seeds and plotted in gray the standard errors (both in the rate and distortion direction). To compute the area under the curve we used the trapezoidal rule on each of the RI curves obtained by a single seed, we then aggregated to area under the curve for the 5 seeds to obtain the mean and standard error. Experiment: <ref type="figure" target="#fig_24">Fig. 7</ref>. Here augmentations are translations on the x-axis. The BINCE model was trained using Algorithm 1, i.e., without assuming knowledge of the maximal invariant. For VIC we used M : x ? [0, x 2 ] T as the maximal invariant.</p><p>Experiment: <ref type="figure" target="#fig_26">Fig. 8</ref>. Here augmentations are translations on the y-axis. For VIC we used M : x ? [x 1 , 0] T as the maximal invariant. To plot of the induced distribution in M (here the x-axis), we sample 1024000 new points, pass them through our encoder and decoder to obtains the codes, and then plot a histogram of the obtained codes (shown in salmon). In blue we also plot the (approximate) distribution of the source when marginalized our invariances (i.e. only consider the x component).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 General Image framework</head><p>Here we discuss the framework which we used for most of our image experiments. Unless stated otherwise, for all (ours or standard) neural image compressors we use the same general framework and architectures.</p><p>Specifically, an image X first passes through a ResNet18 <ref type="bibr" target="#b36">[35]</ref> to obtain a 128 dimensional representation Z that t.v.i R 128 . We then pass Z through an entropy bottleneck with a scaled hyperprior entropy model from Ball? et al. <ref type="bibr" target="#b29">[28]</ref> which gives us the quantized?. For our entropy bottleneck we used B?gaint et al.'s <ref type="bibr" target="#b148">[138]</ref> implementation which is a Pytorch re-implementation of <ref type="bibr" target="#b29">[28]</ref>. Note that the choice of entropy model and quantizer is orthogonal to our work, and any choice that works neural compression would work for us.</p><p>In the case where we have to decode an image (VIC and VC models), we pass the quantized? through a linear layer to reshape it to a latent image in R 2?2?256 . The latent image then passes through a 4-layer transposed CNN decoder where after each layer the number of channels gets divided by two and the width and height of the image doubles. The last layer outputs an image with the correct number of channels (1 for MNIST, 3 for other datasets), which is treated as the reconstruction X of the augmented input (for standard compressors) or the non-augmented input (for our VIC).</p><p>To simulate how well you could perform on downstream tasks of interest (that are not known when learning the compressors), we evaluate how well a model can classify the labels from the dataset. Specifically, once the models are trained we freeze them, apply them to the dataset and train neural network to classify the inputs using either the quantized representation? or the reconstructionX. In the former case we a |Z| ? 2048 ? 2048 ? |Y| MLP with preactivation batch normalization <ref type="bibr" target="#b149">[139]</ref>.</p><p>In the latter case we use a ResNet18 for predictions.</p><p>Finally, we obtain the desired bit-rate by considering the expected log loss of the trained entropy model on the test distribution (i.e. theoretical bit-rate). The desired distortion is obtained by evaluating the predictor on the compressed test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 MNIST</head><p>For our MNIST <ref type="bibr" target="#b150">[140]</ref> experiments we compare again our VIC (as described in Algorithm 2) against a standard neural compressor.</p><p>Data. In order to evaluate our framework in a relatively well understood setting we use the well known MNIST <ref type="bibr" target="#b150">[140]</ref> dataset, which we rescale to 32 ? 32 pixels. For this toy setting we want to understand how our model performs when trained with augmentations that induce the equivalent relation w.r.t. which we are invariant, i.e., we assume that we know the "correct" augmentation. To do so we augment both the training and the test set in the same way. Specifically, we apply random rotations sampled from [? <ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b46">45]</ref> degrees, random translations between [0, 25] percentage of pixels, random shearing between [0, 25] degrees, and random scaling by a factor in [0.6, 1.4].</p><p>Experiment: <ref type="figure" target="#fig_0">Fig. 1 and Fig. 6b</ref>. For a fair comparison we took trained a standard compressor and a VIC so that the downstream accuracy on augmented MNIST is the closest possible to 99% accuracy (note that augmented MNIST is slightly harder than standard MNIST). We then randomly sampled reconstructions for the source, standard reconstructions, and VIC reconstructions which we plot. The quantitative results are average over 5 runs and standard errors are provided in <ref type="figure" target="#fig_6">Fig. 6b</ref>. Experiment: <ref type="figure" target="#fig_6">Fig. 6a</ref>.</p><p>For the rate-error curve we swept over ? = 0.001, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 100 and plotted the curves and computed the area under the curve in the same way as previously discussed for the Banana rate-invariance curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 STL10</head><p>Data. We use the STL10 dataset <ref type="bibr" target="#b38">[37]</ref> which is similar to CIFAR but with fewer labeled training examples. There are 10 classes of 96x96 pixeled, colored images. There are 500 labeled and 100000 unlabeled examples for training, 800 labels for test. Note that the unlabeled images come from a broader distribution of images. For augmentations we use horizontal flips, resizing and cropping, the color transform and we randomly transform the image to gray scale with a likelihood of 0.2. As for MNIST we augmented both the train and the test distribution. The compressors were trained on the unlabeled data, while the predictors were trained on the train distribution.</p><p>Hyperparameters. We used an entropy bottleneck with a scaled hyperprior entropy model from <ref type="bibr" target="#b29">[28]</ref>. When training with BINCE, VIC or VC, the encoder is a ResNet18 architecture. For hyper-parameter tuning we randomly sampled 100 hyperparameters from the following search space: latent dimension size (32 ? 512), rate-distortion trade-off ? (10 ?13 , 100), the optimizer's (ADAM) learning rate (10 ?4 , 10 ?3 ), the learning rate schedule(exponential decay or cosine decay), and the batch size <ref type="bibr">(64 ? 128)</ref>. For prediction on the learned features? we trained an MLP with 1024 ? 4096 hidden units, one or two layers, and dropout probability between 0.0 ? 0.5. We optimized again the learning rate of the ADAM optimizer as before. For predictions from the reconstructionsX , we trained a ResNet18, with the same optimizer parameters as above.</p><p>Experiment: <ref type="table" target="#tab_2">Table 1</ref>. In this experiment we compare the compression performance of PNG <ref type="bibr" target="#b39">[38]</ref>, WebP <ref type="bibr" target="#b40">[40]</ref>, JPEG [39], VC, VIC and BINCE. Since VC and VIC allow to predict either on features or on reconstructions we test both. We sweep uniformly over the log-scale of ? = 10 ?5 , 100 for the neural compressors and sweep the classical compressors over an equivalent quality range. The extensive results from which <ref type="table" target="#tab_2">Table 1</ref> is derived are in <ref type="table" target="#tab_16">Table 8</ref>. The rate-distortion curves belonging to this experiment are <ref type="figure" target="#fig_0">Fig. 10</ref>. The rate-distrotion curves correspond to the pareto optimal curves of the encoders and predictors from the 1000 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Galaxy Zoo</head><p>Data. Celestial objects and events emit radio frequencies. These frequencies are recorded through large antennas. Modern radio astronomy relies on the aggregation of radio signals in time and space. This means that one antenna records over long stretches of time. Due to the rotation of the earth, this translates to many spatial measurements. Further, the inclusion of many antennas in various locations can provide a dense net of observations. The entire system is refereed to as aperture synthesis telescope (AST). Images of the sky are generated by combining the sequences of observations stemming from different antennas. ASTs generate enormous amounts of data, much of which is redundant and further will never be observed by humans. In fact commonly applied techniques to the observations, such as weighting (e.g. Briggs weighting) and blurring of signals removes information from the original observations. Our approach is thus a natural extension to the techniques already present in the radio astronomy community. However, the process of image reconstruction from radio frequency observation series is too complex for the scope of this paper. We thus work on the Galaxy Zoo 2 (GZ2) dataset, that contains of already inferred images of celestial objects. We believe that good rates on this dataset should hint at even better possible rates when working directly with the raw data. GZ2 contains 37 classification tasks, such as answering queries about shapes and counts information of galaxies. Although the tasks are classification tasks, we use the standard GZ2 evaluation that consists in regressing (RMSE evaluation) the expected (over different labellers) label probability. Our data is hosted on the kaggle platform. This means we have no access to test labels but only for total test loss. This is why we compute summary statistics on the validation data set. We choose to reduce the original dataset by center cropping to 256 pixels per dimension. We applied random rotations, horizontal and vertical flips, scaling (1 ? 1.3?) and color transforms to this data. We used CNN encoders <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b108">106]</ref> and an entropy bottleneck with a scaled hyperprior entropy model from <ref type="bibr" target="#b29">[28]</ref>.</p><p>Hyperparameters. For all experiments we used ResNet50 when predicting from images (i.e. encoders and predictors from reconstructions). As with the STL10 experiments, we trained each model and baseline by selecting a set of 100 hyperparameters randomly selected from a large search space. When training with BINCE, we sampled the latent dimension size (32 ? 2048), rate-distortion trade-off ? (1e ? 12, 1e ? 4), the optimizer's (ADAM) learning rate (1e ? 4, 1e ? 3), the learning rate schedule(exponential decay or cosine decay) and the batch size <ref type="bibr" target="#b66">(64,</ref><ref type="bibr" target="#b137">128)</ref>. For prediction on the learned features we would train an MLP with 2048 hidden unit, two layers and dropout probability (0.0 ? 0.5). We optimized the again the learning rate of the ADAM optimizer as before. For the classical compressors we trained a ResNet50 on their reconstructions, with the same optimizer parameters as above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6 Pretrained CLIP</head><p>Data. In addition to the pretrained CLIP, we trained the entropy bottleneck. As we do not have access to the dataset from CLIP, we could not train the entropy bottleneck on the initial data. Instead we had to use a different dataset. We used MSCOCO <ref type="bibr" target="#b44">[43]</ref> for image captioning, as we initially thought that we would need access to pairs of images and sentences to finetune CLIP. <ref type="bibr" target="#b17">16</ref> Note that in our experiments the choice of dataset for training the entropy bottleneck (e.g. CIFAR10 <ref type="bibr" target="#b151">[141]</ref>) had very little impact on the quality of the final compressor.</p><p>To evaluate our compressor in the most realistic setting possible, we selected 10 different datasets. The datasets were chosen so that (i) the source images are of very different shapes and content; (ii) they are easily be accessible online; (iii) images are already compressed by JPEG; (iv) neither the entropy bottleneck nor CLIP should have been trained on the selected datasets; (v) the task are very different classification tasks. To ensure that CLIP was (nearly) not pretrained on the selected datasets we selected a subset of the datasets that CLIP was evaluated on and which did not show significant data overlap (see <ref type="bibr">Radford et al.'s [41]</ref> section 5 for a discussion about data overlap). <ref type="table" target="#tab_11">Table 5</ref> shows the details about the 10 datasets that we use for evaluating our model. When there is no prespecified validation split, we randomly sampled 10% of the training data for validation. Reproducing the results. For clarity and reproducebility we also provide a self contained script to train a very similar version to our compressor in Source Code 2 and Source Code 1. The main changes being that we change the training data (using CIFAR10), the entropy bottleneck (to the simpler factorized prior from <ref type="bibr" target="#b29">[28]</ref>), and use a simpler evaluation pipeline (only use STL10 with a simplified MLP). The entire script (including evaluation and actual compression of a dataset) takes less than ten minutes to run on a single GPU and provides a general zero-shot compressor. The full code that we used is accessible at github.com/YannDubs/lossyless.</p><p>Training the zero-shot compressor. To train our compressor we first download the official pretrained CLIP model <ref type="bibr" target="#b18">17</ref> . Specifically the vision transformer <ref type="bibr" target="#b42">[42]</ref> that they refer to as "ViT-B/32". We then freeze it, and add an entropy bottleneck with Ball? et al.'s <ref type="bibr" target="#b29">[28]</ref> hyperprior. We then train the entropy bottleneck on the MSCOCO dataset.</p><p>To train the entropy bottleneck we need a distortion measure. In theory, to get our BINCE objective, we should use the distortion that CLIP was trained with, i.e., we should compress the representation in such a way that CLIP can still distinguish examples from the same equivalence class. Minimizing such distortion can lead to catastrophic forgetting as the representations only ensure that CLIP can distinguish equivalent example from our very small dataset. <ref type="bibr" target="#b19">18</ref> We instead use a very simple MSE distortion in the representation space. Specifically, we trained the entropy bottleneck to minimize ? z ?? ? log(q ? (z)), where? denotes the reconstructed (quantized) representation. This can be seen in line 22 of Source Code 2.</p><p>One important point to notice is that in standard neural compressors the quantization is a component wise rounding to the closest integer. This typically does not constrain the compressor, as the compressor is trained in an end-to-end fashion so that the encoder can increase or decrease some components of z to effectively increase or decrease the quantization. As our encoder is frozen, it cannot learn to adaptively change the scale of z so we needed to learn the size of the quantization interval instead, i.e., the rounding precision. A simple (and equivalent) way of doing that consists in passing the representation through a (learned) component wise linear transformation (i.e. 2 parameters per component) then through the entropy bottleneck (quantization) and finally we reverse the linear transformation. This can be seen in line 12 and 14 of Source Code 2.</p><p>Generally we found that training the entropy model was very robust to hyperparameter changes. We used the following: 50 epochs, a 512 dimensional z (given by CLIP), a batch size of 64, a learning rate of 1e-3 with 3, a scheduler that decreases the learning rate by 10? every 12 epochs (i.e. uniformly 3 times during training), Adam with decoupled weight decay (AdamW; <ref type="bibr" target="#b157">[147]</ref>) as an optimizer, weight decay of 3e-8 and a 32 dimensional side information for the hyperprior. For the our main CLIP compressor (CLIP+EB) we use an RD hyperparameter ? = 5e-2 which is linearly annealed from 1e-7 to 1e-2 in the first 5 epochs of training (although annealing did not seem important). For our CLIP+EB ? and CLIP+EB + (see <ref type="table" target="#tab_2">Table 10</ref>) we respectively use ? = 1e-2 and ? = 1e-1. We then provide the result of the best model. In <ref type="table" target="#tab_3">Table 2</ref> we compare those results to the same vision transformer that we use for CLIP, but trained directly on the raw images. These results were obtained from Radford et al.'s <ref type="bibr" target="#b41">[41]</ref>  <ref type="table" target="#tab_2">table 10</ref>. For a better comparison to standard SSL models, in <ref type="table" target="#tab_2">Table 10</ref> we also provide the test accuracy of a linear layer (a support vector machine) from the representations. The regularization parameter of the SVM were all selected using 10 values and three fold cross validation. For the rates we compare to the average JPEG size of images in each datasets (all the selected datasets are compressed by default in JPEG). For the rates of the raw CLIP model we losslessly compress the representation using numpy's savez function (zip) <ref type="bibr" target="#b159">[149]</ref>. E.7 Minimal code to train the CLIP compressor in &lt; 5 min.</p><p>In this section we provide minimal code to train our zero-shot compressor and to use our compressor to entropy code an entire dataset. Note that the model is simplified (e.g. using factorized prior instead of a hyperprior, and training on CIFAR10) so the bit-rates is slightly increased but it still achieves orders of magnitude gains compared to JPEG. We use CIFAR10 for training the entropy coder and STL10 for downstream evaluation (as both are downloadable through torchvision). To evaluate the model, we use a linear support vector machine from our representation Z.</p><p>For this minimal code, training takes around 3 minutes on a single GPU. The theoretical bit-rate that we achieve is around 1400, while the practical bit-rate achieved by entropy coding is around 1700. In comparison the bit-rate of JPEG (with 95 quality) is 4.71e4. The entropy coder compresses around 200 images per seconds, and decompresses around around 3 images per seconds. Decompression is slow as we do not perform it in batch (for simplicity of the code), while encoding is batched processed. Downstream classification accuracy on STL10 is 98.7% which is better than the uncompressed representations from CLIP, from which linear probe achieves 98.6% accuracy.</p><p>To run the code you need first need to install the following libraries:  <ref type="bibr">16 17</ref> def step(self, batch, *args, **kwargs): <ref type="bibr" target="#b19">18</ref> z_hat, q_z, _ = self(batch) 19 rate = -torch.log(q_z).sum(-1).mean() 20 distortion = torch.norm(batch[0] -z_hat, p=1, dim=-1).mean() <ref type="bibr" target="#b22">21</ref> self.log_dict({"rate":rate / math.log(2),"distortion":distortion}, prog_bar=True) <ref type="bibr" target="#b23">22</ref> return distortion + self.hparams.lmbda * rate <ref type="bibr">23 24</ref> def training_step(self, batch, _, optimizer_idx=0): <ref type="bibr" target="#b26">25</ref> return self.step(batch) if optimizer_idx == 0 else self.bottleneck.loss() <ref type="bibr">26 27</ref> def predict_step(self, batch, _, __): <ref type="bibr" target="#b29">28</ref> return self.compress(batch[0]), batch <ref type="bibr" target="#b0">[1]</ref>.cpu().numpy() <ref type="bibr">29 30</ref> def compress(self, z): <ref type="bibr" target="#b32">31</ref> if not self.is_updated: <ref type="bibr" target="#b33">32</ref> self.bottleneck.update(force=True) <ref type="bibr" target="#b34">33</ref> self.is_updated = True 34 z = (z + self.biasing) * self.scaling.exp() <ref type="bibr" target="#b36">35</ref> return self.bottleneck.compress(z.unsqueeze(-1).unsqueeze(-1)) <ref type="bibr">36 37</ref> def decompress(self, z_bytes): 38 z_hat = self.bottleneck.decompress(z_bytes, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref>).squeeze() 39 return (z_hat / self.scaling.exp()) -self.biasing <ref type="bibr">40 41</ref> def configure_optimizers(self): </p><formula xml:id="formula_117">pip</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Banana</head><p>In Sec. 5.1 we compared a classical compressor to our VIC in the case of rotation invariant tasks.</p><p>Here show results for invariances to different equivalences and provide more intuition as to what BINCE and VIC achieve. x-translation and BINCE. <ref type="figure" target="#fig_24">Fig. 7</ref> considers the case where downstream tasks are invariant to xtranslations. We used M : x ? [0, x 2 ] T as the maximal during training. We see that our model can essentially perform as well on all downstream tasks for only 60% of the bit-rate. Unsurprisingly we see that the codebook is in shape of horizontal stripes as these can cover the entire distribution with a few codes (small bit rate) while incurring a small invariance distortion (which only depends on the y value).</p><p>We visualized a BINCE model <ref type="figure" target="#fig_24">(Fig. 7c</ref>) in addition to VIC. Although the exact partition for both models is quite different (BINCE does not seem to learn equal sized partitions), both models clearly learn to partition the space into horizontal stripes. Once important difference, is that VIC also provides a codebook (shown with pink dots), as it can reconstruct a quantized version of the input, while BINCE only learns a latent representation and does not provide any reconstructions.  y-translation and induced distribution. <ref type="figure" target="#fig_26">Fig. 8</ref> considers the case where downstream tasks are invariant to y-translations. In this case the maximal invariant used during training is chosen to be M : x ? [0, x 2 ] T . Similarly to the case of x-translation and rotations, we see that our model can perform as well as a standard compressor for a fraction of the rate.</p><p>To provide a better intuition as to why this is the case we also plot the distribution of the reconstructions when marginalized over the y-axis. In other words we plot the distribution of M (X) when applied to the reconstructions, i.e., the x component of the reconstructions. We see that although the partition of the source space is very different for a standard compressor <ref type="figure" target="#fig_26">(Fig. 8a</ref>) and for our VIC <ref type="figure" target="#fig_26">(Fig. 8c</ref>), the induced distribution (and partition) in the marginalized space are actually very similar ( <ref type="figure" target="#fig_26">Fig. 8b</ref> and <ref type="figure" target="#fig_26">Fig. 8d</ref>). This shows where our bit-rate gains come from. Indeed from Prop. 1 we know that in the case of invariant tasks one only needs to model the distribution of M (X) (e.g. the distribution of the x component here), and we see that both the standard compressor and VIC does that similarly well.</p><p>The main difference being that VIC does so in an optimal way while the standard compressor needs to partition the input space in a finer way to achieve a similar induced partition in the M (X) space.</p><p>What is the relation between rate and predictions? Theorem 2 shows that, for log loss, the minimum rate is linearly related to the loss ? in downstream performance. Our theory (Appx. B.6) suggests a logarithmic relationship for MSE. This is seen for VIC and VC in <ref type="figure">Fig. 5</ref> of the main text (log scale x-axis).</p><p>On lossy compression and equivalences. Efficient lossy compression is about learning a partition (e.g. Voronoi diagrams, or <ref type="figure" target="#fig_24">Fig. 7</ref> ) of the input space to map many inputs to the same code. We use the fact that any partition can be constructed from an equivalence relation <ref type="bibr" target="#b160">[150]</ref> to learn compressors that are invariant to desired transformations. The shape of the partitions are then induced by the transformations, which perturb points in their quantization bins (equivalence classes), e.g., rotations in <ref type="figure">Fig. 5</ref> of the main text. The size of the partition, e.g., disks width in <ref type="figure">Fig. 5</ref> of the main text, depend on the desired performance ?. The pink dots are representatives of the partition, i.e., maximal invariants. The key is that using our objectives we can learn arbitrary quantization using only desired transformations, which ML practictioners already use for data augmentations. How do RD curves change if we use classification error instead of log loss? Our theory Theorem 2 only ensures good downstream log loss risk. Nevertheless, we used here the classification error (1 ? accuracy) throughout the main test as it is more commonly used for evaluating classification performance. <ref type="figure">Fig. 9</ref> shows that RD curves are very similar for when using classification error instead of accuracy. This is not very surprising as log loss is the standard (differential) proxy of classification error in ML.</p><p>What is the impact of the choice of augmentations? The rate decreases when A removes more information from X. To illustrate this we trained our VIC and BINCE using three augmentation sets on MNIST, all of which keep the true label invariant but progressively discard more X information: (i) standard image augmentations such as random translations, shears, and rescalings; (ii) those same standard image augmentations, but drawn from larger ranges of possible translations, shears, scales, etc; (iii) supervised "augmentations" line in <ref type="bibr" target="#b161">[151]</ref> that remove everything except label information, i.e., for every image x let x + = A(x) be a random image with the same label. <ref type="table" target="#tab_14">Table 6</ref> shows that using label-preserving augmentations that remove more information about X greatly decreases the rate without hindering classification performance. The fact that the supervised augmentations achieve a much better rate, shows that typical SSL compression is still very far from single-task label compression. SSL compression retains information for at least 2 79 ? 10 23 disjoint labels. How much does end-to-end improve compared to staggered training? We evaluated end-to-end training for both our losses against a staggered version that consists in first optimizing the distortion and then adding an entropy bottleneck to performing lossy compression of the learned representations (as in Sec. 5.3). <ref type="table" target="#tab_15">Table 7</ref> shows that end-to-end training can give large gains compared to the staggered method and that our compression gains with CLIP could be even further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 STL10</head><p>In the main text we used the STL10 data set to answer some principled questions about our method in controlled experiments. We provide additional results here.</p><p>How does the choice of distortion measures or bounds thereof affect RI curves? Supplementing the results in the main text, we show more extensive results comparing the effect of the distortion measures (invariant or not) or bounds thereof (BINCE and VIC are different bounds on R[M (X) | Z]) on RI curves. When predicting from compressed representations? ( <ref type="figure" target="#fig_0">Fig. 10 left)</ref>, BINCE achieves the best RI curves followed by VIC and VC. When predicting from reconstructionsX ( <ref type="figure" target="#fig_0">Fig. 10 right)</ref>, VIC still performs a little better than VC although the gap shrinks. <ref type="table" target="#tab_2">Table 1</ref> shows all quantitative results for best achieved downstream performance (as in <ref type="table" target="#tab_2">Table 1</ref> from the main text).</p><p>Note that VIC and VC achieve much worst downstream performance than BINCE. Based on preliminary results, we believe that this comes from the fact that, for consistency, in all experiments we used ResNet18 encoders. Indeed, ResNet18 have an global averaging pooling layer that averages the "latent image" over spatial dimensions (width and height). As a result, the representations? does not retain any spatial information, which is often useful for improving reconstructions. Preliminary results showed that removing this pooling layer improves downstream predictions significantly. Importantly, this impacts both VIC and VC so although the absolute performance improved by removing this layer the relative error did not seem to.  <ref type="figure" target="#fig_0">Figure 10</ref>: BINCE achieves the best RI curves followed by VIC and then VC for STL10 data. Rate-error curves when predicting downstream tasks from: (left) compressed representations?, (right) reconstructionsX. . This shows that all information about labels is retained, i.e., no images get compressed to the same Z but have different labels. The difference in test accuracy (which is also similar for training) must thus come from the fact that some information is easier to use/decode from <ref type="bibr" target="#b53">[51,</ref><ref type="bibr" target="#b54">52]</ref>. Indeed, our framework only concerns information retention rather than information usability. This is further supported by the fact that BINCE gets very good downstream accuracy, because it has been shown in theory <ref type="bibr" target="#b30">[29]</ref><ref type="bibr" target="#b31">[30]</ref><ref type="bibr" target="#b32">[31]</ref> and in practice <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b33">32]</ref> that contrastive representations are approximately linearly decodable.   <ref type="bibr" target="#b29">[28]</ref> hyperprior entropy model. In our experiments, however, we find that neither of these choices influence the RD curves at typical distortion levels as seen <ref type="figure" target="#fig_0">Fig. 11</ref>. In our experiments we use "H Hyper", which does seem to enable very low rates (high distortions) but seems to perform worst at very high rates (see <ref type="table" target="#tab_17">Table 9</ref>)</p><p>How important is the distribution over augmentations? As discussed in the main text, RD curves of our VIC show negligible difference when the distribution of augmentation shifts from training to test time. We provide this evidence in <ref type="figure" target="#fig_0">Fig. 12</ref>. Here we trained a VIC compressor on data with various augmentations, if these were (jointly) applied or not would be decided by a fair coin flip. At test time, we changed the coin to be biased with p = 0.05, 0.2, 0.5, 0.7, 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Pretrained CLIP</head><p>In <ref type="table" target="#tab_2">Table 10</ref> we provide all the quantitative results for our zero-shot CLIP experiments from which we derived the tables in the main text.</p><p>We note that zero-shot compression can still be analysed using our framework. Indeed CLIP was trained on 400M sampled from a r.v. X over images on the internet. As these datasets are on internet, they are samples from the joint (X, Y ) for a specific task Y . One can see this as a multi-task setting (each dataset is a distinct task).</p><p>What is the effect of using a more powerful predictor from the representation? In our framework we only discuss about information but never whether this information can easily be decoded by the predictors of interest. We investigated the effect of using more powerful predictors from our representation to understand how easy it is to decode the information in our representation. In particular, we evaluated all our CLIP compressors (i.e. at different ?), by considering predictions from our compressed representation using a two layer MLP and using a linear classifier (SVM). <ref type="table" target="#tab_2">Table 10</ref> shows that the advantage of using an MLP compared to a linear model is small, which suggests that our CLIP compressed representation store information in a way that is easily decodable. This is typical from contrastive self-supervised models <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b33">32]</ref>.</p><p>How does compressing SSL compare to compressing features form transfer learning? In previous work, Singh et al. <ref type="bibr" target="#b52">[50]</ref> had considered the case of compressing features from single-task transfer learning instead of self-supervised method. In <ref type="table" target="#tab_2">Table 10</ref> we compare compression of both type of features. Specifically "Transfer + EB" shows compression of a pretrained ResNet50 on ImageNet. We see that It generally performs worst than our CLIP compressor both in terms of test accuracies and bit-rate. One issue with this comparison is that the architectures of both models are not the same is likely that "transfer+EB" does not even perform better than CLIP on ImageNet. Humanity observes earth and sky at high temporal and spatial resolution, this can easily fill entire data centers. What is more, multiple copies of these series often exist over the world. At recording time it is usually not clear what kind of queries need to be answered about the recordings in the future; What was the weather like 10 years ago? Did a glacier resolve here? ect. To investigate our method in such real world scenario we compressed the GalaxyZoo telescope dataset (GZ2) and its 37 classification tasks. In <ref type="table" target="#tab_2">Table 11</ref>, we compare a classical lossless and lossy method, to our BINCE at same distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Galaxy Zoo</head><p>How well does CLIP pretraining work compared to in domain training? In the main paper we have seen that CLIP pretraining gives rise to a compressor that generalizes very well across different datasets. We evaluated how well the CLIP compressor generalizes compare to training BINCE directly end-to-end on GZ2's training set. <ref type="table" target="#tab_2">Table 11</ref> shows that the CLIP compressor works much better (4? rate gains) than the end-to-end BINCE. This suggests that pretraining can really be beneficial for training invariant compressors, and that our CLIP compressor can generalize very well across datasets.</p><p>How does our CLIP compressor generalize to very different images compared to SOTA compressors? In the main text we have seen that our CLIP compressor generalizes very well across different datasets compared to high quality JPEG. To better understand the limits of the generalization capacity of our CLIP compressor, we compared it to a SOTA classical compressor (WebP) on images that are completely different than the ones CLIP was trained on, namely Galaxy images (not typical images on internet). We see in <ref type="table" target="#tab_2">Table 11</ref> that in this challenging setting our CLIP compressor only achieves relatively small gains (30%).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our unsupervised coder improves compression by only keeping information necessary for typical tasks. (left) source augmented MNIST digit; (center) a neural perceptual compressor achieves 130 bit-rate; (right) our invariant compressor achieves 48 bit-rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>; Z] such that D[X, Z] ? ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Maximal invariants M (X) are representatives of equivalence classes. Example M s include the: (a) Euclidean norm for rotations; (b) unit vector for scaling; (c) f when equivalence classes are pre-images by f ; (d) empirical measure for permutations; (e) canonical graph for graph isomorphisms; (f) unaugmented input for data augmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 1 .</head><label>1</label><figDesc>Let (X , ?) be an equivalence relation and M a maximal invariant that takes at most countably many values, with H[M (X)] &lt; ?. Then D T? (2) with log loss is a valid distortion and D T? [X, Z] = R[M (X) | Z] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Rate-Invariance function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>(Left) By reconstructing prototypical digits our VIC (blue) achieves higher compression of augmented MNIST digits than standard neural compressors (VC, orange) without hindering downstream classification. 5 runs. (Right) The source examples (first row) as well as reconstructions for the non-invariant (second row) and invariant compressor (last row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>A Preliminaries 23 A. 1 42 D 49 F</head><label>2314249</label><figDesc>Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 A.2 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 A.3 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B Proofs: optimal bit-rate 26 B.1 Basic properties of equivalence relations and maximal invariants . . . . . . . . . 26 B.2 Proposition 1: simplifying and validating DT for log loss . . . . . . . . . . . . . 27 B.3 Theorem 2: optimal bit-rate under log loss . . . . . . . . . . . . . . . . . . . . 29 B.4 Recovering previous results in the literature . . . . . . . . . . . . . . . . . . . . 31 B.5 Generalizing Theorem 2: optimal bit rate for lossless prediction and any loss . . 31 B.6 Generalizing Theorem 2: optimal bit rate under MSE loss . . . . . . . . . . . . 33 C Variational objectives 37 C.1 Variational upper bound for the rate term I[Z; X] . . . . . . . . . . . . . . . . . 37 C.2 Variational upper bound for the distortion term Rlog[M (X) | Z] . . . . . . . . . 38 C.3 Case study: VIC and BINCE under data augmentations . . . . . . . . . . . . . . 39 C.4 Issue: dealing with unknown M (X) . . . . . . . . . . . . . . . . . . . . . . . 40 C.5 CLIP as BINCE's distortion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 E.2 General Image framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 E.3 MNIST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 E.4 STL10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 E.5 Galaxy Zoo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 E.6 Pretrained CLIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 E.7 Minimal code to train the CLIP compressor in &lt; 5 min. . . . . . . . . . . . . . . Additional experimental results 53 F.1 Banana . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 F.2 MNIST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 F.3 STL10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 F.4 Pretrained CLIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 F.5 Galaxy Zoo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Lemma 4 .</head><label>4</label><figDesc>Let M : X ? M and M : X ? M be two maximal invariants w.r.t. (X , ?). Then there exists a bijective function f : M ? M such that M = f ? M . Proof. From Lemma 3, M is a maximal invariant if and only if there is a bijective function g : X / ? ? M such that the maximal invariant is the composition of g and the projection onto equivalence classes, i.e. M = g ? ? ? . Let g be the corresponding bijection for M . Then we have M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Lemma 11 .</head><label>11</label><figDesc>Let ? denote an equivalence relation on X satisfying Assumption 4. Let M and M be two different maximal invariants w.r.t. (X , ?). Then H[M (X)] = H[M (X)].Proof. Due to Assumption 4, M (X) is a discrete random variable and so H[M (X)] is the discrete entropy, which is invariant to bijective functions<ref type="bibr" target="#b64">[62]</ref>.From Lemma 4 we know that there exists a bijection between M and M from which we conclude that H[M (X)] = H[M (X)] as desired. One of the requirements on Y to be set of downstream tasks T is the finiteness of H[Y ]. Thus, as a consequence of Lemmas 7 and 11, in the case of log loss, all M (X) are always in the set of downstream tasks T . Lemma 12. Let T ? be the invariant tasks of interest w.r.t. (X , ?) and the log loss. Then all maximal invariants are in T ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Proposition 1 (</head><label>1</label><figDesc>Invariant Distortion for log loss). Let T ? be the invariant tasks of interest w.r.t. (X , ?) and the log loss. Let M be any maximal invariant, and Z be a representation of X for T ? . Then the excess distortion w.r.t. log loss, D T? , is a valid distortion and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>uses the fact that Y ? ?M (X) | X (Lemma 6), that Y ? ?Z | X by Definition 6, and that M (X)? ?Z | X because M (X) ? T ? (again using Definition 6). Eq.(28)uses Lemma 6. To go from Eq. (28) to Eq. (30) we use the symmetry of conditional mutual information. Eq. (32) uses the discreteness of M (X) due to Assumption 4, so H[M (X) | Y, Z] ? 0 with equality when Y = M (X) which is possible due to Lemmas 7 and 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>? 1 ?</head><label>1</label><figDesc>X, e.g. a constant. Then, D T? [X, Z] = H[M (X) | Z] = H[M (X)] ? ? and Rate(?) = I[Z; X] = 0.For the second case we need Rate(?) ? H[M (X)] ? ? to be an equality when ? &lt; H[M (X)]. This happens iff inequalities Eq. (45) and Eq. (48) are equalities, i.e. iff X? ?Z | M (X) (for equality of the DPI [64]) and D T? [X, Z] = ?. We do so by starting from Z = M (X) (such that X? ?Z | M (X)) and "erasing" a fraction ? of bits, similarly to binary erasure channels, until D T? [X, Z] = ?. Let Z := M ?{ } for some ? M and let Z be a random variable that t.v.i. in Z and have the following conditional density parametrized by ? ? [0, 1[: ?z ? Z, ?m ? M, p(z | m) = ? ? ? if z = m ? if z = 0 else (51) A simple computation then gives D T? [X, Z] := R log [M (X) | Z] = H[M (X) | Z] = (1 ? ?) H[M (X) | Z = M (X)] + ? H[M (X) | Z = ] = 0 + ? H[M (X)], where the first equality uses Lemma 10 and the last equality uses H[M (X) | M (X)] = 0 due to the discreteness of M (X) (Assumption 4). We can thus achieve D T? [X, Z] = ? by setting ? = ? H[M (X)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>'s source coding theorem Rate(0) = H[M (X)] = H[X].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Lemma 16 .</head><label>16</label><figDesc>Strictly proper scoring rules and point-wise loss functions are information preserving (Definition 8).Proof. First let us consider point-wise loss functions. Suppose thatZ is such that R L [t(X) | Z] = R L [t(X) | X].As L is a point-wise loss function (Eq. (56)) and t is a deterministic function, we have that R L [t(X) | X] = 0. As a result we have R L [t(X) | Z] = 0 which using again Eq. (56) is equivalent to the existence of h s.t. h(Z) a.s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Theorem 17 (</head><label>17</label><figDesc>General invariant source coding). Let ? be an invariance relation on X that satisfies Assumption 4. Let T ? be the invariant tasks of interest w.r.t (X ,?) and any loss function L : Y ? A ? R ?0 as in Definition 4, M be any (X ,?) maximal invariant as in Definition 2, and Z be a representation of X for T ? as in Definition 6. Let Rate(0) denote the minimum achievable bit-rate for transmitting an i.i.d. source of Z such that for any Y ? T ? we have R L [Y | Z] = R L [Y | X]. Then Rate(0) is finite and given by Rate(0) = H[M (X)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>R L [M (X) | Z] = R L [M (X) | X] and can be transmitted with less than H[M (X)] bits. Suppose that R L [M (X) | Z] = R L [M (X) | X], then because L is a meaningful loss function we have there exists function h s.t. h(Z) = M (X). Using the discreteness of M (X) (Assumption 4), we thus have H[M (X) | Z] = 0. Using the RD theorem (Lemma 14) we know that the minimum bit rate for transmitting Z under the constraint H[M (X) | Z] = 0 is I[Z; M (X)] = H[M (X)]?H[M (X) | Z] = H[M (X)] ? 0. We thus find that transmitting a Z which ensures lossless predictions cannot require less than H[M (X)] bits which concludes the proof that Rate(0) = H[M (X)]. To get Eq. (58) we use the same decomposition as in Theorem 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Let B(M) denote the ?-algebra generated by a maximal invariant (all maximal invariants generate the same ?-algebra because they are one-to-one functions of each other Lemma 4), and let M b denote all R k -valued functions that are B(M)-measurable and bounded in L 2 . Then M b ? T ? , and by Lemma 7, M b is non-empty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>L 2 ( 2 E</head><label>22</label><figDesc>B(M)) ? L 2 (B(Z)) ? . The bounded (by B) elements of M b ? L 2 (B(Z)) ? are just the closed ball of radius B, so D T? [X, Z] = suph?L 2 (B(M))?L 2 (B(Z)) ? E X [||h(M (X))|| 2 ]?B X [||h(M (X))|| 2 ] .Now, since neither M b nor L 2 (B(Z)) ? is empty, their intersection is empty if and only if M b ? L 2 (B(Z)), i.e., B(M)yeah ? B(Z): all maximal invariants can be written as functions of Z. In that case, D T? [X, Z] = 0. Alternatively, if M b ? L 2 (B(Z)) ? is not empty, then choose some h * from it such that E X [||h * (M (X))|| 2 ] = B 2 = sup f ?M b R mse [f (M (X)) | Z]. Defining d(x, z) := ||h * (M (x))|| 2 yields a valid distortion D T? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>( 67 )</head><label>67</label><figDesc>Proof. First, by the DPI, for any homeomorphism g of M , I[X; Z] ? I[M (X); Z] = I[g(M (X)); Z] ? I[f * (M (X)); Z] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>?</head><label></label><figDesc>p(M (X)). Let F := {f : M ? Z ? R} be a variational family of discriminators which is used scores how likely M (X), Z are to be sampled from the joint p(M (X), Z) rather than the product of the marginal p(M (X))p(Z), then, lhs = R log [M (X) | Z] (82) = H[M (X) | Z] Lemma 10 (83) = H[M (X)] ? I[M (X); Z]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>(I[X; Z] , R log [M (X) | Z]) that are achieved by solutions of Eq. (72) is equal to the set of RD tuples (I X ; Z , R log [X | Z]) that are achieved by solutions of Eq. (90). In other words, they generate the same segment of the RI function. First, let us show why we can replace X byX, i.e., show that for any ? we have that arg min p(Z|X) I[X; Z] + ? R log [M (X) | Z] is equivalent to arg min p(Z|X) I X ; Z + ? R log M (X) | Z . This can be seen from the fact that the optimal bit rate in Theorem 2 only depends on H[M (X)] and ?. In particular, Rate(?) does not depend on the distribution of the source inside the equivalence classes, p(X|M (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>x = (Rot(?40) ? x) + [?3, ?4] T . For every epoch we resample 1024000 new points, i.e., examples are never seen twice during training).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>49 Source Code 2 :</head><label>492</label><figDesc>42 param = [p for n, p in self.named_parameters() if not n.endswith(".quantiles")] 43 quantile_param = [p for n, p in self.named_parameters() if n.endswith(".quantiles")] 44 optimizer = Adam(param, lr=self.hparams.lr) 45 optimizer_coder = Adam(quantile_param,lr=self.hparams.lr) 46 scheduler = lr_scheduler.StepLR(optimizer, self.hparams.lr_step) scheduler_coder = lr_scheduler.StepLR(optimizer_coder, self.hparams.lr_step) 48 return [optimizer,optimizer_coder], [scheduler,scheduler_coder] Minimal code for training an entropy bottleneck to convert a pretrained SSL model into a powerful zero-shot compressor. For the training and evaluation code see Source Code 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 7 :</head><label>7</label><figDesc>VIC and BINCE improves compression of Banana distribution when downstream tasks are invariant to translation on the x-axis by quantizing the space into horizontal stripes. (a) standard compression with a rate of 4.86 bits and an invariant distortion of 7.51e-2 ; (b) our VIC with a rate of 2.93 bits and an invariant distortion of 7.08e-2. (c) our BINCE with a rate of 2.93 bits and an invariant distortion of 7.08e-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>Standard q(M (X)) (c) y-translation VIC (d) VIC q(M (X))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 8 :</head><label>8</label><figDesc>VIC improves compression of Banana distribution when downstream tasks are invariant to translation on the y-axis by (implicitly) estimating the density p(M (X)). (a) standard compression with a rate of 4.86 bits and an invariant distortion of 7.67e-2 ; (b) the induced marginal distribution q(M (X)) of the x-value of the reconstructions from the standard neural compressor; (c) our compression with a rate of 3.24 bits and an invariant distortion of 7.84e-2. (b) the induced marginal distribution q(M (X)) of the x-value of the reconstructions from the VIC;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>F. 2 MNISTFigure 9 :</head><label>29</label><figDesc>Augmented MNIST RD curves for downstream predictions are very similar when using classification error (left) instead of the log loss (right) from our theory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>Indeed, we have I[X; A(X)] = I[X; M (X)] + I[X; M (X)] = I[X; M (X)] + 0 = H[M (X)], where the second equality comes from Assumption 7 and the last equality from the determinism of M . As a result we can rewrite Theorem 2 as Rate(?) = I[X; A(X)] ? ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>The choice of variational bounds on the rate term I[Z; X] has little effect on RI curves for STL10 data. "MI unitgaussian" is the upper bound on mutual information used in VIB and VAE; "H factorized" is Ball? et al.'s [28] upper bound on H[Z] with a factorized entropy model; "H hyper" is Ball? et al.'s [28] upper bound on H[Z] with a hyperprior entropy model. VIC is robust to distribution shifts in the augmentations as it is invariant to the augmentations. Specifically, test time shifts in augmentation probability seem to have little effect on the rate-distortion curve for the case of STL10 data. I[Z; X] ? D KL [p ? (Z|X)] q ? (Z); (H Factorized) the entropy bound H[Z] ? E p(Z,X) [q ? (Z)] where q ? (Z) is Ball? et al.'s [28] factorized entropy model; (H Hyper) the entropy bound where q ? (Z) is Ball? et al.'s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Invariant compressors (BINCE, VIC) outperform classical (PNG, JPEG, WebP) and neural (VC) compressors on STL10. BINCE achieves lossless prediction but compresses 121? better.</figDesc><table><row><cell></cell><cell cols="7">PNG [38] JPEG [39] WebP [40] VCX VICX VIC Z BINCE</cell></row><row><cell>Decrease in test acc.</cell><cell>0</cell><cell>0.7</cell><cell>1.1</cell><cell>21.0</cell><cell>25.1</cell><cell>16.1</cell><cell>0.0</cell></row><row><cell>Compression gains</cell><cell>1?</cell><cell>3?</cell><cell>13?</cell><cell>63?</cell><cell>269?</cell><cell>175?</cell><cell>121?</cell></row></table><note>How do our BINCE and VIC compare to standard compressors? In Table 1 we compare com- pressors at the lowest downstream error that they achieved. As benchmark, we use PNG's lossless compression. Predicting from PNG corresponds to standard image classification, and obtains a rate of 1.42e4 bits per image for 80.8% accuracy. Classical lossy methods (JPEG, WebP) achieved up to 13? bit-rate gains with little drop in performance. In comparison, our BINCE method achieved 121? compression gains with no impact on predictions. Both our invariant (VIC) and standard (VC) neural compressors significantly decreased classification accuracy, which we believe can be explained by the encoders architecture (ResNet18) that we use for consistency (see Appx. F.3).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Converting a pretrained SSL model into a zero-shot compressor achieves substantial bit-rate gains while allowing test accuracies similar to supervised models predicting from raw images.</figDesc><table><row><cell></cell><cell cols="2">ImageNet STL PCam</cell><cell cols="2">Cars CIFAR10</cell><cell>Food</cell><cell cols="2">Pets Caltech</cell></row><row><cell>Rate gains vs JPEG</cell><cell>1104? 35?</cell><cell cols="2">64? 131?</cell><cell cols="3">7? 109? 150?</cell><cell>126?</cell></row><row><cell>Our Acc. [%]</cell><cell>76.3 98.7</cell><cell>80.9</cell><cell>79.6</cell><cell>95.2</cell><cell>88.3</cell><cell>89.5</cell><cell>93.4</cell></row><row><cell>Supervised Acc. [%]</cell><cell>76.1 99.0</cell><cell>82.6</cell><cell>49.1</cell><cell>96.7</cell><cell>81.8</cell><cell>90.4</cell><cell>94.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Our entropy bottleneck (EB) on CLIP improves compression of representations up to 17? with little impact on predictions. The same compressor is used across datasets. Rates are per image.</figDesc><table><row><cell></cell><cell></cell><cell>ImageNet</cell><cell>STL</cell><cell>PCam</cell><cell cols="2">Cars CIFAR10</cell><cell>Food</cell><cell cols="2">Pets Caltech</cell></row><row><cell></cell><cell>JPEG</cell><cell cols="4">1.49e6 4.71e4 9.60e4 1.92e5</cell><cell cols="3">1.05e4 1.54e5 1.81e5</cell><cell>1.69e5</cell></row><row><cell>Bit-Rate</cell><cell>CLIP +EB high ? +EB ?</cell><cell cols="4">1.52e4 1.52e4 1.52e4 1.52e4 2.47e3 2.46e3 2.61e3 2.59e3 1.35e3 1.34e3 1.49e3 1.47e3</cell><cell cols="3">1.52e4 1.52e4 1.52e4 2.53e3 2.39e3 2.33e3 1.41e3 1.27e3 1.21e3</cell><cell>1.52e4 2.46e3 1.34e3</cell></row><row><cell></cell><cell>+EB low ?</cell><cell cols="4">9.63e2 9.52e2 1.09e3 1.07e3</cell><cell cols="3">1.02e3 8.89e2 8.35e2</cell><cell>9.53e2</cell></row><row><cell>Test Acc.</cell><cell>CLIP +EB high ? +EB ? +EB low ?</cell><cell>76.5 76.6 76.3 76.0</cell><cell>98.6 98.7 98.7 98.7</cell><cell>84.5 82.7 80.9 80.1</cell><cell>80.8 80.4 79.6 78.9</cell><cell>95.3 95.3 95.2 94.8</cell><cell>88.5 88.5 88.3 87.6</cell><cell>89.7 89.6 89.5 88.6</cell><cell>93.2 93.5 93.4 92.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Text-image invariance is better than invariance to standard augmentations for image classification. CLIP and SimCLR are both ResNet50 pretrained with InfoNCE but different augmentations.</figDesc><table><row><cell></cell><cell></cell><cell>ImageNet</cell><cell cols="2">STL PCam</cell><cell cols="3">Cars CIFAR10 Food</cell><cell cols="2">Pets Caltech</cell></row><row><cell>Rate</cell><cell>CLIP+EB SimCLR+EB</cell><cell cols="2">2108 1962 2811 2732</cell><cell cols="2">1949 2421 2769 2751</cell><cell cols="3">2111 1991 1867 2950 2077 2839</cell><cell>1968 2502</cell></row><row><cell>Acc.</cell><cell>CLIP+EB SimCLR+EB</cell><cell>63.2 62.8</cell><cell>92.0 91.9</cell><cell>78.6 81.4</cell><cell>68.0 29.6</cell><cell>65.5 78.6</cell><cell>74.1 60.0</cell><cell>81.8 78.9</cell><cell>83.0 79.0</cell></row><row><cell cols="2">6 Related work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Lemma 14.  (Shannon<ref type="bibr" target="#b18">[17]</ref>; Theorem 7.2.4 and 7.2.5 from Berger<ref type="bibr" target="#b65">[63]</ref>) Let D[X; Z] be a valid distortion. The minimum achievable bit-rate for transmitting an i.i.d. source X with expected distortion less than ? ? 0 is given by the rate-distortion function:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Proposition 18 (Invariant Distortion for MSE). Let T ? be the invariant tasks of interest w.r.t. (X , ?) and w.r.t. the MSE. Fix any maximal invariant M that is also in T ? , and let Z be a representation of X for T ? . Then the excess distortion w.r.t. MSE, D T? , is a valid distortion and D</figDesc><table /><note>T? [X, Z] = sup</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Theorem 19 (Rate-invariance for MSE). Let ? ? 0. Let T ? be the invariant tasks of interest w.r.t. (X ,?) and the MSE, M be any maximal invariant in L 2 (?, H, P), and Z be a representation of X for T ? . Let Rate(?) denote the minimum achievable bit-rate for transmitting an i.i.d. source of Z such that for any</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Datasets used to evaluate our zero-shot compressor. -1 for the shape means variable.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Classes Train size Valid size Test size</cell><cell>Metric</cell><cell>Shape</cell></row><row><cell>ImageNet [16]</cell><cell cols="2">1000 1,281,167</cell><cell></cell><cell>50,000</cell><cell>accuracy</cell><cell>(-1,-1,3)</cell></row><row><cell>CIFAR10 [141]</cell><cell>10</cell><cell>50,000</cell><cell></cell><cell>10,000</cell><cell cols="2">accuracy (32,32,3)</cell></row><row><cell>CIFAR100 [141]</cell><cell>100</cell><cell>50,000</cell><cell></cell><cell>10,000</cell><cell cols="2">accuracy (32,32,3)</cell></row><row><cell>Cars196 [142]</cell><cell>196</cell><cell>8,144</cell><cell></cell><cell>8,041</cell><cell>accuracy</cell><cell>(-1,-1,3)</cell></row><row><cell>Pets37 [143]</cell><cell>37</cell><cell>3,680</cell><cell></cell><cell cols="2">3,669 balanced acc.</cell><cell>(-1,-1,3)</cell></row><row><cell>Caltech101 [144]</cell><cell>102</cell><cell>3,060</cell><cell></cell><cell cols="2">6,085 balanced acc.</cell><cell>(-1,-1,3)</cell></row><row><cell>Food101 [145]</cell><cell>101</cell><cell>75,750</cell><cell></cell><cell>25,250</cell><cell>accuracy.</cell><cell>(-1,-1,3)</cell></row><row><cell>STL10 [37]</cell><cell>10</cell><cell>5000</cell><cell></cell><cell>8000</cell><cell cols="2">accuracy (96,96,3)</cell></row><row><cell>PCam [44, 146]</cell><cell>2</cell><cell>262,144</cell><cell>32,768</cell><cell>32,768</cell><cell cols="2">accuracy (96,96,3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>For data augmentations we used similar ones as used by CLIP namely, normalizing the image by the mean and std form their training dataset (mean=[0.48145466, 0.4578275, 0.40821073] and mean=[0.26862954, 0.26130258, 0.27577711]), resizing the smallest side of the image to 224 ? 224 pixels with bicubic interpolation, then taking a random 224 ? 224 crop. During the evaluation the random cropping is replaced by a center cropping.Evaluating the zero-shot compressor. For evaluating the rate obtained by our compressor, we provide the negative log likelihood of our entropy model for the each test dataset. For evaluating the downstream predictive performance for each dataset we train a 2 hidden layer MLP of dimensions 512 ? 2048 ? 2048 ? |Y| with batch normalization and ReLU activation. For each dataset we provide the best model from 25 randomly sampled models, that arise by randomly sampling the following</figDesc><table><row><cell>hyperparameters:</cell></row><row><cell>? batch size: [32, 64] with logarithmic sampling.</cell></row><row><cell>? optimizer: Adam, SGD, AdamW.</cell></row><row><cell>? weight decay: [1e-7, 1e-4] with logarithmic sampling.</cell></row><row><cell>? learning rate: [1e-5, 1e-3] with logarithmic sampling.</cell></row><row><cell>? scheduler: exponential decay (with total decay by 100 or 1000), decreasing learning rate on</cell></row><row><cell>validation loss plateau, cosine scheduler, decaying learning rate at fixed intervals.</cell></row><row><cell>? dropout [148]: [0, 0.5].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>install git+https://github.com/openai/CLIP.git pip install scikit-learn==0.24.2 lightning-bolts==0.3.3 compressai==1.1.4 The minimal boilerplate code (Source Code 1) downloads the training data and the pretrained CLIP (from line 42), trains the compressor (from line 46), entropy code the evaluation data (from line 57), and finally evaluates downstream performance (from line 67). The actual compressor is defined in Source Code 2. import clip, torch, pytorch_lightning, numpy, tqdm, math, time 2 from torchvision.datasets import STL10,CIFAR10 3 from compressai.entropy_models import EntropyBottleneck 4 from torch.optim import Adam,lr_scheduler 5 from torch.utils.data import DataLoader 6 from pl_bolts.datamodules import SklearnDataModule 7 from sklearn.svm import LinearSVC self.scaling = torch.nn.Parameter(torch.ones(self.hparams.z_dim)) 7 self.biasing = torch.nn.Parameter(torch.zeros(self.hparams.z_dim))</figDesc><table><row><cell>1</cell><cell>class ArrayCompressor(pytorch_lightning.LightningModule):</cell></row><row><cell>2</cell><cell>def __init__(self, *args, **kwargs):</cell></row><row><cell>3</cell><cell>super().__init__()</cell></row><row><cell>4</cell><cell>self.save_hyperparameters()</cell></row><row><cell>5</cell><cell>self.bottleneck = EntropyBottleneck(self.hparams.z_dim)</cell></row><row><cell>8</cell><cell>self.is_updated = False</cell></row><row><cell>9</cell><cell></cell></row><row><cell>10</cell><cell>def forward(self, batch):</cell></row><row><cell>11</cell><cell>z, y = batch</cell></row><row><cell>12</cell><cell>z = (z + self.biasing) * self.scaling.exp()</cell></row><row><cell>13</cell><cell>z_hat, q_z = self.bottleneck(z.unsqueeze(-1).unsqueeze(-1))</cell></row><row><cell>14</cell><cell>z_hat = z_hat.squeeze() / self.scaling.exp() -self.biasing</cell></row><row><cell>15</cell><cell>return z_hat, q_z.squeeze(), y.squeeze()</cell></row></table><note>16</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Using label-preserving augmentations that remove more information about X decreases the rate without hindering classification performance. Single run.</figDesc><table><row><cell></cell><cell>Augmentations</cell><cell cols="2">Rate [ bits img ] Test Acc. [%]</cell></row><row><cell>VIC</cell><cell>Small set Large set</cell><cell>185.3 79.0</cell><cell>96.2 97.9</cell></row><row><cell></cell><cell>Supervised (largest set)</cell><cell>5.7</cell><cell>99.1</cell></row><row><cell>BINCE</cell><cell>Small set Large set</cell><cell>256 131</cell><cell>95.3 97.8</cell></row><row><cell></cell><cell>Supervised (largest set)</cell><cell>5.9</cell><cell>97.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>End-to-end compression of augmented MNIST works much better than staggered compression for both VIC and BINCE. Single run.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Rate [ bits img ] Test Acc. [%]</cell></row><row><cell>VIC</cell><cell>Staggered End-to-end</cell><cell>477 79</cell><cell>98.0 97.9</cell></row><row><cell>BINCE</cell><cell>Staggered End-to-end</cell><cell>358 131</cell><cell>97.4 97.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 :</head><label>8</label><figDesc>We compare classical compression formats (PNG, JPEG, webP) to neural (VC) and invariant (BINCE, VIC) ones. BINCE achieves the same error rate but compresses 121.1? better. For all our models the we estimated (using a naive sample estimate) the mutual information I[Z; Y ] and found that?[Z, X] =?[Y ]</figDesc><table><row><cell>Distortion</cell><cell>Predict from</cell><cell cols="3">Best error [%] Rate [Mb/img] Compression factor</cell></row><row><cell>PNG [38]</cell><cell>reconstructions</cell><cell>19.2</cell><cell>14.20</cell><cell>1?</cell></row><row><cell>JPEG [39]</cell><cell>reconstructions</cell><cell>19.9</cell><cell>4.60</cell><cell>3.0?</cell></row><row><cell>WebP [40]</cell><cell>reconstructions</cell><cell>20.3</cell><cell>1.12</cell><cell>12.7?</cell></row><row><cell>VC</cell><cell>reconstructions</cell><cell>40.2</cell><cell>0.23</cell><cell>62.8?</cell></row><row><cell>VC</cell><cell>features</cell><cell>52.6</cell><cell>1.08</cell><cell>13.2?</cell></row><row><cell>VIC (ours)</cell><cell>reconstructions</cell><cell>44.3</cell><cell>0.05</cell><cell>268.6?</cell></row><row><cell>VIC (ours)</cell><cell>features</cell><cell>35.3</cell><cell>0.08</cell><cell>174.8?</cell></row><row><cell cols="2">BINCE (ours) features</cell><cell>19.2</cell><cell>0.12</cell><cell>121.1?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc>Hierarchical hyperprior works worst (higher rates) for low distortion, when compared to a factorized prior and a mutual information bottleneck on STL10 data. To understand the effect of using other bounds on I[Z; X] and different entropy models q ? (Z). Specifically, inFig. 11we compare three different bounds on mutual information: (MI unitgaussian) the mutual information bound from VAE and VIB</figDesc><table><row><cell>Bottleneck</cell><cell>Entropy model</cell><cell cols="2">Lossless Rate [bits/img] Lossless loss</cell></row><row><cell>Entropy H[Z]</cell><cell>Factorized prior [28]</cell><cell>598.5</cell><cell>0.3117</cell></row><row><cell>Entropy H[Z]</cell><cell>Hierarchical prior [28]</cell><cell>934.7</cell><cell>0.3100</cell></row><row><cell cols="2">Mutual Information I[Z; X] Unit Gaussian</cell><cell>592.5</cell><cell>0.3074</cell></row></table><note>How does the choice of bounds on the rate term I[Z; X] impact RI curves? For the main paper we always used the standard [24] neural compressor's upper bound on I[Z; X], namely, the entropy bound I[Z; X] ? H[Z] ? E p(Z,X) [q ? (Z)].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Converting a pretrained SSL model into a zero-shot compressor achieves substantial bit-rate gains while allowing test accuracies similar to predicting from raw images. CLIP refers to the original CLIP with lossless compression of the representations. CLIP+EB refers to our CLIP compressor. CLIP+EB ? and CLIP+EB + are our CLIP compressors trained respectively for a larger and smaller bit-rate. We provide downstream evaluation using an MLP and a linear (SVM) predictor. Baselines: JPEG and compression of features from a ImageNet pretrained classifier (Transfer + EB).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ImageNet</cell><cell>STL</cell><cell>PCam</cell><cell>Cars</cell><cell>CIFAR10</cell><cell>CIFAR100</cell><cell>Food</cell><cell>Pets</cell><cell>Caltech</cell></row><row><cell>Rate [Bits/img]</cell><cell></cell><cell>JPEG Transfer + EB CLIP CLIP+EB ? CLIP+EB CLIP+EB +</cell><cell>1.49e6 3.95e3 1.52e4 2.47e3 1.35e3 9.63e2</cell><cell>4.71e4 3.33e3 1.52e4 2.46e3 1.34e3 9.52e2</cell><cell>9.60e4 3.99e3 1.52e4 2.61e3 1.49e3 1.49e3</cell><cell>1.92e5 3.18e3 1.52e4 2.59e3 1.47e3 1.52e2</cell><cell>1.05e4 3.92e3 1.52e4 2.53e3 1.41e3 1.02e2</cell><cell>1.05e4 1.52e4 2.54e3 1.42e3 1.09e3</cell><cell>1.54e5 3.26e3 1.52e4 2.39e3 1.27e3 8.89e2</cell><cell>1.81e5 3.70e3 1.52e4 2.33e3 1.21e3 8.35e2</cell><cell>1.69e5 3.40e3 1.52e4 2.46e3 1.34e3 9.53e2</cell></row><row><cell>Test Accuracies [%]</cell><cell>[41] MLP</cell><cell>JPEG CLIP [41] Transfer + EB CLIP CLIP+EB ? CLIP+EB CLIP+EB + CLIP</cell><cell>76.6 76.1 72.7 76.5 76.6 76.3 76.0</cell><cell>99.0 98.3 96.1 98.6 98.7 98.7 98.7 98.6</cell><cell>82.6 83.9 79.4 84.5 82.7 80.9 80.1 83.8</cell><cell>49.1 81.8 42.0 80.8 80.4 79.6 78.9 80.8</cell><cell>96.7 95.1 87.0 95.3 95.3 95.2 94.8 95.0</cell><cell>86.3 80.5 80.9 80.9 80.1 78.6 79.8</cell><cell>81.8 88.8 66.8 88.5 88.5 88.3 87.6 85.0</cell><cell>90.4 90.0 91.3 89.7 89.6 89.5 88.6 89.3</cell><cell>94.5 93.0 89.9 93.2 93.5 93.4 92.9 93.8</cell></row><row><cell></cell><cell>Linear</cell><cell>CLIP+EB ? CLIP+EB CLIP+EB +</cell><cell></cell><cell>98.7 98.7 98.6</cell><cell>83.2 81.1 80.5</cell><cell>80.8 79.9 78.9</cell><cell>95.0 94.8 94.4</cell><cell>79.7 79.0 80.5</cell><cell>85.0 83.6 82.5</cell><cell>89.2 88.3 87.8</cell><cell>93.6 93.7 93.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 11 :</head><label>11</label><figDesc>Comparisons between pretrained CLIP BINCE, a BINCE trained end-to-end, and SOTA perceptual compressors on Galaxyzoo data. CLIP BINCE achieves the smallest bit-rate.</figDesc><table><row><cell>Compressor</cell><cell>rate</cell><cell cols="5">test loss val. loss mean median max min</cell><cell>std</cell></row><row><cell></cell><cell cols="2">[Mb/img] [ ]</cell><cell>[ ]</cell><cell>[10 ?3 ]</cell><cell>[ ]</cell><cell cols="2">[10 ?7 ] [10 ?2 ]</cell></row><row><cell>PNG</cell><cell>53.73</cell><cell>0.007</cell><cell>0.008</cell><cell>0.86</cell><cell cols="2">0.07 1.04</cell><cell>1.62</cell></row><row><cell>JPEG</cell><cell>1.68</cell><cell>0.012</cell><cell>0.013</cell><cell>1.25</cell><cell cols="2">0.11 1.23</cell><cell>2.61</cell></row><row><cell>WebP</cell><cell>0.48</cell><cell>0.010</cell><cell>0.011</cell><cell>1.20</cell><cell cols="2">0.10 1.16</cell><cell>2.29</cell></row><row><cell>BINCE (CLIP)</cell><cell>0.33</cell><cell>0.011</cell><cell>0.011</cell><cell>1.13</cell><cell cols="2">0.10 7.59</cell><cell>2.29</cell></row><row><cell cols="2">BINCE (end to end) 1.77</cell><cell>0.012</cell><cell>0.012</cell><cell>1.43</cell><cell cols="2">0.11 1.31</cell><cell>2.50</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">As a reminder, ? is an equivalence relation iff for all x, x , x ? X : (reflexivity) x ? x, (symmetry)x ? x ?? x ? x, and (transitivity) x ? x and x ? x =? x ? x .<ref type="bibr" target="#b3">3</ref> This extends the definition of maximal invariants<ref type="bibr" target="#b20">[19]</ref> beyond invariances to group actions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We prove in Appx. B.5 that Rate(0) = H[M (X)] for any losses used in ML.<ref type="bibr" target="#b5">5</ref> The number of heads K follow a binomial distribution so H[K] ? O(log n). Here K is also a minimal sufficient statistic for X n . More generally, if P (X ) is invariant to ? and ? is the coarsest such relation, then minimal sufficiency coincides with maximal invariance. In practice, however, P (X ) will rarely be ? invariant.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">As the RI function is not strictly convex (Fig. 3), it should be beneficial to use R[M (X) | Z] 2 to ensure that sweeping over ? is equivalent to sweeping over ?<ref type="bibr" target="#b23">[22]</ref>. We did not see any difference in practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We investigated finetuning CLIP on MSCOCO but it suffered from catastrophic forgetting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">A movie takes around 10GB to store, but the information relevant to humans (e.g., "what happened to the house?", "how did the movie end?") can likely be stored in a detailed movie script and would require 100KB.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">See Flamich et al.<ref type="bibr" target="#b84">[82]</ref> or Schulman<ref type="bibr" target="#b85">[83]</ref> for an O(exp(I[Z; X])) algorithm.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Bits-back coding<ref type="bibr" target="#b86">[84]</ref> can efficiently reach the desired bit-rate only because it is in the lossless setting.<ref type="bibr" target="#b12">11</ref> RL[M (X) | Z] ? inf h?H E p(X)p(Z|X) [L(M (X), h(Z))] which comes from the fact that we are taking an inf over a subset H of all possible predictors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Taking exponentials is not necessary, any function g : M ? R ?0 would work as a discriminator, we use g := exp ?f to ensure positivity as this has a nice softmax interpretation and is standard in practice. Our derivation is still general as we can set f := log ? g.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">Note that the bit rate gains H[X | M (X)] ? ? clearly depend on p(X|M (X)), but not the actual bit-rate H[M (X)] ? ?.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">The standard definition of adequacy from Skibinsky<ref type="bibr" target="#b135">[127]</ref> also requires the statistic to be sufficient, here we use "adequacy in the wide sense" as defined by Takeuchi and Akahira<ref type="bibr" target="#b137">[128]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">Their code can be found at https://github.com/tensorflow/compression/blob/master/models/ toy_sources/toy_sources.ipynb</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">At the end we did not use the sentences as freezing CLIP worked very well.<ref type="bibr" target="#b18">17</ref> https://github.com/openai/CLIP<ref type="bibr" target="#b19">18</ref> We tried many ways of finetuning CLIP with very small learning rates and frozen components, but although the rate gains were large (around 2 to 3?) this lead to significant decrease in performance, most probably due to catastrophic forgetting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">9 def clip_featurize_data(dataset, device):</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We would like to thank Alex Alemi, David Duvenaud, Andriy Mnih, Emile Mathieu, Jonah Philion, Yangjun Ruan, and Ilya Sutskever for their helpful feedback and encouragements. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute. BBR acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC): RGPIN-2020-04995, RGPAS-2020-00095, DGECR-2020-00343.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Donti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Kaack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Milojevic-Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waldman-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tackling Climate Change with Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sherwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Mukkavilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>K?rding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Creutzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>_eprint: 1906.05433</idno>
		<ptr target="http://arxiv.org/abs/1906.05433" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Big Data: Conceptual Analysis and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zgurovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Zaychenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Driving to safety: How many miles of driving would it take to demonstrate autonomous vehicle reliability?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Paddock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part A: Policy and Practice</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="182" to="193" />
			<date type="published" when="2016" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sensor and sensor fusion technology in autonomous vehicles: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Yeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Velasco-Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2140</biblScope>
			<date type="published" when="2021" />
			<publisher>Multidisciplinary Digital Publishing Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transform coding of audio signals using perceptual noise criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="314" to="323" />
			<date type="published" when="1988-02" />
		</imprint>
	</monogr>
	<note>IEEE Journal on Selected Areas in Communications</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A model of perceptual image fidelity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Teo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.1995.537485</idno>
		<ptr target="https://doi.org/10.1109/ICIP.1995.537485" />
	</analytic>
	<monogr>
		<title level="m">Proceedings 1995 International Conference on Image Processing</title>
		<meeting>1995 International Conference on Image Processing<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="343" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceptual Video Compression: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<ptr target="https://infoscience.epfl.ch/record/184275" />
	</analytic>
	<monogr>
		<title level="j">Ieee Journal Of Selected Topics In Signal Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">684</biblScope>
			<date type="published" when="2012" />
			<publisher>Ieee-Inst Electrical Electronics Engineers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/blau19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Research, K. Chaudhuri and R. Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA, ser</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="675" to="685" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Digital Audio Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Technical Journal</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High-Fidelity Generative Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson ; Virtual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/8a50bae297807da9e97722a0b3fd8f27-Abstract.html" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The information bottleneck method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/physics/0004057" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10710-017-9314-z</idno>
		<ptr target="https://doi.org/10.1007/s10710-017-9314-z" />
	</analytic>
	<monogr>
		<title level="j">Genet. Program. Evolvable Mach</title>
		<imprint>
			<biblScope unit="volume">800</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="305" to="307" />
			<date type="published" when="2016" />
			<publisher>Deep learning -The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on Image Data Augmentation for Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-019-0197-0</idno>
		<ptr target="https://doi.org/10.1186/s40537-019-0197-0" />
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6114" />
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<editor>Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<idno>arXiv: 1807.03748</idno>
		<ptr target="http://arxiv.org/abs/1807.03748" />
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="1063" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coding theorems for a discrete source with a fidelity criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Nat. Conv. Rec</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rate distortion theory for sources with abstract alphabets and memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<ptr target="https://linkinghub.elsevier.com/retrieve/pii/S0019995868911236" />
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="273" />
			<date type="published" when="1968-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Group Invariance Applications in Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Eaton</surname></persName>
		</author>
		<ptr target="https://www.jstor.org/stable/4153172" />
	</analytic>
	<monogr>
		<title level="m">Regional Conference Series in Probability and Statistics</title>
		<imprint>
			<publisher>Institute of Mathematical Statistics</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">133</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rashevsky</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02477860</idno>
		<ptr target="https://doi.org/10.1007/BF02477860" />
	</analytic>
	<monogr>
		<title level="m">Life, information theory, and topology</title>
		<imprint>
			<date type="published" when="1955-09" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="229" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Benefiting from Disorder: Source Coding for Unordered Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0708.2310</idno>
		<idno>arXiv: 0708.2310</idno>
		<ptr target="http://arxiv.org/abs/0708.2310" />
		<imprint>
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Caveats for information bottleneck in deterministic scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Tracey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Kuyk</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rke4HiAcY7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-Scale Machine Learning with Stochastic Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-7908-2604-3_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-7908-2604-3_16" />
	</analytic>
	<monogr>
		<title level="m">19th International Conference on Computational Statistics, COMPSTAT 2010</title>
		<editor>Papers, Y. Lechevallier and G. Saporta</editor>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>Physica-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end Optimized Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJxdQ3jeg" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lossy Image Compression with Compressive Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJiNwv9gg" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalized Kraft Inequality and Arithmetic Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Name: IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="198" to="203" />
			<date type="published" when="1976-05" />
		</imprint>
	</monogr>
	<note>IBM Journal of Research and Development</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Asymmetric numeral systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duda</surname></persName>
		</author>
		<idno>_eprint: 0902.0271</idno>
		<ptr target="http://arxiv.org/abs/0902.0271" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Variational image compression with a scale hyperprior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkcQFMZRb" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Theoretical Analysis of Contrastive Unsupervised Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khandeparkar</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/saunshi19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Research, K. Chaudhuri and R. Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA, ser</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5628" to="5637" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contrastive learning, multi-view redundancy, and linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v132/tosh21a.html" />
	</analytic>
	<monogr>
		<title level="m">Virtual Conference, Worldwide, ser. Proceedings of Machine Learning Research</title>
		<editor>V. Feldman, K. Ligett, and S. Sabato</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-03-19" />
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="1179" to="1206" />
		</imprint>
	</monogr>
	<note>Algorithmic Learning Theory</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01064</idno>
		<idno>arXiv: 2008.01064</idno>
		<ptr target="http://arxiv.org/abs/2008.01064" />
		<title level="m">Predicting What You Already Know Helps: Provable Self-Supervised Learning</title>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/chen20j.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On Variational Bounds of Mutual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/poole19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Research, K. Chaudhuri and R. Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA, ser</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5171" to="5180" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09852</idno>
		<idno>arXiv: 2007.09852</idno>
		<ptr target="http://arxiv.org/abs/2007.09852" />
		<title level="m">Multi-label Contrastive Predictive Coding</title>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/7780459/" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonlinear Transform Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<idno>_eprint: 2007.03034</idno>
		<ptr target="https://arxiv.org/abs/2007.03034" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Graphics</surname></persName>
		</author>
		<ptr target="hhttp://www.libpng.org/pub/png/spec/iso/index-object.html" />
		<title level="m">ISO/IEC 15948</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Webp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>published: developers.google.com/speed/webp</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<idno>arXiv: 2103.00020</idno>
		<ptr target="http://arxiv.org/abs/2103.00020" />
		<imprint>
			<date type="published" when="2021-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>_eprint: 2010.11929</idno>
		<ptr target="https://arxiv.org/abs/2010.11929" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<idno>arXiv: 1405.0312</idno>
		<ptr target="http://arxiv.org/abs/1405.0312" />
		<title level="m">Microsoft COCO: Common Objects in Context</title>
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Rotation Equivariant CNNs for Digital Pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Linmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03962</idno>
		<idno>arXiv: 1806.03962</idno>
		<ptr target="http://arxiv.org/abs/1806.03962" />
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05788</idno>
		<idno>arXiv: 1803.05788</idno>
		<ptr target="http://arxiv.org/abs/1803.05788" />
		<title level="m">DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression Framework</title>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08487</idno>
		<idno>arXiv: 1904.08487</idno>
		<ptr target="http://arxiv.org/abs/1904.08487" />
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recognizable or Not: Towards Image Semantic Quality Assessment for Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensing and Imaging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s11220-016-0152-5</idno>
		<ptr target="https://doi.org/10.1007/s11220-016-0152-5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On The Classification-Distortion-Perception Tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/6c29793a140a811d0c45ce03c1c93a28-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch? Buc, E. B. Fox, and R. Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="1204" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Observer Dependent Lossy Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Renggli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-71278-5_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-71278-5_10" />
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition -42nd DAGM German Conference, DAGM GCPR 2020</title>
		<editor>Z. Akata, A. Geiger, and T. Sattler</editor>
		<meeting><address><addrLine>T?bingen, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-10-01" />
			<biblScope unit="volume">12544</biblScope>
			<biblScope unit="page" from="130" to="144" />
		</imprint>
	</monogr>
	<note>Proceedings, ser. Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Compressible Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP40778.2020.9190860</idno>
		<ptr target="https://doi.org/10.1109/ICIP40778.2020.9190860" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3349" to="3353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A Theory of Usable Information under Computational Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2020. [Online</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning Optimal Representations with the Decodable Information Bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/d8ea5f53c1b1eb087ac2e356253395d8-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>virtual, H. Larochelle, M. Ranzato, R. Hadsell, M.-F. Balcan, and H.-T. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On the Shannon theory of information transmission in the case of continuous signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conference Name: IRE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="1956-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="102" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Pinsker</surname></persName>
		</author>
		<title level="m">Information and Information Stability of Random Variables and Processes</title>
		<imprint>
			<publisher>Holden-Day, Inc</publisher>
			<date type="published" when="1964-01" />
		</imprint>
	</monogr>
	<note>first edition ed., A. Feinstein</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Information Theory and Statistical Mechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
		<idno type="DOI">https:/link.aps.org/doi/10.1103/PhysRev.106.620</idno>
		<ptr target="https://link.aps.org/doi/10.1103/PhysRev.106.620" />
	</analytic>
	<monogr>
		<title level="j">Physical Review</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="620" to="630" />
			<date type="published" when="1957-05" />
			<publisher>American Physical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Romano</surname></persName>
		</author>
		<title level="m">ser. Springer texts in statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Testing statistical hypotheses</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Mac</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Birkhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Algebra</forename></persName>
		</author>
		<idno>ID: L6FENd8GHIUC</idno>
		<imprint>
			<date type="published" when="1999" />
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Probabilistic Symmetries and Invariant Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bloem-Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno>pp. 90:1-90:61</idno>
		<ptr target="http://jmlr.org/papers/v21/19-322.html" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Minimum Excess Risk in Bayesian Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raginsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14868</idno>
		<idno>arXiv: 2012.14868</idno>
		<ptr target="http://arxiv.org/abs/2012.14868" />
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Foundations of Modern Probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kallenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">535</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Strictly Proper Scoring Rules, Prediction, and Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gneiting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<idno type="DOI">http:/www.tandfonline.com/doi/abs/10.1198/016214506000001437</idno>
		<ptr target="http://www.tandfonline.com/doi/abs/10.1198/016214506000001437" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">477</biblScope>
			<biblScope unit="page" from="359" to="378" />
			<date type="published" when="2007-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Estimating Mutual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stoegbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:cond-mat/0305641</idno>
		<ptr target="http://arxiv.org/abs/cond-mat/0305641" />
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Rate Distortion Theory: A Mathematical Basis for Data Compression, ser. Prentice-Hall electrical engineering series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<ptr target="https://books.google.ch/books?id=-HV1QgAACAAJ" />
		<imprint>
			<date type="published" when="1971" />
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of information theory</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Compression of graphical structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/5205736/" />
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Symposium on Information Theory</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="364" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learnability for the Information Bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="DOI">10.3390/e21100924</idno>
		<ptr target="https://doi.org/10.3390/e21100924" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">924</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The Conditional Entropy Bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="DOI">10.3390/e22090999</idno>
		<ptr target="https://doi.org/10.3390/e22090999" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">999</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Rational Decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Good</surname></persName>
		</author>
		<ptr target="https://www.jstor.org/stable/2984087" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="114" />
			<date type="published" when="1952" />
			<publisher>Royal Statistical Society, Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PROBABILITY</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950-01" />
			<publisher>American Meteorological Society Section</publisher>
		</imprint>
	</monogr>
	<note>Monthly Weather Review</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Measuring information and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Good</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Statistical Inference</title>
		<editor>Robert J. Buehler</editor>
		<imprint>
			<biblScope unit="page" from="337" to="339" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
	<note>Comment on</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Injective Hilbert Space Embeddings of Probability Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Scoring rules, Divergences and Information in Bayesian Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Making and Evaluating Point Forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gneiting</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0912.0902</idno>
		<idno>arXiv: 0912.0902</idno>
		<ptr target="http://arxiv.org/abs/0912.0902" />
		<imprint>
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
	<note>math, stat</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<idno type="DOI">10.1002/j.1538-7305.1948.tb00917.x</idno>
		<ptr target="https://doi.org/10.1002/j.1538-7305.1948.tb00917.x" />
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jacod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Protter</surname></persName>
		</author>
		<title level="m">Probability Essentials</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Generalized Lagrange Multiplier Method for Solving Problems of Optimum Allocation of Resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Everett</surname></persName>
		</author>
		<idno type="DOI">https:/pubsonline.informs.org/doi/abs/10.1287/opre.11.3.399</idno>
		<ptr target="https://pubsonline.informs.org/doi/abs/10.1287/opre.11.3.399" />
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="399" to="417" />
			<date type="published" when="1963-06" />
			<publisher>INFORMS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Rate-Distortion Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<idno type="DOI">https:/onlinelibrary.wiley.com/doi/abs/10.1002/0471219282.eot142</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/0471219282.eot142" />
	</analytic>
	<monogr>
		<title level="m">Wiley Encyclopedia of Telecommunications</title>
		<imprint>
			<publisher>American Cancer Society</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Estimation of Entropy and Mutual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paninski</surname></persName>
		</author>
		<idno type="DOI">https:/www.mitpressjournals.org/doi/abs/10.1162/089976603321780272</idno>
		<ptr target="https://www.mitpressjournals.org/doi/abs/10.1162/089976603321780272" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1191" to="1253" />
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Formal Limitations on the Measurement of Mutual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v108/mcallester20a.html" />
	</analytic>
	<monogr>
		<title level="m">The 23rd International Conference on Artificial Intelligence and Statistics</title>
		<editor>Research, S. Chiappa and R. Calandra</editor>
		<meeting><address><addrLine>Online [Palermo, Sicily, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-08-28" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="875" to="884" />
		</imprint>
	</monogr>
	<note>ser. Proceedings of Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Deep Variational Information Bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Universally Quantized Neural Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/92049debbe566ca5782a3045cf300a3c-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>virtual, H. Larochelle, M. Ranzato, R. Hadsell, M.-F. Balcan, and H.-T. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Compressing Images by Encoding Their Latent Representations with Relative Entropy Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Flamich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/ba053350fe56ed93e64b3e769062b680-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>virtual, H. Larochelle, M. Ranzato, R. Hadsell, M.-F. Balcan, and H.-T. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Sending Samples Without Bits-Back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<ptr target="http://joschu.net/blog/sending-samples.html" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Proceedings, ser. Lecture Notes in Computer Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-53504-7_63</idno>
		<ptr target="https://doi.org/10.1007/3-540-53504-7_63" />
	</analytic>
	<monogr>
		<title level="m">Advances in Computing and Information -ICCI&apos;90, International Conference on Computing and Information</title>
		<editor>S. G. Akl, F. Fiala, and W. W. Koczkodaj</editor>
		<meeting><address><addrLine>Niagara Falls, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990" />
			<biblScope unit="volume">468</biblScope>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
	<note>Classification by Minimum-Message-Length Inference</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v9/gutmann10a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2010-03" />
			<biblScope unit="page" from="1938" to="7228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1405" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="3698" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08010</idno>
		<idno>arXiv: 1810.08010</idno>
		<ptr target="http://arxiv.org/abs/1810.08010" />
		<title level="m">Variational Noise-Contrastive Estimation</title>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Building symmetries into feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1989 First IEE International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="1989-10" />
			<biblScope unit="page" from="158" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Representation Theory and Invariant Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1016/0166-218X(95)00075-3</idno>
		<ptr target="https://doi.org/10.1016/0166-218X(95" />
	</analytic>
	<monogr>
		<title level="j">Discret. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="75" to="78" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Invariant Scattering Convolution Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.230</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2012.230" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Group Equivariant Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/cohenc16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<editor>F. Balcan and K. Q. Weinberger</editor>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Deep Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>I. Guyon, U. v. Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/kondor18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>J. G. Dy and A. Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2752" to="2760" />
		</imprint>
	</monogr>
	<note>ser. Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">A Kernel Theory of Modern Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">A Group-Theoretic Framework for Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dobriban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10905</idno>
		<idno>arXiv: 1907.10905</idno>
		<ptr target="http://arxiv.org/abs/1907.10905" />
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">On the Benefits of Invariance in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bloem-Reddy</surname></persName>
		</author>
		<idno>_eprint: 2005.00178</idno>
		<ptr target="https://arxiv.org/abs/2005.00178" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Compression of Graphical Structures: Fundamental Limits, Algorithms, and Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/6145505/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="620" to="638" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A history of graph entropy measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mowshowitz</surname></persName>
		</author>
		<ptr target="https://linkinghub.elsevier.com/retrieve/pii/S0020025510004147" />
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="78" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Compression and Symmetry of Small-World Graphs and Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kontoyiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papakonstantinopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15981</idno>
		<idno>arXiv: 2007.15981</idno>
		<ptr target="http://arxiv.org/abs/2007.15981" />
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Symmetry-Based Scalable Lossless Compression of 3D Medical Image Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abugharbieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nasiopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conference Name: IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2009-07" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1062" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Compression of 3D MRI images based on symmetry in prediction-error field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amraee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirani</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICME.2011.6011897</idno>
		<ptr target="https://doi.org/10.1109/ICME.2011.6011897" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE International Conference on Multimedia and Expo</title>
		<meeting>the 2011 IEEE International Conference on Multimedia and Expo<address><addrLine>Barcelona, Catalonia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011-07-15" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Symmetry in 3D Geometry: Extraction and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12010</idno>
		<ptr target="https://doi.org/10.1111/cgf.12010" />
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Representation of signals by local symmetry decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gnutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guerrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 23rd European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="983" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Symmetry-Based Biomedical Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bairagi</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4636716/" />
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="718" to="726" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02731</idno>
		<idno>arXiv: 1611.02731</idno>
		<ptr target="http://arxiv.org/abs/1611.02731" />
	</analytic>
	<monogr>
		<title level="j">Variational Lossy Autoencoder</title>
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Joint Autoregressive and Hierarchical Priors for Learned Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Montr?al</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Grauman</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/53edebc543333dfbf7c5933af792c9c4-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>Cesa-Bianchi, and R. Garnett</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
			<biblScope unit="page" from="10" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Computationally Efficient Neural Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<idno>_eprint: 1912.08771</idno>
		<ptr target="http://arxiv.org/abs/1912.08771" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Improving Inference for Neural Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bamler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt ; Virtual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/066f182b787111ed4cb65ed437f0855b-Abstract.html" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Variational Bayesian Quantization</title>
		<idno>pp. 10 670-10 680</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
				<ptr target="http://proceedings.mlr.press/v119/yang20a.html" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Channel-Wise Autoregressive Entropy Models for Learned Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3339" to="3343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICIP40778.2020.9190935</idno>
		<ptr target="https://doi.org/10.1109/ICIP40778.2020.9190935" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Context-adaptive Entropy Model for End-to-end Optimized Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Beack</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyxKIiAqYQ" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Perceptually Optimizing Deep Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Norkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno>_eprint: 2007.02711</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Generative Adversarial Networks for Extreme Learned Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00031</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00031" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<idno>arXiv: 2103.03230</idno>
		<ptr target="http://arxiv.org/abs/2103.03230" />
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
	<note>cs, q-bio</note>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04906</idno>
		<idno>arXiv: 2105.04906</idno>
		<ptr target="http://arxiv.org/abs/2105.04906" />
		<imprint>
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Learning and generalization with the information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">411</biblScope>
			<biblScope unit="issue">29-30</biblScope>
			<biblScope unit="page" from="2696" to="2711" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.tcs.2010.04.006</idno>
		<ptr target="https://doi.org/10.1016/j.tcs.2010.04.006" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">The Role of Information Complexity and Randomization in Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Vega</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05355</idno>
		<idno>arXiv: 1802.05355</idno>
		<ptr target="http://arxiv.org/abs/1802.05355" />
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">An Information Theoretic Framework for Multi-view Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st Annual Conference on Learning Theory -COLT</title>
		<editor>R. A. Servedio and T. Zhang</editor>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2008-07-09" />
			<biblScope unit="page" from="403" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
				<ptr target="http://colt2008.cs.helsinki.fi/papers/94-Sridharan.pdf" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Self-supervised Learning from a Multi-view Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05576</idno>
		<idno>arXiv: 2006.05576</idno>
		<ptr target="http://arxiv.org/abs/2006.05576" />
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Representation Learning via Invariant Causal Mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=9p2ekP904Rs" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Uncertainty, Information, and Sequential Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Degroot</surname></persName>
		</author>
		<ptr target="https://projecteuclid.org/euclid.aoms/1177704567" />
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="404" to="419" />
			<date type="published" when="1962-06" />
			<publisher>Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:math/0410076</idno>
		<ptr target="http://arxiv.org/abs/math/0410076" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1367" to="1433" />
			<date type="published" when="2004-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Multiclass classification, information, divergence and surrogate risk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ruan</surname></persName>
		</author>
		<ptr target="https://projecteuclid.org/euclid.aos/1536631273" />
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6B</biblScope>
			<biblScope unit="page" from="3246" to="3275" />
			<date type="published" when="2018-12" />
			<publisher>Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">A Minimax Approach to Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Farnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tse</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/hash/7b1ce3d73b70f1a7246e7b76a35fb552-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. v. Luxburg, I. Guyon, and R. Garnett</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="4233" to="4241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Application of the Radon-Nikodym Theorem to the Theory of Sufficient Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Halmos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Savage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="241" />
			<date type="published" when="1949-06" />
			<publisher>Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title/>
		<idno type="DOI">https:/projecteuclid.org/journals/annals-of-mathematical-statistics/volume-20/issue-2/Application-of-the-Radon-Nikodym-Theorem-to-the-Theory-of/10.1214/aoms/1177730032.full</idno>
		<ptr target="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-20/issue-2/Application-of-the-Radon-Nikodym-Theorem-to-the-Theory-of/10.1214/aoms/1177730032.full" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Sufficiency and Statistical Decision Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Bahadur</surname></persName>
		</author>
		<idno type="DOI">https:/projecteuclid.org/journals/annals-of-mathematical-statistics/volume-25/issue-3/Sufficiency-and-Statistical-Decision-Functions/10.1214/aoms/1177728715.full</idno>
		<ptr target="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-25/issue-3/Sufficiency-and-Statistical-Decision-Functions/10.1214/aoms/1177728715.full" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="462" />
			<date type="published" when="1954-09" />
			<publisher>Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Adequate Subfields and Sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Skibinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="161" />
			<date type="published" when="1967-02" />
			<publisher>Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title/>
		<idno type="DOI">https:/projecteuclid.org/journals/annals-of-mathematical-statistics/volume-38/issue-1/Adequate-Subfields-and-Sufficiency/10.1214/aoms/1177699065.full</idno>
		<ptr target="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-38/issue-1/Adequate-Subfields-and-Sufficiency/10.1214/aoms/1177699065.full" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Characterizations of Prediction Sufficiency (Adequacy) in Terms of Risk Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Akahira</surname></persName>
		</author>
		<ptr target="https://www.jstor.org/stable/3035531" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1018" to="1024" />
			<date type="published" when="1975" />
			<publisher>Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<title level="m">LEARNING SUMMARY STATISTIC FOR APPROXIMATE BAYESIAN COMPUTATION VIA DEEP NEURAL NETWORK</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title/>
		<ptr target="https://www.jstor.org/stable/26384090" />
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1595" to="1618" />
			<date type="published" when="2017" />
			<publisher>Academia Sinica</publisher>
		</imprint>
		<respStmt>
			<orgName>Institute of Statistical Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Minimal Achievable Sufficient Statistic Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cvitkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koliander</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/cvitkovic19a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="2640" to="3498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Information Dropout: Learning Optimal Representations Through Noisy Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2784440</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2784440" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2897" to="2905" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7676</idno>
		<idno>arXiv: 1411.7676</idno>
		<ptr target="http://arxiv.org/abs/1411.7676" />
		<title level="m">Visual Representations: Defining Properties and Deep Approximations</title>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Minimum Rates of Approximate Sufficient Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Y F</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2017.2775612</idno>
		<ptr target="https://doi.org/10.1109/TIT.2017.2775612" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="875" to="888" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Fine Asymptotics for Universal One-to-One Compression of Parametric Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Iri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kosut</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2019.2898659</idno>
		<ptr target="https://doi.org/10.1109/TIT.2019.2898659" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2442" to="2458" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Y. Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.123" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch? Buc, E. B. Fox, and R. Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">CompressAI: a PyTorch library and evaluation platform for end-to-end compression research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>B?gaint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Racap?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feltman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pushparaja</surname></persName>
		</author>
		<idno>_eprint: 2011.03029</idno>
		<ptr target="https://arxiv.org/abs/2011.03029" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>F. R. Bach and D. M. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ser. JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
			<publisher>Ieee</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">3D Object Representations for Fine-Grained Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCVW.2013.77</idno>
		<ptr target="https://doi.org/10.1109/ICCVW.2013.77" />
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision Workshops, ICCV Workshops</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013-12-01" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Cats and Dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2005.09.012</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2005.09.012" />
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Automated Flower Classification over a Large Number of Classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICVGIP.2008.47</idno>
		<ptr target="https://doi.org/10.1109/ICVGIP.2008.47" />
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<meeting><address><addrLine>Bhubaneswar, India</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008-12-19" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Ehteshami</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Johannes Van Diest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A W M T C</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Consortium</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2017.14585</idno>
		<ptr target="https://doi.org/10.1001/jama.2017.14585" />
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2199" to="2210" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2670313" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Array programming with NumPy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kerkwijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haldane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>R?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>G?rard-Marchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gohlke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-020-2649-2</idno>
		<ptr target="https://doi.org/10.1038/s41586-020-2649-2" />
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">Handbook of Analysis and its Foundations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schechter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Supervised Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18" to="661" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
