<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Robustness and Accuracy via Relative Information Encoding in 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date>October 20-24, 2021. October 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkang</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Digital Media</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshe</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Digital Media</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Information Technology R&amp;D Innovation Center</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Shaoxing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfeng</forename><surname>Zhang</surname></persName>
							<email>xfzhang@ucas.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<email>wgao@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Digital Media</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkang</forename><surname>Shan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshe</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><forename type="middle">Gao</forename></persName>
						</author>
						<title level="a" type="main">Improving Robustness and Accuracy via Relative Information Encoding in 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th ACM International Conference on Multimedia (MM &apos;21)</title>
						<meeting>the 29th ACM International Conference on Multimedia (MM &apos;21) <address><addrLine>China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event</publisher>
							<date type="published">October 20-24, 2021. October 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475504</idno>
					<note>ACM, New York, NY, USA, 13 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Computer vision problems</term>
					<term>Supervised learning KEYWORDS 3D human pose estimation</term>
					<term>neural networks</term>
					<term>robustness</term>
					<term>accuracy * Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of the existing 3D human pose estimation approaches mainly focus on predicting 3D positional relationships between the root joint and other human joints (local motion) instead of the overall trajectory of the human body (global motion). Despite the great progress achieved by these approaches, they are not robust to global motion, and lack the ability to accurately predict local motion with a small movement range. To alleviate these two problems, we propose a relative information encoding method that yields positional and temporal enhanced representations. Firstly, we encode positional information by utilizing relative coordinates of 2D poses to enhance the consistency between the input and output distribution. The same posture with different absolute 2D positions can be mapped to a common representation. It is beneficial to resist the interference of global motion on the prediction results. Second, we encode temporal information by establishing the connection between the current pose and other poses of the same person within a period of time. More attention will be paid to the movement changes before and after the current pose, resulting in better prediction performance on local motion with a small movement range. The ablation studies validate the effectiveness of the proposed relative information encoding method. Besides, we introduce a multi-stage optimization method to the whole framework to further exploit the positional and temporal enhanced representations. Our method outperforms state-of-the-art methods on two public datasets. Code is available at https://github.com/paTRICK-swk/Pose3D-RIE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>3D human pose estimation aims to localize the positions of human joints in 3D space from a given RGB image or video. It is an active research topic in computer vision since it can be used in a broad range of domains, such as object detection <ref type="bibr" target="#b36">[37]</ref>, video surveillance <ref type="bibr" target="#b23">[24]</ref>, and augmented reality <ref type="bibr" target="#b20">[21]</ref>. 3D human pose estimation is a challenging ill-posed task due to the ambiguity that multiple 3D poses can have the same projection in 2D space. Traditional methods <ref type="bibr">[1]</ref><ref type="bibr">[2]</ref><ref type="bibr">[3]</ref> extend Pictorial Structure (PS) model to 3D pose estimation or utilize hand-crafted features to recover human poses. With the help of 3D large-scale motion capture datasets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref>, learning-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>, which utilize neural networks to regress the 3D coordinates of human joints, have achieved promising results.</p><p>Recent approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref> follow the two-step principle for 3D pose reasoning. The first step is to localize 2D human keypoints, while the second step is to predict the corresponding 3D joint locations from the results of the previous step. These approaches focus on the process of lifting 2D keypoints to a 3D skeleton. In this way, the prior information of 2D coordinates can be utilized to facilitate 3D human pose estimation. Our work falls under this category.</p><p>As illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, the motion of the human body can be divided into global and local motion. Most of the previous works <ref type="bibr">[4,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> pay attention to the prediction of local motion instead of global motion. They represent 3D human joints in the form of relative coordinates with respect to the root joint (i.e. pelvis). Despite the great success achieved by these approaches, there remain two drawbacks. First, existing methods lack robustness to global motion. They only use the absolute positions of 2D poses as inputs, resulting in a gap between the distribution of the input and output. In real cases, the camera position often shifts, which can be regarded as the global motion of 2D keypoints. This brings a serious problem that the same posture of a person with different absolute 2D positions will correspond to different outputs. In other words, prediction results are affected by the movement of the camera. Second, previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref>   a period of time to predict the 3D pose of a person in a certain frame. These works treat every pose equally, ignoring the relationships between the current pose and all the others. This indicates that the network is insensitive to small changes in local motion, resulting in poor prediction performance on poses with a small movement range.</p><p>In this paper, we propose a relative information encoding method to alleviate the aforementioned problems. We begin with a baseline called Feature Fusion Network, utilizing the human body grouping strategy <ref type="bibr" target="#b29">[30]</ref>. The relative information encoding produces positional and temporal enhanced representations as additional inputs to the baseline. First, we encode the positional information by using relative 2D coordinates with respect to the root joint at the input to ensure consistency with the output. In this way, the posturerelated information can be distilled without being disturbed by the absolute position of the human body. When the positions of 2D keypoints in the image plane are globally shifted, the results after positional information encoding is still the same. Therefore, the 3D pose prediction becomes more robust to global motion. Second, to achieve a better estimation performance on local motion with a small movement range, we encode the temporal information by explicitly propagating the influence of the current pose to other poses. The temporal information encoding can be modeled as any vector operator, such as inner-product and subtraction. The position changes in the contextual poses relative to the current pose are emphasized instead of the absolute position of each pose. In the case of local motion with a small movement range, changes between the current pose and the others will be magnified, contributing to a more accurate prediction result.</p><p>After the positional and temporal enhanced representations are yielded, the postural information in these representations should be extracted independently in each body group in Feature Fusion Network. Subsequently, prior knowledge about human body structure needs to be transferred between groups without disturbing the previous process. To this end, we propose a multi-stage optimization method for the whole framework. This method sustains the independence of the preceding two processes by optimizing them in sequence.</p><p>In summary, the contributions of this paper are threefold:</p><p>? We propose a positional information encoding method to yield a prediction result that is more robust to global motion.</p><p>? We propose a temporal information encoding method to yield a prediction result that is more accurate on local motion with a small movement range. ? To better utilize the positional and temporal information within each human body group, we introduce a multi-stage optimization method to the whole framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>There are two categories of methods for 3D human pose estimation.</p><p>Most of the early works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>, which directly regress 3D human joints from original images or videos, are known as onestep methods. Recent approaches <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>, which firstly detect 2D keypoints and then infer the corresponding 3D poses, are known as two-step methods. We briefly review the learning-based two-step approaches in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anatomical Grouping Strategy</head><p>Many methods <ref type="bibr">[4,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> leverage the internal structural constraints that exist between human joints to improve the performance of 3D pose estimation. Recently, the anatomical grouping strategy, which partitions the human body into local groups to capture the commonality of related joints, is widely exploited. Park et al. <ref type="bibr" target="#b29">[30]</ref> capture the inter-dependencies among joints within a group via a hierarchical relational network. Fang et al. <ref type="bibr" target="#b10">[11]</ref> develop a pose grammar to encode high-order relations among human body parts. Wang et al. <ref type="bibr" target="#b37">[38]</ref> divide a human body into different parts with varying levels of degrees of freedom and explicitly model the bi-directional dependencies among parts. Zheng et al. <ref type="bibr" target="#b44">[45]</ref> regard each joint as an independent group, in which the local features are extracted. Zeng et al. <ref type="bibr" target="#b42">[43]</ref> propose a split-and-recombine approach to incorporate low-dimensional global context into the local group. Similarly, we categorize the body parts into five groups and build a baseline called Feature Fusion Network. We come up with a Feature Fusion Module (FFM) to explicitly model the associations between these groups. In contrast to <ref type="bibr" target="#b42">[43]</ref>, which exchanges information in every convolution layer, our approach implements feature fusion only after the encoder networks. In this way, the processes of local feature extraction and feature fusion are separated. Through the proposed multi-stage optimization method, the encoder in each group can work independently without interference from other groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Positional Information</head><p>Many recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46]</ref> propose novel reparameterization methods for 3D human poses at the output end. Sun et al. <ref type="bibr" target="#b34">[35]</ref> use bones instead of joints to represent human poses, and exploit the joint connection structure to define a compositional loss function for training. Mehta et al. <ref type="bibr" target="#b26">[27]</ref> represent joint positions in various ways, such as positions relative to the root, or to the first order parent. Other works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref> decompose the coordinates of human joints into axis and angle in the polar coordinate system relative to the parent joints, making it easier to yield a unified bone length for a particular person. However, all of the existing approaches only focus on searching for better 3D representations, which capture the posture of the human body. Few efforts have been made to ensure the uniformity of the input and output distribution. As a result, global motion, such as the offset of the camera position, will cause performance degradation. In contrast, we propose a positional information encoding method that represents poses in the relative coordinate system with respect to the root joint both on the input and output side, eliminating the negative effects of the global motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Temporal Information</head><p>In order to maintain temporal consistency and mitigate jitter in videos, many approaches <ref type="bibr">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref> have been proposed to exploit temporal information for 3D pose inferring in video sequences. Recently, LSTM models <ref type="bibr" target="#b13">[14]</ref> are introduced to 3D human pose estimation. Hossain et al. <ref type="bibr" target="#b14">[15]</ref> propose a sequenceto-sequence model composed of LSTM unit to estimate the 3D pose sequences. Xu et al. <ref type="bibr" target="#b41">[42]</ref> propose to refine the 3D trajectories using a bi-directional LSTM network. Lin et al. <ref type="bibr" target="#b21">[22]</ref> process all input frames concurrently, and utilize matrix factorization to deal with the drift problem of LSTM models. Besides, Pavllo et al. <ref type="bibr" target="#b31">[32]</ref> propose a Temporal Convolutional Network (TCN) that performs 1D convolutions over time sequences. A sequence of 2D poses is utilized to jointly estimate the current 3D pose. Compared with the methods using LSTMs, TCN can achieve higher accuracy with fewer parameters. However, all of the 2D poses are treated equally by the temporal 1D convolution used by TCN. The discrepancy between the current pose and other poses is overlooked. Although Liu et al. <ref type="bibr" target="#b24">[25]</ref> apply the attention mechanism to TCN, only tensors in the middle layers are weighted, and the importance of the current pose is still not emphasized explicitly. The relationships of the current pose and the others are implicitly modeled, which increases the burden on the network. As a result, small differences between poses are less likely to be captured by the network, resulting in bad prediction results. Unlike these works, we explicitly encode the temporal information by building a direct correlation between the current frame and the others. This method boosts the estimation performance on local motion with a small movement range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>An overview of our framework is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. It takes a sequence of 2D keypoints as input and yields a single 3D pose in the current frame as output. To take advantage of the structural information of the human body, we build a baseline called Feature Fusion Network based on an auto-encoder paradigm. We assign spatially associated joints into the same group, in which shared features can be learned to model the commonality of the grouped joints. Given the fact that predicting joint positions in each group independently may produce physically impossible body movements, we propose a Feature Fusion Module (FFM), similar to <ref type="bibr" target="#b42">[43]</ref>, to depict the dependencies between groups. This module is beneficial for the establishment of structural associations between different groups, and ensure that the generated 3D poses are reasonable. To mitigate the problem of lacking robustness to global motion and inaccurate prediction results of local motion, we propose a relative information encoding method. We utilize this method to yield positional and temporal enhanced representations (K , K ), which serve as additional inputs to the baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline</head><p>To incorporate the relative information encoding into 3D human pose estimation, we propose a Feature Fusion Network as the baseline in view of the physiological structure of the human body. For each pose, we adopt the grouping strategy used in <ref type="bibr" target="#b29">[30]</ref>. We denote a sequence of 2D vectorized poses in image space as K = k , k = (x , y ) ? R 2 , and the corresponding 3D poses in the absolute</p><formula xml:id="formula_0">coordinate system as P = p , p = (X , Y , Z ) ? R 3 , where = 1, 2, . . . , , = 1, 2, . . . ,</formula><p>. is the number of human joints, and is the length of the sequence. For each pose, all human joints are divided into = 5 groups, including torso, left arm, right arm, left leg, and right leg. The 2D human joints in each group can be</p><formula xml:id="formula_1">expressed as K = k =1</formula><p>, where is the number of joints in group .</p><p>Local features are extracted from grouped 2D keypoints by the local feature encoder, which uses TCN <ref type="bibr" target="#b31">[32]</ref> as the backbone. This process can be expressed as:</p><formula xml:id="formula_2">F = E (K , )<label>(1)</label></formula><p>where E (?, ) stands for the local feature encoder in group . With the intention of enabling the framework to comprehend the global coherence of all human joints that is not interfered by other frames, the global features are extracted from the current pose</p><formula xml:id="formula_3">K = k 2 =1</formula><p>. This process can be expressed as:</p><formula xml:id="formula_4">F = E (K , )<label>(2)</label></formula><p>where E (?, ) stands for the global feature encoder. Although the spatially meaningful patterns are preserved in each group, the connection between different groups is excluded. When inferring 3D poses, the joint positions of other groups are completely unknown to the current group, which is not good for maintaining the consistency of the overall body posture. Ideally, the networks are supposed to account for the continuity of the joints between groups. This motivates us to propose a Feature Fusion Module (FFM) that transfers the information of other groups to the current group. This module can work effectively in combination with the proposed multi-stage optimization method, which will be discussed in Section 3.4. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates the structure of the FFM. The fused features are obtained by a fusion block:</p><formula xml:id="formula_5">F = G (M , )<label>(3)</label></formula><p>where M = F | = 1, 2, . . . , , ? , is the number of groups, and G (?, ) stands for the fusion block. Subsequently, all of the features are concatenated and sent to the decoder to estimate the 3D pose:P</p><formula xml:id="formula_6">= D (F ? F ? F , )<label>(4)</label></formula><p>where ? is the concatenation operator, and D (?, ) stands for the decoder. It should be noticed that the global feature encoder, decoders, and fusion block in the FFM share the same network structure that is proposed by <ref type="bibr" target="#b25">[26]</ref>. It consists of two fully connected layers as well as a skip connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Positional Information Encoding</head><p>To yield 3D poses that are less sensitive to global motion, we propose a positional information encoding method. Most of the previous approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref> estimate 3D joint positions with respect to the root joint. They eliminate the 3D global trajectory for restricting the networks to focus on estimating the posture of the human body. However, none of them consider the uniformity of the input and output forms. At a certain time , the goal of the previous networks can be formulated as a function:</p><formula xml:id="formula_7">F : (x , y ) ? (X ? X 0 , Y ? Y 0 , Z ? Z 0 ) (5) where = 1, 2, . . . , . (X 0 , Y 0 , Z 0 )</formula><p>represent the 3D coordinates of the root joint (i.e. pelvis). is omitted for simplicity. There is inconsistency in the distribution of the input and output, which will deteriorate the robustness to global motion. For example, when a hand-held camera is capturing a person with the same posture, the camera is prone to be shifted. The overall 2D positions of the person in the videos taken by the camera, before and after the shift, are different, while the relative 2D positions of the corresponding joints stay the same. The overall shift of the 2D coordinates can be regarded as the global motion in this situation, thus equation <ref type="formula">(5)</ref> can be reformulated as: where</p><formula xml:id="formula_8">F : K ? P<label>(6)</label></formula><formula xml:id="formula_9">K = (x + ?x, y + ?y) (7) P = { (X + ?X ? (X 0 + ?X), Y + ?Y ? (Y 0 + ?Y), Z + ?Z ? (Z 0 + ?Z)) } = { (X ? X 0 , Y ? Y 0 , Z ? Z 0 ) }<label>(8)</label></formula><p>The global motion affects the input, but the output is still the same. In addition to inferring the 3D posture of the human body, the network is supposed to learn a mapping function from multiple inputs to the same output. Namely, the network needs to complete the conversion from the absolute 2D coordinates in image space to the relative coordinates in 3D space. When the network fails to learn this mapping relationship, a person with the same posture shot by the camera before and after the shift will correspond to different 3D pose estimation results, which is undesirable.</p><p>To address this issue, we encode the positional information in the input side by subtracting the coordinates of the root joint from the positions of all other joints. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>(a), the positional information encoding can be expressed as:</p><formula xml:id="formula_10">K = k ? k 0 =1 (9)</formula><p>is omitted for simplicity. Then the goal of the network can be rewritten as:</p><formula xml:id="formula_11">F : (x ? x 0 , y ? y 0 ) ? (X ? X 0 , Y ? Y 0 , Z ? Z 0 )<label>(10)</label></formula><p>where (x 0 , y 0 ) represent the 2D coordinates of the root joint. When the global motion occurs, equation (10) can be reformulated as:</p><formula xml:id="formula_12">F : K ? P<label>(11)</label></formula><p>where</p><formula xml:id="formula_13">K = { (x + ?x ? (x 0 + ?x), y + ?y ? (y 0 + ?y)) } = { (x ? x 0 , y ? y 0 ) }<label>(12)</label></formula><p>and P remains the same. Hence, F = F . In this way, the same posture with different absolute 2D coordinates will correspond to a common positional enhanced representation, which reduces the difficulty for the network to yield the same prediction result. The positional information encoding forces the network to only capture meaningful information related to the human posture instead of 2D global trajectory. This is conducive for the network to become more robust to global motion. Besides, the 2D global trajectory cannot be discarded as it is beneficial to 3D pose inferring. Its necessity is assessed in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Temporal Information Encoding</head><p>In addition to enhancing the positional information, we propose a temporal information encoding method to alleviate the issue of bad performance on local motion with a small movement range. Previous methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref> use Temporal Convolutional Network (TCN) that takes a sequence of 2D poses as input and produces a single 3D pose. <ref type="figure" target="#fig_5">Figure 5</ref> shows two shortcomings of TCN: 1) Although several 2D poses within a short period of time are fed into the TCN, only a single pose in the middle of the sequence is supposed to be estimated. All other poses play an auxiliary role in maintaining the continuity of the current pose in the time domain. Therefore, they are less important than the current pose. TCN treats all poses equally, not knowing which pose needs to be lifted to 3D space. The relative positional relationships between the current pose and the others are not explicitly emphasized. 2) According to the attribute of convolution operation, the shallow layers in neural networks have a small receptive field, and only aggregate information in a local region, while deep layers have a larger receptive field. This indicates that poses far away from the current pose cannot obtain any information related to the current pose till deeper layers are reached. These two shortcomings will cause that the network doesn't pay enough attention to the changes occurring around the current pose. When the movement range of poses is relatively small, these changes are difficult to be captured by the network, resulting in bad prediction performance on small movements. As we use TCN as the backbone of the local feature encoders, we propose a temporal information encoding method to overcome these shortcomings. We produce the temporal enhanced representation to model the relationships between the current pose and other poses. This method drives the network to learn the impact of the current pose on other poses. It allows all poses to start focusing on the temporal correlation with the current pose, whether they are far from it or near to it in the time domain, from the shallow layers of the network. In other words, the network concentrates on the position changes around the current pose rather than the absolute position of each pose. When the local motion with a small movement range occurs, these changes will be magnified, which is helpful for fine-grained modeling of 3D poses. As shown in <ref type="figure" target="#fig_4">Figure 4(b)</ref>, the temporal information encoding can be formulated as follows.</p><formula xml:id="formula_14">K = k ? k 2 =1<label>(13)</label></formula><p>where ? can be any vector operator, such as inner-product, crossproduct, cosine similarity, and subtraction. is omitted for simplicity. Experiments are conducted to evaluate the effectiveness of different operators in the supplementary material.</p><p>The positional and temporal enhanced representations produced by the proposed relative information encoding method are utilized as additional inputs to the Feature Fusion Network. Thus, equation (1) can be reformulated as:</p><formula xml:id="formula_15">F = E (K , K , K , )<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-Stage Optimization</head><p>We propose a multi-stage optimization method to fully exploit the positional and temporal information in each group for the following consideration. The information exchange should be implemented between groups selectively and comprehendingly under the condition of preserving the independence of the encoding process.</p><p>In <ref type="bibr" target="#b42">[43]</ref>, the processes of capturing the internal commonality in each group and exchanging useful information between groups are blended together. This will cause that the features in each group are interfered with by other groups. Therefore, we separate the preceding two processes in the Feature Fusion Network and apply the proposed multi-stage optimization method to our framework. We retain the independence of the local feature encoding and feature fusion through this method, thus clarifying the role of these two processes.</p><p>Feature Fusion Network can be divided into three parts: encoders, decoders, and FFM. In the first stage of optimization, We temporarily delete the FFM so that the local feature encoders can extract regional features of related joints independently. At the same time, the global feature encoder accounts for the overall coherence of the human body. The local and global features are concatenated and sent to the decoders, which ensure the integrity of the framework for end-to-end training. The parameters of the local and global feature encoders are preserved for the second stage of optimization. In the second stage, only the FFM and decoders participate in optimization. The parameters of the encoders are loaded and fixed, while the parameters of the decoders in the first stage are abandoned. The decoders are retrained in this stage. In this way, the FFM sufficiently comprehends the inter-group dependencies  without disturbing the optimization process of the encoders. In the third stage, the entire framework is finetuned to make the encoders work with the decoders and FFM more effectively. With the help of the proposed multi-stage optimization method, posture-related information in positional and temporal representations can be extracted independently in each group, and positive interaction is formed between groups to facilitate 3D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets and Evaluation Protocols</head><p>We train and evaluate our method on two publicly available datasets: Human3.6M <ref type="bibr" target="#b15">[16]</ref> and HumanEva-I <ref type="bibr" target="#b33">[34]</ref>.</p><p>Human3.6M is one of the largest indoor datasets. It contains 3.6 million different human poses for 11 subjects from four synchronized cameras, organized into 15 scenarios. Following the previous approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>, we use five subjects for training (S1, S5, S6, S7, S8) and two subjects for testing (S9 and S11).</p><p>HumanEva-I is a smaller dataset compared to Human3.6M. It consists of three subjects with six actions. In the same manner as <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>, we conduct the training and evaluating process by subject on two actions: Walk and Jog.</p><p>Two commonly used protocols are utilized for evaluation in our experiments. Protocol#1 refers to Mean Per Joint Position Error  (MPJPE) that measures the average Euclidean distance between the estimated joint positions and the ground truth positions. Protocol#2, which is denoted as P-MPJPE, refers to the error after the predicted pose is aligned to the ground truth using Procrustes analysis <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>In order to verify the effectiveness of the proposed framework, we analyze in detail the role played by each module in this section. The ablation studies are conducted on Human3.6M dataset under Protocol#1. Our framework is trained based on the ground truth of 2D poses. We use subtraction as the temporal information encoding operator. <ref type="table" target="#tab_0">Table 1</ref> reports the performance improvement brought by all components. We can observe that the positional and temporal enhanced representations reduce the error by 0.7mm and 1.0mm respectively. These two representations result in a total performance gain of 1.8mm. Besides, it can be seen that the multi-stage optimization method brings performance improvement to the entire framework as it solves the problem of negative interaction between the encoders and the FFM.</p><p>Positional information encoding. To validate the effectiveness of the positional information encoding, we carry out experiments based on the same posture with different absolute 2D positions. We manually add a random global offset to the input poses to simulate the global motion caused by camera movement. The absolute coordinates of 2D keypoints change, but the relative 2D coordinates with respect to the root joint remain fixed. In this case, multiple 2D inputs correspond to the same 3D pose.</p><p>Since the input coordinates are normalized to lie between -1 and 1, we need to make sure that the shifted input will not go outside this range either. We randomly select an offset vector (?x, ?y), where ?x, ?y ? (? , ). is set to 0.2 empirically. The shifted input can be expressed as equation <ref type="formula">(7)</ref>, where x + ?x, y + ?y ? (?1, 1). We denote different 3D poses as follows.</p><p>? P : the 3D pose predicted from the original input.</p><p>? P : the 3D pose predicted from the shifted input.</p><p>? P : the ground truth of the 3D pose. MPJPE is computed between: P and P , P and P , P and P . The purpose of computing MPJPE between P and P is to examine the testing error of P . The purpose of computing MPJPE between P and P is to examine the consistency of the output results under the circumstance of global motion caused by simulated camera movement. <ref type="figure" target="#fig_6">Figure 6</ref> shows the results of the baseline and baseline with the positional enhanced representation (baseline+P-ENH) on six randomly selected offset vectors. The magnitude of the offset vector refers to ?? (?x) 2 + (?y) 2 . The MPJPE between P and P of the baseline and baseline+P-ENH is 34.1mm and 33.4mm respectively. The red and green lines show the MPJPE between P and P . It can be seen that the baseline+P-ENH is superior to the baseline when the input is shifted. As the magnitude of the offset vector becomes larger, the accuracy of the 3D pose estimation drops significantly and the advantage of baseline+P-ENH is more prominent. The yellow and cyan lines show the MPJPE between P and P . It is shown that the baseline+P-ENH achieves better consistency between P and P than the baseline. As the magnitude of the offset vector becomes larger, the consistency gap between these two methods is more noticeable. When the magnitude of the offset vector is 0.375, the baseline+P-ENH achieves 17.1mm (about 15.3%) and <ref type="bibr" target="#b16">17</ref>.5mm (about 17.6%) improvement in MPJPE between P and P as well as P and P respectively against the baseline. In short, the baseline with the positional enhanced representation is more robust to global motion than the baseline.</p><p>Temporal information encoding. We examine the impact of the temporal information encoding by carrying out experiments on local motion with different movement ranges. As the proposed framework takes a sequence of poses as input, we utilize the Mean Square Error (MSE) between the current pose and other poses in the same sequence to define the Movement Range (MR):</p><formula xml:id="formula_16">MR = 1 1 ?? =1 ?? =1 k ? k 2 2<label>(15)</label></formula><p>where k ? k 2 2 is the Euclidean distance between k and k 2 . A large mean of the differences between the current pose and the others corresponds to a large MR of this sequence, and vice versa. We divide the poses in the testing set into tens subsets, which are arranged in ascending order according to their MR. <ref type="figure" target="#fig_7">Figure 7</ref> shows the results of the baseline and baseline with the temporal enhanced representation (baseline+T-ENH) on these subsets. It is shown that poses with a small movement range (left side) are harder to predict than poses with a large movement range (right side). The reason is that there is less information available for the networks to utilize in the time domain. The changes of the absolute 2D coordinates of poses with a large movement range are significant, so the baseline can easily learn the relationships between the current pose and the others. However, the absolute 2D coordinates of poses with a small movement range are basically unchanged within the input sequence, resulting in a lot of redundant information sent to the network. It is difficult for the baseline to concentrate on the subtle changes between different poses in this case. In contrast, the temporal enhanced representation can magnify the differences in poses with a small movement range and improve the performance of the network to a greater extent. Compared to the baseline, the baseline+T-ENH decreases the MPJPE by 1.6mm in the subset with the smallest movement range and 0.5mm in the subset with the largest movement range. This demonstrates the effectiveness of the temporal information encoding on local motion with a small movement range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Art Methods</head><p>Results on Human3.6M. <ref type="table" target="#tab_1">Table 2-3</ref> show the comparison results of the proposed method and the previous approaches on Human3.6M dataset under two protocols. Our method is compatible with any 2D keypoint detector. Specifically, we use the results of Cascaded Pyramid Network (CPN) <ref type="bibr">[6]</ref> as the inputs to the proposed framework. Compared to the state-of-the-art method <ref type="bibr" target="#b42">[43]</ref>, our model achieves promising results with 44.3mm in MPJPE and 35.0mm in P-MPJPE. We also train our model on the ground truth of 2D poses. Our model achieves 30.1mm in MPJPE and improves the lower bound of <ref type="bibr" target="#b42">[43]</ref> by about 5.9%. The qualitative results are provided in the supplementary material.</p><p>Results on HumanEva-I. As shown in <ref type="table" target="#tab_3">Table 4</ref>, we evaluate our method in terms of Protocol#2 on HumanEva-I dataset. We use Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> as the 2D detector. Besides, we choose = 27 in this experiment. Our model achieves 13.6mm in P-MPJPE, outperforming the previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we introduce a relative information encoding method that provides explicit and strong priors of the positional relationships within all human joints as well as the temporal relationships between poses in a sequence. The proposed method facilitates plausible 3D pose estimation on local motion with a small movement range and yields a more robust result to global motion. Additionally, a multi-stage optimization method is proposed for the whole framework. Experiments show that our method outperforms stateof-the-art performance on Human3.6M and HumanEva-I datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>1 QUALITATIVE RESULTS <ref type="figure" target="#fig_1">Figure 1</ref> shows the results of the baseline and baseline+P-ENH on global motion caused by simulated camera movement. When the input is shifted, the baseline+P-ENH produces a result closer to the 3D pose estimated from the original input than the baseline in the foot region. <ref type="figure" target="#fig_2">Figure 2</ref> shows the results of the baseline and baseline+T-ENH on local motion with a small movement range. It can be seen that the baseline+T-ENH produces a more accurate result than the baseline in the hand region. <ref type="figure" target="#fig_3">Figure 3</ref> shows the results of our method and the approach carried out by Liu et al. <ref type="bibr">[1]</ref>. The magnitude of the offset vector is 0.282. Compared to Liu's approach, our method obtains better consistency between P and P by means of positional information encoding. Besides, our method achieves a higher prediction accuracy on small movements using temporal information encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IMPLEMENTATION DETAILS</head><p>The proposed method is implemented using PyTorch <ref type="bibr">[3]</ref>. For training and testing, we set the length of the pose sequence = 243, which is the same as <ref type="bibr">[4]</ref>. We use the proposed multi-stage optimization method to separate the optimization process of the encoders and FFM to ensure that the feature extraction is not affected. In the first stage, Feature Fusion Network without the FFM is trained for 80 epochs with the initial learning rate set to 1 ?3 . In the second stage, the parameters of the encoders are loaded from the first stage and fixed. Only the FFM and the decoders are trained for another 80 epochs with the initial learning rate set to 1 ?3 . In the third stage, the whole framework is finetuned for 20 epochs with the initial learning rate set to 5 ?4 . The learning rate decreases by 5% after each epoch for all stages. Leaky ReLU <ref type="bibr">[6]</ref> is adopted as the activation function. We adopt 1024 as the batch size. The channel dimension and dropout rate of the TCN are set to 512 and 0.2, and those of the global feature encoder, decoders, and fusion block in the FFM are set to 1024 and 0.25 respectively. AdamW <ref type="bibr">[2]</ref> is used as the optimizer. The dimension of the local, global, and fused features is set to 512. All experiments were conducted on a single Nvidia GeForce GTX 3080 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYSIS ON ABSOLUTE 2D POSITIONS</head><p>We conduct experiments to analyze the effect of absolute 2D positions on the input side. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. It is found that the absolute 2D positions of joints are necessary to the inference of 3D poses. In the case of positional information encoding, discarding absolute 2D positions will result in the loss of some visual priors, for example, the lower part of the image usually corresponds to the area closer to the camera, and the upper part corresponds to the area farther away. The lack of these priors will affect the depth prediction of each joint, and lead to a sharp performance drop. In the case of temporal information encoding, only retaining the temporal enhanced representation will cause the input of the current pose to be 0. Therefore, we utilize the absolute 2D position in the current frame as well as the temporal enhanced   representation in other frames as inputs. However, this will lead to another problem that the object of the convolution operation includes both the absolute 2D position and temporal enhanced representation. The scale of these two is different, which brings a burden to the learning process of the network and deteriorates the prediction performance. In summary, the absolute 2D position is indispensable, so we use the original input together with the positional and temporal enhanced representations as the inputs to the Feature Fusion Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CHOICES FOR TEMPORAL INFORMATION ENCODING OPERATOR</head><p>We evaluate the performance of different temporal information encoding operators in <ref type="table" target="#tab_1">Table 2</ref>. The notations are defined as:</p><p>? CP: cross-product.</p><p>? IP: inner-product.</p><p>? CS: cosine similarity.</p><p>? SUB: subtraction.</p><p>? SUB+SUB_S: the geometric descriptor used in <ref type="bibr">[7]</ref>. It consists of the differences between the current pose and the others as well as the quadratic deformation of the differences. ? SUB(81f): only use the differences in 81 frames around the current pose (243 frames in total). It is shown that subtraction is a more appropriate operator than cross-product, inner-product, and cosine similarity. The long-term dependencies will not have a negative impact on the prediction of the current pose as the performance of SUB is better than SUB(81f). Additional information, like the quadratic deformation of the difference, is not needed. Therefore, we choose subtraction as the temporal information encoding operator in our paper.</p><p>The intuition why subtraction is a relatively good choice is as follows. Since 3D pose estimation from 2D poses is an ill-posed problem due to the inherent ambiguity in depth, perceptual evidence needs to be added to assist the estimation of depth. Previous work <ref type="bibr">[5]</ref> shows that motion parallax provides a reliable cue for monocular depth perception. The differences between the current pose and the others in the time domain play a similar role as the motion parallax. They are beneficial to estimate the depth of each joint under monocular conditions, and help the networks converge to a better result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION ON THE EFFECT OF DIFFERENT COMPONENTS ON THE PERFORMANCE</head><p>As analyzed in the ablation studies, P-ENH, T-ENH, and M-S OPT reduce the error by 0.7mm, 1.0mm, and 2.2mm respectively. The reason why the M-S OPT method has the strongest effect on the performance is as follows. M-S OPT is designed for the overall framework, so it brings a relatively large performance improvement. P-ENH and T-ENH are proposed to alleviate two different problems in specific scenarios. The performance improvement brought by P-ENH is secondary because it mainly focuses on improving the robustness to global motion. T-ENH focuses on improving prediction accuracy on small movements but has a limited effect on large movements. The test set includes both large and small movements, so the reduction in the mean error is not as significant as that of the experiments conducted on small movements. Besides, we analyze the effect of the M-S OPT method on P/T-ENH. As shown in <ref type="table" target="#tab_2">Table 3</ref>, we apply this method to the baseline, baseline+P-ENH, baseline+T-ENH, and baseline+P-ENH+P-ENH. The M-S OPT method improves upon the baseline by 0.8mm, while improves upon the baseline+P-ENH and baseline+T-ENH by 0.9mm and 1.3mm. This shows the effectiveness of M-S OPT in exploiting the positional and temporal information.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>produce inaccurate prediction results on local motion. They use 2D coordinates within arXiv:2107.13994v1 [cs.CV] 29 Jul 2021 Local motion Global motion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the global and local motion. Global motion is the overall offset of all human joints. Local motion is the movement of each joint with respect to the root joint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An overview of our framework. The positional and temporal information is enhanced by the proposed relative information encoding method. Then, the enhanced representations, together with the original input, are fed into the Feature Fusion Network in which human joints are partitioned into groups. The local features within a group are captured by the local feature encoder. Additionally, the global features are extracted from the current pose. All features are fused by the Feature Fusion Module and sent to the decoders to yield the final 3D pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the Feature Fusion Module. Local features in the current group, fused features produced by the fusion block, and global features produced by the global feature encoder are concatenated and fed into the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the relative information encoding method, including (a) positional information encoding and (b) temporal information encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Shortcomings of TCN. Green rectangle: TCN treats all poses equally, not knowing which one needs to be lifted to 3D space. Blue rectangle: the convolution operation in shallow layers only aggregates information in a local region that does not include the current pose (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Testing error of P (red and green) as well as the similarity measurement between P and P (yellow and cyan) in the case of global motion caused by simulated camera movement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Testing error of the baseline and the baseline+T-ENH in ten subsets with different movement ranges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 :</head><label>1</label><figDesc>Qualitative results of the baseline and baseline+P-ENH on global motion caused by simulated camera movement. Both the original and shifted input are drawn in the raw image. Top: baseline. Bottom: baseline+P-ENH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Qualitative results of the baseline and baseline+T-ENH on local motion with a small movement range. Comparison between our method and Liu's approach[1]. Top: the case of local motion with a small movement range. Bottom: the case of global motion caused by simulated camera movement. The original and shifted input are drawn in the raw image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation analysis of the proposed framework when different components are applied. P-ENH and T-ENH refer to positional and temporal enhanced representations respectively. M-S OPT: multi-stage optimization.</figDesc><table><row><cell>Method</cell><cell cols="2">MPJPE(mm) ?</cell></row><row><cell>Baseline</cell><cell>34.1</cell><cell>-</cell></row><row><cell>+P-ENH</cell><cell>33.4</cell><cell>0.7</cell></row><row><cell>+T-ENH</cell><cell>33.1</cell><cell>1.0</cell></row><row><cell>+P-ENH+T-ENH</cell><cell>32.3</cell><cell>1.8</cell></row><row><cell>+P-ENH+T-ENH(M-S OPT)</cell><cell>30.1</cell><cell>4.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on Human3.6M in millimeter under Protocol#1 (MPJPE). Top table: 2D poses obtained by CPN are used as inputs. Bottom table: the ground truth of 2D poses are used as inputs. The best result is shown in bold, and the second-best result is underlined.</figDesc><table><row><cell>Method</cell><cell cols="16">Dir. Disc. Eat Greet Phone Photo Pose Pur. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg</cell></row><row><cell cols="5">Martinez et al. [26] ICCV'17 51.8 56.2 58.1 59.0</cell><cell>69.5</cell><cell>78.4</cell><cell cols="4">55.2 58.1 74.0 94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell cols="5">Pavlakos et al. [31] CVPR'17 48.5 54.4 54.4 52.0</cell><cell>59.4</cell><cell>65.3</cell><cell cols="4">49.9 52.9 65.8 71.1</cell><cell>56.6</cell><cell>52.9</cell><cell>60.9</cell><cell>44.7</cell><cell>47.8</cell><cell>56.2</cell></row><row><cell>Fang et al. [11] AAAI'18</cell><cell cols="4">50.1 54.3 57.0 57.1</cell><cell>66.6</cell><cell>73.3</cell><cell cols="4">53.4 55.7 72.8 88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell cols="5">Hossain et al. [15] ECCV'18 48.4 50.7 57.2 55.2</cell><cell>63.1</cell><cell>72.6</cell><cell cols="4">53.0 51.7 66.1 80.9</cell><cell>59.0</cell><cell>57.3</cell><cell>62.4</cell><cell>46.6</cell><cell>49.6</cell><cell>58.3</cell></row><row><cell>Lee et al. [18] ECCV'18</cell><cell cols="4">40.2 49.2 47.8 52.6</cell><cell>50.1</cell><cell>75.0</cell><cell cols="4">50.2 43.0 55.8 73.9</cell><cell>54.1</cell><cell>55.6</cell><cell>58.2</cell><cell>43.3</cell><cell>43.3</cell><cell>52.8</cell></row><row><cell>Pavllo et al. [32] CVPR'19</cell><cell cols="4">45.2 46.7 43.3 45.6</cell><cell>48.1</cell><cell>55.1</cell><cell cols="4">44.6 44.3 57.3 65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell>Lin et al. [22] BMVC'19</cell><cell cols="4">42.5 44.8 42.6 44.2</cell><cell>48.5</cell><cell>57.1</cell><cell cols="4">42.6 41.4 56.5 64.5</cell><cell>47.4</cell><cell>43.0</cell><cell>48.1</cell><cell>33.0</cell><cell>35.1</cell><cell>46.6</cell></row><row><cell>Xu et al. [42] CVPR'20</cell><cell cols="4">37.4 43.5 42.7 42.7</cell><cell>46.6</cell><cell cols="5">59.7 41.3 45.1 52.7 60.2</cell><cell>45.8</cell><cell>43.1</cell><cell>47.7</cell><cell>33.7</cell><cell>37.1</cell><cell>45.6</cell></row><row><cell>Liu et al. [25] CVPR'20</cell><cell cols="4">41.8 44.8 41.1 44.9</cell><cell>47.4</cell><cell>54.1</cell><cell cols="4">43.4 42.2 56.2 63.6</cell><cell>45.3</cell><cell>43.5</cell><cell>45.3</cell><cell>31.3</cell><cell>32.2</cell><cell>45.1</cell></row><row><cell>Zeng et al. [43] ECCV'20</cell><cell cols="4">46.6 47.1 43.9 41.6</cell><cell>45.8</cell><cell cols="5">49.6 46.5 40.0 53.4 61.1</cell><cell>46.1</cell><cell>42.6</cell><cell>43.1</cell><cell>31.5</cell><cell>32.6</cell><cell>44.8</cell></row><row><cell>Ours ( = 243 CPN)</cell><cell cols="4">40.8 44.5 41.4 42.7</cell><cell>46.3</cell><cell>55.6</cell><cell cols="4">41.8 41.9 53.7 60.8</cell><cell>45.0</cell><cell>41.5</cell><cell>44.8</cell><cell>30.8</cell><cell>31.9</cell><cell>44.3</cell></row><row><cell cols="5">Martinezet al. [26] ICCV'17 37.7 44.4 40.3 42.1</cell><cell>48.2</cell><cell>54.9</cell><cell cols="4">44.4 42.1 54.6 58.0</cell><cell>45.1</cell><cell>46.4</cell><cell>47.6</cell><cell>36.4</cell><cell>40.4</cell><cell>45.5</cell></row><row><cell cols="5">Hossain et al. [15] ECCV'18 35.2 40.8 37.2 37.4</cell><cell>43.2</cell><cell>44.0</cell><cell cols="4">38.9 35.6 42.3 44.6</cell><cell>39.7</cell><cell>39.7</cell><cell>40.2</cell><cell>32.8</cell><cell>35.5</cell><cell>39.2</cell></row><row><cell>Lee et al. [18] ECCV'18</cell><cell cols="4">32.1 36.6 34.4 37.8</cell><cell>44.5</cell><cell>49.9</cell><cell cols="4">40.9 36.2 44.1 45.6</cell><cell>35.3</cell><cell>35.9</cell><cell>37.6</cell><cell>30.3</cell><cell>35.5</cell><cell>38.4</cell></row><row><cell>Pavllo et al. [32] CVPR'19</cell><cell cols="4">35.2 40.2 32.7 35.7</cell><cell>38.2</cell><cell>45.5</cell><cell cols="4">40.6 36.1 48.8 47.3</cell><cell>37.8</cell><cell>39.7</cell><cell>38.7</cell><cell>27.8</cell><cell>29.5</cell><cell>37.8</cell></row><row><cell>Lin et al. [22] BMVC'19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.8</cell></row><row><cell>Liu et al. [25] CVPR'20</cell><cell cols="4">34.5 37.1 33.6 34.2</cell><cell>32.9</cell><cell>37.1</cell><cell cols="4">39.6 35.8 40.7 41.4</cell><cell>33.0</cell><cell>33.8</cell><cell>33.0</cell><cell>26.6</cell><cell>26.9</cell><cell>34.7</cell></row><row><cell>Zeng et al. [43] ECCV'20</cell><cell cols="4">34.8 32.1 28.5 30.7</cell><cell>31.4</cell><cell>36.9</cell><cell cols="4">35.6 30.5 38.9 40.5</cell><cell>32.5</cell><cell>31.0</cell><cell>29.9</cell><cell>22.5</cell><cell>24.5</cell><cell>32.0</cell></row><row><cell>Ours ( = 243 GT)</cell><cell cols="4">29.5 30.8 28.8 29.1</cell><cell>30.7</cell><cell cols="5">35.2 31.7 27.8 34.5 36.0</cell><cell>30.3</cell><cell>29.4</cell><cell>28.9</cell><cell>24.1</cell><cell>24.7</cell><cell>30.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on Human3.6M after rigid alignment in millimeter under Protocol#2 (P-MPJPE).</figDesc><table><row><cell></cell><cell>5 43.2 46.4 47.0</cell><cell>51.0</cell><cell>56.0</cell><cell>41.4 40.6 56.5 69.4</cell><cell>49.2</cell><cell>45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell cols="2">Pavlakos et al. [31] CVPR'17 34.7 39.8 41.8 38.6</cell><cell>42.5</cell><cell>47.5</cell><cell>38.0 36.6 50.7 56.8</cell><cell>42.6</cell><cell>39.6</cell><cell>43.9</cell><cell>32.1</cell><cell>36.5</cell><cell>41.8</cell></row><row><cell>Fang et al. [11] AAAI'18</cell><cell>38.2 41.7 43.7 44.9</cell><cell>48.5</cell><cell>55.3</cell><cell>40.2 38.2 54.5 64.4</cell><cell>47.2</cell><cell>44.3</cell><cell>47.3</cell><cell>36.7</cell><cell>41.7</cell><cell>45.7</cell></row><row><cell cols="2">Hossain et al. [15] ECCV'18 35.7 39.3 44.6 43.0</cell><cell>47.2</cell><cell>54.0</cell><cell>38.3 37.5 51.6 61.3</cell><cell>46.5</cell><cell>41.4</cell><cell>47.3</cell><cell>34.2</cell><cell>39.4</cell><cell>44.1</cell></row><row><cell>Pavllo et al. [32] CVPR'19</cell><cell>34.1 36.1 34.4 37.2</cell><cell>36.4</cell><cell>42.2</cell><cell>34.4 33.6 45.0 52.5</cell><cell>37.4</cell><cell>33.8</cell><cell>37.8</cell><cell>25.6</cell><cell>27.3</cell><cell>36.5</cell></row><row><cell>Xu et al. [42] CVPR'20</cell><cell>31.0 34.8 34.7 34.4</cell><cell>36.2</cell><cell cols="2">43.9 31.6 33.5 42.3 49.0</cell><cell>37.1</cell><cell>33.0</cell><cell>39.1</cell><cell>26.9</cell><cell>31.9</cell><cell>36.2</cell></row><row><cell>Liu et al. [25] CVPR'20</cell><cell>32.3 35.2 33.3 35.8</cell><cell>35.9</cell><cell cols="2">41.5 33.2 32.7 44.6 50.9</cell><cell>37.0</cell><cell>32.4</cell><cell>37.0</cell><cell>25.2</cell><cell>27.2</cell><cell>35.6</cell></row><row><cell>Ours ( = 243 CPN)</cell><cell>32.5 36.2 33.2 35.3</cell><cell>35.6</cell><cell>42.1</cell><cell>32.6 31.9 42.6 47.9</cell><cell>36.6</cell><cell>32.1</cell><cell>34.8</cell><cell>24.2</cell><cell>25.8</cell><cell>35.0</cell></row></table><note>Method Dir. Disc. Eat Greet Phone Photo Pose Pur. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg Martinez et al. [26] ICCV'17 39.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on HumanEva-I after rigid alignment in millimeter under Protocol#2 (P-MPJPE). ( * ) : the high error on "Walk" of S3 is due to corrupted mocap data, thus this value is not involved in the calculation of the average.</figDesc><table><row><cell>Method</cell><cell>S1</cell><cell>Walk S2</cell><cell>S3</cell><cell>S1</cell><cell>Jog S2</cell><cell>S3</cell><cell>Avg</cell></row><row><cell cols="4">Martinez et al. [26] 19.7 17.4 46.8</cell><cell cols="4">26.9 18.2 18.6 24.6</cell></row><row><cell cols="8">Pavlakos et al. [31] 22.3 19.5 29.7 28.9 21.9 23.8 24.4</cell></row><row><cell>Lee et al. [18]</cell><cell cols="3">18.6 19.9 30.5</cell><cell cols="4">25.7 16.8 17.7 21.5</cell></row><row><cell>Pavllo et al. [32]</cell><cell cols="7">13.9 10.2 46.6  *  20.9 13.1 13.8 14.3</cell></row><row><cell>Liu et al. [25]</cell><cell cols="7">13.8 10.0 46.4  *  20.2 12.3 12.9 13.8</cell></row><row><cell>Ours ( = 27)</cell><cell cols="7">13.5 9.4 47.2  *  20.3 12.1 12.8 13.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>The effect of absolute 2D positions on the prediction results. w/o ABS: without absolute 2D positions</figDesc><table><row><cell>Method</cell><cell>MPJPE(mm)</cell><cell>?</cell></row><row><cell>Baseline</cell><cell>34.1</cell><cell>-</cell></row><row><cell>+P-ENH</cell><cell>33.4</cell><cell>0.7</cell></row><row><cell>+P-ENH(w/o ABS)</cell><cell>36.2</cell><cell>-2.1</cell></row><row><cell>+T-ENH</cell><cell>33.1</cell><cell>1.0</cell></row><row><cell>+T-ENH(w/o ABS)</cell><cell>36.9</cell><cell>-2.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparison between different temporal information encoding operators.</figDesc><table><row><cell>Method</cell><cell>MPJPE(mm)</cell><cell>?</cell></row><row><cell>Baseline</cell><cell>34.1</cell><cell>-</cell></row><row><cell>+CP</cell><cell>33.9</cell><cell>0.2</cell></row><row><cell>+IP</cell><cell>33.6</cell><cell>0.5</cell></row><row><cell>+CS</cell><cell>33.5</cell><cell>0.6</cell></row><row><cell>+SUB</cell><cell>33.1</cell><cell>1.0</cell></row><row><cell>+SUB+SUB_S</cell><cell>34.2</cell><cell>-0.1</cell></row><row><cell>+SUB(81f)</cell><cell>33.7</cell><cell>0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The influence of the M-S OPT method on P/T-ENH.</figDesc><table><row><cell>Method</cell><cell cols="2">MPJPE(mm) ?</cell></row><row><cell>Baseline</cell><cell>34.1</cell><cell>-</cell></row><row><cell>Baseline(M-S OPT)</cell><cell>33.3</cell><cell>0.8</cell></row><row><cell>+P-ENH</cell><cell>33.4</cell><cell>-</cell></row><row><cell>+P-ENH(M-S OPT)</cell><cell>32.5</cell><cell>0.9</cell></row><row><cell>+T-ENH</cell><cell>33.1</cell><cell>-</cell></row><row><cell>+T-ENH(M-S OPT)</cell><cell>31.8</cell><cell>1.3</cell></row><row><cell>+P-ENH+T-ENH</cell><cell>32.3</cell><cell>-</cell></row><row><cell>+P-ENH+T-ENH(M-S OPT)</cell><cell>30.1</cell><cell>2.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D Human Pose from Silhouettes by Relevance Vector Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pictorial Structures Revisited: People Detection and Articulated Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D Pictorial Structures for Multiple Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
	<note>Bernt Schiele, Nassir Navab, and Slobodan Ilic</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting Spatial-Temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation = 2D Pose Estimation + Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7035" to="7043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded Pyramid Network for Multi-person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Occlusion-Aware Networks for 3D Human Pose Estimation in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimizing Network Structure for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2262" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning 3D Human Pose from Structure and Motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Alletto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7204" to="7213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Pose Grammar to Encode Human Body Configuration for 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Generalized Procrustes Analysis</title>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting Temporal Information for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly-Supervised 3D Human Pose Learning via Multi-View Images in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5243" to="5252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Propagating LSTM: 3D Pose Estimation Based on Joint Interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungoh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation from Monocular Images with Deep Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cascaded Deep Monocular 3D Human Pose Estimation With Evolutionary Training Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6173" to="6183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Augmented Reality with Human Body Interaction Based on Monocular 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung</forename><surname>Huei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Wen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="321" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Trajectory Space Factorization for Deep Video-Based 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent 3D Pose Sequence Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="810" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pose Transferrable Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4099" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sen-ching Cheung, and Vijayan Asari. 2020. Attention Mechanism Exploits Temporal Contexts: Real-Time 3D Human Pose Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Simple Yet Effective Baseline for 3d Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monocular 3D Human Pose Estimation in the Wild Using Improved CNN Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vnect: Real-Time 3D Human Pose Estimation with a Single RGB Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation from a Single Image via Distance Matrix Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2823" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation with Relational Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungheon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation in Video With Temporal Convolutions and Semi-Supervised Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-Classification-Regression for Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3433" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Compositional Human Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">2020. Faster R-CNN for Multi-Class Fruit Detection Using a Robotic Vision System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotirios</forename><surname>Goudos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page">107036</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Not All Parts Are Created Equal: 3D Pose Estimation by Modeling Bi-Directional Dependencies of Body Parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7771" to="7780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Motion Guided 3D Pose Estimation from Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="764" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">DRPose3D: Depth Ranking in 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="978" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Hierarchical Poselets for Human Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1705" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep Kinematics Analysis for Monocular 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Srnet: Improving Generalization in 3D Human Pose Estimation with a Split-and-Recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semantic Graph Convolutional Networks for 3D Human Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Joint Relationship Aware Neural Network for Single-Image 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiumei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4747" to="4758" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sen-ching Cheung, and Vijayan Asari. 2020. Attention Mechanism Exploits Temporal Contexts: Real-Time 3D Human Pose Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation in Video With Temporal Convolutions and Semi-Supervised Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Motion Parallax as an Independent Cue for Depth Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maureen</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="125" to="134" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<title level="m">Empirical Evaluation of Rectified Activations in Convolutional Network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation in the Wild by Adversarial Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
