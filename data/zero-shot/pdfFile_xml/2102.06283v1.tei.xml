<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPEECH-LANGUAGE PRE-TRAINING FOR END-TO-END SPOKEN LANGUAGE UNDERSTANDING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Cognitive Services Research Group</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximo</forename><surname>Bian</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Cognitive Services Research Group</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoyuki</forename><surname>Kanda</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Cognitive Services Research Group</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Cognitive Services Research Group</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Cognitive Services Research Group</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Cognitive Services Research Group</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SPEECH-LANGUAGE PRE-TRAINING FOR END-TO-END SPOKEN LANGUAGE UNDERSTANDING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-spoken language understanding</term>
					<term>end-to-end ap- proach</term>
					<term>pre-training</term>
					<term>transfer learning</term>
					<term>self-supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end (E2E) spoken language understanding (SLU) can infer semantics directly from speech signal without cascading an automatic speech recognizer (ASR) with a natural language understanding (NLU) module. However, paired utterance recordings and corresponding semantics may not always be available or sufficient to train an E2E SLU model in a real production environment. In this paper, we propose to unify a well-optimized E2E ASR encoder (speech) and a pre-trained language model encoder (language) into a transformer decoder. The unified speech-language pre-trained model (SLP) is continually enhanced on limited labeled data from a target domain by using a conditional masked language model (MLM) objective, and thus can effectively generate a sequence of intent, slot type, and slot value for given input speech in the inference. The experimental results on two public corpora show that our approach to E2E SLU is superior to the conventional cascaded method. It also outperforms the present state-of-the-art approaches to E2E SLU with much less paired data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The conventional SLU generally contains two components: the automatic speech recognizer (ASR) to decode the input speech into recognized text and the natural language understanding (NLU) module to transform the ASR hypothesis into a concept or semantic label that can drive subsequent responses in a task-oriented spoken dialog system (SDS). These two components are optimized with different criteria. Nowadays, the performance of ASR has been significantly improved by using a huge amount of training data with a deep learning architecture. Transfer learning based on transformerbased LMs like BERT <ref type="bibr" target="#b0">[1]</ref> or GPT <ref type="bibr" target="#b1">[2]</ref> has also largely boosted the accuracy of NLU. This cascaded process is beneficial in reusing the well-optimized modules to accelerate the development of the SLU system. However, the cascaded process could propagate errors that occurred in the current module to the following modules and results in generating inappropriate responses or even causing the task failure.</p><p>In recent few years, an end-to-end (E2E) modeling approach has been widely applied to speech recognition, language recognition, etc. It enables us to utilize as little a prior knowledge as possible and train a single model for the whole target system, skipping the intermediate modules in conventional pipeline designs. ASR-free E2E SLU has also been exploited in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, where either the raw waveforms or the acoustic features (e.g., filterbank features) are directly used as the inputs of SLU to infer semantic meanings. In <ref type="bibr" target="#b4">[5]</ref>, * Work done while the second author was an intern at Microsoft. multiple E2E SLU approaches, i.e., direct model, joint model, multitask model, and multi-stage model, have been proposed to jointly optimize the ASR and NLU components in a manner of sequenceto-sequence mapping. On the other hand, the performance of the end-to-end approach is always suffering from a dearth of data, i.e., it is difficult to obtain large amounts of paired speech utterances and corresponding semantic labels from real production environments when we are developing new SDS applications from scratch.</p><p>Substantial work has shown that transfer learning with a pretrained model requires much less task-specific data than conventional methods. It is also extended to cross-modal learning, e.g., VLP <ref type="bibr" target="#b8">[9]</ref> and SpeechBERT <ref type="bibr" target="#b9">[10]</ref> proposed to build two-modality joint embeddings for image captioning, spoken question answering, etc. There are many studies to use either speech model or language model as a pre-trained model to address the issue of insufficient data for SLU but few tries to leverage both and jointly optimize them on the paired data. BERT or GPT is leveraged as the pre-trained language model <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. The speech model is pre-trained by using ASR task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref>, i.e., to predict phonemes, sub-words or words, or by self-supervised learning on the unlabeled speech data <ref type="bibr" target="#b13">[14]</ref>. Language embeddings were assumed to have better representation for lexical semantics than speech embeddings, so a loss function, e.g., mean squared error (MSE), L1 norm, or triplet loss, is employed to make speech embeddings closer to language embeddings in the stage of jointly training semantic representations across these two modalities <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. This kind of alignment for speech and language embeddings could be applied at the sentence level or token level. Sentence-level alignment is more feasible than that of token-level since obtaining accurate boundaries of sub-word for speech embedding sequence is nontrivial. Consequently, most of the studies focus on sentence-level SLU, i.e., domain/intent classification, or using a fixed dimension vector of logits corresponding to the different semantic labels.</p><p>In this paper, we exploit leveraging transfer learning based on well-trained encoders from both speech and language for sentencelevel and word-level SLU, i.e., intent classification and slot filling. The proposed approach can compensate for insufficient paired training data in the SLU tasks and avoid train a model from scratch so that the resultant model has a better generalization capability. The major contribution of this paper is two-fold:</p><p>1. We unify two encoders from speech and language separately into a decoder, continually pre-train it by using a conditional masked language model (MLM) objective, and finetune it to generate a sequence of intent, slot type, and slot value.</p><p>2. Slot filling, conventionally regarded as a classification or tagging problem on the word/token level, is formulated as a conditional sequence to sequence generation problem, given the contextual embeddings of input speech. The effectiveness of this approach is demonstrated by a comparison with the con-ventional method on public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Inspired by the transfer Learning with a unified text-to-text transformer (T5) <ref type="bibr" target="#b14">[15]</ref> where the input and output are always text strings for NLP tasks, fine-tuning a pre-trained causal LM (e.g., GPT), for NLP tasks like natural language understanding, natural language generation, or end-to-end task completion have been explored for dialog systems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. However, most of them aim at processing the user's written responses or the human transcriptions of spoken responses. Speech to semantics mapping was also defined as a sequence-to-sequence problem in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>, but it lacks a mechanism to leverage pre-trained models from either speech or language to further improve the performance of E2E SLU. Speech synthesis is thus explored to generate large training data from multiple artificial speakers to cover the shortage of paired data in E2E SLU <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>. The most similar work to ours has been presented in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>, where it initializes the speech-to-intent (S2I) model with an ASR model and improves it by leveraging BERT and text-to-speech augmented S2I data. The significant difference between ours and theirs is that they employ sentence-level embedding from both speech and language to jointly classify intent, while we use an attention-based autoregressive generation model jointly trained by using frame-level speech embedding and token level text embedding to generate multiple intents and slots/values given an utterance input. It is difficult to use their approach to handle more complicated SLU tasks, e.g., slot filling in the ATIS dataset, since a classification layer with a fixed-length output is impossible to cover various slots/values across utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SPEECH-LANGUAGE PRE-TRAINING FOR SLU</head><p>Our proposed model architecture for speech-language pre-training is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. The model inputs include speech embedding sequence, X, masked text token sequence, Y , and three special tokens. Speech embeddings are extracted from the encoder of an attention-based encoder-decoder ASR model. We tokenize the texts into subword units by WordPiece <ref type="bibr" target="#b21">[22]</ref>, and add the corresponding token embeddings with position embeddings. Three special tokens: [BOS], [SEP], and [EOS] indicate the start of speech input, the boundary between the speech input and the text input, and the end of text input, separately. These three delimiter tokens only give the model weak information about which segment (i.e., speech input or text input) each embedding belongs to. We add the segment embeddings to both speech embeddings and text embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-training</head><p>We initialize the parameters of the model with pre-trained BERT and continually pre-train it with the speech and the corresponding transcriptions from the target dataset. The parameters of the model are updated by using a cloze task with a conditional masked language modeling (C-MLM) objective. It aims to predict the masked token y m n in Y = (y1, .., yN ), conditioned on a sequence of input X. It is usually trained by Maximum Likelihood Estimation (MLE), or equivalently, minimizing the cross-entropy loss as follows:</p><formula xml:id="formula_0">L(?) = ? m log P ? (y m t |y1:t?1, X)<label>(1)</label></formula><p>where the conditional probability is calculated by using an attention mechanism in transformer-based network architecture. We randomly select some tokens in the input and replace each of them with a special token [Mask] or a random token or the original token, and then predict the masked tokens by feeding the output vectors from the transformer network into a softmax classifier. We mask one token or a bigram/trigram at random. For the prediction, the tokens in the speech part can attend to each other, while the tokens in the text part can only attend to the left-side context (including all the tokens in the speech part) and itself. It is different from what BERT uses, i.e., all tokens are allowed to be attended in the prediction.</p><p>A self-attention mask matrix, as defined in <ref type="bibr" target="#b22">[23]</ref>,</p><formula xml:id="formula_1">Mij = 0, allow to attend ??,</formula><p>prevent from attending <ref type="formula">(2)</ref> is applied to control which contextual tokens can be attended to. The l-th self-attention output, A l , is computed as:</p><formula xml:id="formula_2">A l = softmax( QK ? d k + M )V<label>(3)</label></formula><p>where we use a single attention head in the self-attention module to simplify the description. V , Q and K are the intermediate variables as defined in <ref type="bibr" target="#b23">[24]</ref> for representing values, queries, and keys, respectively. d k is the dimension of the keys. Our continual pre-training approach can be regarded as turning the BERT initialized encoder, which has learned better text representations by encoding contextual information from both directions, into an autoregressive decoder that satisfies the property of sequence-to-sequence generation in the inference stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fine-tuning for SLU</head><p>We fine-tune the pre-trained speech-language model by using the same objective as that of pre-training but with the speech and the annotated semantic labels from the target dataset. Unlike the conventional slot IOB labels (i.e., tagging inside, outside, beginning words in a slot type ) assigned to every word in the utterance, sequence representation, such as <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference</head><p>In the inference stage, we first encode the speech embeddings along with the special [BOS] and [SEP] tokens and then use a greedy sampling approach or a beam search to generate text token-by-token. The generation starts with appending a [MASK] token to the input sequence, replaces it with a sampled token, and repeats the process till the [EOS] token is chosen. Given speech embeddings, the pretrained speech-language model can directly generate the corresponding transcription and the fine-tuned model can produce the matching semantic label sequence. The model can also generate ASR transcription and SLU semantic labels sequentially if the input text token in <ref type="figure" target="#fig_0">Figure 1</ref> is the concatenation of transcription and semantic labels during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our approach of speech-language pre-training to E2E SLU on two public SLU datasets: Fluent Speech Commands (FSC) <ref type="bibr" target="#b5">[6]</ref> and Air Travel Information System (ATIS) <ref type="bibr" target="#b24">[25]</ref>. FSC dataset was recorded by 77 speakers for a smart home or virtual assistant. There are 248 distinct sentences. Each sentence, e.g.,{turn on the lights in the kitchen}, was spoken by multiple speakers in both training set and validation/test sets. Each audio file is labeled with three slots, e.g., {action: "active", object: "lights", location: "kitchen"}. We follow the usage suggested in <ref type="bibr" target="#b5">[6]</ref>, i.e., combining slot values as the intent of utterance and using 31 distinct intents in total.</p><p>ATIS pilot corpus is the most commonly used dataset for SLU research. It comprises the acoustic speech data for a query, the transcriptions of that query, and the corresponding semantic frames, i.e., an intent and slots filled with phrases. For example, Query: {What flights are available from Pittsburgh to Baltimore on Thursday morning}; Intent: {flight info}; Slots: {from city: "Pittsburgh", to city: "Baltimore", depart date: "Thursday", depart time: "morning"} The statistics of these two datasets are shown in <ref type="table" target="#tab_1">Table 1</ref>, where lists the number of audio files in training, validation and test sets, and the number of intents and slots. We use the same data division as <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16]</ref> for FSC and ATIS datasets, respectively. Some utterances have multiple intents in ATIS corpus, so we employed 21 intents rather than 18 intents in some papers, where only one intent was forcedly assigned to an utterance. Slot labels in the IOB format (120 labels in total) and human transcriptions have been widely used in the studies (e.g., <ref type="bibr" target="#b15">[16]</ref>) for ATIS corpus. We find 39 utterances have the transcriptions, but the audio files are missing. We use total of 4,439 training utterances and the corresponding 79 original slot types in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental setup</head><p>The experiments are configured as follows:</p><p>E2E with speech-language pre-training (E2E SLP) This is our approach described in Section 3. We construct mode with the architecture shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The model parameters are initialized by using the English uncased BERT-Base model, which is a multilayer bidirectional transformer encoder consisting of 12 layers, 768 hidden states, and 12 heads, and is trained on BooksCorpus (800M words) and English Wikipedia (2,500M words).</p><p>The input to the model is a concatenation of text embeddings, speech embeddings, and three special tokens: [BOS], [SEP], and <ref type="bibr">[EOS]</ref>. Text embeddings are the sums of WordPiece embeddings with a 30,000 token vocabulary, positional embeddings, and segment embeddings. Speech embeddings are extracted from the outputs of the encoder of an attention-based encoder-decoder (AED) ASR model, which comprises of 6 layers of 1,024-dim bidirectional long short-term memory (BLSTM) for the encoder, 2 layers of 1,024-dim unidirectional LSTM for the decoder, and a conventional locationaware content-based attention <ref type="bibr" target="#b26">[27]</ref> with a single head in between the encoder and the decoder. Layer normalization <ref type="bibr" target="#b27">[28]</ref> was applied after every BLSTM in the encoder but not after every LSTM in the decoder. The input feature is a sequence of 80-dim log Mel filter bank with a stride of 10 milliseconds (ms). Three of them are stacked together to form a 240-dim super-frame. We feed into the encoder on top of the stacked features. 4,000 WordPiece tokens are used as the output targets. This model was trained on 75 thousand hours of Microsoft anonymized training data. A linear layer is applied to the speech embeddings for converting 1,024-dim to 768-dim in order to keep the same dimension as those of text embeddings.</p><p>Adam optimizer, 64 batch size, and 1e-4 learning rate are used for both pre-training and fine-tuning. We trim long utterance and pad short utterances to 30 words for the text embeddings and 500 frames for the speech embeddings. During the training, 15% of input text tokens are masked and replaced with the special token [Mask], the random token, or the original token with the probabilities of 80%, 10%, and 10%, separately. One or two/three successive tokens are masked at random with the chances of 80% and 20%, respectively. A beam search with beam size 4 is used in the inference stage.</p><p>E2E Joint Modeling Similar to the AED-based ASR model, the approach of E2E joint modeling to SLU also employs an encoderattention-decoder architecture and trains the model in an end-to-end manner. We use the same architecture and input acoustic features as those of the above AED-based ASR model to train E2E SLU. The output of this model is the concatenation of transcription and its corresponding semantics rather than just the transcription, i.e., the decoder produces the transcription followed by intent, slot types, and slot values. We test this approach by using only the FSC corpus. The training data in ATIS is too insufficient to deliver a decent E2E model.</p><p>Cascaded ASR+NLU It is a conventional approach to SLU. We first use ASR described in the above section to recognize the spoken input into text and then fine-tune the pre-trained BERT model with the ASR transcriptions of training utterances. The objective of the fine-tuning is to jointly optimize model parameters for intent classification and slot filling <ref type="bibr" target="#b15">[16]</ref>. This approach can leverage BERT to improve the generalization capability of NLU models and achieve the state-of-the-art performance on the human transcriptions of ATIS corpus. The same English uncased BERT-Base model as that employed in our approach is used for fine-tuning. The hyperparameters are also set as those in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and discussion</head><p>Word error rate (WER) is used to evaluate the performance of ASR. The performance metrics for semantic labeling are intent accuracy (Intent-Acc), slot precision (Slot-Pre), slot recall (Slot-Rec), and slot F1 (Slot-F1). <ref type="table" target="#tab_2">Table 2</ref> shows the performance of different systems. The slots/values of each utterance are combined as the intent of the utterance in the FSC corpus, so there are no results related to the slots in <ref type="table" target="#tab_2">Table 2</ref>. Meanwhile, the results of E2E joint modeling for ATIS corpus are also blank due to the inadequate training data for building an E2E SLU model. The E2E joint modeling approach to SLU achieves the intent accuracy of 99.13% on the FSC testing set. This is the best performance we have seen by building a model from scratch, i.e., using only training data of FSC without any helps from the pre-trained models.</p><p>We apply two strategies in building E2E SLP modes. Two-step means that we firstly pre-train SLP with paired speech and the corresponding transcriptions, and then fine-tune the resultant model with paired speech and the semantic labels. One-step indicates that we just fine-tune the SLP model. The output targets can be semantic labels, i.e., one-step (SLU ), or the concatenation of transcriptions and semantic labels, i.e., one-step (ASR ? SLU ). The experimental results in <ref type="table" target="#tab_2">Table 2</ref> show that the performance of one-step (ASR?SLU ) is equivalent to that of two-step (ASR, SLU ) for the FSC corpus but two-step significantly outperforms one-step for the ATIS corpus. We think 1) the size of training data in FSC is much larger than that in ATIS so that both strategies can converge to the same performance; 2) the concatenations of transcriptions and semantic labels in ATIS corpus are much longer than those in FSC corpus. It may cause performance degradation for sequence generation.</p><p>Three kinds of transcription: 1) The ground-truth transcription (Ref?BERT) as a upper-bound reference; 2) ASR transcription (ASR?BERT) by using the AED-based ASR system mentioned in Section 4.2; 3) ASR transcription from pre-trained SLP model (SLP ASR?BERT), are fed into the BERT model to build cascaded SLU system. The pre-trained SLP model can reduce ASR WER from 2.82%, 14.82% to 0.42%, 8.68% for FSC and ATIS, respectively. The refined ASR hypotheses are critical for improving the performance of intent prediction and slot filing, e.g., the slot-F1 for ATIS is significantly improved from 84.13% to 90.12%. In the pre-training stage, we continually train the SLP model with the speech embeddings from the encoder of the AED-based ASR model and the tokenized transcription by using FSC or ATIS dataset. This procedure can be thought of as domain adaptation for the acoustic model (AM) and language model (LM) in the point of view of ASR customization. LM represented by the decoder in the AED-based ASR model is replaced by BERT based LM and further adapted with the training transcriptions of FSC or ATIS. It brings most of the WER reductions for the generated hypotheses. By comparing the performance of different systems shown in <ref type="table" target="#tab_2">Table 2</ref>, we find that our proposed approach achieves the best performance in terms of all evaluation metrics for both corpora. In the previous studies, E2E SLU is difficult to beat the cascaded SLU, which can leverage large module-specific data to optimize individual modules. Our approach can unify well-optimized modules into a single model and further optimize the model with a unified objective of SLU. The size of training data in the FSC corpus is relatively larger than other commonly used SLU corpora. We investigate whether we can reduce the number of utterances for transfer learning with the pre-trained SLP. The <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the intent accuracy along with the number of training samples by using our proposed approach, i.e., E2E SLP one-step (ASR ? SLU ). It shows that using only 3,000 samples by a random selection from the training set can reach a similar performance to that of using all 23,132 training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>The cascaded SLU can leverage ASR and NLU modules optimized with a large amount of module-specific training data and deliver a decent performance. It is a challenge for E2E SLU to beat it due to a lack of labeled data. We propose to leverage the pre-trained speech embeddings from ASR and the semantic representations from BERT and jointly optimize them for E2E SLU. We have demonstrated the effectiveness of our approach on FSC and ATIS datasets in this study. In the future, we will extend our method to multi-domain and multilingual SLU corpora and evaluate its generalization capability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A schematic diagram of our approach to speech-language pre-training for E2E SLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The number of training samples from FSC corpus vs. the intent accuracy by using the approach of E2E SLP one-step (ASR ? SLU )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>intent] &amp; [slot1 type] [slot1 value] &amp; [slot2 type] [slot2 value] &amp; ..., is employed to represent the intents and the slots/values for the whole utterance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Number of audio files in training, validation and test sets, and the number of intents and slots</figDesc><table><row><cell>#</cell><cell cols="5">Training Validation Testing Intents Slots</cell></row><row><cell>FSC</cell><cell>23,132</cell><cell>3,118</cell><cell>3,793</cell><cell>31</cell><cell></cell></row><row><cell>ATIS</cell><cell>4,439</cell><cell>500</cell><cell>893</cell><cell>21</cell><cell>79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance(%) of different systems</figDesc><table><row><cell></cell><cell>Systems</cell><cell></cell><cell>FSC</cell><cell></cell><cell></cell><cell>ATIS</cell><cell></cell><cell></cell></row><row><cell>Approaches</cell><cell>Configurations</cell><cell cols="7">WER Intent-Acc WER Intent-Acc Slot-Pre Slot-Rec Slot-F1</cell></row><row><cell>E2E Joint</cell><cell>ASR ? SLU</cell><cell>0.86</cell><cell>99.13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>E2E SLP</cell><cell>one-step (SLU )</cell><cell></cell><cell>98.58</cell><cell></cell><cell>94.96</cell><cell>89.21</cell><cell>85.58</cell><cell>87.36</cell></row><row><cell>E2E SLP</cell><cell>one-step (ASR ? SLU )</cell><cell>0.42</cell><cell>99.71</cell><cell>14.47</cell><cell>95.30</cell><cell>86.71</cell><cell>82.20</cell><cell>84.40</cell></row><row><cell>E2E SLP</cell><cell>two-step (ASR, SLU )</cell><cell>0.42</cell><cell>99.71</cell><cell>8.68</cell><cell>96.30</cell><cell>91.13</cell><cell>90.76</cell><cell>90.95</cell></row><row><cell>Cascaded</cell><cell>Ref ? BERT</cell><cell></cell><cell>100.0</cell><cell></cell><cell>97.65</cell><cell>95.89</cell><cell>96.30</cell><cell>96.10</cell></row><row><cell>Cascaded</cell><cell>ASR ? BERT</cell><cell>2.82</cell><cell>96.84</cell><cell>14.82</cell><cell>95.18</cell><cell>86.53</cell><cell>81.86</cell><cell>84.13</cell></row><row><cell>Cascaded</cell><cell>SLP ASR ? BERT</cell><cell>0.42</cell><cell>99.47</cell><cell>8.68</cell><cell>95.52</cell><cell>90.46</cell><cell>89.78</cell><cell>90.12</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Exploring ASR-free end-to-end modeling to improve spoken language understanding in a cloud-based dialog system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rutuja</forename><surname>Ubale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Ramanaryanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Suendermann-Oeft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keelan</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Tsuprun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spoken language understanding without speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6189" to="6193" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From audio to semantics: Approaches to end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in SLT</title>
		<imprint>
			<biblScope unit="page" from="720" to="726" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Speech model pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikrant</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards end-toend spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5754" to="5758" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale unsupervised pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7999" to="8003" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unified vision-language pre-training for image captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11059</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Speechbert: An audio-and-text jointly learned language model for end-to-end spoken question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Sung</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Shan Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11559</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning spoken language representations with neural lattice language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3764" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning for optimizing ASR and semantic labeling in taskoriented spoken dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">To be appeared in Interspeech</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Leveraging unpaired text data for training end-to-end speech-to-intent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7984" to="7988" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semisupervised speech-language joint pre-training for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02295</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT for joint intent classification and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Few-shot natural language generation for task-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12328</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Transferable multi-domain state generator for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08743</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hello, it&apos;s gpt-2-how can i help you? towards the use of pretrained language models for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawe?</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05774</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using speech synthesis to train end-to-end spoken language understanding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="8499" to="8503" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pretrained semantic speech embeddings for end-to-end spoken language understanding via cross-modal teacher-student learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Denisov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01836</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13063" to="13075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The ATIS spoken language systems pilot corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hemphill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DARPA Speech and Natural Language Workshop</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Slot-gated modeling for joint slot filling and intent prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chih-Wen Goo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Kai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Li</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Chieh</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="753" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Hinton</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
