<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ICON: Implicit Clothed humans Obtained from Normals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Xiu</surname></persName>
							<email>yuliang.xiu@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Yang</surname></persName>
							<email>jinlong.yang@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
							<email>dtzionas@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ICON: Implicit Clothed humans Obtained from Normals</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Images to avatars. ICON robustly reconstructs 3D clothed humans in unconstrained poses from individual video frames (Left). These are used to learn a fully textured and animatable clothed avatar with realistic clothing deformations (Right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn an avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair and clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON ("Implicit Clothed humans Obtained from Normals"), which, instead, uses local features. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use a modified version of SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables avatar creation directly from video with personalized pose-dependent cloth deformation. Models and code are available for research at https://icon.is.tue.mpg.de.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Realistic virtual humans will play a central role in mixed and augmented reality, forming a key foundation for the "metaverse" and supporting remote presence, collaboration, education, and entertainment. To enable this, new tools are needed to easily create 3D virtual humans that can be readily animated. Traditionally, this requires significant artist effort and expensive scanning equipment. Therefore, such approaches do not scale easily. A more practical approach would enable individuals to create an avatar from one or more images. There are now several methods that take a single image and regress a minimally clothed 3D human model <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b46">48]</ref>. Existing parametric body models, however, lack important details like clothing and hair <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b62">64]</ref>. In contrast, we present a method that robustly extracts 3D scan-like data from images of people in arbitrary poses and uses this to construct an animatable avatar.</p><p>We base our approach on implicit functions (IFs), which go beyond parametric body models to represent fine shape details and varied topology. IFs allow recent methods to infer detailed shape from an image <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b68">70]</ref>. Despite promising results, state-of-the-art (SOTA) methods struggle with in-the-wild data and often produce humans with broken or disembodied limbs, missing details, high-frequency noise, or non-human shape; see <ref type="figure" target="#fig_3">Fig. 2</ref> for examples.</p><p>The issues with previous methods are twofold: (1) Such methods are typically trained on small, handcurated, 3D human datasets (e.g. Renderpeople <ref type="bibr" target="#b1">[3]</ref>) with very limited pose, shape and clothing variation.</p><p>(2) They typically feed their implicit-function module with features of a global 2D image or 3D voxel encoder, but these are sensitive to global pose. While more, and more varied, 3D training data would help, such data remains limited. Hence, we take a different approach and improve the model.</p><p>Specifically, our goal is to reconstruct a detailed clothed 3D human from a single RGB image with a method that is training-data efficient and robust to in-the-wild images and out-of-distribution poses. Our method, called ICON, stands for Implicit Clothed humans Obtained from Normals. ICON replaces the global encoder of existing methods with a more data-efficient local scheme; <ref type="figure" target="#fig_0">Fig. 3</ref> shows a model overview. ICON takes as input an RGB image of a segmented clothed human and a SMPL body estimated from the image <ref type="bibr" target="#b30">[32]</ref>. The SMPL body is used to guide two of ICON's modules: one infers detailed clothed-human surface normals (front and back views), and the other infers a visibility-aware implicit surface (iso-surface of an occupancy field). Errors in the initial SMPL estimate, however, might misguide inference. Thus, at inference time, an iterative feedback loop refines SMPL (i.e., its 3D shape, pose, and translation) using the inferred detailed normals, and vice versa, leading to a refined implicit shape with better 3D details.</p><p>We evaluate ICON quantitatively and qualitatively on challenging datasets, namely AGORA <ref type="bibr" target="#b45">[47]</ref> and CAPE <ref type="bibr" target="#b39">[41]</ref>, as well as on in-the-wild images. Results show that ICON has two advantages w.r.t. the state of the art:</p><p>(1) Generalization. ICON's locality helps it generalize to in-the-wild images and out-of-distribution poses and clothes better than previous methods. Representative cases are shown in <ref type="figure" target="#fig_3">Fig. 2</ref>; notice that, although ICON is trained on full-body images only, it can handle images with outof-frame cropping, with no fine tuning or post processing.</p><p>(2) Data efficacy. ICON's locality helps it avoid spurious correlations between pose and surface shape. Thus, it needs less data for training. ICON significantly outperforms baselines in low-data regimes, as it reaches SOTA performance when trained with as little as 12% of the data.</p><p>We provide an example application of ICON for creating an animatable avatar; see <ref type="figure">Fig. 1</ref> for an overview. We first apply ICON on the individual frames of a video sequence, <ref type="figure" target="#fig_3">Figure 2</ref>. SOTA methods for inferring 3D humans from in-the-wild images, e.g., PIFu, PIFuHD, PaMIR, and ARCH++, struggle with challenging poses and out-of-frame cropping (E), resulting in various artifacts including non-human shapes (A,G), disembodied parts (B,H), missing body parts (C,D), missing details (E), and high-frequency noise (F). ICON deals with these challenges and produces high-quality results, highlighted with a green shadow Front view (blue) and rotated view (bronze).</p><formula xml:id="formula_0">(A) (B) (D) (F) (E) (C) (B) (D) (F)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICON vs PIFu</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICON vs PIFuHD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICON vs PaMIR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICON vs ARCH++</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(G) (H) (H)</head><p>to obtain 3D meshes of a clothed person in various poses. We then use these to train a poseable avatar using a modified version of SCANimate <ref type="bibr" target="#b54">[56]</ref>. Unlike 3D scans, which SCANimate takes as input, our estimated shapes are not equally detailed and reliable from all views. Consequently, we modify SCANimate to exploit visibility information in learning the avatar. The output is a 3D clothed avatar that moves and deforms naturally; see <ref type="figure">Fig. 1</ref>-right and <ref type="figure">Fig. 8b</ref>. ICON takes a step towards robust reconstruction of 3D clothed humans from in-the-wild photos. Based on this, fully textured and animatable avatars with personalized poseaware clothing deformation can be created directly from video frames. Models and code are available at https: //icon.is.tue.mpg.de.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Mesh-based statistical models. Mesh-based statistical body models <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b62">64]</ref> are a popular explicit representation for 3D human reconstruction. This is not only because such models capture the statistics across a human population, but also because meshes are compatible with standard graphics pipelines. A lot of work <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b64">66]</ref> estimates 3D body meshes from an RGB image, but these have no clothing. Other work estimates clothed humans, instead, by modeling clothing geometry as 3D offsets on top of body geometry <ref type="bibr">[4-7, 34, 50, 63, 72]</ref>. The resulting clothed 3D humans can be easily animated, as they naturally inherit the skeleton and surface skinning weights from the underlying body model. An important limitation, though, is modeling clothing such as skirts and dresses; since these differ a lot from the body surface, simple body-to-cloth offsets are insufficient. To address this, some methods <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b25">27]</ref> use a classifier to identify cloth types in the input image, and then perform cloth-aware inference for 3D reconstruction. However, such a remedy does not scale up to a large variety of clothing types. Another advantage of mesh-based statistical models, is that texture information can be easily accumulated through multi-view images or image sequences <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b10">12]</ref>, due to their consistent mesh topology. The biggest limitation, though, is that the state of the art does not generalize well w.r.t. clothing-type variation, and it estimates meshes that do not align well to input-image pixels.</p><p>Deep implicit functions. Unlike meshes, deep implicit functions <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b44">46]</ref> can represent detailed 3D shapes with arbitrary topology, and have no resolution limitations. Saito et al. <ref type="bibr" target="#b52">[54]</ref> introduce deep implicit functions for clothed 3D human reconstruction from RGB images and, later <ref type="bibr" target="#b53">[55]</ref>, they significantly improve 3D geometric details. The estimated shapes align well to image pixels. However, their shape reconstruction lacks regularization, and often produces artifacts like broken or disembodied limbs, missing details, or geometric noise. He et al. <ref type="bibr" target="#b20">[22]</ref> add a coarse-occupancy prediction branch, and Li et al. <ref type="bibr" target="#b35">[37]</ref> and Dong et al. <ref type="bibr" target="#b18">[20]</ref> use depth information captured by an RGB-D camera to further regularize shape estimation and provide robustness to pose variation. Li et al. <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b34">36]</ref> speed up inference through an efficient volumetric sampling scheme. A limitation of all above methods is that the estimated 3D humans cannot be reposed, because implicit shapes (unlike statistical models) lack a consistent mesh topology, a skeleton, and skinning weights. To address this, Bozic et al. <ref type="bibr" target="#b12">[14]</ref> infer an embedded deformation graph to manipulate implicit functions, while Yang et al. <ref type="bibr" target="#b63">[65]</ref> also infer a skeleton and skinning fields.</p><p>Statistical models &amp; implicit functions. Mesh-based statistical models are well regularized, while deep implicit functions are much more expressive. To get the best of both worlds, recent methods <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b68">70]</ref> combine the two representations. Given a sparse point cloud of a clothed person, IPNet <ref type="bibr" target="#b8">[10]</ref> infers an occupancy field with body/clothing layers, registers SMPL to the body layer with inferred body-part segmentation, and captures clothing as offsets from SMPL to the point cloud. Given an RGB image of a clothed person, ARCH <ref type="bibr" target="#b22">[24]</ref> and ARCH++ <ref type="bibr" target="#b21">[23]</ref> reconstruct 3D human shape in a canonical space by warping query points from the canonical to the posed space, and projecting them onto the 2D image space. However, to train these models, one needs to unpose scans into the canonical pose with an accurately fitted body model; inaccurate poses cause artifacts. Moreover, unposing clothed scans using the "undressed" model's skinning weights alters shape details. For the same RGB input, Zheng et al. <ref type="bibr" target="#b67">[69,</ref><ref type="bibr" target="#b68">70]</ref> condition the implicit function on a posed and voxelized SMPL mesh for robustness to pose variation and reconstruct local details from the image pixels, similar to PIFu <ref type="bibr" target="#b52">[54]</ref>. However, these methods are sensitive to global pose, due to their 3D convolutional encoder. Thus, for training data with limited pose variation, they struggle with out-of-distribution poses and in-the-wild images.</p><p>Positioning ICON w.r.t. related work. ICON combines the statistical body model SMPL with an implicit function, to reconstruct clothed 3D human shape from a single RGB image. SMPL not only guides ICON's estimation, but is also optimized "in the loop" during inference to enhance its pose accuracy. Instead of relying on the global body features, ICON exploits local body features that are agnostic to global pose variations. As a result, even when trained on heavily limited data, ICON achieves state-of-the-art performance and is robust to out-of-distribution poses. This work links monocular 3D clothed human reconstruction to scan/depth based avatar modeling algorithms <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b60">62</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>ICON is a deep-learning model that infers a 3D clothed human from a color image. Specifically, ICON takes as input an RGB image with a segmented clothed human (following the suggestion of PIFuHD's repository <ref type="bibr" target="#b47">[49]</ref>), along with an estimated human body shape "under clothing" (SMPL), and outputs a pixel-aligned 3D shape reconstruction of the clothed human. ICON has two main modules (see <ref type="figure" target="#fig_0">Fig. 3</ref>) for: (1) SMPL-guided clothed-body normal prediction and (2) local-feature based implicit surface reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Body-guided normal prediction</head><p>Inferring full-360 ? 3D normals from a single RGB image of a clothed person is challenging; normals for the occluded parts need to be hallucinated based on the observed parts. This is an ill-posed task and is challenging for deep networks. Unlike model-free methods <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b57">59]</ref>, ICON takes into account a SMPL <ref type="bibr" target="#b36">[38]</ref> "body-under-clothing" mesh to reduce ambiguities and guide front and (especially) back clothed-body normal prediction. To estimate the SMPL mesh M(?, ?) ? R N ?3 from image I, we use PyMAF <ref type="bibr" target="#b66">[68]</ref> due to its better mesh-to-image alignment compared to other methods. SMPL is parameterized by shape, ? ? R 10 , and pose, ? ? R 3?K , where N = 6, 890 vertices and K = 24 joints. ICON is also compatible with SMPL-X <ref type="bibr" target="#b46">[48]</ref>.</p><p>Under a weak-perspective camera model, with scale s ? R and translation t ? R 3 , we use the PyTorch3D <ref type="bibr" target="#b49">[51]</ref> dif- ferentiable renderer, denoted as DR, to render M from two opposite views, obtaining "front" (i.e., observable side) and "back" (i.e., occluded side) SMPL-body normal maps</p><formula xml:id="formula_1">N b = {N b front , N b back }. Given N b and the original color image I, our normal networks G N = {G N front , G N back } predict clothed- body normal maps, denoted as N c = { N c front , N c back }: DR(M) ? N b ,<label>(1)</label></formula><formula xml:id="formula_2">G N (N b , I) ? N c .<label>(2)</label></formula><p>We train the normal networks, G N , with the following loss:</p><formula xml:id="formula_3">L N = L pixel + ? VGG L VGG ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">L pixel = |N c v ? N c v |, v = {front, back}</formula><p>, is a loss (L1) between ground-truth and predicted normals (the two G N in <ref type="figure" target="#fig_0">Fig. 3</ref> have different parameters), and L VGG is a perceptual loss <ref type="bibr" target="#b26">[28]</ref> weighted by ? VGG . With only L pixel , the inferred normals are blurry, but adding L VGG helps recover details.</p><p>Refining SMPL. Intuitively, a more accurate SMPL body fit provides a better prior that helps infer better clothed-body normals. However, in practice, human pose and shape (HPS) regressors do not give pixel-aligned SMPL fits. To account for this, during inference, the SMPL fits are optimized based on the difference between the rendered SMPL-body normal maps, N b , and the predicted clothed-body normal maps, N c , as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Specifically we optimize over SMPL's shape, ?, pose, ?, and translation, t, parameters to minimize:</p><formula xml:id="formula_5">L SMPL = min ?,?,t (? N diff L N diff + L S diff ),<label>(4)</label></formula><formula xml:id="formula_6">L N diff = |N b ? N c |, L S diff = |S b ? S c |,<label>(5)</label></formula><p>where L N diff is a normal-map loss (L1), weighted by ? N diff ; L S diff is a loss (L1) between the silhouettes of the  SMPL body normal-map S b and the human mask S c segmented <ref type="bibr" target="#b50">[52]</ref> from I. We ablate L N diff , L S diff in Appx Refining normals. The normal maps rendered from the refined SMPL mesh, N b , are fed to the G N networks. The improved SMPL-mesh-to-image alignment guides G N to infer more reliable and detailed normals N c . Refinement loop. During inference, ICON alternates between: (1) refining the SMPL mesh using the inferred N c normals and (2) re-inferring N c using the refined SMPL. Experiments show that this feedback loop leads to more reliable clothed-body normal maps for both (front/back) sides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local-feature based implicit 3D reconstruction</head><p>Given the predicted clothed-body normal maps, N c , and the SMPL-body mesh, M, we regress the implicit 3D surface of a clothed human based on local features F P :</p><formula xml:id="formula_7">F P = [F s (P), F b n (P), F c n (P)],<label>(6)</label></formula><p>where F s is the signed distance from a query point P to the closest body point P b ? M, and F b n is the barycentric surface normal of P b ; both provide strong regularization against self occlusions. Finally, F c n is a normal vector extracted from N c front or N c back depending on the visibility of P b :</p><formula xml:id="formula_8">F c n (P) = N c front (?(P)) if P b is visible N c back (?(P)) else,<label>(7)</label></formula><p>where ?(P) denotes the 2D projection of the 3D point P. Please note that F P is independent of global body pose. Experiments show that this is key for robustness to out-of-distribution poses and efficacy w.r.t. training data.</p><p>We feed F P into an implicit function, IF, parameterized by a Multi-Layer Perceptron (MLP) to estimate the occupancy at point P, denoted as o(P). A mean squared error loss is used to train IF with ground-truth occupancy, o(P). Then the fast surface localization algorithm <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b37">39]</ref> is used to extract meshes from the 3D occupancy inferred by IF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baseline models</head><p>We compare ICON primarily with PIFu <ref type="bibr" target="#b52">[54]</ref> and PaMIR <ref type="bibr" target="#b68">[70]</ref>. These methods differ from ICON and from each other w.r.t. the training data, the loss functions, the network structure, the use of the SMPL body prior, etc. To isolate and evaluate each factor, we re-implement PIFu and PaMIR by "simulating" them based on ICON's architecture. This provides a unified benchmarking framework, and enables us to easily train each baseline with the exact same data and training hyper-parameters for a fair comparison. Since there might be small differences w.r.t. the original models, we denote the "simulated" models with a "star" as:</p><formula xml:id="formula_9">? PIFu * : {f2D(I, N )} ? O, ? PaMIR * : {f2D(I, N ), f3D(V)} ? O, ? ICON : {N , ?(M)} ? O,</formula><p>where f 2D denotes the 2D image encoder, f 3D denotes the 3D voxel encoder, V denotes the voxelized SMPL, O denotes the entire predicted occupancy field, and ? is the mesh-based local feature extractor described in Sec. 3.2. The results are summarized in Tab. 2-A, and discussed in Sec. 4.3-A. For reference, we also report the performance of the original PIFu <ref type="bibr" target="#b52">[54]</ref>, PIFuHD <ref type="bibr" target="#b53">[55]</ref>, and PaMIR <ref type="bibr" target="#b68">[70]</ref>; our "simulated" models perform well, and even outperform the original ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>Several public or commercial 3D clothed-human datasets are used in the literature, but each method uses different subsets and combinations of these, as shown in Tab. 1.</p><p>Training data. To compare models fairly, we factor out differences in training data as explained in Sec. 4.1. Following previous work <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b53">55]</ref>, we retrain all baselines on the same 450 Renderpeople scans (subset of AGORA). Methods that require the 3D body prior (i.e., PaMIR, ICON) use the SMPL-X meshes provided by AGORA. ICON's G N and IF modules are trained on the same data.</p><p>Testing data. We evaluate primarily on CAPE <ref type="bibr" target="#b39">[41]</ref>, which no method uses for training, to test their generelizability. Specifically, we divide the CAPE dataset into the "CAPE-FP" and "CAPE-NFP" sets that have "fashion" and "non-fashion" poses, respectively, to better analyze the generalization to complex body poses; for details on data splitting please see Appx To evaluate performance without a domain gap between train/test data, we also test all models on "AGORA-50" <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b53">55]</ref>, which contains 50 samples from AGORA that are different from the 450 used for training.</p><p>Generating synthetic data. We use the OpenGL scripts of MonoPort <ref type="bibr" target="#b34">[36]</ref> to render photo-realistic images with dynamic lighting. We render each clothed-human 3D scan (I and N c ) and their SMPL-X fits (N b ) from multiple views by using a weak perspective camera and rotating the scan in front of it. In this way we generate 138, 924 samples, each containing a 3D clothed-human scan, its SMPL-X fit, an RGB image, camera parameters, 2D normal maps for the scan and the SMPL-X mesh (from two opposite views) and SMPL-X triangle visibility information w.r.t. the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation</head><p>We use 3 evaluation metrics, described in the following: "Chamfer" distance. We report the Chamfer distance between ground-truth scans and reconstructed meshes. For this, we sample points uniformly on scans/meshes, to factor out resolution differences, and compute average bi-directional point-to-surface distances. This metric captures large geometric differences, but misses smaller geometric details.   "P2S" distance. CAPE has raw scans as ground truth, which can contain large holes. To factor holes out, we additionally report the average point-to-surface (P2S) distance from scan points to the closest reconstructed surface points. This metric can be viewed as a 1-directional version of the above metric. "Normals" difference. We render normal images for reconstructed and ground-truth surfaces from fixed viewpoints (Sec. 4.2, "generating synthetic data"), and calculate the L2 error between them. This captures errors for high-frequency geometric details, when Chamfer and P2S errors are small. A. ICON -vs-SOTA. ICON outperforms all original state-of-the-art (SOTA) methods, and is competitive to our "simulated" versions of them, as shown in Tab. 2-A. We use AGORA's SMPL-X <ref type="bibr" target="#b45">[47]</ref> ground truth (GT) as a reference. We notice that our re-implemented PaMIR * outperform the SMPL-X GT for images with in-distribution body poses ("AGORA-50" and "CAPE-FP"), However, this is not the case for images with out-of-distribution poses ("CAPE-NFP"). This shows that, although conditioned on GT SMPL-X fits, PaMIR * is still sensitive to global body pose due to its global feature encoder, and fails to generalize to out-of-distribution poses. On the contrary, ICON generalizes well to out-of-distribution poses, because its local features are independent from global pose (see Sec. 3.2).</p><p>B. Body-guided normal prediction. We evaluate the conditioning on SMPL-X-body normal maps, N b , for guiding inference of clothed-body normal maps, N c (Sec. 3.1). <ref type="table">Table 2</ref>-B shows performance with ("ICON") and without ("ICON N ? ") conditioning. With no conditioning, errors on "CAPE" increase slightly. Qualitatively, guidance by body normals heavily improves the inferred normals, especially for occluded body regions; see We use a 2-stack hourglass model <ref type="bibr" target="#b23">[25]</ref>, whose receptive field expands to 46% of the image size. This takes a large image area into account and produces features sensitive to global body pose. This worsens reconstruction performance for out-of-distribution poses, such as in "CAPE-NFP". For an evaluation of PaMIR's receptive field size, see Appx</p><p>We compare ICON to state-of-the-art (SOTA) models for a varying amount of training data in <ref type="figure">Fig. 6</ref>. The "Dataset scale" axis reports the data size as the ratio w.r.t. the 450 scans of the original PIFu methods <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b53">55]</ref>; the left-most side corresponds to 56 scans and the right-most side corresponds to 3, 709 scans, i.e., all the scans of AGORA <ref type="bibr" target="#b45">[47]</ref> and THuman <ref type="bibr" target="#b69">[71]</ref>. ICON consistently outperforms all methods. Importantly, ICON achieves SOTA performance even when trained on just a fraction of the data. We attribute this to the local nature of ICON's point features; this helps ICON generalize well in the pose space and be data efficient.  <ref type="figure">Figure 6</ref>. Reconstruction error w.r.t. training-data size. "Dataset size" is defined as the ratio w.r.t. the 450 scans used in <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b53">55]</ref>. The "8x" setting is all 3, 709 scans of AGORA <ref type="bibr" target="#b45">[47]</ref> and THuman <ref type="bibr" target="#b69">[71]</ref>.</p><p>D. Robustness to SMPL-X noise. SMPL-X estimated from an image might not be perfectly aligned with body pixels in the image. However, PaMIR and ICON are conditioned on this estimation. Thus, they need to be robust against various noise levels in SMPL-X shape and pose. To evaluate this, we feed PaMIR * and ICON with groundtruth and perturbed SMPL-X, denoted with () and () in Tab. 2-A,D. ICON conditioned on perturbed () SMPL-X produces larger errors w.r.t. conditioning on ground truth (). However, adding the body refinement module ("ICON +BR") of Sec. 3.1, refines SMPL-X and improves performance. As a result, "ICON +BR" conditioned on noisy SMPL-X () performs comparably to PaMIR * conditioned on ground-truth SMPL-X (); it is slightly worse/better for in-/out-of-distribution poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Applications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Reconstruction from in-the-wild images</head><p>We collect 200 in-the-wild images from Pinterest that show people performing parkour, sports, street dance, and kung fu. These images are unseen during training. We show qualitative results for ICON in <ref type="figure">Fig. 8a</ref> and comparisons to SOTA in <ref type="figure" target="#fig_3">Fig. 2</ref>; for more results see our video and Appx</p><p>To evaluate the perceived realism of our results, we compare ICON to PIFu * , PaMIR * , and the original PIFuHD <ref type="bibr" target="#b53">[55]</ref> in a perceptual study. ICON, PIFu * and PaMIR * are trained on all 3, 709 scans of AGORA <ref type="bibr" target="#b45">[47]</ref> and THuman <ref type="bibr" target="#b69">[71]</ref> ("8x" setting in <ref type="figure">Fig. 6</ref>). For PIFuHD we use its pre-trained model. In the study, participants were shown an image and either a rendered result of ICON or of another method. Participants were asked to choose the result that best represents the shape of the human in the image. We report the percentage of trails in which participants preferred the baseline methods over ICON in Tab. 3; p-values correspond to the null-hypothesis that two methods perform equally well. For details on the study, example stimuli, catch trials, etc. see Appx</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loose Clothes</head><p>Body Fitting Failure Unseen Camera <ref type="figure">Figure 7</ref>. Failure cases of ICON for extreme clothing, pose, or camera view. We show the front (blue) and rotated (bronze) views.  <ref type="table">Table 3</ref>. Perceptual study. Numbers denote the chance that participants prefer the reconstruction of a competing method over ICON for in-the-wild images. ICON is judged significantly more realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Animatable avatar creation from video</head><p>Given a sequence of images with the same subject in various poses, we create an animatable avatar with the help of SCANimate <ref type="bibr" target="#b54">[56]</ref>. First, we use ICON to reconstruct a 3D clothed-human mesh per frame. Then, we feed these meshes to SCANimate. ICON's robustness to diverse poses enables us to learn a clothed avatar with pose-dependent clothing deformation. Unlike raw 3D scans, which are taken with multi-view systems, ICON operates on a single image and its reconstructions are more reliable for observed body regions than for occluded ones. Thus, we reformulate the loss of SCANimate to downweight occluded regions depending on camera viewpoint. Results are shown in <ref type="figure">Fig. 1 and Fig. 8b</ref>; for animations see the video on our webpage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented ICON, which robustly recovers a 3D clothed person from a single image with accuracy and realism that exceeds prior art. There are two keys: (1) Regularizing the solution with a 3D body model while optimizing that body model iteratively. <ref type="bibr" target="#b0">(2)</ref> Using local features to eliminate spurious correlations with global pose. Thorough ablation studies validate these choices. The quality of results is sufficient to build a 3D avatar from monocular image sequences. Limitations and future work. Due to the strong body prior exploited by ICON, loose clothing that is far from the body may be difficult to reconstruct; see <ref type="figure">Fig. 7</ref>. Although ICON is robust to small errors of body fits, significant failure of body fits leads to reconstruction failure. Because it is trained on orthographic views, ICON has trouble with strong perspective effects, producing asymmetric limbs or anatomically improbable shapes. A key future application is to use images alone to create a dataset of clothed avatars. Such a dataset could advance research in human shape generation <ref type="bibr" target="#b13">[15]</ref>, be valuable to fashion industry, and facilitate graphics applications.  <ref type="figure">Figure 8</ref>. ICON results for two applications (Sec. 5). We show two views for each mesh, i.e., a front (blue) and a rotated (bronze) view.</p><p>Possible negative impact. While the quality of virtual humans created from images is not at the level of facial "deep fakes", as this technology matures, it will open up the possibility for full-body deep fakes, with all the attendant risks. These risks must also be balanced by the positive use cases in entertainment, tele-presence, and future metaverse applications. Clearly regulation will be needed to establish legal boundaries for its use. In lieu of societal guidelines today, we have made our code available with an appropriate license. Disclosure. https://files.is.tue.mpg.de/black/CoI CVPR 2022.txt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>We provide more details for the method and experiments, as well as more quantitative and qualitative results, as an extension of Sec. 3, Sec. 4 and Sec. 5 of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Method &amp; Experiment Details</head><p>A.1. Dataset (Sec. 4.2) Dataset size. We evaluate the performance of ICON and SOTA methods for a varying training-dataset size ( <ref type="figure">Fig. 6</ref> and Tab. 9). For this, we first combine AGORA <ref type="bibr" target="#b45">[47]</ref> (3, 109 scans) and THuman <ref type="bibr" target="#b69">[71]</ref> (600 scans) to get 3, 709 scans in total. This new dataset is 8x times larger than the 450 Renderpeople ("450-Rp") scans used in <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b53">55]</ref>. Then, we sample this "8x dataset" to create smaller variations, for 1/8x, 1/4x, 1/2x, 1x, and 8x the size of "450-Rp". Dataset splits. For the "8x dataset", we split the 3, 109 AGORA scans into a new training set (3, 034 scans), validation set (25 scans) and test set (50 scans). Among these, 1, 847 come from Renderpeople <ref type="bibr" target="#b1">[3]</ref> (see <ref type="figure" target="#fig_6">Fig. 10a</ref>), 622 from AXYZ <ref type="bibr" target="#b6">[8]</ref>, 242 from Humanalloy <ref type="bibr" target="#b0">[2]</ref>, 398 from 3DPeople [1], and we sample only 600 scans from THuman (see <ref type="figure" target="#fig_6">Fig. 10b</ref>), due to its high pose repeatability and limited identity variants (see Tab. 1), with the "select-cluster" scheme described below. These scans, as well as their SMPL-X fits, are rendered after every 10 degrees rotation around the yaw axis, to totally generate (3109 AGORA + 600 THuman + 150 CAPE) ? 36 = 138, 924 samples. Dataset distribution via "select-cluster" scheme. To create a training set with a rich pose distribution, we need to select scans from various datasets with poses different from AGORA. Following SMPLify <ref type="bibr" target="#b11">[13]</ref>, we first fit a Gaussian Mixture Model (GMM) with 8 components to all AGORA poses, and select 2K THuman scans with low likelihood. Then, we apply M-Medoids (n cluster = 50) on these selections for clustering, and randomly pick 12 scans per cluster, collecting 50?12 = 600 THuman scans in total; see <ref type="figure" target="#fig_6">Fig. 10b</ref>. This is also used to split CAPE into "CAPE-FP" <ref type="figure" target="#fig_6">(Fig. 10c</ref>) and "CAPE-NFP" <ref type="figure" target="#fig_6">(Fig. 10d</ref>), corresponding to scans with poses similar (in-distribution poses) and dissimilar (out-of-distribution poses) to AGORA ones, respectively. Perturbed SMPL. To perturb SMPL's pose and shape parameters, random noise is added to ? and ? by:</p><formula xml:id="formula_10">? += s ? * ?, ? += s ? * ?,<label>(8)</label></formula><p>where ? ? [?1, 1], s ? = 0.15 and s ? = 0.5. These are set empirically to mimic the misalignment error typically caused by off-the-shell HPS during testing.</p><p>Discussion on simulated data. The wide and loose clothing in CLOTH3D++ <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b41">43]</ref> demonstrates strong dynamics, which would complement commonly used datasets of commercial scans. Yet, the domain gap between CLOTH3D++ and real images is still large. Moreover, it is unclear how to train an implicit function from multi-layer non-watertight meshes. Consequently, we leave it for future research.</p><p>A.2. Refining SMPL (Sec. 3.1) <ref type="figure">Figure 9</ref>. SMPL refinement error (y-axis) with different losses (see colors) and noise levels, s ? , of pose parameters (x-axis).</p><p>To statistically analyze the necessity of L N diff and L S diff in Eq. (4), we do a sanity check on AGORA's validation set. Initialized with different pose noise, s ? (Eq. (8)), we optimize the {?, ?, t} parameters of the perturbed SMPL by minimizing the difference between rendered SMPL-body normal maps and ground-truth clothed-body normal maps for 2K iterations. As <ref type="figure">Fig. 9</ref> shows, L N diff + L S diff always leads to the smallest error under any noise level, measured by the Chamfer distance between the optimized perturbed SMPL mesh and the ground-truth SMPL mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Perceptual study (Tab. 3)</head><p>Reconstruction on in-the-wild images. We perform a perceptual study to evaluate the perceived realism of the reconstructed clothed 3D humans from in-the-wild images. ICON is compared against 3 methods, PIFu <ref type="bibr" target="#b52">[54]</ref>, PIFuHD <ref type="bibr" target="#b53">[55]</ref>, and PaMIR <ref type="bibr" target="#b68">[70]</ref>. We create a benchmark of 200 unseen images downloaded from the internet, and apply all the methods on this test set. All the reconstruction results are evaluated on Amazon Mechanical Turk (AMT), where each participant is shown pairs of reconstructions from ICON and one of the baselines, see <ref type="figure" target="#fig_7">Fig. 11</ref>. Each reconstruction result is rendered in four views: front, right, back and left. Participants are asked to choose the reconstructed 3D shape that better represents the human in the given color image. Each participant is given 100 samples to evaluate. To teach participants, and to filter out the ones that do not understand the task, we set up 1 tutorial sample, followed by 10 warm-up samples, and then the evaluation samples along with catch  <ref type="table" target="#tab_9">Table 4</ref>. Quantitative errors (cm) for several ICON variants conditioned on perturbed SMPL-X fits (s ? = 0.15, s ? = 0.5).</p><p>trial samples inserted every 10 evaluation samples. Each catch trial sample shows a color image along with either (1) the reconstruction of a baseline method for this image and the ground-truth scan that was rendered to create this image, or (2) the reconstruction of a baseline method for this image and the reconstruction for a different image (false positive), see <ref type="figure" target="#fig_7">Fig. 11c</ref>. Only participants that pass 70% out of 10 catch trials are considered. This leads to 28 valid participants out of 36 ones. Results are reported in Tab. 3. Normal map prediction. To evaluate the effect of the body prior for normal map prediction on in-the-wild images, we conduct a perceptual study against prediction without the body prior. We use AMT, and show participants a color image along with a pair of predicted normal maps from two methods. Participants are asked to pick the normal map that better represents the human in the image. Front-and backside normal maps are evaluated separately. See <ref type="figure" target="#fig_3">Fig. 12</ref> for some samples. We set up 2 tutorial samples, 10 warm-up samples, 100 evaluation samples and 10 catch trials for each subject. The catch trials lead to 20 valid subjects out of 24 participants. We report the statistical results in Tab. 5. A chi-squared test is performed with a null hypothesis that the body prior does not have any influence. We show some results in <ref type="figure" target="#fig_0">Fig. 13</ref>, where all participants unanimously prefer one method over the other. While results of both methods look generally similar on front-side normal maps, using the body prior usually leads to better back-side normal maps.   Test-time details. During inference, to iteratively refine SMPL and the predicted clothed-body normal maps, we perform 50 iterations (each iteration takes ? 460 ms on a Quadro RTX 5000 GPU) and set ? N = 2.0 in Eq. (4). We conduct an experiment to show the influence of the number of iterations (#iterations) on accuracy, see Tab. 7. The resolution of the queried occupancy space is 256 3 . We use rembg 1 to segment the humans in in-the-wild images, and use Kaolin 2 to compute per-point the signed distance, F s , and barycentric surface normal, F b n . Discussion on receptive field size. As Tab. 8 shows, simply reducing the size of receptive field of PaMIR does not lead to better performance. This shows that our informative 3D features as in Eq. (6) and normal maps N c also play important roles for robust reconstruction. A more sophisticated design of smaller receptive field may lead to better performance and we would leave it for future research.</p><p>B. More Quantitative Results (Sec. 4.3)  <ref type="table">Table 9</ref>. Reconstruction error (cm) w.r.t. training-data size. "Training set scale" is defined as the ratio w.r.t. the 450 scans used in <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b53">55]</ref>. The "8x" setting is all 3, 709 scans of AGORA <ref type="bibr" target="#b45">[47]</ref> and THuman <ref type="bibr" target="#b69">[71]</ref>. Results outperform ground-truth SMPL-X, which has 1.158 cm and 1.125 cm for Chamfer and P2S in Tab. 2.</p><p>C. More Qualitative Results (Sec. 5) <ref type="figure" target="#fig_2">Figures 14 to 16</ref> show reconstructions for in-the-wild images, rendered from four different view points; normals are color coded. <ref type="figure">Figure 17</ref> shows reconstructions for images with out-of-frame cropping. <ref type="figure">Figure 18</ref>     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>ICON's architecture contains two main modules for: (1) body-guided normal prediction, and (2) local-feature based implicit 3D reconstruction. The dotted line with an arrow is a 2D or 3D query function. The two G N networks (purple/orange) have different parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>SMPL refinement using a feedback loop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 .</head><label>2</label><figDesc>Quantitative evaluation (cm) for: (A) performance w.r.t. SOTA; (B) body-guided normal prediction; (C) local-feature based implicit reconstruction; and (D) robustness to SMPL-X noise. Inference conditioned on: () SMPL-X ground truth (GT); () perturbed SMPL-X GT; () no SMPL-X condition. SMPL-X ground truth is provided by each dataset. CAPE is not used for training, and tests generalizability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Figure 5 .</head><label>55</label><figDesc>We also ablate the effect of the body-normal feature (Sec. 3.2), F b n , by removing it; this worsens results, see "ICON w/o F b n " in Tab. 2-B. C. Local-feature based implicit reconstruction. To evaluate the importance of our "local" features (Sec. 3.2), F P , we replace them with "global" features produced by 2D Normal prediction ( N c ) w/ and w/o SMPL prior (N b ). convolutional filters. These are applied on the image and the clothed-body normal maps ("ICON enc(I, N c ) " in Tab. 2-C), or only on the normal maps ("ICON enc( N c ) " in Tab. 2-C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) ICON reconstructions for in-the-wild images with extreme poses (Sec. 5.1).(b) Avatar creation from images with SCANimate (Sec. 5.2). The input per-frame meshes are reconstructed with ICON.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>shows additional representative failures. The video on our website shows animation examples created with ICON and SCANimate.(a) Renderpeople [3] (450 scans) (b) THuman [71] (600 scans) (c) "CAPE-FP" [41] (fashion poses, 50 scans) (d) "CAPE-NFP" [41] (non fashion poses, 100 scans) Representative poses for different datasets.(a) A tutorial sample. (b) An evaluation sample. (c) Two samples of catch trials. Left: result from this image (top) vs from another image (bottom). Right: ground-truth (top) vs reconstruction mesh (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Some samples in the perceptual study to evaluate reconstructions on in-the-wild images. (a) The two tutorial samples. (b) Two evaluation samples. (c) Two catch trial samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Some samples in the perceptual study to evaluate the effect of the body prior for normal prediction on in-the-wild images. Examples of perceptual preference on front normal maps. Unanimously preferred results are in ? ? ? black boxes . The back normal maps are for reference. Examples of perceptual preference on back normal maps. Unanimously preferred results are in ? ? ? black boxes . The front normal maps are for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .Figure 14 .Figure 15 .Figure 16 .Figure 17 .Figure 18 .</head><label>131415161718</label><figDesc>Qualitative results to evaluate the effect of body prior for normal prediction on in-the-wild images. Qualitative comparison of reconstruction for ICON vs SOTA. Four view points are shown per result. Qualitative comparison of reconstruction for ICON vs SOTA. Four view points are shown per result. Qualitative comparison of reconstruction for ICON vs SOTA. Four view points are shown per result. Qualitative comparison (ICON vs SOTA) on images with out-of-frame cropping. More failure cases of ICON.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Datasets for 3D clothed humans. Gray color indicates datasets used by ICON. The bottom "number of scans" row indicates the number of scans each method uses. The cell format is number of scans [method]. ICON is denoted as [IC]. The symbol ? corresponds to the "8x" setting in Fig. 6.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Train &amp; Validation Sets</cell><cell></cell><cell>Test Set</cell></row><row><cell></cell><cell>Renderp.</cell><cell>Twindom</cell><cell>AGORA</cell><cell>THuman</cell><cell>BUFF</cell><cell>CAPE</cell></row><row><cell></cell><cell>[3]</cell><cell>[61]</cell><cell>[47]</cell><cell>[71]</cell><cell>[67]</cell><cell>[41, 50]</cell></row><row><cell>Free &amp; public</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Diverse poses</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Diverse identities</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SMPL(-X) poses</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>High-res texture</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">450 [54, 55] 1000 [70]</cell><cell>450 [IC]</cell><cell>600 [IC  ? ]</cell><cell cols="2">5 [54, 55] 150 [IC]</cell></row><row><cell>Number of scans</cell><cell>375 [24]</cell><cell></cell><cell>3109 [IC  ? ]</cell><cell>600 [70]</cell><cell>26 [24]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>300 [36, 70]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Chamfer ? P2S ? Normals ? Chamfer ? P2S ? Normals ? Chamfer ? P2S ? Normals ? Chamfer ? P2S ? Normals ?</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell>SMPL-X</cell><cell></cell><cell>AGORA-50</cell><cell></cell><cell></cell><cell>CAPE-FP</cell><cell></cell><cell></cell><cell>CAPE-NFP</cell><cell></cell><cell></cell><cell>CAPE</cell></row><row><cell cols="4">condition. Ours ICON</cell><cell>1.204</cell><cell>1.584</cell><cell>0.060</cell><cell>1.233</cell><cell>1.170</cell><cell>0.072</cell><cell>1.096</cell><cell>1.013</cell><cell>0.063</cell><cell>1.142</cell><cell>1.065</cell><cell>0.066</cell></row><row><cell></cell><cell>PIFu</cell><cell>[54]</cell><cell></cell><cell>3.453</cell><cell>3.660</cell><cell>0.094</cell><cell>2.823</cell><cell>2.796</cell><cell>0.100</cell><cell>4.029</cell><cell>4.195</cell><cell>0.124</cell><cell>3.627</cell><cell>3.729</cell><cell>0.116</cell></row><row><cell></cell><cell cols="2">PIFuHD [55]</cell><cell></cell><cell>3.119</cell><cell>3.333</cell><cell>0.085</cell><cell>2.302</cell><cell>2.335</cell><cell>0.090</cell><cell>3.704</cell><cell>3.517</cell><cell>0.123</cell><cell>3.237</cell><cell>3.123</cell><cell>0.112</cell></row><row><cell>A</cell><cell cols="2">PaMIR [70] SMPL-X GT</cell><cell>N/A</cell><cell>2.035 1.518</cell><cell>1.873 1.985</cell><cell>0.079 0.072</cell><cell>1.936 1.335</cell><cell>1.263 1.259</cell><cell>0.078 0.085</cell><cell>2.216 1.070</cell><cell>1.611 1.058</cell><cell>0.093 0.068</cell><cell>2.122 1.158</cell><cell>1.495 1.125</cell><cell>0.088 0.074</cell></row><row><cell></cell><cell>PIFu  *</cell><cell></cell><cell></cell><cell>2.688</cell><cell>2.573</cell><cell>0.097</cell><cell>2.100</cell><cell>2.093</cell><cell>0.091</cell><cell>2.973</cell><cell>2.940</cell><cell>0.111</cell><cell>2.682</cell><cell>2.658</cell><cell>0.104</cell></row><row><cell></cell><cell>PaMIR  *</cell><cell></cell><cell></cell><cell>1.401</cell><cell>1.500</cell><cell>0.063</cell><cell>1.225</cell><cell>1.206</cell><cell>0.055</cell><cell>1.413</cell><cell>1.321</cell><cell>0.063</cell><cell>1.350</cell><cell>1.283</cell><cell>0.060</cell></row><row><cell>B</cell><cell cols="2">ICON N  ? ICON w/o F b n</cell><cell></cell><cell>1.153 1.259</cell><cell>1.545 1.667</cell><cell>0.057 0.062</cell><cell>1.240 1.344</cell><cell>1.226 1.336</cell><cell>0.069 0.072</cell><cell>1.114 1.180</cell><cell>1.097 1.172</cell><cell>0.062 0.064</cell><cell>1.156 1.235</cell><cell>1.140 1.227</cell><cell>0.064 0.067</cell></row><row><cell>C</cell><cell cols="2">ICON enc(I, N c ) ICON enc( N c )</cell><cell></cell><cell>1.172 1.180</cell><cell>1.350 1.450</cell><cell>0.053 0.055</cell><cell>1.243 1.202</cell><cell>1.243 1.196</cell><cell>0.062 0.061</cell><cell>1.254 1.180</cell><cell>1.122 1.067</cell><cell>0.060 0.059</cell><cell>1.250 1.187</cell><cell>1.229 1.110</cell><cell>0.061 0.060</cell></row><row><cell></cell><cell>ICON</cell><cell></cell><cell></cell><cell>1.583</cell><cell>1.987</cell><cell>0.079</cell><cell>1.364</cell><cell>1.403</cell><cell>0.080</cell><cell>1.444</cell><cell>1.453</cell><cell>0.083</cell><cell>1.417</cell><cell>1.436</cell><cell>0.082</cell></row><row><cell>D</cell><cell cols="2">ICON + BR PaMIR  *</cell><cell></cell><cell>1.554 1.674</cell><cell>1.961 1.802</cell><cell>0.074 0.075</cell><cell>1.314 1.608</cell><cell>1.356 1.625</cell><cell>0.070 0.072</cell><cell>1.351 1.803</cell><cell>1.390 1.764</cell><cell>0.073 0.079</cell><cell>1.339 1.738</cell><cell>1.378 1.718</cell><cell>0.072 0.077</cell></row><row><cell></cell><cell cols="2">SMPL-X perturbed</cell><cell>N/A</cell><cell>1.984</cell><cell>2.471</cell><cell>0.098</cell><cell>1.488</cell><cell>1.531</cell><cell>0.095</cell><cell>1.493</cell><cell>1.534</cell><cell>0.098</cell><cell>1.491</cell><cell>1.533</cell><cell>0.097</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>P2S ? Normal ? Chamfer ? P2S ? Normal ? Chamfer ? P2S ? Normal ? Chamfer ? P2S ? Normal ?</figDesc><table><row><cell></cell><cell>Methods</cell><cell>SMPL-X</cell><cell cols="2">AGORA-50</cell><cell></cell><cell></cell><cell>CAPE-FP</cell><cell></cell><cell></cell><cell>CAPE-NFP</cell><cell></cell><cell></cell><cell>CAPE</cell></row><row><cell></cell><cell cols="3">condition. Chamfer ? ICON 1.583</cell><cell>1.987</cell><cell>0.079</cell><cell>1.364</cell><cell>1.403</cell><cell>0.080</cell><cell>1.444</cell><cell>1.453</cell><cell>0.083</cell><cell>1.417</cell><cell>1.436</cell><cell>0.082</cell></row><row><cell></cell><cell>SMPL-X perturbed</cell><cell>1.984</cell><cell></cell><cell>2.471</cell><cell>0.098</cell><cell>1.488</cell><cell>1.531</cell><cell>0.095</cell><cell>1.493</cell><cell>1.534</cell><cell>0.098</cell><cell>1.491</cell><cell>1.533</cell><cell>0.097</cell></row><row><cell>D</cell><cell>ICON enc(I,N) ICON enc(N)</cell><cell>1.569 1.564</cell><cell></cell><cell>1.784 1.854</cell><cell>0.073 0.074</cell><cell>1.379 1.368</cell><cell>1.498 1.484</cell><cell>0.070 0.071</cell><cell>1.600 1.526</cell><cell>1.580 1.524</cell><cell>0.078 0.078</cell><cell>1.526 1.473</cell><cell>1.553 1.511</cell><cell>0.075 0.076</cell></row><row><cell></cell><cell>ICON N  ?</cell><cell>1.575</cell><cell></cell><cell>2.016</cell><cell>0.077</cell><cell>1.376</cell><cell>1.496</cell><cell>0.076</cell><cell>1.458</cell><cell>1.569</cell><cell>0.080</cell><cell>1.431</cell><cell>1.545</cell><cell>0.079</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Perceptual study on normal prediction. Our body-guided normal prediction network uses the same architecture as PIFuHD<ref type="bibr" target="#b53">[55]</ref>, originally proposed in<ref type="bibr" target="#b26">[28]</ref>, and consisting of residual blocks with 4 down-sampling layers. The image encoder for PIFu * , PaMIR * , and ICON enc is a stacked hourglass<ref type="bibr" target="#b43">[45]</ref> with 2 stacks, modified according to<ref type="bibr" target="#b23">[25]</ref>. Tab. 6 lists feature dimensions for various methods; "total dims" is the neuron number for the first MLP layer (input). The number of neurons in each MLP layer is: 13 (7 for ICON), 512, 256, 128, and 1, with skip connections at the 3rd, 4th, and 5th layers.</figDesc><table><row><cell>A.4. Implementation details (Sec. 4.1)</cell></row><row><cell>Network architecture.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .Table 8 .</head><label>68</label><figDesc>Feature dimensions for various approaches. "pixel dims" and "point dims" denote the feature dimensions encoded from pixels (image/normal maps) and 3D body prior, respectively. PaMIR's receptive field Training details. For training G N we do not use THuman due to its low-quality texture (see Tab. 1). On the contrary, IF is trained on both AGORA and THuman. The front-side and back-side normal prediction networks are trained individually with batch size of 12 under the objective function defined in Eq. (3), where we set ? VGG = 5.0. We use the ADAM optimizer with a learning rate of 1.0 ? 10 ?4 until convergence at 80 epochs.</figDesc><table><row><cell># iters (460ms/it)</cell><cell>0</cell><cell>10</cell><cell>50</cell><cell cols="2">Receptive field 139 271 403</cell></row><row><cell>Chamfer ?</cell><cell cols="3">1.417 1.413 1.339</cell><cell>Chamfer ?</cell><cell>1.418 1.478 1.366</cell></row><row><cell>P2S ?</cell><cell cols="3">1.436 1.515 1.378</cell><cell>P2S ?</cell><cell>1.236 1.320 1.214</cell></row><row><cell>Normal ?</cell><cell cols="3">0.082 0.077 0.074</cell><cell>Normal ?</cell><cell>0.083 0.084 0.078</cell></row><row><cell cols="4">Table 7. ICON errors w.r.t. iterations</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 compares</head><label>4</label><figDesc>several ICON variants conditioned on perturbed SMPL-X meshes. For the plot of Fig. 6 of the main paper (reconstruction error w.r.t. training-data size), extended quantitative results are shown in Tab. 9.</figDesc><table><row><cell cols="2">Training set scale</cell><cell>1/8x</cell><cell>1/4x</cell><cell>1/2x</cell><cell>1x</cell><cell>8x</cell></row><row><cell>PIFu  *</cell><cell cols="5">Chamfer ? 3.339 2.968 2.932 2.682 P2S ? 3.280 2.859 2.812 2.658</cell><cell>1.760 1.547</cell></row><row><cell cols="6">PaMIR  *  Chamfer ? 2.024 1.780 1.479 1.350 P2S ? 1.791 1.778 1.662 1.283</cell><cell>1.095 1.131</cell></row><row><cell>ICON</cell><cell cols="4">Chamfer ? 1.336 1.266 1.219 P2S ? 1.286 1.235 1.184</cell><cell>1.142 1.065</cell><cell>1.036 1.063</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/danielgatis/rembg 2 https://github.com/NVIDIAGameWorks/kaolin</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">HumanAlloy. humanalloy.com</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">RenderPeople. renderpeople.com</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to reconstruct people in clothing from a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><forename type="middle">Lal</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1175" to="1186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detailed human avatars from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="98" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video based reconstruction of 3D people models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tex2Shape: Detailed full human body geometry from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Magnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">AXYZ. secure.axyz-design.com</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CLOTH3D: Clothed 3D humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Bertiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meysam</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">12365</biblScope>
			<biblScope unit="page" from="344" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining implicit function learning and parametric models for 3D human reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bharat Lal Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12347</biblScope>
			<biblScope unit="page" from="311" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LoopReg: Self-supervised learning of implicit surface correspondences, pose and shape for 3D human mesh registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bharat Lal Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-Garment Net: Learning to dress 3D people from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garvita</forename><surname>Bharat Lal Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5419" to="5429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9909</biblScope>
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural deformation graphs for globally-consistent non-rigid reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljaz</forename><surname>Bozic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">R</forename><surname>Palafox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1450" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">gDNA: Towards generative detailed neural avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SNARF: Differentiable forward skinning for animating non-rigid neural implicit shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11594" to="11604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5939" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate 3D body shape regression via linguistic attributes and anthropometric measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural Articulated Shape Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Jeruzalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12352</biblScope>
			<biblScope unit="page" from="612" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PINA: Learning a personalized implicit neural avatar from a single RGB-D video sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collaborative regression of expressive bodies using moderation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="792" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geo-PIFu: Geometry and pixel aligned implicit functions for single-view human reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">P</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ARCH++: Animation-ready clothed human reconstruction revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11046" to="11056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ARCH: Animatable reconstruction of clothed humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D human body reconstruction from a single image via volumetric regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manafas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan Roth Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops (ECCVw)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11132</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning high fidelity depths of dressed humans by watching social media dance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasamin</forename><surname>Jafarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="12753" to="12762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BCNet: Learning body and cloth shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="18" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Total capture: A 3D deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8320" to="8329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5252" to="5262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PARE: Part attention regressor for 3D human body estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11127" to="11137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">360-Degree textures of people in clothing from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verica</forename><surname>Lazova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Volumetric human teleportation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno>1-1. 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2020 Real-Time Live</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Monocular real-time volumetric performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12368</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust 3D self-portraits in Seconds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>248:1-248:16</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Marching cubes: A high resolution 3D surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harvey</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="163" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SCALE: Modeling clothed humans with a surface codec of articulated local elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16082" to="16093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to dress 3D people in generative clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The power of points for modeling humans in clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10974" to="10984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning cloth dynamics: 3D + texture garment reconstruction benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meysam</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Bertiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wafa</forename><surname>Bouzouita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NeurIPS 2020 Competition and Demonstration Track</title>
		<meeting>the NeurIPS 2020 Competition and Demonstration Track</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="57" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3D reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DeepSDF: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">AGORA: Avatars in geography optimized for regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">T</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pifuhd Code On Github</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<ptr target="https://github.com/facebookresearch/pifuhd" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ClothCap: seamless 4D clothing capture and retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonny</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>73:1-73:15</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08501,2020.3</idno>
		<title level="m">Accelerating 3D deep learning with PyTorch3D</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Rembg: A tool to remove images background</title>
		<idno>2022. 4</idno>
		<ptr target="https://github.com/danielgatis/rembg" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>245:1-245:17</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">PIFu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">PIFuHD: Multi-level pixel-aligned implicit function for high-resolution 3D human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">SCANimate: Weakly supervised learning of skinned clothed avatar networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">FACSIMILE: Fast and accurate scans from an image in less than a second</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Mavroidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5330" to="5339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Putting people in their place: Monocular regression of 3D people in depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Siyu Zhu, and Ping Tan. A neural network for detailed human depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feitong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7750" to="7759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Neural-GIF: Neural generalized implicit functions for animating people in clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garvita</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11708" to="11718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Twindom. twindom.com</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">MetaAvatar: Learning animatable clothed human models from few depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Mihajlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">MonoClothCap: Towards temporally coherent clothing capture from monocular RGB video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Prada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">GHUM &amp; GHUML: Generative 3D human shape and articulated pose models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6183" to="6192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">S3: Neural shape, skeleton, and skinning fields for 3D human modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivabalan</forename><surname>Manivasagam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Human-aware object placement for visual environment reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Detailed, accurate, human shape estimation from clothed 3D scan sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5484" to="5493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11446" to="11456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">DeepMultiCap: Performance capture of multiple characters using sparse multiview cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6239" to="6249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">PaMIR: Parametric model-conditioned implicit representation for image-based human reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">DeepHuman: 3D human reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Detailed human shape estimation from a single image by hierarchical mesh deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
