<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ReSSL: Relational Self-Supervised Learning with Weak Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Zheng</surname></persName>
							<email>zhengmingkai@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
							<email>youshan@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">Tsinghua University (THUAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
							<email>wangfei@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<email>qianchen@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">Tsinghua University (THUAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
							<email>c.xu@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sensetime</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">ReSSL: Relational Self-Supervised Learning with Weak Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most of methods mainly focus on the instance level information (i.e., the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduced a novel SSL paradigm, which we term as relational self-supervised learning (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as relation metric, which is thus utilized to match the feature embeddings of different augmentations. Moreover, to boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum strategy for practical efficiency. Experimental results show that our proposed ReSSL significantly outperforms the previous stateof-the-art algorithms in terms of both performance and training efficiency. Code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, self-supervised learning (SSL) has shown its superiority and achieved promising results for unsupervised visual representation learning in computer vision tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. The purpose of a typical self-supervised learning algorithm is to learn general visual representations from a large amount of data without human annotations, which can be transferred or leveraged in downstream tasks (e.g., classification, detection, and segmentation). Some previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> even have proven that a good unsupervised pretraining can lead to a better downstream performance than supervised pretraining.</p><p>Among various SSL algorithms, contrastive learning <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b6">7]</ref> serves as a state-of-the-art framework, which mainly focuses on learning an invariant feature from different views. For example, instance discrimination is a widely adopted pre-text task as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46]</ref>, which utilizes the noisy contrastive estimation (NCE) to encourage two augmented views of the same image to be pulled closer on the embedding space but pushes apart all the other images away. Deep Clustering <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b5">6]</ref> is an alternative pre-text task that forces different augmented views of the same instance to be clustered into the same class. However, instance discrimination based methods will inevitably induce a class collision problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b10">11]</ref>, where similar images should be pulled closer instead of being pushed away. Deep clustering based methods cooperated with traditional clustering algorithms to assign a label for each instance, which relaxed the constraint of instance discrimination, but most of these algorithms adopt a strong assumption, i.e., the labels must induce an equipartition of the data, which might introduce some noise and hurt the learned representations.</p><p>In this paper, we introduce a novel Relational Self-Supervised Learning framework (ReSSL), which does not encourage explicitly to push away different instances, but uses relation as a manner to investigate the inter-instance relationships and highlight the intra-instance invariance. Concretely, we aim to maintain the consistency of pairwise similarities among different instances for two different augmentations. For example, if we have three instances x 1 , x 2 , y and z where x 1 , x 2 are two different augmentations of x, y and z are different samples. Then, if x 1 is similar to y but different to z, we wish x 2 can maintain such relationship and vice versa. In this way, the relation can be modelled as a similarity distribution between a set of augmented images, and then use it as a metric to align the same images with different augmentations, so that the relationship between different instances could be maintained across different views.</p><p>However, this simple manner induces unexpectedly horrible performance if we follow the same training recipe as other contrastive learning methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>. We argue that construction of a proper relation matters for ReSSL; aggressive data augmentations as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41]</ref> are usually leveraged by default to generate diverse positive pairs that increase the difficulty of the pre-text task. However, this hurts the reliability of the target relation. Views generated by aggressive augmentations might cause the loss of semantic information, so the target relation might be noisy and not that reliable. In this way, we propose to leverage weaker augmentations to represent the relation, since much lesser disturbances provide more stable and meaningful relationships between different instances. Besides, we also sharpen the target distribution to emphasize the most important relationship and utilize the memory buffer with a momentum-updated network to reduce the demand of large batch size for more efficiency. Experimental results on multiple benchmark datasets show the superiority of ReSSL in terms of both performance and efficiency. For example, with 200 epochs of pre-training, our ReSSL achieved 69.9% on ImageNet <ref type="bibr" target="#b14">[15]</ref> linear evaluation protocol, which is 2.4% higher than our baseline method (MoCoV2 <ref type="bibr" target="#b8">[9]</ref>). When working with the Multi-Crop strategy (200 epochs), ReSSL achieved new state-of-the-art 74.7% Top-1 accuracy, which is 1.4% higher than CLSA-Multi <ref type="bibr" target="#b44">[45]</ref>.</p><p>Our contributions can be summarized as follows.</p><p>? We proposed a novel SSL paradigm, which we term it as relational self-supervised learning (ReSSL). ReSSL maintains the relational consistency between the instances under different augmentations instead of explicitly pushing different instances away.</p><p>? Our proposed weak augmentation and sharpening distribution strategy provide a stable and high quality target similarity distribution, which makes the framework works well.</p><p>? ReSSL is a simple and effective SSL framework since it replaces the widely adopted contrastive loss with our proposed relational consistency loss. It achieved state-of-the-art performance under the same training cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Self-Supervised Learning. Early works in self-supervised learning methods rely on all sorts of pretext to learn visual representations. For example, colorizing gray-scale images <ref type="bibr" target="#b48">[49]</ref>, image jigsaw puzzle <ref type="bibr" target="#b37">[38]</ref>, image super-resolution <ref type="bibr" target="#b31">[32]</ref>, image inpainting <ref type="bibr" target="#b18">[19]</ref>, predicting a relative offset for a pair of patches <ref type="bibr" target="#b16">[17]</ref>, predicting the rotation angle <ref type="bibr" target="#b32">[33]</ref>, and image reconstruction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>. Although these methods have shown their effectiveness, they lack the generality of the learned representations.</p><p>Instance Discrimination. The recent contrastive learning methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref> have made a lot of progress in the field of self-supervised learning. Most of the previous contrastive learning methods are based on the instance discrimination <ref type="bibr" target="#b45">[46]</ref> task in which positive pairs are defined as different views of the same image, while negative pairs are formed by sampling views from different images. SimCLR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> shows that image augmentation (e.g.Grayscale, Random Resized Cropping, Color Jittering, and Gaussian Blur), nonlinear projection head and large batch size plays a critical role in contrastive learning. Since large batch size usually requires a lot of GPU memory, which is not very friendly to most of researchers. MoCo <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref> proposed a momentum contrast mechanism that forces the query encoder to learn the representation from a slowly progressing key encoder and maintain a memory buffer to store a large number of negative samples. InfoMin <ref type="bibr" target="#b40">[41]</ref> proposed a set of stronger augmentation that reduces the mutual information between views while keeping task-relevant information intact. AlignUniform <ref type="bibr" target="#b43">[44]</ref> shows that alignment and uniformity are two critical properties of contrastive learning.</p><p>Deep Clustering. In contrast to instance discrimination which treats every instance as a distinct class, deep clustering <ref type="bibr" target="#b4">[5]</ref> adopts the traditional clustering method (e.g.KMeans) to label each image iteratively. Eventually, similar samples will be clustered into the same class. Simply apply the KMeans algorithm might lead to a degenerate solution where all data points are mapped to the same cluster; SeLa <ref type="bibr" target="#b46">[47]</ref> solved this issue by adding the constraint that the labels must induce equipartition of the data and proposed a fast version of the Sinkhorn-Knopp to achieve this. SwAV <ref type="bibr" target="#b5">[6]</ref> further extended this idea and proposed a scalable online clustering framework. PCL <ref type="bibr" target="#b33">[34]</ref> reveals the class collision problem and simply performed instance discrimination and unsupervised clustering simultaneously; although it gets the same linear classification accuracy with MoCoV2, it has better performance on downstream tasks.</p><p>Contrastive Learning Without Negatives. Most previous contrastive learning methods prevent the model collapse in an explicit manner (e.g.push different instances away from each other or force different instances to be clustered into different groups.) BYOL <ref type="bibr" target="#b22">[23]</ref> can learn a high-quality representation without negatives. Specifically, it trains an online network to predict the target network representation of the same image under a different augmented view and using an additional predictor network on top of the online encoder to avoiding the model collapse. SimSiam <ref type="bibr" target="#b9">[10]</ref> shows that simple Siamese networks can learn meaningful representations even without the use of negative pairs, large batch size, and momentum encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we will first revisit the preliminary work on contrastive learning; then, we will introduce our proposed relational self-supervised learning framework. After that, the algorithm and the implementation details will also be explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries on Self-supervised Learning</head><p>Given N unlabeled samples x, we randomly apply a composition of augmentation functions T (?) to obtain two different views x 1 and x 2 through T (x, ? 1 ) and T (x, ? 2 ) where ? is the random seed for T . Then, a convolutional neural network based encoder F(?) is employed to extract the information from these samples, i.e., h = F(T (x, ?)). Finally, a two-layer non-linear projection head g(?) is utilized to map h into embedding space, which can be written as: z = g(h). SimCLR <ref type="bibr" target="#b6">[7]</ref> and MoCo <ref type="bibr" target="#b23">[24]</ref> style framework adopt the noise contrastive estimation (NCE) objective for discriminating different instances in the dataset. Suppose z 1 i and z 2 i are the representations of two augmented views of x i and z k is a different instance. The NCE objective can be expressed by Eq.</p><p>(1), where the similarity function sim(?) represents the dot product between L 2 normalized vectors sim(u, v) = u T v/ u v and ? is the temperature parameter.</p><formula xml:id="formula_0">L N CE = ? log exp(sim(z 1 , z 2 )/? ) exp(sim(z 1 i , z 2 i )/? ) + N k=1 exp(sim(z 1 i , z k )/? ) .<label>(1)</label></formula><p>BYOL <ref type="bibr" target="#b22">[23]</ref> and SimSiam <ref type="bibr" target="#b9">[10]</ref> style framework add an additional non-linear predictor head q(?) which further maps z to p. The model will minimize the negative cosine similarity (equivalent to minimize the L2 distance) between z to p.</p><formula xml:id="formula_1">L cos = ? p 1 p 1 ? z 2 z 2 , L mse = p 1 ? z 2 2 2 .<label>(2)</label></formula><p>Tricks like stop-gradient and momentum teacher are often applied to avoid model collapsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relational Self-Supervised Learning</head><p>In classical self-supervised learning, different instances are to be pushed away from each other, and augmented views of the same instance is expected to be of exactly the same features. However, both Contrastive Augmentation</p><formula xml:id="formula_2">1 = ? t ( 1 ) 2 = ? s ( 2 )</formula><p>Weak Augmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exponential</head><p>Moving Average Memory Buffer</p><formula xml:id="formula_3">? t ( 1 ) ? s ( 2 ) z 1 z 2 Append SoftMax SoftMax Target No Gradient</formula><p>Backprop Sharper <ref type="figure">Figure 1</ref>: The overall framework of our proposed method. We adopt the student-teacher framework where the student is trained to predict the representation of the teacher, and the teacher is updated with a "momentum update" (exponential moving average) of the student. The relationship consistency is achieve by align the conditional distribution for student and teacher model. Please see more details in our method part.</p><p>constrains are too restricted because of the existence of similar samples and the distorted semantic information if aggressive augmentation is adopted. In this way, we do not encourage explicit negative instances (those to be pushed away) for each instance; instead, we leverage the pairwise similarities as a manner to explore their relationships. And we pull the features of two different augmentations in this sense of relation metric. As a result, our method relaxes both <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref>, where different instances do not always need to be pushed away from each other; and augmented views of the same instance only need to share the similar but not exactly the same features.</p><p>Concretely, given a image x in a batch of samples , two different augmented views can be obtained by x 1 = T (x, ? 1 ), x 2 = T (x, ? 2 ) and calculate the corresponds embedding z 1 = g(F(x 1 )), z 2 = g(F(x 2 )). Then, we calculate the similarities between the instances of the first augmented images. Which can be measured by sim(z 1 , z i ). A softmax layer can be adopted to process the calculated similarities, which then produces a relationship distribution:</p><formula xml:id="formula_4">p 1 i = exp(sim(z 1 , z i )/? t ) K k=1 exp(sim(z 1 , z k )/? t , )</formula><p>.</p><p>where ? t is the temperature parameter. At the same time, we can calculate the relationship between x 2 and the i-th instance as sim(z 2 , z i ). The resulting relationship distribution can be written as:</p><formula xml:id="formula_6">p 2 i = exp(sim(z 2 , z i )/? s ) K k=1 exp(sim(z 2 , z k )/? s , ) .<label>(4)</label></formula><p>where ? s is a different temperature parameter. We propose to push the relational consistency between p 1 i and p 2 i by minimizing the Kullback-Leibler divergence, which can be formulated as:</p><formula xml:id="formula_7">L relation = D KL (p 1 ||p 2 ) = H(p 1 , p 2 ) ? H(p 1 ).<label>(5)</label></formula><p>Since the p 1 will only be used as a target, the gradient will be clipped here to avoid the model collapsing, thus we only minimize the cross-entropy term H(p 1 , p 2 ) in our implementation.</p><p>More efficiency with Momentum targets. However, the quality of the target similarity distribution p 1 is crucial, to make the similarity distribution reliable and stable, we usually require a large batch size which is very unfriendly to GPU memories. To resolve this issue, we utilize a "momentum update" network as in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref>, and maintain a large memory buffer Q of K past samples {z k |k = 1, ..., K} (follow the FIFO principle) for storing the feature embeddings from the past batches, which can then be used for simulating the large batch size relationship and providing a stable similarity distribution.</p><formula xml:id="formula_8">F t ? mF t + (1 ? m)F s , g t ? mg t + (1 ? m)g s ,<label>(6)</label></formula><p>where F s and g s denote the most latest encoder and head, respectively, so we name them as the student model with a subscript s. On the other hand, F t and g t stand for ensembles of the past encoder and head, respectively, so we name them as the teacher model with a subscript t. m represents the momentum coefficient which controls how fast the teacher F t will be updated.</p><p>Sharper Distribution as Target. Note, the value of ? t has to be smaller than ? s since ? t will be used to generate the target distribution. A smaller ? will result in a "sharper" distribution which can be interpreted as highlight the most similar feature for z 1 . Align p 2 with p 1 can be regarded as pulling z 2 towards the features that are similar with z 1 .</p><p>Weak Augmentation Strategy for Teacher. To further improve the quality and stability of the target distribution, we adopt a weak augmentation strategy for the teacher model since the standard contrastive augmentation is too aggressive, which introduced too many disturbances and will mislead the student network. Please refer to more details in our empirical study.</p><p>Compare with SEED and CLSA. SEED <ref type="bibr" target="#b20">[21]</ref> follows the standard Knowledge Distillation (KD) paradigm <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref> where it aims to distill the knowledge from a larger network into a smaller architecture. The knowledge transfer happens in the same view but between different models. In our framework, we are trying to maintain the relational consistency between different augmentations; the knowledge transfer happens between different views but in the same network. CLSA <ref type="bibr" target="#b44">[45]</ref> also introduced the concept of using weak augmentation to guide a stronger augmentation. However, the "weak" augmentation in CLSA is equivalent to the "strong" augmentation in our method (We do not use any stronger augmentations such as <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>). On the other hand, CLSA still adopts the InfoNCE loss (1) for instance discrimination, where our proposed method only utilized the relational consistency loss <ref type="bibr" target="#b4">(5)</ref>. Finally, CLSA requires at least one additional sample during training, which will slow down the training speed.</p><p>Algorithm 1: Relational Self-supervised Learning with Weak Augmentation (ReSSL) Input : x: a batch of samples. T w (?): Weak augmentation function. T c (?): Contrastive augmentation function. F t and F s : the teacher and student backbone network. g t and g s : the non-linear projection head for teacher and student. Q: the memory buffer while network not converge do for i=1 to step do Fetch x from current batch B z 1 = g t (F t (T w (x, ? 1 ))); z 2 = g s (F s (T c (x, ? 2 ))); p 1 = SoftMax(z 1 Q T / ? t ); p 2 = SoftMax(z 2 Q T / ? s ) ; // Eq. (3)(4) Calculate L relation loss by CrossEntropy(p 1 , p 2 ) ; // Eq. (5) Update F s and g s with loss L relation Update F t and g t by F t ? mF t + (1 ? m)F s , g t ? mg t + (1 ? m)g s ; // Eq. (6) Update the memory buffer Q by z 1 end end Output :The well trained model F s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Study</head><p>In this section, we will empirically study our proposed method on 4 popular self-supervised learning benchmarks and compare to previous state-of-the-art algorithms (SimCLR <ref type="bibr" target="#b6">[7]</ref>, BYOL <ref type="bibr" target="#b22">[23]</ref>, SimSiam <ref type="bibr" target="#b9">[10]</ref>, MoCoV2 <ref type="bibr" target="#b8">[9]</ref>).</p><p>Small Dataset. CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b29">[30]</ref>. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. CIFAR-100 is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class.</p><p>Medium Dataset. STL-10 <ref type="bibr" target="#b11">[12]</ref> and Tiny ImageNet <ref type="bibr" target="#b0">[1]</ref>. STL10 <ref type="bibr" target="#b11">[12]</ref> dataset is composed of 96x96 resolution images of 10 classes, 5K labeled training images, 8K validation images, and 100K unlabeled images. The Tiny ImageNet dataset is composed of 64x64 resolution images of 200 classes with 100K training images and 10k validation images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We adopt the ResNet18 <ref type="bibr" target="#b24">[25]</ref> as our backbone network. Because most of our dataset contains low-resolution images, we replace the first 7x7 Conv of stride 2 with 3x3 Conv of stride 1 and remove the first max pooling operation for a small dataset. For data augmentations, we use the random resized crops (the lower bound of random crop ratio is set to 0.2), color distortion (strength=0.5) with a probability of 0.8, and Gaussian blur with a probability of 0.5. The images from the small and medium datasets will be resized to 32x32 and 64x64 resolution respectively. Our method is based on MoCoV2 <ref type="bibr" target="#b8">[9]</ref>; in order to simulate the shuffle BN trick on one GPU, we simply divide a batch of data into different groups and then calculate BN statistics within each group. The momentum value and memory buffer size are set to 0.99/0.996 and 4096/16384 for small and medium datasets respectively. Moreover, The model is trained using SGD optimizer with a momentum of 0.9 and weight decay of 5e ?4 . We linear warm up the learning rate for 5 epochs until it reaches 0.06 ? BatchSize/256, then switch to the cosine decay scheduler <ref type="bibr" target="#b34">[35]</ref>. For the implementation details of other methods, please refers to the supplementary material.</p><p>Evaluation Protocol. All the models will be trained for 200 epochs. For testing the representation quality, we evaluate the pre-trained model on the widely adopted linear evaluation protocol -We will freeze the encoder parameters and train a linear classifier on top of the average pooling features for 100 epochs. To test the classifier, we use the center crop of the test set and computes accuracy according to predicted output. We train the classifier with a learning rate of 30, no weight decay, and momentum of 0.9. The learning rate will be times 0.1 in 60 and 80 epochs. Note, for STL-10; the pretraining will be applied on both labeled and unlabeled images. During the linear evaluation, only the labeled 5K images will be used. Result. As we can see the result in <ref type="table" target="#tab_0">Table 1</ref>, our proposed method outperforms the previous method on all four benchmarks. Reminder, most of the previous method requires twice back-propagation, which results in a much higher training cost than MoCoV2 and our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Properly Sharpened Relation is A Better Target</head><p>The temperature parameter is very crucial in most contrastive learning algorithms. To verify the effective of ? s and ? t for our proposed method, we fixed ? s = 0.1 or 0.2, and sweep over ? t = {0.01, 0.02, ..., 0.07}. The result is shown in <ref type="table" target="#tab_1">Table 2</ref>. For ? t , the optimal value is either 0.04 or 0.05 across all different datasets. As we can see, the performance is increasing when we increase ? t from 0 to 0.04 and 0.05. After that, the performance will start to decrease. Note, ? t ? 0 correspond to the Top-1 or argmax operation which produce a one-hot distribution as the target. On the other hand, when ? t ? 0.1, the target will be a much flatter distribution that cannot highlight the most similar features for students. Hence, ? t can not be either too small or too large, but it has to be smaller than ? s (p 1 has to be sharper than p 2 ) so that the target distribution can provide effective guidance to the student model. For ? t , it is clearly to see that the result of ? s = 0.1 can always result a much higher performance than ? s = 0.2, which is different to MoCoV2 where ? s = 0.2 is the optimal value. According to <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b15">16]</ref>, a greater temperature will result in a larger angular margin in the hypersphere. Since MoCoV2 adopts instance discrimination as the pretext task, a large temperature can enhance the compactness for the same instance and discrepancy for different instances. In contrast to instance discrimination, our method can be interpreted as pulling similar instances closer on the hypersphere; when the ground truth label is not available, the large angular margin might hurt the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Weak Augmentation Makes Better Relation</head><p>As we have mentioned, the weaker augmentation strategy for the teacher model is the key to the success of our framework. Here, We implement the weak augmentation as a random resized crop  <ref type="figure">Figure 2</ref>: Visualization of the 10 nearest neighbour of the query image. The top half is the result when we apply the weak augmentation. The bottom half is the case when the typical contrastive augmentation is adopted. Note, we use the red square to highlight the images that has different ground truth label with the query image.</p><p>(the random ratio is set to (0.2, 1)) and a random horizontal flip. For temperature parameter, we simply adopt the same setting as in <ref type="table" target="#tab_1">Table 2</ref> and report the performance of the best setting. The result is shown in <ref type="table" target="#tab_2">Table 3</ref>, as we can see that when we use the weak augmentation for the teacher model, the performance is significantly boosted across all datasets. We believe that this phenomenon is because relatively small disturbances in the teacher model can provide more accurate similarity guidance to the student model. To further verify this hypothesis, we random sampled three image from STL-10 training set as the query images, and then find the 10 nearest neighbour based on the weak / contrastive augmented query. We visualized the result in <ref type="figure">Figure 2</ref>, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dimension of the Relation</head><p>Since we also adopt the memory buffer as in MoCo <ref type="bibr" target="#b23">[24]</ref>, the buffer size will be equivalent to the dimension of the distribution p 1 p 2 . Thus, it will be one of the crucial points in our framework. To verify the effect the memory buffer size, we simply keep ? s = 0.1 and ? t = 0.04, then varying the memory buffer size from 256 to 32768. The result is shown in <ref type="table" target="#tab_3">Table 4</ref>, as we can see that a larger memory buffer can significantly boost the performance. However, a further increase in the buffer size can only bring a marginal improvement when the buffer is large enough. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization of Learned Representations</head><p>We also show the t-SNE <ref type="bibr" target="#b35">[36]</ref> visualizations of the representations learned by our proposed method and MoCov2 on the training set of CIFAR-10. Our proposed relational consistency loss leads to better class separation than the contrastive loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results on Large-scale Datasets</head><p>We also performed our algorithm on the large-scale ImageNet-1k dataset <ref type="bibr" target="#b14">[15]</ref>. In the experiments, we adopt a learning rate of 0.05 * BatchSize/256, a memory buffer size of 130k, and a 2-layer non-linear projection head with a hidden dimension 4096 and output dimension 512. For ? t and ? s , we simply adopt the best setting from <ref type="table" target="#tab_1">Table 2</ref> where ? t = 0.04 and ? s = 0.1. Linear Evaluation. For the linear evaluation of ImageNet-1k, we strictly follow the setting in SwAV <ref type="bibr" target="#b5">[6]</ref>. The result is shown in table 5. As we can see clearly that in 1x backprop methods, our ReSSL is significantly better than all previous algorithms. Comparing to the 2x backprop methods, our ReSSL can outperform CLSA, SimCLR, and SwAV with much lesser training costs.</p><p>Working with Multi-Crop Strategy. We also performed ReSSL with Multi-Crop strategy. The result is shown below in <ref type="table" target="#tab_5">Table 6</ref>. Specifically, the result of 4 crops is trained with the resolution of 224 ? 224, 160 ? 160, 128 ? 128, 96 ? 96. For the result of 5 crops, we add an additional 192 ? 192 image which is exactly the same with AdCo <ref type="bibr" target="#b27">[28]</ref>. As we can see our proposed ReSSL is significantly better than previous state-of-the-art methods. Working with Smaller Architecture. We also applied our proposed method on the smaller architecture (ResNet-18). The result is shown in <ref type="table" target="#tab_6">Table 7</ref>. Following the same training recipe of the ResNet-50 in above, our proposed method has a higher performance than SEED <ref type="bibr" target="#b20">[21]</ref> without a larger pretrained teacher network. We test all the methods on V100 GPU with 32G memory. It is clear to see that our ReSSL is just slightly slower (3%) than MoCo v2, but we get 2.4% improvements. On the other hand, the training speed of ReSSL (4 crops) is on pair with 2x backprop method (i.e.SimCLR and BYOL), but our performance is significantly better than all state-of-the-art methods. Transfer Learning. Finally, we further evaluate the quality of the learned representations by transferring them to other datasets. Following <ref type="bibr" target="#b33">[34]</ref>, we perform linear classification on the PASCAL VOC2007 dataset <ref type="bibr" target="#b19">[20]</ref>. Specifically, we resize all images to 256 pixels along the shorter side and taking a 224 ? 224 center crop. Then, we train a linear SVM on top of corresponding global average pooled final representations. To study the transferability of the representations in few-shot scenarios, we vary the number of labeled examples K and report the mAP. <ref type="table" target="#tab_8">Table 9</ref> shows the comparison between our method with previous works. We report the average performance over 5 runs (except for k=full).It's clearly to see that our proposed method is consistently outperform MoCo v2 and PCL v2 across all different K. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details on Small and Medium Dataset</head><p>We adopt the same backbone and data augmentation for all methods as we already described in Section 4. For SimCLR and BYOL, we use the LARS optimizer with a momentum of 0.9 and weight decay of 1e ? 4; the learning rate will be linearly warmed up for 5 epochs until it reaches 1.0 ? BatchSize/256. For linear evaluation, we use a standard SGD optimizer with a momentum of 0.9, weight decay of 0, and a learning rate of 0.2 ? BatchSize/256; the learning rate will be cosine decayed for 100 epochs. For SimSiam, the optimizer, learning rate, weight decay, and the linear evaluation details are the same as our MoCo and ReSSL implementation (as in Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experiments on Temperature</head><p>In this section, we add more experiments for different ? t (an extension for <ref type="table" target="#tab_1">Table 2</ref>). As we can see, when ? t ? ? s , the model is simply collapsed, which further verified that ? t has to be properly sharpened. Note, as we have mentioned in <ref type="table" target="#tab_1">Table 2</ref>, the optimal value for ? t is 0.04?0.05. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Experiments on Weak Augmentation</head><p>Since the weak augmentation for the teacher model is one of the crucial points in ReSSL, we further analyze the effect of applying different augmentations on the teacher model. In this experiment, we simply set ? t = 0.04 and report the linear evaluation performance on the Tiny ImageNet dataset. The results are shown in <ref type="table" target="#tab_0">Table 11</ref>. The first row is the baseline, where we simply resize all images to the same resolution (no extra augmentation is applied). Then, we applied random resized crops, random flip, color jitter, grayscale, gaussian blur, and various combinations. We empirically find that if we use no augmentation (e.g., no random resized crops) for the teacher model, the performance tends to degrade. This might result from that the gap of features between two views is way too smaller, which undermines the learning of representations. However, too strong augmentations of teacher model will introduce too much noise and make the target distribution inaccurate (see <ref type="figure">Figure 2</ref>). Thus mildly weak augmentations are better option for the teacher, and random resized crops with random flip is the combination with the highest performance as <ref type="table" target="#tab_0">Table 11</ref> shows. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>t-SNE visualizations on CIFAR-10. Classes are indicated by colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Compare to other SSL algorithms on small and medium dataset.</figDesc><table><row><cell>Method</cell><cell>BackProp</cell><cell>EMA</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>STL-10</cell><cell>Tiny ImageNet</cell></row><row><cell>Supervised</cell><cell>-</cell><cell>-</cell><cell>94.22</cell><cell>74.66</cell><cell>82.55</cell><cell>59.26</cell></row><row><cell>SimCLR [7]</cell><cell>2x</cell><cell>No</cell><cell>84.92</cell><cell>59.28</cell><cell>85.48</cell><cell>44.38</cell></row><row><cell>BYOL [23]</cell><cell>2x</cell><cell>Yes</cell><cell>85.82</cell><cell>57.75</cell><cell>87.45</cell><cell>42.70</cell></row><row><cell>SimSiam [10]</cell><cell>2x</cell><cell>No</cell><cell>88.51</cell><cell>60.00</cell><cell>87.47</cell><cell>37.04</cell></row><row><cell>MoCoV2 [9]</cell><cell>1x</cell><cell>Yes</cell><cell>86.18</cell><cell>59.51</cell><cell>85.88</cell><cell>43.36</cell></row><row><cell>ReSSL (Ours)</cell><cell>1x</cell><cell>Yes</cell><cell>90.20</cell><cell>63.79</cell><cell>88.25</cell><cell>46.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effect of different ? t and ? s for ReSSLDataset   ?s ?t = 0.01 ?t = 0.02 ?t = 0.03 ?t = 0.04 ?t = 0.05 ?t = 0.06 ?t = 0.07</figDesc><table><row><cell>CIFAR-10</cell><cell>0.1</cell><cell>89.35</cell><cell>89.74</cell><cell>90.09</cell><cell>90.04</cell><cell>90.20</cell><cell>90.18</cell><cell>88.67</cell></row><row><cell>CIFAR-10</cell><cell>0.2</cell><cell>89.52</cell><cell>89.67</cell><cell>89.24</cell><cell>89.50</cell><cell>89.22</cell><cell>89.40</cell><cell>89.50</cell></row><row><cell>CIFAR-100</cell><cell>0.1</cell><cell>62.34</cell><cell>62.79</cell><cell>62.71</cell><cell>63.79</cell><cell>63.46</cell><cell>63.20</cell><cell>61.31</cell></row><row><cell>CIFAR-100</cell><cell>0.2</cell><cell>60.37</cell><cell>60.05</cell><cell>60.24</cell><cell>60.09</cell><cell>59.09</cell><cell>59.12</cell><cell>59.76</cell></row><row><cell>STL-10</cell><cell>0.1</cell><cell>86.65</cell><cell>86.96</cell><cell>87.16</cell><cell>87.32</cell><cell>88.25</cell><cell>87.83</cell><cell>87.08</cell></row><row><cell>STL-10</cell><cell>0.2</cell><cell>85.17</cell><cell>86.12</cell><cell>85.01</cell><cell>85.67</cell><cell>85.21</cell><cell>85.51</cell><cell>85.28</cell></row><row><cell cols="2">Tiny ImageNet 0.1</cell><cell>45.20</cell><cell>45.40</cell><cell>46.30</cell><cell>46.60</cell><cell>45.08</cell><cell>45.24</cell><cell>44.18</cell></row><row><cell cols="2">Tiny ImageNet 0.2</cell><cell>43.28</cell><cell>42.98</cell><cell>43.58</cell><cell>42.12</cell><cell>42.70</cell><cell>42.76</cell><cell>42.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effect of weak augmentation guided ReSSL</figDesc><table><row><cell cols="6">Teacher Aug Student Aug CIFAR-10 CIFAR-100 STL-10 Tiny ImageNet</cell></row><row><cell>Contrastive</cell><cell>Contrastive</cell><cell>86.17</cell><cell>57.60</cell><cell>84.71</cell><cell>40.38</cell></row><row><cell>Weak</cell><cell>Contrastive</cell><cell>90.20</cell><cell>63.79</cell><cell>88.25</cell><cell>46.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Effect of different memory buffer size on small and medium dataset</figDesc><table><row><cell>Dataset (Small)</cell><cell cols="6">K = 256 K = 512 K = 1024 K = 4096 K = 8192 K = 16384</cell></row><row><cell>CIFAR-10</cell><cell>89.37</cell><cell>89.53</cell><cell>89.83</cell><cell>90.04</cell><cell>90.15</cell><cell>90.35</cell></row><row><cell>CIFAR-100</cell><cell>61.17</cell><cell>62.47</cell><cell>63.20</cell><cell>63.79</cell><cell>63.84</cell><cell>64.06</cell></row><row><cell cols="7">Dataset (Medium) K = 256 K = 1024 K = 4096 K = 8192 K = 16384 K = 32768</cell></row><row><cell>STL-10</cell><cell>85.88</cell><cell>87.23</cell><cell>87.72</cell><cell>87.42</cell><cell>87.32</cell><cell>87.47</cell></row><row><cell>Tiny ImageNet</cell><cell>43.08</cell><cell>45.32</cell><cell>45.78</cell><cell>45.42</cell><cell>46.60</cell><cell>46.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Top-1 accuracy under the linear evaluation on ImageNet with the ResNet-50 backbone. The table compares the methods over 200 epochs of pretraining.</figDesc><table><row><cell>Method</cell><cell cols="7">Arch Backprop EMA Batch Size Param Epochs Top-1</cell></row><row><cell>Supervised</cell><cell>R50</cell><cell>1x</cell><cell>No</cell><cell>256</cell><cell>24</cell><cell>120</cell><cell>76.5</cell></row><row><cell>1x Backprop Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>InstDisc [46]</cell><cell>R50</cell><cell>1x</cell><cell>No</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>58.5</cell></row><row><cell>LocalAgg [50]</cell><cell>R50</cell><cell>1x</cell><cell>No</cell><cell>128</cell><cell>24</cell><cell>200</cell><cell>58.8</cell></row><row><cell>MoCo v2 [9]</cell><cell>R50</cell><cell>1x</cell><cell>Yes</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>67.5</cell></row><row><cell>MoCHi [29]</cell><cell>R50</cell><cell>1x</cell><cell>Yes</cell><cell>512</cell><cell>24</cell><cell>200</cell><cell>68.0</cell></row><row><cell>CPC v2 [31]</cell><cell>R50</cell><cell>1x</cell><cell>No</cell><cell>512</cell><cell>24</cell><cell>200</cell><cell>63.8</cell></row><row><cell>PCL v2 [34]</cell><cell>R50</cell><cell>1x</cell><cell>Yes</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>67.6</cell></row><row><cell>AdCo [28]</cell><cell>R50</cell><cell>1x</cell><cell>Yes</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>68.6</cell></row><row><cell>ReSSL(Ours)</cell><cell>R50</cell><cell>1x</cell><cell>Yes</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>69.9</cell></row><row><cell>2x Backprop Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLSA-Single [45]</cell><cell>R50</cell><cell>2x</cell><cell>Yes</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>69.4</cell></row><row><cell>SimCLR [7]</cell><cell>R50</cell><cell>2x</cell><cell>No</cell><cell>4096</cell><cell>24</cell><cell>200</cell><cell>66.8</cell></row><row><cell>SwAV [6]</cell><cell>R50</cell><cell>2x</cell><cell>No</cell><cell>4096</cell><cell>24</cell><cell>200</cell><cell>69.1</cell></row><row><cell>SimSiam [23]</cell><cell>R50</cell><cell>2x</cell><cell>No</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>70.0</cell></row><row><cell>BYOL [23]</cell><cell>R50</cell><cell>2x</cell><cell>Yes</cell><cell>4096</cell><cell>24</cell><cell>200</cell><cell>70.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Working with Multi-Crop Strategy (Linear Evaluation on ImageNet)</figDesc><table><row><cell>Method</cell><cell cols="6">Arch EMA Batch Size Param Epochs Top-1</cell></row><row><cell>SwAV [6]</cell><cell>R50</cell><cell>No</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>72.7</cell></row><row><cell>AdCo [28]</cell><cell>R50</cell><cell>No</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>73.2</cell></row><row><cell cols="2">CLSA-Multi [45] R50</cell><cell>Yes</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>73.3</cell></row><row><cell>ReSSL (4 crops)</cell><cell>R50</cell><cell>Yes</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>73.8</cell></row><row><cell>ReSSL (5 crops)</cell><cell>R50</cell><cell>Yes</cell><cell>256</cell><cell>24</cell><cell>200</cell><cell>74.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Experiments on ResNet-18 (Linear Evaluation on ImageNet) Training Cost. Fair comparison should be performed under the same training cost. In table 8, we show the training speed and linear evaluation accuracy on ImageNet.</figDesc><table><row><cell>Method</cell><cell>Epochs</cell><cell>Student</cell><cell>Teacher</cell><cell>Acc</cell></row><row><cell>MoCo v2</cell><cell>200</cell><cell>ResNet-18</cell><cell>EMA</cell><cell>52.2</cell></row><row><cell>SEED</cell><cell>200</cell><cell>ResNet-18</cell><cell>ResNet-50 (MoCoV2)</cell><cell>57.6</cell></row><row><cell>ReSSL</cell><cell>200</cell><cell>ResNet-18</cell><cell>EMA</cell><cell>58.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Training Cost Comparison, the result is reported in averaged GPU hours</figDesc><table><row><cell>Method</cell><cell cols="2">Epochs Batch Size</cell><cell>GPU</cell><cell cols="3">GPU Memory (GPU?Time)/Epoch Acc</cell></row><row><cell>MoCo v2</cell><cell>200</cell><cell>256</cell><cell>8 x V100</cell><cell>40 G</cell><cell>2.25</cell><cell>67.5</cell></row><row><cell>ReSSL (Ours)</cell><cell>200</cell><cell>256</cell><cell>8 x V100</cell><cell>42 G</cell><cell>2.33</cell><cell>69.9</cell></row><row><cell>SimCLR</cell><cell>200</cell><cell>4096</cell><cell>32 x V100</cell><cell>858 G</cell><cell>3.55</cell><cell>66.8</cell></row><row><cell>BYOL</cell><cell>200</cell><cell>4096</cell><cell>32 x V100</cell><cell>863 G</cell><cell>3.88</cell><cell>70.6</cell></row><row><cell>ReSSL (4 Crops)</cell><cell>200</cell><cell>256</cell><cell>8 x V100</cell><cell>80 G</cell><cell>3.62</cell><cell>73.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Transfer learning on low-shot image classificationIn this work, we propose relational self-supervised learning (ReSSL), a new paradigm for unsupervised visual representation learning framework that maintains the relational consistency between instances under different augmentations. Our proposed ReSSL relaxes the typical constraints in contrastive learning where different instances do not always need to be pushed away on the embedding space, and the augmented views do not need to share exactly the same feature. An extensive empirical study shows the effect of each component in our framework. The experiments on large-scaled datasets demonstrate the efficiency and state-of-the-art performance for unsupervised representation learning.</figDesc><table><row><cell>Method</cell><cell>Epochs</cell><cell>ImageNet</cell><cell>K=16</cell><cell>K=32</cell><cell>K=64</cell><cell>Full</cell></row><row><cell>Random</cell><cell>-</cell><cell>-</cell><cell>10.10</cell><cell>11.34</cell><cell>11.96</cell><cell>12.42</cell></row><row><cell>Supervised</cell><cell>90</cell><cell>76.1</cell><cell>82.26</cell><cell>84.00</cell><cell>85.13</cell><cell>87.27</cell></row><row><cell>MoCo V2 [9]</cell><cell>200</cell><cell>67.5</cell><cell>76.14</cell><cell>79.16</cell><cell>81.52</cell><cell>84.60</cell></row><row><cell>PCL V2 [34]</cell><cell>200</cell><cell>67.5</cell><cell>78.34</cell><cell>80.72</cell><cell>82.67</cell><cell>85.43</cell></row><row><cell>ReSSL (Ours)</cell><cell>200</cell><cell>69.9</cell><cell>79.17</cell><cell>81.96</cell><cell>83.81</cell><cell>86.31</cell></row><row><cell>6 Conclusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>More experiments for different ? t (Top-1 accuracy on small and medium dataset)</figDesc><table><row><cell>?s</cell><cell>?t</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>STL-10</cell><cell>Tiny ImageNet</cell></row><row><cell>0.1</cell><cell>0.08</cell><cell>10.00</cell><cell>1.00</cell><cell>83.05</cell><cell>39.38</cell></row><row><cell>0.1</cell><cell>0.09</cell><cell>10.00</cell><cell>1.00</cell><cell>10.00</cell><cell>0.50</cell></row><row><cell>0.1</cell><cell>0.10</cell><cell>10.00</cell><cell>1.00</cell><cell>10.00</cell><cell>0.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Effect of different augmentation for teacher model(Tiny ImageNet)</figDesc><table><row><cell>Random Resized Crops</cell><cell>Random Flip</cell><cell>Color Jitter</cell><cell>GrayScale</cell><cell>Gaussian Blur</cell><cell>Acc</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31.74</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30.98</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>29.46</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>29.68</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.60</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.44</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>42.28</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.88</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>43.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>42.28</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.52</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Densenet models for tiny imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Abai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajmalwar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<idno>ArXiv abs/1902.09229</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autoencoders, unsupervised learning and deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Unsupervised and Transfer Learning Workshop</title>
		<meeting>the 2011 International Conference on Unsupervised and Transfer Learning Workshop</meeting>
		<imprint>
			<publisher>UTLW&apos;11</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="37" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1809.11096</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments (2020) 1, 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning (2020) 1, 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Debiased contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf2" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8765" to="8775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v15/coates11a.html5" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="11" to="13" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Autoaugment: Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image inpainting: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Elharrouss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Almaadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-M?adeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Akbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The PAS-CAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html9" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">{SEED}: Self-supervised distillation for visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=AHm3dbp7D1D" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf2" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<title level="m">Bootstrap your own latent: A new approach to self</title>
		<imprint/>
	</monogr>
	<note>supervised learning (2020) 1, 3, 5, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<title level="m">Momentum contrast for unsupervised visual representation learning</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08435</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS) (2020)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Contrastive predictive coding based feature for automatic speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Lai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01575</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-supervised label augmentation via input transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=KmykpuSrjcq2" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10242</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<title level="m">Contrastive learning with stronger augmentations (2021) 2, 5</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hyx-jyBFPr1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning from multiple teacher networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
