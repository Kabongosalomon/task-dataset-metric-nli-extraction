<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Empirical Evaluation of Pretraining Strategies for Supervised Entity Linking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-28">28 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
							<email>tfevry@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><forename type="middle">Baldini</forename><surname>Soares</surname></persName>
							<email>liviobs@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
						</author>
						<title level="a" type="main">Empirical Evaluation of Pretraining Strategies for Supervised Entity Linking</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-28">28 May 2020</date>
						</imprint>
					</monogr>
					<note>Automated Knowledge Base Construction (2020) Conference paper Google Research, NYC</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present an entity linking model which combines a Transformer architecture with large scale pretraining from Wikipedia links. Our model achieves the stateof-the-art on two commonly used entity linking datasets: 96.7% on CoNLL and 94.9% on TAC-KBP. We present detailed analyses to understand what design choices are important for entity linking, including choices of negative entity candidates, Transformer architecture, and input perturbations. Lastly, we present promising results on more challenging settings such as end-to-end entity linking and entity linking without in-domain training data. * . Denotes equal contribution. ?. Work conducted during Google AI Residency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Traditionally, entity linking approaches have relied on knowledge bases, complicated modelling and task-specific hand-engineered features to achieve high performance. More recently, <ref type="bibr" target="#b0">Broscheit 2019</ref><ref type="bibr" target="#b17">, Ling et al. 2020</ref><ref type="bibr" target="#b32">and Wu et al. 2019</ref> show that using large-scale pretrained language models like BERT <ref type="bibr" target="#b5">[Devlin et al., 2018]</ref>, pretraining on Wikipedia entity links, and fine-tuning on a specific entity linking corpus leads to state-of-the-art performance without relying on such features. However, <ref type="bibr" target="#b17">Ling et al. 2020</ref> focused mainly on constructing general-purpose entity representations, <ref type="bibr" target="#b32">Wu et al. 2019</ref> on building strong zero-shot entity linking systems, and Broscheit 2019 on end-to-end linking, so that the limits of pretraining for entity disambiguation have not been fully explored.</p><p>In this paper we present a thorough study of pretraining strategies for supervised entity linking. We establish new upper bounds for performance on the widely studied CoNLL and TAC-KBP 2010 entity linking tasks. We also show that our pretraining approach yields a very competitive entity linking system without any further domain specific tuning. We present a detailed analysis of significant design choices including the choice of negative candidates used during training, and the document context encoded for each mention. We find that the optimal choice of negative candidates is dependent on whether or not the final linking system has access to an alias table. For a system that will use an alias table at inference, it is helpful to pretrain the models with lexically similar candidates. However, when no alias table is used for the downstream task, ensuring candidates are random improves the model ability to distinguish the right entity among all possible entities.</p><p>In our studies we found two surprising results: (1) it is possible to achieve optimal entity linking results with a four layer transformer, which is one third of the size of BERT-base, and (2) given the abundance of supervision from Wikipedia links, we did not get any gains in performance from training with an auxiliary language modeling loss. However, we did find that the input perturbations introduced by <ref type="bibr" target="#b5">[Devlin et al., 2018]</ref> themselves increase the quality of our pretraining approach and we present an analysis of how these perturbations add robustness to the model. Finally, to demonstrate the generality of our model, we present results on the end-toend entity linking task in which both mention location and identity are predicted. On this task, our model outperforms all but the tailored methods introduced by <ref type="bibr" target="#b15">Kolitsas et al. 2018</ref><ref type="bibr" target="#b0">, Broscheit 2019</ref>. We argue that our model is more practical, and easier to integrate, than heavily engineered existing approaches, and we believe that downstream tasks such as information extraction and question answering can benefit from this robust standalone end-to-end entity linker (e.g. ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early entity linking systems <ref type="bibr" target="#b1">[Bunescu and</ref><ref type="bibr">Pasca, 2006, Mihalcea and</ref><ref type="bibr" target="#b20">Csomai, 2007]</ref> focused on matching the context of the mention with that of the entity page. In addition to context features, systems have relied on P(e|m), the prior probability that mention m refers to entity e, computed from Wikipedia mention counts. The set of entities in a document should be globally coherent, and several approaches have introduced sophisticated global disambiguation methods <ref type="bibr" target="#b9">[Globerson et al., 2016</ref><ref type="bibr" target="#b2">, Cheng and Roth, 2013</ref><ref type="bibr" target="#b28">, Sil and Yates, 2013</ref> that consider all mentions in a document to make predictions. In contrast, we do not model document-level disambiguation explicitly. However, our long context windows contain several mentions which should allow such disambiguation. Other approaches have also sought to incorporate types and knowledge base information in their modelling, such as <ref type="bibr">Radhakrishnan et al. 2018, Raiman and</ref><ref type="bibr" target="#b26">Raiman 2018.</ref> We pretrain our model by learning distributed representation of entities directly from Wikipedia text, similarly to <ref type="bibr" target="#b33">Yamada et al. 2016</ref><ref type="bibr" target="#b8">, Gillick et al. 2019</ref><ref type="bibr" target="#b17">, Ling et al. 2020</ref><ref type="bibr">. Unlike Wu et al. 2019</ref>, our embeddings are learned directly rather than generated through entity descriptions. In contrast with <ref type="bibr" target="#b33">Yamada et al. 2016</ref><ref type="bibr" target="#b34">Yamada et al. , 2017</ref>, we do not use additional features, such as prior probabilities or string match features. Our method is therefore most similar to <ref type="bibr" target="#b17">Ling et al. 2020 and</ref><ref type="bibr" target="#b0">Broscheit 2019</ref> who also use a transformer. We differ from <ref type="bibr" target="#b17">Ling et al. 2020</ref> by simultaneously considering all mentions and entities in a context and, in contrast to both, only use a four layer, randomly-initialized transformer instead of twelve layers initialized from large scale language modelling pretraining.</p><p>In addition, we experiment with end-to-end entity linking <ref type="bibr" target="#b28">[Sil and Yates, 2013</ref><ref type="bibr" target="#b19">, Luo et al., 2015</ref><ref type="bibr" target="#b15">, Kolitsas et al., 2018</ref>, where instead of predicting the entity for gold spans, a system must both predict the span and its label. A closely related task is multilingual entity linking. Approaches have used multilingual embeddings to link text in several languages <ref type="bibr">[Sil et al., 2018, Tsai and</ref><ref type="bibr" target="#b30">Roth, 2016]</ref>. Zero-shot entity linking <ref type="bibr" target="#b18">[Logeswaran et al., 2019</ref><ref type="bibr" target="#b32">, Wu et al., 2019</ref> is another relevant task. In that setting, entities predicted at test time are not seen in training. Instead, the system relies on the entity name and description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>Let E = {e 1 . . . e N } be a predefined set of entities, and let V = {[mask], w 1 . . . w M } be a vocabulary of words. A context x = [x 0 . . . x t ] is a sequence of words x i ? V. A span s = (s start , s end ), is a tuple with 0 ? s start , s end &lt; t which defines a contiguous sequence of tokens in a given context. A mention label l = (s k , e k ) consists of a span s i and an entity label, e i ? E ? ?. We use l to denote a set of such mention labels. The NULL-symbol ? indicates a span that is labeled as a mention, but without an entity linking label.</p><p>Our training data, D = {(x 0 , l 0 ) . . . (x N , l N )}, is a corpus of contexts, each paired with a set of mention labels, one for each mention in the context. Given an input context x i , our goal is to predict the set of entity mentions l i . In Entity Disambiguation, we are given the set of spans, and predict the entity linked by each span. In End-to-End Entity Linking, we must predict both the set of mention spans, and their linked entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contextual Language Representation</head><p>Our model is built using the now-standard Transformer-based architecture <ref type="bibr" target="#b31">[Vaswani et al., 2017]</ref>. The model computes a matrix representation? ? R t?d of a text sequence through successive application of a Transformer block to the output of the previous layer:</p><formula xml:id="formula_0">H i = TransformerBlock(H i?1 ) = MLP(MultiHeadAttention(H i?1 , H i?1 , H i?1 ))</formula><p>H 0 is a sequence of context-independent token embeddings and? = H n , where n is the number of Transformer layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entity Disambiguation</head><p>Each entity e ? E is mapped directly onto a dedicated vector in R d via a |E| ? d dimensional embedding matrix. In our experiments, we have a distinct embedding for every concept that has an English Wikipedia page, resulting in approximately 5.7m entity embeddings.</p><p>In order to perform entity linking for a particular span with word-piece token indices (i, j), we (following <ref type="bibr" target="#b16">Lee et al. 2016</ref>) first obtain a representation of the span by concatenating the representation at the span start and end, and pass this through a multi-layer perceptron which projects the span representation into the same space as the entity embeddings.?</p><formula xml:id="formula_1">s i = MLP([H n,sstart , H n,s end ])</formula><p>(1)</p><p>Our model scores each span-entity pair by taking the dot-product between the projected span representation and the embedding of e c . Thus, the conditional probability that the span s i refers to entity e c is defined as:</p><formula xml:id="formula_2">P(e c |s i ) = exp(? s i ? e c ) c ? ?E exp(? s i ? e c ? )<label>(2)</label></formula><p>In practice, this is expensive to compute for large |E|. Therefore, for every l we select a set C l of k candidates, which contains the entity labels for all l ? l as well as a set of negative candidates. We do not have an entity linking loss on mentions that do not have a label. Therefore, our per-example entity linking loss is:</p><formula xml:id="formula_3">l linking (l) = l i ?l exp(? s i ? e i )? e i !=? c?C l exp(? s i ? e c )<label>(3)</label></formula><p>We will discuss further how we select C l in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mention Detection</head><p>For many entity linking tasks, the target spans are provided. In order to be able to do endto-end entity linking, we additionally train our model to predict mentions, independently of entity linking. One way to do this would be to score every possible span-entity pair, and either use a score threshold to filter spans where no entity link achieves a sufficiently high score, or to additionally score a special NULL-link embedding. However, enumerating all spans for the long contexts we use in our model would be prohibitively expensive. We take the approach of encoding mentions as a BIO sequence, and train an MLP on the context representation to predict this sequence with a standard cross-entropy loss. Our final loss sums the mention detection loss and the linking loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Wikipedia Pretraining</head><p>We build a training corpus of contexts paired with entity mention labels from the 2019-04-14 dump of English Wikipedia. We first divide each article into chunks of 1000 unicode characters, resulting in a corpus of over 17.5 million contexts with over 17 million entity mentions covering over 5.7 million entities. These are processed with the BERT tokenizer, limited to 256 word-piece tokens. In addition to the Wikipedia links, we annotate each sentence with unlinked mention spans using a state-of-the-art named entity recognizer. These are used as additional signal for our mention detection component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Candidates Selection</head><p>Training the model with a full softmax over all 5.7 million entities for every mention is computationally expensive. A common solution is to use a noise contrastive loss <ref type="bibr" target="#b11">[Gutmann and</ref><ref type="bibr">Hyv?rinen, 2012, Mnih and</ref><ref type="bibr" target="#b21">Kavukcuoglu, 2013]</ref> and sample candidates according to their relative frequency, as in <ref type="bibr" target="#b17">Ling et al. [2020]</ref>. In this work, we experiment with other approaches to candidate generation that might provide better negatives in training. In addition to negatives selected uniformly at random from the entire entity vocabulary, we define two types of hard negatives:</p><p>1. Page candidates, which is the set of all entities linked to in the article from which the given context was taken. This is meant to capture semantically related concepts.</p><p>2. Phrase table candidates, the set of lexically related entities for each mention candidate, obtained from the Phrase <ref type="table">Table provided</ref> by SLING <ref type="bibr" target="#b27">[Ringgaard et al., 2017]</ref>.</p><p>Throughout the paper, we will use |C l | = 768. In our base setup, we use up to 256 page candidates, and 384 phrase table candidates, equally divided between each mention in the example. Any remaining room in the set of 768 is filled with random candidates sampled uniformly from the entity vocabulary (meaning a minimum of 128 random candidates per example). We will study the impact of different candidate selection methods in Section 6.2. In addition to those candidates, for every example in a batch, we use the candidates of other examples as additional negatives.</p><p>Input Noising We also add noise to the input data. We apply the same noise function as is used in <ref type="bibr" target="#b5">Devlin et al. 2018</ref>: 15% of the tokens are chosen to be modified. 80% of those tokens are changed to the [mask] token, 10% are changed to a random token and 10% are left unmodified.</p><p>Pretraining hyperparameters We use Adam [Kingma and Ba, 2014] with a learning rate of 1e-4 to optimize our model. We use a linear warmup schedule for the first 10% of training, decay the learning rate afterwards and use gradient clipping with a norm of 1.0. We train from scratch for up to a million steps and use a large batch size of 8192 for pretraining. We follow BERT <ref type="bibr" target="#b5">[Devlin et al., 2018]</ref> base for many of our model parameters, though we do not use large-scale language-modeling pretraining and only use four layers, as we did not find more to layers to further improve performance. We use the same wordpiece vocabulary as the lowercase version of BERT. We use entity embeddings of size 256 unless mentioned otherwise. We weight both the entity disambiguation loss and the mention detection loss to 1. We use a context window of 256 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Linking Datasets</head><p>We evaluate our model on two popular entity linking benchmarks: AIDA CoNLL YAGO <ref type="bibr" target="#b12">[Hoffart et al., 2011]</ref> and TAC-KBP 2010 <ref type="bibr" target="#b13">[Ji et al., 2010]</ref>. The first is comprehensively annotated with approximately 34,000 mentions on 1,393 newswire document on the full Wikipedia vocabulary, while the second is sparsely annotated for target entities only on a smaller entity vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">AIDA CoNLL-YAGO Dataset</head><p>Textual Context Most CoNLL documents do not fit in our limit of 256 tokens. Therefore, we split the document into "sentences" at each newline in the document. We experiment with three methods to add document context to these sentences: (i) taking the sentence as-is, (ii) adding the title of the document to the sentence, (iii) adding the title of the document as well as the first two sentences to the sentence. Throughout our experiments we will use (iii), though we show the impact of this choice in Section 6.4.  We find that through careful resolution of unicode and Wikipedia redirects, we achieve a slightly higher conversion rate than reported by Globerson et al. 2016 (statistics provided in <ref type="table">Table 1</ref>). This leads to a higher gold recall, but also a larger number of candidate for each mention, meaning our system must distinguish between more candidates. We report results using both alias tables.</p><p>Finetuning We finetune our model -including the entity embeddings -on the CoNLL training set, using the alias table candidates for each mention. We used a batch size of 256, a learning rate of 1e-6, and train for 2000 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">TAC-KBP 2010 dataset</head><p>TAC-KBP 2010 is another widely used dataset for evaluating entity disambiguation systems. In contrast with CoNLL, the mentions are sparsely annotated among documents. It contains 1074 annotated entities in the training set and 1020 in the evaluation set. The entities for this dataset are part of the TAC Knowledge Base, containing 818,741 entities. Due to the reduced entity vocabulary, we can fine-tune without resorting to an alias table and we adopt this setting throughout our results. This is consistent with the prior state-of-the-art approach of <ref type="bibr" target="#b32">Wu et al. 2019</ref>. To select the context for a mention, we take the 256 bytes before and after the first occurrence of the mention in the document.</p><p>We select the fine-tuning parameters on training by doing cross-validation on the training set. We used a batch size of 32, trained for 1,000 steps and found it was best to freeze the entity embeddings. Our final model is trained on all the training data with the parameters selected in cross-validation. However, we report the result on the evaluation (test) set number in all tables, including ablations. Indeed, we found that this was more reflective of task performance, as the training set is significantly easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">End-to-end entity linking</head><p>We also experiment with end-to-end entity linking on CoNLL (TAC-KBP is not suitable due to its sparse annotations). In this case, we do not use an alias table. In this setting, we follow the hyperparameters of Section 6.1. Instead of using candidates, we train our model to predict BIO-tagged mention boundaries and to disambiguate among all entities. At training and fine-tuning time, gold spans are used for the disambiguation task. At inference, we use the BIO-tagged predictions as our spans and predict entities for each span among all possible entities. We use the standard strong matching micro-F1 score. <ref type="table">Table 1</ref> shows that our approach outperforms all prior approaches on CoNLL and TAC-KBP 2010. On CoNLL, we outperform methods in both alias-table settings. Additionally, we note that unlike many previous systems, we do not use alias priors, knowledge-base features, or other entity features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Entity Linking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">End-to-end entity linking</head><p>For end-to-end entity linking, we do not use the alias table. Instead of using candidates, we predict BIO-tagged mention boundaries and disambiguate mentions among all entities. At training and fine-tuning time, gold spans are used for the disambiguation task. At inference, we use the BIO predictions as our spans and predict entities for all these spans. <ref type="table">Table 2</ref> shows our model fares well against other models, with the exception of Broscheit 2019 and <ref type="bibr" target="#b15">Kolitsas et al. 2018</ref>. The former use a much larger Transformer model, and also initialize from BERT-base model, which is pretrained on a corpus of unlabeled text much larger than our training data. <ref type="bibr" target="#b15">Kolitsas et al. 2018</ref> relies on an alias table to generate candidate mentions at both training and inference time. In addition, it introduces a clever mechanism to jointly optimize and select mention boundaries and entity candidates, whereas we use a simpler pipelined approach. Finally, they also introduce a document-level disambiguation coherence penalty and a coreference resolution heuristic. We believe the use of  an alias table as well as the aforementioned differences explain the gap between our method and <ref type="bibr" target="#b15">Kolitsas et al. 2018</ref>, and we will look to bridge this gap in future work. Nevertheless, our model stands as a strong baseline of what can be achieved with simple modelling and low inference cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Classifying all entities or classifying candidates</head><p>We trained our model to distinguish the correct linked entity among candidates. An alternative approach is to predict among all entities. This is computationally more expensive as it requires doing a softmax over 5.7 million entities for every mention in the batch. Thus we use a batch size to 2048 and set the entity embedding dimension to 64 for both this model and the one trained with candidates. whereas for TAC-KBP, setups that use random candidates are more successful as they are closer to the full classification setup used in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Impact of candidate selection</head><p>In  <ref type="table">)</ref> candidates is particularly important, as this is similar to the disambiguation task the model has to perform. Page candidates are semantically related but generally not lexically related and thus do not bring the same benefits. In fact, they might even distort the distribution of negative candidates as the model performs worse in this setting than with random candidates. After fine-tuning, all models fare similarly, which we believe is due to CoNLL having enough fine-tuning data so that all our models approach the performance upper-bound on this task (see Section 6.5). For TAC-KBP 2010, where we do not use an alias table at fine-tuning or inference time, the results are markedly different. After pre-training, ours which has less random candidates performs worse than all other alternatives. This is likely because having more random candidates is closer to the full classification setup used in TAC-KBP 2010. Given TAC-KBP's small training set, these differences carry over in fine-tuning performance.</p><p>6.3 Impact of adding noise during pretraining <ref type="table" target="#tab_6">Table 5</ref> shows adding noise in pretraining helps both pre-training and fine-tuning performance. We hypothesize that input noise implicitly trains the model to generalize to alternative aliases: for instance, given the mention "Yuri Gagarin", the model might have to learn to recognize "Yuri [MASK]" or "[MASK] Gagarin".</p><p>Encouraged by the success of <ref type="bibr" target="#b5">Devlin et al. 2018</ref>, we experimented with also pretraining our model with a masked-language modelling objective, with the expectation that such pretraining would help our model learn better representations of language. We tried different architectures and loss weights (including a 4 layer transformer with both objectives at layer 4, a 12 layer with the entity linking loss at layer 4, etc.) but overall found this to not improve further on simply adding noise in pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>No fine-tuning Fine-tuned With noise 92.2 96.9 Without noise 88.1 96.1  6.4 Impact of context selection methods on CoNLL <ref type="table" target="#tab_7">Table 6</ref> shows the impact of varying CoNLL's context type in pre-training and fine-tuning. We find that larger contexts, especially those that include the beginning of the document, considerably boost performance before fine-tuning. However, similarly to our observations in Sections 6.2 and 6.3, we find that improvements are less marked after fine-tuning, likely because our performance is already very high. <ref type="figure">Figure 2</ref> shows three sample errors on the CoNLL development set. Most errors are due to varying levels of specificity in the CoNLL labels. Some errors are due to changes in Wikipedia. For instance, in text A, the Bulgaria U21 soccer team Wikipedia page was built in 2013, after CoNLL. Also, in text C, our model correctly disambiguates between Austin, Michaella and Richard Krajicek, which are all three tennis players (only Richard is a dutch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Error analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper we present a thorough study of pretraining strategies for supervised entity linking, achieving state-of-the-art performance on both CoNLL and TAC-KBP 2010 with a four-layer Transformer-based model. Given the limited headroom remaining in these datasets, and the strong impact of alias tables in simplifying the problem, we believe the creation of new datasets, and more difficult entity linking settings, such as zero-shot and low resource domains, are crucial areas for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Entity</head><label></label><figDesc>Candidates Selection Our candidates for CoNLL come from alias tables -resources which provide a list of possible strings for a given entity. A key challenge with evaluating entity linking systems on the CoNLL dataset is inconsistent use of alias tables. Globerson et al. 2016 describe the difficulty of resolving older resources due to changes in Wikipedia links and unicode, and provide statistics for two commonly-used alias tables: The YAGO extended "means" mapping of Hoffart et al. 2011, and the "PPRforNED" mapping of Pershina et al. 2015. Alias</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table Conversion</head><label>Conversion</label><figDesc>Statistics for alias table conversions, computed on the CoNLL test split. Gold recall is the percentage of mentions for which the gold entity is included in the candidate set. Average ambiguity is the total number of candidates divided by the number of mentions.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Gold recall Avg. ambig.</cell></row><row><cell>Hoffart et al. 2011</cell><cell>Globerson et al. 2016 Ours</cell><cell>96.19 99.33</cell><cell>65.9 67.4</cell></row><row><cell>Pershina et al. 2015</cell><cell>Globerson et al. 2016 Ours</cell><cell>99.84 99.75</cell><cell>12.6 13.8</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 1: Test accuracy on the CoNLL and TAC-KBP entity disambiguation tasks. CoNLL H refers to papers using the Hoffart et al. 2011 "means" alias table while P refers to the Pershina et al. 2015 table. *It is not clear which alias table, if any, is used by Raiman and Raiman 2018. ?-marked systems do not use features beyond the text and alias table.</figDesc><table><row><cell>System</cell><cell cols="3">CoNLL H CoNLL P TAC-KBP 2010</cell></row><row><cell>Chisholm and Hachey 2015</cell><cell>88.7</cell><cell>-</cell><cell>80.7</cell></row><row><cell>Ganea et al. 2016</cell><cell>87.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Globerson et al. 2016</cell><cell>91.0</cell><cell>-</cell><cell>87.2</cell></row><row><cell>Pershina et al. 2015</cell><cell>-</cell><cell>91.8</cell><cell>-</cell></row><row><cell>Globerson et al. 2016</cell><cell>-</cell><cell>92.7</cell><cell>87.2</cell></row><row><cell>Yamada et al. 2016</cell><cell>91.5</cell><cell>93.1</cell><cell>85.2</cell></row><row><cell>Raiman and Raiman 2018  *</cell><cell></cell><cell>94.9</cell><cell>90.9</cell></row><row><cell>Yamada et al. 2017</cell><cell>-</cell><cell>94.3</cell><cell>87.7</cell></row><row><cell>Ling et al. 2020 ?</cell><cell>-</cell><cell>94.9</cell><cell>89.8</cell></row><row><cell>Wu et al. 2019</cell><cell>-</cell><cell>-</cell><cell>94.0</cell></row><row><cell>ours ?</cell><cell>92.5</cell><cell>96.7</cell><cell>94.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Entity disambiguation accuracy on the CoNLL development set for different pretraining and fine-tuning setups. Numbers in the first part of the table do not use an alias table, whereas the ones on the second part use Pershina et al. 2015's table.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>shows impressive accuracy without an alias table for the system classifying among all entities. However, it does not fare better than the model trained with candidates when using one. Given the considerable cost of doing the full sofmax for every mention, we use candidates for our other experiments. Note that our model gets 88.4% accuracy when trained only on the CoNLL data.</figDesc><table><row><cell></cell><cell>CoNLL</cell><cell></cell><cell>TAC-KBP</cell><cell></cell></row><row><cell>Candidates source</cell><cell cols="4">No fine-tuning Fine-tuned No fine-tuning Fine-tuned</cell></row><row><cell>ours</cell><cell>92.2</cell><cell>96.9</cell><cell>87.3</cell><cell>91.4</cell></row><row><cell>Phrase table and random</cell><cell>88.0</cell><cell>96.9</cell><cell>91.6</cell><cell>94.7</cell></row><row><cell>Page and random</cell><cell>83.4</cell><cell>96.9</cell><cell>92.4</cell><cell>94.4</cell></row><row><cell>Random</cell><cell>85.3</cell><cell>97.2</cell><cell>91.7</cell><cell>94.9</cell></row><row><cell cols="5">Table 4: Impact of the candidate selection method on development performance on CoNLL</cell></row><row><cell cols="5">and TAC-KBP 2010. Unsurprisingly, pretraining methods that are closer to the</cell></row><row><cell cols="5">final task setup perform better: For CoNLL, we emphasize the importance of</cell></row><row><cell cols="5">phrase table candidates in pre-training to emulate the use of the alias table ,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>, we show the impact of different candidate selection pretraining strategies. On CoNLL, where we do use an alias table for evaluation, we find that our candidate selection heuristic seems to help the model in pre-training, achieving better performance than any of our ablations. Training with lexically related (phrase table</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Impact of adding BERT-style input noise during pre-training. We report development accuracy on CoNLL using the<ref type="bibr" target="#b22">Pershina et al. 2015</ref> alias table.</figDesc><table><row><cell></cell><cell cols="3">None Title Title &amp; first two sents</cell></row><row><cell cols="2">No Fine-tuning 85.5</cell><cell>89.4</cell><cell>92.2</cell></row><row><cell>Fine-tuned</cell><cell>96.2</cell><cell>96.9</cell><cell>96.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Impact of additional context beyond a single sentence used for entity disambiguation performance on the CoNLL task. We report development accuracy on CoNLL using the<ref type="bibr" target="#b22">Pershina et al. 2015</ref> alias table.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors wish to thank Dan Bikel and Eunsol Choi for their helpful comments in the preparation of this paper, as well as the anonymous reviewers. Text b: Scottish labour party narrowly backs referendum. Stirling, Scotland 1996-08-31. Conservatives have only 10 of the 72 Scottish seats in parliament and consistently run third in opinion polls in Scotland behind labour and the independence-seeking Scottish national party. Incorrectly predicted "labour party" as Scottish Labour Party. Gold Label: Labour party (UK).</p><p>Text c: Edberg refuses to qo (sic) quietly. Richard Finn "it doesn't look all that bad" Edberg said of his path through the draw starting next with a match against Krajicek's dutch countryman Paul Haarhuis.</p><p>Correctly predicted "Edberg" as Stefan Edberg and "Krajicek" as Richard Krajicek and "Paul Haarhuis". Incorrectly predicted "dutch" as Dutch people. Second prediction is Netherlands and correct. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Investigating entity knowledge in bert with simple neural end-to-end entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoNLL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>EACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relational inference for wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Entity disambiguation with web links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">TACL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving efficiency and accuracy in multilingual entity extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Entities as experts: Sparse memory access with entity supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07202</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Probabilistic bag-of-hyperlinks model for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Octavian-Eugen Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
		<title level="m">Learning dense representations for entity retrieval</title>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Collective entity resolution with multi-focal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Amir Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Ringaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Entity linking via joint encoding of types, descriptions, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyv?rinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>F?rstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Overview of the tac 2010 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Ellis</surname></persName>
		</author>
		<idno>TAC 2010</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-End neural entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Kolitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimi</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno>1611.01436</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio Baldini</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Learning cross-context entity representations from text. arXiv preprint 2001.03765</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Zero-shot entity linking by reading entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>1906.07348</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wikify! linking documents to encyclopedic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Csomai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IKM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noisecontrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Personalized page rank for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pershina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno>1909.04164</idno>
		<title level="m">Knowledge enhanced contextual word representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From tagme to wat: a new entity annotator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Workshop on Entity Recognition &amp; Disambiguation</title>
		<meeting>the First International Workshop on Entity Recognition &amp; Disambiguation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Elden: Improved entity linking using densified knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeptype: multilingual entity linking by neural type system evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Raphael Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Michel Raiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sling: A framework for frame semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<idno>1710.07032</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Re-ranking for joint named-entity recognition and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IKM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural cross-lingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourab</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-lingual wikification using multilingual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Zeroshot entity linking with dense entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint learning of the embedding of words and entities for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoNLL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning distributed representations of texts and entities from knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
