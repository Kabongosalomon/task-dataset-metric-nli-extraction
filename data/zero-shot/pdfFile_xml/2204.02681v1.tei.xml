<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncai</forename><surname>Peng</surname></persName>
							<email>pengjuncai@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Hao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutao</forename><surname>Chu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guowei</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewu</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Dang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baohua</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiwen</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Yanjun</roleName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Baidu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename></persName>
						</author>
						<title level="a" type="main">PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world applications have high demands for semantic segmentation methods. Although semantic segmentation has made remarkable leap-forwards with deep learning, the performance of real-time methods is not satisfactory. In this work, we propose PP-LiteSeg, a novel lightweight model for the real-time semantic segmentation task. Specifically, we present a Flexible and Lightweight Decoder (FLD) to reduce computation overhead of previous decoder. To strengthen feature representations, we propose a Unified Attention Fusion Module (UAFM), which takes advantage of spatial and channel attention to produce a weight and then fuses the input features with the weight. Moreover, a Simple Pyramid Pooling Module (SPPM) is proposed to aggregate global context with low computation cost. Extensive evaluations demonstrate that PP-LiteSeg achieves a superior tradeoff between accuracy and speed compared to other methods. On the Cityscapes test set, PP-LiteSeg achieves 72.0% mIoU/273.6 FPS and 77.5% mIoU/102.6 FPS on NVIDIA GTX 1080Ti. Source code and models are available at Pad-dleSeg: https://github.com/PaddlePaddle/PaddleSeg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation aims to precisely predict the label of each pixel in an image. It has been widely applied in real-world applications, e.g. medical imaging <ref type="bibr" target="#b29">[30]</ref>, autonomous driving <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>, video conferencing <ref type="bibr" target="#b4">[5]</ref>, semiautomatic annotation <ref type="bibr" target="#b8">[9]</ref>. As a fundamental task in computer vision, semantic segmentation has attracted a lot of attention from researchers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>With the remarkable progress of deep learning, a lot of semantic segmentation methods have been proposed based on convolutional neural network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. FCN <ref type="bibr" target="#b17">[18]</ref> is the first fully convolutional network trained in an endto-end and pixel-to-pixel way. It also presents the primitively encoder-decoder architecture in semantic segmenta- tion, which is widely adopted in subsequent methods. To achieve higher accuracy, PSPNet <ref type="bibr" target="#b28">[29]</ref> utilizes a pyramid pooling module to aggregate global context and SFNet <ref type="bibr" target="#b14">[15]</ref> proposes a flow alignment module to strengthen the feature representations.</p><p>Yet, these models are not suitable for real-time applications because of their high computation cost. To accelerate the inference speed, Espnetv2 <ref type="bibr" target="#b20">[21]</ref> utilizes light-weight convolutions to extract features from an enlarged receptive field. BiSeNetV2 <ref type="bibr" target="#b25">[26]</ref> proposes bilateral segmentation network and extracts the detail features and semantic features separately. STDCSeg <ref type="bibr" target="#b7">[8]</ref> designs a new backbone named STDC to improve the computation efficiency. However, these models do not achieve satisfactory trade-off between accuracy and speed.</p><p>In this work, we propose a real-time and hand-craft net- work named PP-LiteSeg. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, PP-LiteSeg adopts the encode-decoder architecture and consists of three novel modules: Flexible and Lightweight Decoder (FLD), Unified Attention Fusion Module (UAFM) and Simple Pyramid Pooling Module (SPPM). The motivations and details of these modules are introduced as follows. The encoder in semantic segmentation models extracts hierarchical features, and the decoder fuses and unsamples features. For the features from low level to high level in encoder, the number of channels increases and the spatial size decreases, which is an efficient design. For the features from high level to low level in decoder, the spatial size increases, while the number of channels are the same in recent models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. Therefore, we present a Flexible and Lightweight Decoder (FLD), which gradually reduces the channels and increases the spatial size for the features. Besides, the volume of proposed decoder can be easily adjusted according to the encoder. The flexible design balances the computation complexity of encoder and decoder, which makes the overall model more efficient.</p><p>Strengthening feature representations is a crucial way to improve segmentation accuracy <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref>. It is usually achieved through fusing the low-level and high-level features in a decoder. However, the fusion modules in existing methods usually suffer from high computation cost. In this work, we propose a Unified Attention Fusion Module (UAFM) to strengthen feature representations efficiently. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, UAFM first takes advantage of the attention module to produce weight ?, and then fuses the input features with ?. In UAFM, there are two kinds of attention modules, i.e. spatial and channel attention modules, which exploit inter-spatial and inter-channel relationships of the input features.</p><p>Contextual aggregation is another key to promote seg-mentation accuracy, but previous aggregation modules are time-consuming for real-time networks. Based on the framework of PPM <ref type="bibr" target="#b28">[29]</ref>, we design a Simple Pyramid Pooling Module (SPPM), which reduces the intermediate and output channels, removes the short-cut, and replaces the concatenate operation with an add operation. Experimental results show SPPM contributes to the segmentation accuracy with low computation cost. We evaluate the proposed PP-LiteSeg through extensive experiments on Cityscapes and CamVid dataset. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, PP-LiteSeg achieves a superior tradeoff between segmentation accuracy and inference speed. Specifically, PP-LiteSeg achieves 72.0% mIoU/273.6 FPS and 77.5% mIoU/102.6 FPS on the Cityscapes test set.</p><p>Our main contributions are summarized as follows:</p><p>? We propose a Flexible and Lightweight Decoder (FLD), which mitigates the redundancy of the decoder and balances the computation cost of the encoder and decoder.</p><p>? We present a Unified Attention Fusion Module (UAFM) that utilizes channel and spatial attention to strengthen the feature representations.</p><p>? We propose a Simple Pyramid Pooling Module (SPPM) to aggregate global context. SPPM promotes the segmentation accuracy with minor extra inference time.</p><p>? Based on the above modules, we propose PP-LiteSeg, a real-time semantic segmentation model. Extensive experiments demonstrate our SOTA performance in terms of accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Segmentation</head><p>Deep learning has helped semantic segmentation make remarkable leap-forwards. FCN <ref type="bibr" target="#b17">[18]</ref> is the first full convolutional network for semantic segmentation. It is trained in an end-to-end and pixel-to-pixel way. Besides, images with arbitrary size can be segmented by FCN. Following the design of FCN, various methods have been proposed in later. Segnet <ref type="bibr" target="#b0">[1]</ref> applies the indices of max-pooling operation in encoder to upsampling operation in decoder. Therefore, the information in decoder is reused and the decoder produces refined features. PSPNet <ref type="bibr" target="#b28">[29]</ref> proposes the pyramid pooling module to aggregate local and global information, which is effective for segmentation accuracy. Besides, recent semantic segmentation methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> utilize the transformer architecture to achieve better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Real-time Semantic Segmentation</head><p>To fulfill the real-time demands of semantic segmentation, lots of methods have been proposed, e.g lightweight module design <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, dual-branch architecture <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, early-downsampling strategy <ref type="bibr" target="#b22">[23]</ref>, multiscale image cascade network <ref type="bibr" target="#b27">[28]</ref>. ENet <ref type="bibr" target="#b22">[23]</ref> uses an early-downsampling strategy to reduce the computation cost of processing large images and feature maps. For efficiency, ICNet <ref type="bibr" target="#b27">[28]</ref> designs a multi-resolution image cascade network. Based on bilateral segmentation network, Bisenet <ref type="bibr" target="#b25">[26]</ref> extracts the detail features and semantic features separately. The bilateral network is lightweight, so the inference speed is fast. STDCSeg <ref type="bibr" target="#b7">[8]</ref> proposes the channel-reduced receptive field-enlarged STDC module and designs an efficient backbone, which can strengthen the feature representations with low computation cost. To eliminate the redundancy in two-branch network, STDCSeg guides the features with detailed ground truth, so the efficiency is further improved. Espnetv2 <ref type="bibr" target="#b20">[21]</ref> uses group point-wise and depth-wise dilated separable convolutions to learn features from an enlarged receptive field in a computation friendly manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Feature Fusion Module</head><p>The feature fusion module is commonly used in semantic segmentation to strengthen feature representations. In addition to the element-wise summation and concatenation methods, researchers propose several methods as follows. In BiSeNet <ref type="bibr" target="#b25">[26]</ref>, the BGA module employs element-wise mul method to fuse the features from the spatial and contextual branches. To enhance the features with high-level context, DFANet <ref type="bibr" target="#b13">[14]</ref> fuses features in a stage-wise and subnet-wise way. To tackle the problem of misalignment, SFNet <ref type="bibr" target="#b14">[15]</ref> and AlignSeg <ref type="bibr" target="#b11">[12]</ref> first learn the transformation offsets through a CNN module, and then apply the transformation offsets to grid sample operation to gener- ate the refined feature. In detail, SFNet designs the flow alignment module. AlignSeg designs aligned feature aggregation module and the aligned context modeling module. FaPN <ref type="bibr" target="#b10">[11]</ref> solves the feature misalignment problem by applying the transformation offsets to deformable convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we first introduce Flexible and Lightweight Decoder (FLD), Unified Attention Fusion Module (UAFM) and Simple Pyramid Pooling Module (SPPM), respectively. Then, we present the architecture of PP-LiteSeg for real-time semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Flexible and Lightweight Decoder</head><p>Encoder-decoder architecture has been proved to be effective for semantic segmentation. In general, the encoder utilizes a series of layers grouped into several stages to extract hierarchical features. For the features from low level to high level, the number of channels gradually increases and the spatial size of the features decreases. This design balances the computation cost of each stage, which ensures the efficiency of the encoder. The decoder also has several stages, which are responsible for fusing and upsampling features. Although the spatial size of features increases from high level to low level, the decoder in recent lightweight models keeps the feature channels the same in all levels. Therefore, the computation cost of shallow stage is much larger than that of the deep stage, which leads to the computation redundancy in shallow stage. To improve the efficiency of decoder, we present a Flexible and Lightweight Decoder (FLD). As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, FLD gradually decreases the channels of the features from high level to low level. FLD can easily adjusts the computation cost to achieve better balance between encoder and decoder. Although the channels of features in FLD are decreasing, our experiments show that PP-LiteSeg achieves competitive accuracy compared to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unified Attention Fusion Module</head><p>As discussed above, fusing multi-level features is essential to achieve high segmentation accuracy. In addition to the element-wise summation and concatenation methods, researchers propose several methods, e.g. SFNet <ref type="bibr" target="#b14">[15]</ref>, FaPN <ref type="bibr" target="#b10">[11]</ref> and AttaNet <ref type="bibr" target="#b24">[25]</ref>. In this work, we propose a Unified Attention Fusion Module (UAFM) that applies channel and spatial attention to enrich the fused feature representations. UAFM Framework. As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref> (a), UAFM utilizes an attention module to produce the weight ?, and fuses the input features with ? by Mul and Add operations. In detail, the input features are denoted as F high and F low . F high is the output of the deeper module, and F low is the counterpart from the encoder. Note that they have same channels. UAFM first makes use of bilinear interpolation operation to upsample F high to the same size of F low , while the upsampled feature is denoted as F up . Then, the attention module takes F up and F low as input and produces the weight ?. Note that, the attention module can be a plugin, such as spatial attention module, channel attention module, etc. After that, to obtain attention-weighted features, we apply the element-wise Mul operation to F up and F low , respectively. Finally, UAFM performs element-wise addition for the attention-weighted features and outputs the fused feature. We can formulate the above procedure as equation 1.</p><formula xml:id="formula_0">F up = U psample(F high ) ? = Attention(F up , F low ) F out = F up ? ? + F low ? (1 ? ?)<label>(1)</label></formula><p>Spatial Attention Module. The motivation of the spatial attention module is exploiting the inter-spatial relationship to produce a weight, which represents the importance of each pixel in the input features. As shown in <ref type="figure" target="#fig_3">Figure 4</ref> (b), given the input features, i.e., F up ? R C?H?W and F low ? R C?H?W , we first perform mean and max operations along the channel axis to generates four features, of which the dimension is R 1?H?W . Afterwards, these four features are concatenated to a feature F cat ? R 4?H?W . For the concatenated feature, the convolution and sigmoid operations are applied to output ? ? R 1?H?W . The formulation of the spatial attention module is shown in equation 2. Furthermore, the spatial attention module can be flexible, e.g. removing the max operation to reduce computation cost.  </p><p>Channel Attention Module. The key concept of the channel attention module is leveraging the inter-channel relationship to generate a weight, which indicates the importance of each channel in the input features. As shown in <ref type="figure" target="#fig_3">Figure 4</ref> (b), the proposed channel attention module utilizes average-pooling and max-pooling operations to squeeze the spatial dimension of input features. This procedure generates four features with the dimension R C?1?1 . Then, it concatenates these four features along the channel axis and performs convolution and sigmoid operations to produce a weight ? ? R C?1?1 . In short, the procedures of the channel attention module can be formulated as equation 3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Simple Pyramid Pooling Module</head><p>As shown in <ref type="figure" target="#fig_7">Figure 5</ref>, we propose a Simple Pyramid Pooling Module (SPPM). It first leverages the pyramid pooling module to fuse the input feature. The pyramid pooling module has three global-average-pooling operations and the bin sizes are 1 ? 1, 2 ? 2, and 4 ? 4 respectively. Afterwards, the output features are followed by the convolution and upsampling operations. For the convolution operation, the kernel size is 1?1 and the output channel is smaller than the input channel. Finally, we add these upsampled features and apply a convolution operation to produce the refined feature. Compared to original PPM, SPPM reduces the intermediate and output channels, removes the short-cut, and replaces the concatenate operation with an addition operation. Consequently, SPPM is more efficient and suitable for real-time models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Architecture</head><p>The architecture of the proposed PP-LiteSeg is demonstrated in <ref type="figure" target="#fig_1">Figure 2</ref>. PP-LiteSeg mainly comprises three modules: encoder, aggregation and decoder.</p><p>Firstly, given an input image, PP-Lite utilizes a common lightweight network as encoder to extract hierarchical features. In detail, we choose the STDCNet <ref type="bibr" target="#b7">[8]</ref> for its outstanding performance. The STDCNet has 5 stages, the stride for each stage is 2, so the final feature size is 1/32 of the input image. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we present two versions of PP-LiteSeg, i.e., PP-LiteSeg-T and PP-LiteSeg-B, of which the encoder is STDC1 and STDC2 respectively. The PP-LiteSeg-B achieves higher segmentation accuracy, while the inference speed of PP-LiteSeg-T is faster. It is worth noting that we apply the SSLD <ref type="bibr" target="#b6">[7]</ref> method to the training of the encoder and obtain enhanced pre-trained weights, which is beneficial for the convergence of segmentation training.</p><p>Secondly, PP-LiteSeg adopts SPPM to model the longrange dependencies. Taking the output feature of the encoder as input, SPPM produces a feature that contains global context information.</p><p>Finally, PP-LiteSeg utilizes our proposed FLD to gradually fuse multi-level features and output the resulting image. Specifically, FLD consists of two UAFM and a segmentation head. For efficiency, we adopt spatial attention module in UAFM. Each UAFM takes two features as input, i.e., a low-level feature extracted by the stages of the encoder, a high-level feature generated by SPPM or the deeper fusion module. The latter UAFM outputs fused features with the down-sample ratio of 1/8. In the segmentation head, we perform Conv-BN-Relu operation to reduce the channels of the 1/8 down-sample feature to the number of classes. An upsampling operation is followed to expand the feature size to the input image size, and an argmax operation predicts the label of each pixel. The cross entropy loss with Online Hard Example Mining is adopted to optimize our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the datasets and implementation details. Then, we compare the experimental results in terms of accuracy and inference speed with other state-of-the-art real-time methods. Finally, we demonstrate the effectiveness of the proposed modules with the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Implementation Details</head><p>Cityscapes. The Cityscapes <ref type="bibr" target="#b5">[6]</ref> is a large-scale dataset for urban segmentation. It contains 5,000 fine annotated images, which are further split into 2975, 500, and 1525 images for training, validation and testing, respectively. The resolution of the images is 2048 ? 1024, which poses great challenges for the real-time semantic segmentation methods. The annotated images have 30 classes and our experiments only use 19 classes for a fair comparison with other methods. CamVid. Cambridge-driving Labeled Video Database (CamVid) <ref type="bibr" target="#b1">[2]</ref> is a small-scale dataset for road scene segmentation. There are 701 images with high-quality pixellevel annotations, in which 367, 101 and 233 images are chosen for training, validation and testing respectively. The images have the same resolution of 960 ? 720. The annotated images provide 32 categories, of which the subset of 11 categories are used in our experiments. Training Settings. Following the common setting, the stochastic gradient descent (SGD) algorithm with 0.9 momentum is chosen as an optimizer. We also adopt the warm-up strategy and the "poly" learning rate scheduler. For Cityscapes, the batch size is 16, the max iterations are 160,000, the initial learning rate is 0.005, and the weight decay in the optimizer is 5e ?4 . For CamVid, the batch size is 24, the max iterations is 1,000, the initial learning rate is 0.01, and the weight decay is 1e ?4 . For data augmentation, we make use of random scaling, random cropping, random </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation study</head><p>Ablation experiments are conducted to demonstrate the effectiveness of the proposed modules. The experiments choose PP-LiteSeg-B2 in the comparison and use the same training and inference setting. The baseline model is the PP-LiteSeg-B2 without the proposed modules, while the number of features channels is 96 in decoder and the fusion method is element-wise summation. <ref type="table">Table 3</ref> presents the quantitative results of our ablation study. We can find that the FLD in PP-LiteSeg-B2 improves the mIoU by 0.17%. Adding SPPM and UAFM also improve the segmentation accuracy, while the inference speed slightly decreases. Based on three proposed modules, PP-LiteSeg-B2 achieves 78.21 mIoU with 102.6 FPS. The mIoU is boosted by 0.71% compared to the baseline model. <ref type="figure" target="#fig_8">Figure 6</ref> provides the qualitative comparisons. We can observe that the predicted image becomes more consistent with the ground truth when adding FLD, SPPM and UAFM one by one. In short, our proposed modules are effective for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on CamVid</head><p>To further demonstrate the capability of PP-LiteSeg, we also conduct experiments on the CamVid dataset. Similar to other works, the input resolution for training and inference is 960 ? 720. As shown in <ref type="table" target="#tab_2">Table 4</ref>, PP-LiteSeg-T achieves 222.3 FPS, which is over 12.5% faster than other methods. PP-LiteSeg-B achieves the best accuracy, i.e., 75.0% mIoU  with 154.8 FPS. Overall, the comparisons show PP-LiteSeg achieves a state-of-the-art trade-off between accuracy and speed on Camvid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we focus on designing a novel real-time network for semantic segmentation. First, Flexible and Lightweight Decoder (FLD) is proposed to improve the efficiency of previous decoder. Then, we present a Unified Attention Fusion Module (UAFM), which is effective for strengthening feature representations. Furthermore, we propose a Simple Pyramid Pooling Module (SPPM) to aggregate global context with low computation cost. Based on these novel modules, we propose PP-LiteSeg, a realtime semantic segmentation network. Extensive experiments demonstrate that PP-LiteSeg achieves a state-of-theart trade-off between segmentation accuracy and inference speed. In future work, we plan to apply our methods into more tasks, such as matting and interactive segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The comparison of segmentation accuracy (mIoU) and inference speed (FPS) on the Cityscapes test set. The red dots represent our proposed PP-LiteSeg. The testing device is NVIDIA GTX 1080Ti. The experimental results demonstrate that PP-LiteSeg achieves state-of-the-art trade-off between accuracy and speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture overview. PP-LiteSeg consists of three modules: encoder, aggregation and decoder. A lightweight network is used as encoder to extract the features from different levels. The Simple Pyramid Pooling Module (SPPM) is responsible for aggregating the global context. The Flexible and Lightweight Decoder (FLD) fuses the detail and semantic features from high level to low level and outputs the result. Remarkably, FLD uses the Unified Attention Fusion Module (UAFM) to strengthen feature representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(a) The conventional encoder-decoder, in which the decoder keeps the features channels the same. (b) The encoder and proposed Flexible and Lightweight Decoder (FLD). FLD gradually reduces the channels for the features from high level to low level. Moreover, the volume of FLD is adjusted to conform to the encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>(a) The framework of Unified Attention Fusion Module (UAFM). (b) The Spatial Attention Module. (c) The Channel Attention Module. UAFM first uses spatial or channel attention modules to produce the weight ?, and then fuses the input features with Mul and Add operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F</head><label></label><figDesc>cat = Concat(M ean(F up ), M ax(F up ), M ean(F low ), M ax(F low )) ? = Sigmoid(Conv(F cat ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F</head><label></label><figDesc>cat = Concat(AvgP ool(F up ), M axP ool(F up ), AvgP ool(F low ), M axP ool(F low )) ? = Sigmoid(Conv(F cat ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Simple Pyramid Pooling Module (SPPM). Conv denotes convolution, batch norm and relu operations. The bin sizes of global-average-pooling are 1 ? 1, 2 ? 2 and 4 ? 4 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>The qualitative comparison on the Cityscapes validation set. (a)-(e) represent the predicted image of baseline, baseline+FLD, baseline+FLD+SPPM, baseline+FLD+UAFM and baseline+FLD+SPPM+UAFM respectively, (f) denotes the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The details of our proposed PP-LiteSeg.</figDesc><table><row><cell>Model</cell><cell cols="2">Encoder Channels in Decoder</cell></row><row><cell cols="2">PP-LiteSeg-T STDC1</cell><cell>32, 64, 128</cell></row><row><cell cols="2">PP-LiteSeg-B STDC2</cell><cell>64, 96, 128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>resized to 1024 ? 512 and 1536 ? 768, then the inference model takes the scaled image and produces the predicted image, finally, the predicted image is resized to the original size of the input image. The cost of the three steps is counted as the inference time. For CamVid, the inference model takes the original image as input, while the resolution is 960 ? 720.</figDesc><table><row><cell>Model ENet [23] ICNet [28] ESPNet [20]</cell><cell>Encoder -PSPNet50 ESPNet</cell><cell>Resolution 512 ? 1024 1024 ? 2048 512 ? 1024</cell><cell cols="3">mIoU (%) val test -58.3 -69.5 -60.3 112.9 FPS 76.9 30.3</cell><cell>Model Baseline PP-LiteSeg-B2 PP-LiteSeg-B2</cell><cell>FLD SPPM UAFM mIoU(%) 77.50 77.67 77.76</cell><cell>FPS 110.9 109.7 106.3</cell></row><row><cell>ESPNetV2 [21]</cell><cell>ESPNetV2</cell><cell>512 ? 1024</cell><cell cols="2">66.4 66.2</cell><cell>-</cell><cell>PP-LiteSeg-B2</cell><cell>77.89</cell><cell>105.5</cell></row><row><cell>SwiftNet [22]</cell><cell>ResNet18</cell><cell cols="3">1024 ? 2048 75.4 75.5</cell><cell>39.9</cell><cell>PP-LiteSeg-B2</cell><cell>78.21</cell><cell>102.6</cell></row><row><cell>BiSeNetV1 [27]</cell><cell>Xception39</cell><cell>768 ? 1536</cell><cell cols="3">69.0 68.4 105.8</cell><cell></cell><cell></cell></row><row><cell>BiSeNetV1-L [27] BiSeNetV2 [26] BiSeNetV2-L [26]</cell><cell>ResNet18 --</cell><cell>768 ? 1536 512 ? 1024 512 ? 1024</cell><cell cols="2">74.8 74.7 73.4 72.6 75.8 75.3</cell><cell>65.5 156 47.3</cell><cell cols="3">Table 3. Ablation study for our proposed modules on the vali-dation set of Cityscapes. The baseline model is PP-LiteSeg-B2</cell></row><row><cell>FasterSeg [4]</cell><cell>-</cell><cell cols="4">1024 ? 2048 73.1 71.5 163.9</cell><cell cols="2">without the proposed modules.</cell></row><row><cell>SFNet [15]</cell><cell>DF1</cell><cell>1024 ? 2048</cell><cell>-</cell><cell>74.5</cell><cell>121</cell><cell></cell><cell></cell></row><row><cell>STDC1-Seg50 [8]</cell><cell>STDC1</cell><cell>512 ? 1024</cell><cell cols="3">72.2 71.9 250.4</cell><cell></cell><cell></cell></row><row><cell>STDC2-Seg50 [8] STDC1-Seg75 [8] STDC2-Seg75 [8]</cell><cell>STDC2 STDC1 STDC2</cell><cell>512 ? 1024 768 ? 1536 768 ? 1536</cell><cell cols="3">74.2 73.4 188.6 74.5 75.3 126.7 77.0 76.8 97.0</cell><cell cols="3">presents the model information, input resolution, mIoU, and FPS of various approaches. Figure 1 provides an intuitive</cell></row><row><cell>PP-LiteSeg-T1</cell><cell>STDC1</cell><cell>512 ? 1024</cell><cell cols="3">73.1 72.0 273.6</cell><cell cols="3">comparison of segmentation accuracy and inference speed.</cell></row><row><cell>PP-LiteSeg-B1 PP-LiteSeg-T2 PP-LiteSeg-B2</cell><cell>STDC2 STDC1 STDC2</cell><cell>512 ? 1024 768 ? 1536 768 ? 1536</cell><cell cols="3">75.3 73.9 195.3 76.0 74.9 143.6 78.2 77.5 102.6</cell><cell cols="3">The experimental evaluations demonstrate that the proposed PP-LiteSeg achieves a state-of-the-art trade-off between ac-curacy and speed against other methods. Specifically, we</cell></row><row><cell cols="6">Table 2. The comparisons with state-of-the-art real-time methods</cell><cell cols="3">can observe PP-LiteSeg-T1 achieves 273.6 FPS and 72.0%</cell></row><row><cell cols="6">on Cityscapes. The training and inference setting refer to the im-</cell><cell cols="3">mIoU, which means the fastest inference speed and com-</cell></row><row><cell cols="2">plementation details.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">petitive accuracy. With the resolution of 768 ? 1536, PP-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">LiteSeg-B2 achieves the best accuracy, i,e. 78.2% mIoU for</cell></row><row><cell cols="6">horizontal flipping, random color jittering and normaliza-tion. The random scale ranges in [0.125, 1.5], [0.5, 2.5] for Cityscapes and Camvid respectively. The cropped resolu-</cell><cell cols="3">the validation set, 77.5% mIoU for the test set. In addition, with same encoder and input resolution as STDC-Seg, PP-LiteSeg shows better performance.</cell></row><row><cell cols="6">tion of Cityscapes is 1024?512, and the cropped resolution</cell><cell></cell><cell></cell></row><row><cell cols="6">of CamVid is 960 ? 720. All of our experiments are con-</cell><cell></cell><cell></cell></row><row><cell cols="6">ducted on Tesla V100 GPU using PaddlePaddle 1 [19]. Code</cell><cell></cell><cell></cell></row><row><cell cols="6">and pretrained models are available at PaddleSeg 2 [16].</cell><cell></cell><cell></cell></row><row><cell cols="6">Inference Settings. For a fair comparison, we export PP-</cell><cell></cell><cell></cell></row><row><cell cols="6">LiteSeg to ONNX and utilize TensorRT to execute the</cell><cell></cell><cell></cell></row><row><cell cols="6">model. Similar to other methods [8, 26], an image from</cell><cell></cell><cell></cell></row><row><cell cols="6">Cityscapes is first We conduct all inference exper-</cell><cell></cell><cell></cell></row><row><cell cols="6">iments under CUDA 10.2, CUDNN 7.6, TensorRT 7.1.3 on</cell><cell></cell><cell></cell></row><row><cell cols="6">NVIDIA 1080Ti GPU. We employ the standard mIoU for</cell><cell></cell><cell></cell></row><row><cell cols="6">segmentation accuracy comparison and FPS for inference</cell><cell></cell><cell></cell></row><row><cell cols="2">speed comparison.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">4.2. Experiments on Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">4.2.1 Comparisons with State-of-the-arts</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">With the training and inference setting mentioned above,</cell><cell></cell><cell></cell></row><row><cell cols="6">we compare the proposed PP-LiteSeg with previous state-</cell><cell></cell><cell></cell></row><row><cell cols="6">of-the-art real-time models on Cityscapes. For fair compar-</cell><cell></cell><cell></cell></row><row><cell cols="6">ison, we evaluate PP-LiteSeg-T and PP-LiteSeg-B at two</cell><cell></cell><cell></cell></row><row><cell cols="6">resolutions, i.e., 512 ? 1024 and 768 ? 1536. Table 2</cell><cell></cell><cell></cell></row><row><cell cols="3">1 https://github.com/PaddlePaddle/Paddle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">2 https://github.com/PaddlePaddle/PaddleSeg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The comparisons with state-of-the-art real-time methods on CamVid test set. The input resolution of all methods is 960 ? 720.</figDesc><table><row><cell>Model</cell><cell>Encoder</cell><cell>mIoU (%)</cell><cell>FPS</cell></row><row><cell>ENet [23]</cell><cell>-</cell><cell>51.3</cell><cell>61.2</cell></row><row><cell>ICNet [28]</cell><cell>PSPNet50</cell><cell>67.1</cell><cell>34.5</cell></row><row><cell>DFANet A [14]</cell><cell>Xception A</cell><cell>64.7</cell><cell>120</cell></row><row><cell>SwiftNet [22]</cell><cell>ResNet18</cell><cell>72.58</cell><cell>-</cell></row><row><cell>BiSeNetV1 [27]</cell><cell>Xception39</cell><cell>65.6</cell><cell>175</cell></row><row><cell>BiSeNetV1-L [27]</cell><cell>ResNet18</cell><cell>68.7</cell><cell>116.3</cell></row><row><cell>BiSeNetV2 [26]</cell><cell>-</cell><cell>72.4</cell><cell>124.5</cell></row><row><cell>BiSeNetV2-L [26]</cell><cell>-</cell><cell>73.2</cell><cell>32.7</cell></row><row><cell>STDC1-Seg [8]</cell><cell>STDC1</cell><cell>73.0</cell><cell>197.6</cell></row><row><cell>STDC2-Seg [8]</cell><cell>STDC2</cell><cell>73.9</cell><cell>152.2</cell></row><row><cell>PP-LiteSeg-T</cell><cell>STDC1</cell><cell>73.3</cell><cell>222.3</cell></row><row><cell>PP-LiteSeg-B</cell><cell>STDC2</cell><cell>75.0</cell><cell>154.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fasterseg: Searching for faster real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10917</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pp-humanseg: Connectivity-aware portrait segmentation with a large-scale teleconferencing video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncai</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baohua</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="202" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Beyond self-supervision: A simple yet effective network distillation alternative to improve backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05959</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking bisenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Achieving practical interactive segmentation with edge-guided flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1551" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fapn: Feature-aligned pyramid network for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Alignseg: Featurealigned segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="550" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Survey on semantic segmentation using deep learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Lateef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ruichek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="321" to="348" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic flow for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houlong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="775" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Paddleseg: A high-efficient development toolkit for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baohua</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Hao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06175</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09883</idno>
		<title level="m">Swin transformer v2: Scaling up capacity and resolution</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Paddlepaddle: An open-source deep learning platform from industrial practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Data and Domputing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="115" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (ECCV)</title>
		<meeting>the european conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9190" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comparative study of real-time semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moemen</forename><surname>Abdel-Razek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="587" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attanet: Attentionaugmented network for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangfu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3051" to="3068" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Mahfuzur Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
