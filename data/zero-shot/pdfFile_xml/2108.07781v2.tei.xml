<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Dense Video Captioning with Parallel Decoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Wang</surname></persName>
							<email>tengwang@connect.hku.hkruimao.zhang@ieee.orgluzhichaocn@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
							<email>f.zheng@ieee.orgranchengcn@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Dense Video Captioning with Parallel Decoding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dense video captioning aims to generate multiple associated captions with their temporal locations from the video. Previous methods follow a sophisticated "localizethen-describe" scheme, which heavily relies on numerous hand-crafted components. In this paper, we proposed a simple yet effective framework for end-to-end dense video captioning with parallel decoding (PDVC), by formulating the dense caption generation as a set prediction task. In practice, through stacking a newly proposed event counter on the top of a transformer decoder, the PDVC precisely segments the video into a number of event pieces under the holistic understanding of the video content, which effectively increases the coherence and readability of predicted captions. Compared with prior arts, the PDVC has several appealing advantages: (1) Without relying on heuristic non-maximum suppression or a recurrent event sequence selection network to remove redundancy, PDVC directly produces an event set with an appropriate size; (2) In contrast to adopting the two-stage scheme, we feed the enhanced representations of event queries into the localization head and caption head in parallel, making these two sub-tasks deeply interrelated and mutually promoted through the optimization; (3) Without bells and whistles, extensive experiments on ActivityNet Captions and YouCook2 show that PDVC is capable of producing high-quality captioning results, surpassing the state-of-the-art two-stage methods when its localization accuracy is on par with them. Code is available at https://github.com/ttengwang/PDVC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As an emerging branch of video understanding, video captioning has received an increasing attention in the recent past <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, aiming to generate a natural sentence to describe one main event of <ref type="figure">Figure 1</ref>: The defacto two-stage pipeline vs. the proposed PDVC. The two-stage "localize-then-describe" pipeline requires a denseto-sparse proposal generation and selection process before captioning, which contains hand-crafted components and can not effectively exploit the potential mutual benefits between localization and captioning. PDVC adopts the vision transformer to learn attentive interaction of different frames, where the learnable event queries are embedded to capture the relevance between the frames and the events. Two prediction heads run in parallel over query features, leveraging the mutual benefits between two tasks and improving their performance together. a short video. However, since realistic videos are usually long, untrimmed, and composed of a variety of events with irrelevant background contents, the above single-sentence captioning methods tend to generate sentences of blandness with less information. To circumvent the above dilemma, dense video captioning (DVC) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b37">37]</ref> is developed for automatically localizing and captioning multiple events in the video, which could reveal detailed visual contents and generate the coherent and complete descriptions.</p><p>Intuitively, dense video captioning can be divided into two subtasks, termed event localization and event captioning. As shown in <ref type="figure">Fig. 1</ref>, the previous methods usually solve this problem by a two-stage "localize-then-describe" pipeline. It firstly predicts a set of event proposals with accurate boundaries. By extracting fine-grained semantic cues and visual contexts of the proposal, the detailed sentence description is finally decoded by the caption generator. The above scheme is straightforward but suffers from the following issues: 1) By considering the captioning as the downstream task, the performance of such a scheme highly relies on the quality of the generated event proposals, which limits the mutual promotion of these two sub-tasks. 2) The performance of proposal generators in previous methods depends on careful anchor design <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">34]</ref> and proposal selection post-processing (e.g., non-maximum suppression <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">34]</ref>). These hand-crafted components introduce additional hyper-parameters that highly rely on manual thresholding strategies, hindering the progress toward a fully end-to-end captioning generation.</p><p>To tackle the above issues, this paper proposes a pure end-to-end dense Video Captioning framework with Parallel Decoding termed PDVC. As shown in <ref type="figure">Fig. 1</ref>, instead of invoking the two-stage scheme, we directly feed the intermediate representation used for proposal generation into a captioning head that is parallel to the localization head. By doing so, PDVC aims to directly exploit inter-task association at the feature level. The intermediate feature vectors and the target events could be matched in a one-to-one correspondence, making the feature representations more discriminative to identify a specific event.</p><p>In practice, we consider the dense video captioning task as a set prediction problem. The proposed PDVC directly decodes the frame features, which are extracted from a Vision Transformer, into an event set with their locations and corresponding captions by applying two parallel prediction heads, i.e., localization head and captioning head. Since the appropriate size of the event set is an essential indicator for dense captioning quality <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b48">48]</ref>, a newly proposed event counter is also stacked on the top of the Transformer decoder to further predict the number of final events. By introducing such a simple module, PDVC could precisely segment the video into a number of event pieces under the holistic understanding of the video content, avoiding the information missing as well as the replicated caption generation caused by unreliable event number estimation. We evaluate our model on two large-scale video benchmarks, ActivityNet Captions and YouCook2. Even with a lightweight caption head (vanilla LSTM), our method can achieve comparable performance against state-of-theart methods which adopts well-designed attention-based LSTM <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b34">34]</ref> or Transformer <ref type="bibr" target="#b31">[31]</ref>. In addition, we show quantitatively and qualitatively that the generated proposals gain benefit from the paralleling decoding design. Even with a weakly supervised setting (without location annotations), we show our model can implicitly learn the locationaware features from captions.</p><p>To summarize, the major contributions of this paper are three folds. 1) We propose a novel end-to-end dense video captioning framework named PDVC by formulating DVC as a parallel set prediction task, significantly simplifying the traditional pipeline which highly depends on hand-crafted components. 2) We further improve PDVC with a novel event counter to estimate the number of events in the video, greatly increasing the readability of generated captions by avoiding the unrealistic event number estimation. 3) Extensive experiments on ActivityNet Captions and YouCook2 show state-of-the-art performance over existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Temporal event proposals. Temporal event proposals (TEP), also called temporal action proposals, aims to predict temporal segments containing event instances in untrimmed videos. Mainstream approaches can be divided into two categories: anchor-based and boundary-based. Anchor-based methods <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b47">47]</ref> pre-define a vast number of anchors at different scales with regular intervals, followed by the evaluation network to score each anchor. However, the pre-defined scales and intervals can not cover all temporal patterns, especially in videos with variable temporal scales. The boundary-based methods <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b53">53]</ref> combine salient frames with high confidences to form proposals in a local-to-global manner. Both types of methods contain hand-crafted designs (e.g., NMS and rule-based label assignment), which require careful manual threshold selection and are not a strictly end-to-end method.</p><p>Dense video captioning. Dense video captioning is a multitask problem that combines event localization and event captioning. <ref type="bibr">Krishna et al. [5]</ref> propose the first dense video captioning model, containing a multi-scale proposal module for localization and an attention-based LSTM for contextaware caption generation. Some of the following work aims to enrich the event representations by context modeling <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b28">28]</ref>, event-level relationships <ref type="bibr" target="#b34">[34]</ref>, or multi-modal feature fusion <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36]</ref>, enabling more accurate and informative captioning generation.</p><p>One of the limitations of the above approaches is that the localization module can not benefit from the captioning module. Some researchers try to explore the interaction between two sub-tasks. Li et al. <ref type="bibr" target="#b6">[7]</ref> introduce a proxy task, i.e., predicting language rewards of generated sentences, as an additional optimization goal of the localization module. Zhou et al. <ref type="bibr" target="#b31">[31]</ref> propose a differential masking mechanism to link the gradient flow from captioning loss to proposals' boundaries, enabling a joint optimization of two tasks. We argue that neither a binary mask vector <ref type="bibr" target="#b31">[31]</ref> nor a scalar descriptiveness score <ref type="bibr" target="#b6">[7]</ref> carries enough informative gradients of linguistic cues to guide the internal feature representation in the proposal module during the back-propagation training. Instead, the proposed PDVC exploits the intertask interactions by enforcing the two sub-tasks share the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Encoding</head><p>Parallel Decoding rank~ attention weights <ref type="figure">Figure 2</ref>: Overview of the proposed method. Firstly, we adopt a pre-trained video feature extractor and a transformer encoder to obtain a sequence of frame-level features. A transformer decoder and three prediction heads are then proposed to predict the locations, captions, and the number of events given learnable event queries. We provide two types of caption heads, which are based on vanilla LSTM and deformable soft-attention enhanced LSTM, respectively. In the testing stage, we select the top detected events by ranking the captioning score and localization score, with no need to remove redundancy by non-maximum suppression.</p><p>same intermediate features. Moreover, we adopt one-to-one matching between intermediate feature vectors and target event instances to obtain the discriminative features for captioning, significantly different from previous methods with a many-to-one anchor assignment strategy.</p><p>Another promising direction focuses on the coherence of generated captions. Early work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b24">24]</ref> usually generates a large number of proposal-caption pairs (10 times more than the number of ground-truth events) for a high recall, where massive redundancy greatly reduces the readability and coherence of generated captions. SDVC <ref type="bibr" target="#b8">[9]</ref> is the first to tackle this problem by introducing a "localizeselect-describe" pipeline. Given the output proposals produced by a TEP model, they develop an RNN-based event sequence generation network (ESGN) to select a small set of proposals, reducing the predicted proposal number from 100 to 2.85 on average. Though promising performance is achieved, SDVC is not an end-to-end model, making a multi-step training strategy necessary. The recurrent nature also restricts the application of ESGN to handle long videos with a large number of events. We parallelize localization, selection, and captioning tasks in a single end-to-end framework, largely simplifying the pipeline while enabling generating accurate and coherent captions.</p><p>Transformer-based detector. Transformer <ref type="bibr" target="#b16">[17]</ref> is an encoder-decoder architecture based on an attention mechanism for natural language processing. Benefit from the significant ability to capture long-range relationships, Transformer has been successfully applied and shows promising performance in computer vision <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b45">45]</ref>. Detection Transformer (DETR) <ref type="bibr" target="#b39">[39]</ref> is a newly emerging solution to object detection, which considers object detection as a set prediction task and does not rely on any hand-crafted components. Though it offers promising performance, DETR suffers from the high training time due to the slow convergence of global attention mechanism. Deformable Transformer <ref type="bibr" target="#b38">[38]</ref> is proposed to speed up the network training and gain better performance by attending to sparse spatial locations of images and incorporating multi-scale feature representation. Inspired by the simple design and promising performance of DETR-style detectors in the image domain, we extend Deformable Transformer into a more challenging dense video captioning task in the video domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>To simplify the dense video captioning pipeline and explore the mutual benefits between localization task and captioning task, we directly detect a set of temporally-localized captions with an appropriate size {(t s j , t e j , S j )} Nset j=1 , where t s j , t e j , S j represent the starting time, ending time, and the caption of an event, respectively. The set size N set is also predicted by PDVC.</p><p>Specifically, a deformable transformer with an encoderdecoder structure is adopted to capture the inter-frame, inter-event, and event-frame interactions by attention mechanism and produce a set of event query features. Then, two parallel prediction heads predict the boundaries and captions for each event query simultaneously. An event counter predicts the event number N set from a global view. The final results are obtained by select top N set events with high confidence to ensure a complete and coherent story. <ref type="figure">Fig. 2</ref> shows the overview of the proposed PDVC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary: Deformable Transformer</head><p>Deformable Transformer <ref type="bibr" target="#b38">[38]</ref> is an encoder-decoder architecture based on multi-scale deformable attention (MS-DAtt). MSDAtt mitigates the slow convergency problem of the self-attention <ref type="bibr" target="#b16">[17]</ref> in Transformer when processing image feature maps, by attending to a sparse set of sampling points around reference points. Given multi-scale feature maps X = {x l } L l=1 where x l ? R C?H?W , a query element q j and a normalized reference point p j ? [0, 1] 2 , MSDAtt outputs a context vector by the weighted sum of K?L sampling points across feature maps at L scales:</p><formula xml:id="formula_0">MSDAtt(q j , p j , X) = L l=1 K k=1 A jlk Wx lp jlk p jlk = ? l (p j ) + ?p jkl ,<label>(1)</label></formula><p>wherep jkl and A jkl are the position and the attention weight of k-th sampled key at l-th scale for j-th query element, respectively. W is the projection matrix for key elements. ? l projects normalized reference points into the feature map at l-th level. ?p jkl are sampling offsets w.r.t. ? l (p j ). Both A jkl and ?p jkl are obtained by linear projection onto the query element. Note that the original MSDAtt applies multi-head attention mechanism, while here we illustrate the single-head version for better understanding.</p><p>Deformable Transformer replaces the self-attention modules in the Transformer encoder and the cross-attention modules in the Transformer decoder with deformable attention modules, enabling a fast convergence rate and better representation ability in object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Encoding</head><p>To capture rich spatio-temporal features in a video, we first adopt a pre-trained action recognition network (e.g., C3D <ref type="bibr" target="#b15">[16]</ref>, TSN <ref type="bibr" target="#b22">[22]</ref>) to extract the frame-level features. We re-scale the feature map's temporal dimension to a fixed number T by interpolation to facilitate batch processing. Then, to better utilize multi-scale features for predicting multi-scale events, we add L temporal convolutional layers (stride=2, kernel size=3) to get feature sequences across multiple resolutions, from T to T /2 L . The multi-scale frame features with their positional embedding <ref type="bibr" target="#b16">[17]</ref> are fed into the deformable transformer encoder, extracting the frame-frame relationship across multiple scales. The output frame features are denoted as {f l } L l=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Parallel Decoding</head><p>The decoding network contains a deformable transformer decoder and three parallel heads, a captioning head for caption generation, a localization head to predict events' boundaries with confidence scores, and an event counter to predict a suitable event number. The decoder aims to directly query the event-level features from frame features conditioned on N learnable embedding (termed event queries) {q j } N j=1 , and their corresponding scalar referent points p j . Note that p j is predicted by a linear projection with a sigmoid activation over q j . The event queries and reference points serve as the initial guess of the events' features and locations (center points), and they will be refined iteratively at each decoding layer, as in <ref type="bibr" target="#b38">[38]</ref>. The output query features and the reference point are denoted asq j ,p j .</p><p>Localization head. Localization head performs box prediction and binary classification for each event query. Box prediction aims to predict the 2D relative offsets (center and length) of the ground-truth segment w.r.t. the reference point. Binary classification aims to generate the foreground confidence of each event query. Both box prediction and binary classification are implemented by multi-layer perceptron. After that, we obtain a set of tuples {t s j , t e j , c loc j } N j=1 to represent the detected events, where c loc j is the localization confidence of the event queryq j .</p><p>Captioning head. We provide two captioning heads, a lightweight one and a standard one. The lightweight head simply feedsq j into a vanilla LSTM at each timestamp. The word w jt is predicted by an FC layer followed by a softmax activation over the hidden state h jt of LSTM.</p><p>However, the lightweight captioning head only receives an event-level representationq j , lacking the interactions between linguistic cues and frame features. Soft attention (SA) <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b8">9]</ref> is a widely-used module in video captioning, which can dynamically determine the importance of each frame when generating a word. Traditional twostage methods <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b8">9]</ref> aligns the event segments and their captions by restricting the attention area being within the event boundaries, but our captioning head can not access events' boundaries, increasing the optimization difficulty to learn relationships between the linguistic word and frames. To alleviate this problem, we propose the deformable soft attention (DSA) to enforce the soft attention weights focus on a small area around the reference points. Specifically, when generating the t-th word w t , we first generate K sampling points from each f l conditioned on both language query h jt and event queryq j , following Eqn. 1, where h jt denotes the hidden state in LSTM. Then we consider K?L sampling points as the key/value and [h jt ,q j ] as the query in soft attention. Since the sampling points are distributed around the reference pointp j , the output features z jt of DSA are restricted to attend on a relatively small region. The LSTM takes as input the concatenation of the context features z jt , event query featuresq j and previous words w j,t?1 . The probability for next word w jt is obtained by an FC layer with softmax activation over h jt . With the evolving of LSTM, we obtain a sentence S j = {w j1 , ..., w jMj }, where M j is the sentence length.</p><p>Event counter. Considering that an appropriate event number is an essential indicator for dense captioning quality: too many events cause replicated captions and bad readability; too few detected events mean information missing and incomplete story. The event counter aims to detect the event number of the video. It contains a max-pooling layer and an FC layer with softmax activation, which first compress the most salient information of event queries {q j } to a global feature vector, and then predict a fix-size vector r len , where each value refers to the possibility of a specific number. During the inference stage, predicted event number is obtained by N set = argmax(r len ). The final outputs are obtained by selecting the top N set events with accurate boundaries and good captions from N event queries. The confidence of each event query is calculated by:</p><formula xml:id="formula_1">c j = c loc j + ? M j ? Mj t=1 log(c cap jt ),<label>(2)</label></formula><p>where c cap jt is the probability of the generated word. We observe that the average word confidence is not a convincing measurement of sentence-level confidence since the captioning head tends to produce overestimated confidence for short sentences. Thus, we add a modulation factor ? to rectify the influence of caption length. ? is the balance factor.</p><p>Set prediction loss. During training, PDVC produces a set of N events with their locations and captions. To match predicted events with ground truths in a global scheme, we use the Hungarian algorithm following <ref type="bibr" target="#b39">[39]</ref> to find the best bipartite matching results. The matching cost is defined as C = ? giou L giou + ? cls L cls , where L giou represents the generalized IOU <ref type="bibr" target="#b61">[61]</ref> between predicted temporal segments and ground-truth segments, L cls represents the focal loss <ref type="bibr" target="#b60">[60]</ref> between the predicted classification score and the groundtruth label. The matched pairs are selected to calculate the set prediction loss, which is the weighted sum of gIOU loss, classification loss, countering loss, and caption loss:</p><formula xml:id="formula_2">L = ? giou L giou + ? cls L cls + ? ec L ec + ? cap L cap ,<label>(3)</label></formula><p>where L cap measures the cross-entropy between the predicted word probability and the ground truth normalized by the caption length, L ec is also the cross-entropy loss between the predicted count distribution and the ground truth.</p><p>Note that we follow <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b38">38]</ref> to add prediction heads to each layer of the transformer decoder. The final loss is the summation of the set prediction losses of all decoder layers.</p><p>PDVC for paragraph captioning. Paragraph captioning <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b27">27]</ref> is a simplified version of dense video captioning, which focuses on generating a coherent paragraph and does not need to predict the temporal location of each sentence. PDVC can easily extend to paragraph captioning by removing the localization function and taking the pre-extracted proposals as input event queries. Specifically, we consider the linear embeddings of proposals' position as the event queries and use the proposals' center as reference points. Then PDVC is trained with caption loss only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Datasets. We use two large-scale benchmark datasets, ActivityNet Captions <ref type="bibr" target="#b4">[5]</ref>, and YouCook2 <ref type="bibr" target="#b30">[30]</ref> to evaluate the effectiveness of the proposed PDVC. ActivityNet Captions contains 20k long untrimmed videos of various human activities. On average, each video lasts 120s and is annotated with 3.65 temporally-localized sentences. We follow the standard split with 10009/4925/5044 videos for training, validation, and test. YouCook2 has 2000 untrimmed videos of cooking procedures with an average duration of 320s. Each video has 7.7 annotated segments with associated sentences. We use the official split with 1333/457/210 videos for training, validation, and test.</p><p>Evaluation metrics. We evaluate our method in three aspects: 1) For localization performance, we use the average precision, average recall across IOU at {0.3, 0.5, 0.7, 0.9} and their harmonic mean, F1 score. 2) For dense captioning performance, we follow the official evaluation tool provided by ActivityNet Challenge 2018, which calculates the average precision (measured by BLEU4 <ref type="bibr" target="#b10">[11]</ref>, METEOR <ref type="bibr" target="#b5">[6]</ref>, and CIDEr <ref type="bibr" target="#b17">[18]</ref>) of the matched pairs between generated captions and the ground truth across IOU thresholds of {0.3, 0.5, 0.7, 0.9}. However, the official scorer does not consider the storytelling quality, i.e., how well the generated captions can cover the video's whole story. We further adopt SODA c <ref type="bibr" target="#b48">[48]</ref> for an overall evaluation. 3) For paragraph captioning performance, we form a paragraph by sorting generated captions according to their starting time and report the paragraph-level captioning performance. Note that ActivityNet Captions has two sets of annotations for the validation set. For SODA c, we evaluate it by two sets independently and report their average score.</p><p>Implementation details. For ActivityNet Captions, we use a C3D <ref type="bibr" target="#b15">[16]</ref> pre-trained on Sports1M <ref type="bibr" target="#b52">[52]</ref> to extract framelevel features. To fairly compare with state-of-the-art methods, we also test our model based on TSN <ref type="bibr" target="#b22">[22]</ref> features provided by <ref type="bibr" target="#b31">[31]</ref>, and I3D+VGGish features provided by <ref type="bibr" target="#b36">[36]</ref>. For YouCook2, we use the same TSN features as in <ref type="bibr" target="#b31">[31]</ref>.</p><p>We use a two-layer deformable transformer with multiscale (4 levels) deformable attention. The deformable transformer uses a hidden size of 512 in MSDAtt layers and 2048 in feed-forward layers. The number of event queries is 10/100 for ActivityNet Captions/YouCook2. We implement a lightweight PDVC (termed PDVC light) with the vanilla LSTM captioner and the standard PDVC with the LSTM-DSA captioner. The LSTM hidden dimension in captioning heads is 512. For the event counter, we choose the maximum count as 10/20 for ActivityNet Captions/YouCook2. In Eqn. 2, the length modulation factor ? is set to 2, and the trade-off ratio ? is set to 0.3/1.0 for PDVC light/PDVC.    The cost ratios in the bipartite matching are ? giou :? cls =2:1 and the loss ratios are ? giou :? cls :? ec :? cap =2:1:1:1. We use Adam <ref type="bibr" target="#b3">[4]</ref> optimizer with an initial learning rate of 5e-5 and the mini-batch size of 1 video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art Methods</head><p>Localization performance. The comparison of event localization quality is shown in <ref type="table">Table 1</ref>. SDVC and MFT generate event proposals by a sophisticated "localize-selectdescribe" workflow. In contrast, PDVC removes the handcrafted designs in the traditional proposal modules and directly outputs the proposals in a parallel manner, which is more efficient to deal with long sequences than recurrent counterparts. We surpass MFT by a large margin and achieve similar (slightly better) performance to SDVC, which shows the effectiveness of parallel set prediction in our method. Besides, the choice of the captioning head can slightly influence the balance of precision and recall.</p><p>Dense captioning performance. In <ref type="table" target="#tab_3">Table 3</ref>, we list the performance of state-of-the-art models with crossentropy training 1 on ActivityNet Captions. With ground- <ref type="bibr" target="#b0">1</ref> A few methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">34]</ref> incorporates Reinforcement Learning (RL) <ref type="bibr" target="#b13">[14]</ref> after the cross-entropy training to further boost the performance. Note that we do not compare with these methods since RL training requires a truth proposals, PDVC achieves considerable improvement over the state-of-the-art on BLEU4 and CIDEr, which shows a deformable transformer plus an LSTM captioner can give good caption quality. With predicted proposals, PDVC with C3D features achieves the best performance on BLEU4/METEOR/CIDEr/SODA c, giving a 22.22%/4.31%/75.87%/63.35% relative improvement over state-of-the-art scores. We find that PDVC with groundtruth proposals does not show much superiority over ECHR on METEOR but surpasses ECHR with predicted proposals, indicating the generated proposals of PDVC are much better. Even with a lightweight LSTM as a captioner, PDVC light can surpass most two-stage approaches on BLEU4/CIDEr/SODA c. The reason mainly comes from the parallel decoding of the captioning head and localization head, which helps to generate proposals with high descriptiveness and discriminative internal representation. <ref type="table" target="#tab_2">Table 2</ref> shows the dense captioning performance on the YouCook2 validation set. Our method achieve state-of-theart performance with a considerable performance gain over other methods on all metrics.</p><p>Paragraph captioning performance. <ref type="table" target="#tab_4">Table 4</ref> shows the comparison between PDVC and state-of-the-art paragraph captioning methods. With ground-truth proposals, PDVC with a deformable transformer plus an attention-based LSTM can surpass several transformer-based captioning models, like MART, VTrans, and Trans-XL, indicating the strong representation ability of deformable attention in the encoder-decoder and the LSTM-DSA. It is promising for PDVC to get a further performance boost by incorporating a transformer captioner. We leave this for future work.</p><p>Even with predicted proposals, we observe PDVC has a comparable performance with previous methods with ground-truth proposals, indicating that query features contain rich information covering main segments in videos. While most previous paragraph captioning methods require ground-truth annotation at testing, our model reduces the captioning module's dependence on accurate proposals by parallel decoding, raising the possibility of generating good paragraphs without human annotations of the test video.</p><p>Efficiency. We compare the inference time of PDVC against two-stage methods TDA-CG <ref type="bibr" target="#b24">[24]</ref>, MT <ref type="bibr" target="#b31">[31]</ref> under the same hardware environment in <ref type="table" target="#tab_6">Table 5</ref>. Our methods are more efficient since: 1) Only a few event proposals with their captions are predicted in parallel; 2) We do not require a dense-to-sparse proposal selection like NMS; 3) MSDAtt is an efficient operator due to the sparse sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Interaction between Localization &amp; Captioning</head><p>In this part, we go deeper into the mutual effect between two subtasks. It is straightforward that localization can aid captioning since the localization supervision guides the query features to specific ground-truth regions, which contains rich semantics matching the target captions. Therefore, we focus on how captioning task affects proposals' quality, which is less explored in the previous literature.</p><p>Captioning supervision helps generate proposals with descriptiveness. To better study the quality of proposals generated by PDVC, we use the same pre-trained event captioning model <ref type="bibr" target="#b26">[26]</ref> to evaluate the descriptiveness of generated proposals of different models. We also reimplemented two mainstream proposal generation modules SST and SST+ESGN for comparison. Both SST and SST+ESGN are trained with localization loss only, while PDVC is trained with both localization and captioning loss. As shown in Table 6, PDVC achieves a slightly lower F1 score but the best descriptiveness score among the four models.</p><p>We match each generated proposal with one groundtruth segment with the highest overlap. <ref type="figure" target="#fig_0">Fig. 3</ref> demonstrates the statistics of matching results. Surprisingly, incorporating caption supervision yields a considerable boost in caption quality of high-precision proposals (i.e., IOU&gt;0.9). The reason may be that the captioning head is trained based    When further incorporating ESGN for adaptively proposal selection, the majority of proposals are with 0.6&lt;IOU&lt;0.9. Ours and Ours loc only achieve a similar IOU distribution to SST+ESGN, but do not introduce any hand-crafted components like anchor generation and NMS. Generally speaking, descriptiveness is positively correlated to the precision of proposals with an ideal captioner. However, the performance of existing captioners is still far from satisfactory, which means they generate wrong or boring captions for some proposals. To reduce improper captions of the final results, it is essential to generate not only location-accurate but caption-aware proposals. Our model provides an effective solution to explore the mutual benefits between localization and captioning by parallel decoding.</p><p>Captioning supervision helps learn location-aware features. Another advantage of parallel decoding is that we can directly remove the localization head to study the behavior of captioning head. We train an event proposal generation module based on merely captioning supervision, by   making some modifications to the original PDVC to stabilize training, such as fixing the sampling offsets in the decoder and using the captioning cost in bipartite matching. More details can be found in the supplementary material. After iterative refinement in the decoder, we directly regard the reference points corresponding to event queries in the last decoder layer as event proposals. <ref type="figure" target="#fig_1">Fig. 4</ref> shows the position distribution of predicted proposals on YouCook2. We also report quantitative results such as recall and precision.</p><p>As the training epoch increases, proposals' center tends to spread uniformly, and the proposals' length tends to focus on a relatively small value. Though a noticeable gap exists between the distributions of predicted proposals and ground-truth proposals, we see that the predicted proposals are gradually approaching the ground truth during training. The recall/precision at epoch 13 is 30.32/14.24, which is better than that at initialization (24.14/13.56). Based on the above findings, we argue that our method can implicitly capture the location-aware features from caption supervision, helping the optimization of the event localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>Deformable components. As shown in <ref type="table" target="#tab_9">Table 7a</ref>, when removing deformable operations from deformable transformer or LSTM-DSA, the performance degrades considerably. We conclude that: 1) Adding locality into transformer helps to extract temporally-sensitive features for localization-aware tasks; 2) Focusing on a small segment around the proposals rather than the whole video helps the optimization of the event captioning.</p><p>Query number &amp; event counter. As shown in <ref type="table" target="#tab_9">Table 7b</ref>, only a few queries are sufficient for good performance. Too many queries lead to high precision and METEOR, but low Recall and SODA c. We choose an appropriate query number for striking a balance of recall and precision. The final event number also controls the balance of precision and recall. The event counter can predict a reasonable number of event instances, making the generated captions reveal a whole story in the video.</p><p>Length modulation. <ref type="table" target="#tab_9">Table 7c</ref> shows that modulating the caption length (?&gt;1) obtains a better trade-off between METEOR &amp; SODA c and Precision &amp; Recall than averaging (?=1) or summing (?=0) the word scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents PDVC, an end-to-end dense video captioning framework with parallel decoding, which formulates dense video captioning as a set prediction task. PDVC directly produces a set of temporally-localized sentences without a dense-to-sparse proposal generation and selection process, significantly simplifying the traditional "localizethen-describe" pipeline. Prediction heads for event localization and event captioning run in parallel to exploit the inter-task mutual benefits. Experiments on two benchmark datasets show that PDVC can generate high-quality captions and surpass state-of-the-art methods.</p><formula xml:id="formula_3">log(c cap jj ? t ),</formula><p>where ? = 2 is the modulation factor of the caption length. The final cost matrix for bipartite matching is:</p><formula xml:id="formula_4">C = C cap + ? cls L cls ,<label>(4)</label></formula><p>where ? cls = 0.5 is the balance factor. Based on the above modification, we train PDVC light with merely captioning loss on YouCook2. We choose the lightweight captioning head to ease the optimization difficulty. During inference, we directly use the reference points in the last layer as the predicted proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Visualization</head><p>Predicted proposals. We visualize the distribution of generated proposals of PDVC in <ref type="figure" target="#fig_2">Fig. 5</ref>. For the ActivityNet Captions dataset, ground-truth proposals are distributed evenly across different positions and different lengths. However, for YouCook2, the length of most ground-truth proposals is relatively small (less than 25% of the video duration). From the figure, we conclude that: 1) Each query describes a specific mode of the proposals' location. 2) All queries can predict video-wide proposals with coherence and low redundancy and generate a similar distribution with ground truth. 3) Event queries serve as a location prior for localization tasks, which are trained to learn location patterns of events from human annotations.</p><p>Activity types. The dense captioning performance of PDVC varies in different activity types. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the METEOR score of PDVC with predicted/ground-truth proposals on 200 activity classes. Our model seems to generate more accurate captions with activities containing distinct scene cues or large objects, like "riding bumper cars", "playing squash", and "calf roping". However, activities that rely more on fine-grained action cues or small objects tend to get a worse METEOR, like "doing karate", "gargling mouthwash", and "rollerblading". It is promising to achieve a performance improvement to incorporate the finegrained object features and a more powerful action recognition model.</p><p>Temporally-localized captions. <ref type="figure" target="#fig_5">Fig. 7</ref> shows the generated captions with their temporal locations of different models. The captions of MT <ref type="bibr" target="#b31">[31]</ref> are generated based on ground-truth proposals, while PDVC light and PDVC are with predicted proposals. For the second video, MT and PDVC light misrecognize the shoes as a dog and a tire, respectively. Instead, PDVC can generate accurate and meaningful captions with predicted proposals, which verifies the effectiveness of the proposed parallel decoding mechanism and the captioning head with deformable soft attention.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>e1: A man is standing in a room. e2: He has a ball on a tennis racket. e3: He throws the ball in the air and hits it with the racket. e1 e2 e3 e1 e2 e3 e3 e2 e1 PDVC e1: a man is standing on a court. e2: a man is seen standing on a tennis court holding a tennis racket. e3: the man then serves the ball and hits the ball.</p><p>PDVC_light e1: he throws the ball back and forth. e2: he is then seen spinning around and throwing a ball. e3: he throws the ball back and forth.</p><p>MT e1: a man is seen standing on a court holding a tennis racket. e2: a man is standing on a court. e3: the man serves the ball with the racket. Ground Truth e1: A close up of a candle is shown as well as a picture of a man praying in the desert. e2: A person is then seen taking off a pair of shoes in front of him. e3: The man sets the shots in between his legs and speaks to the camera. e1 e2 e3 e1 e2 e3 e3 e2 e1 PDVC e1: a close up of a piece of shoes are shown followed by a person putting a piece of shoes. e2: the person is then seen putting the shoes on the ground. e3: the man then puts the shoes on the shoes and begins to the camera.</p><p>PDVC_light e1: a person is seen kneeling down on a table and begins to the camera. e2: the person is then seen putting a tire on the floor and begins to the camera. e3: the person is then shown on the floor.</p><p>MT e1: a close up of a &lt;unk&gt; is shown followed by a person walking into frame e2: the man then begins to &lt;unk&gt; the dog 's leg. e3: the man then grabs a pair of &lt;unk&gt; and begins to &lt;unk&gt; the shoe. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Caption quality vs. IOU. We omit the pairs with IOU&lt; 0.3 (less than 2% of all pairs). on event query features corresponding to accurate proposals, so PDVC learns to enhance the descriptiveness of the high-precision proposals. The last subfigure shows the IOU distribution of matched pairs. Most proposals produced by SST are not very accurate (mainly with 0.5&lt;IOU&lt;0.8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The distribution of predicted proposals without localization supervision. We plot the predicted proposals of 200 randomly sampled videos in the YouCook2 validation set. Horizontal and vertical axes represent the re-scaled center position and re-scaled length of proposals, respectively. Each sub-figure contains 30 clusters with different colors corresponds to 30 input event queries. R and P refer to recall and precision of the 30 generated proposals, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The distribution of predicted proposals and ground-truth proposals. Horizontal and vertical axes represent the normalized center position and normalized length of proposals, respectively. For each dataset, we report the results of 200 randomly sampled videos on the validation set. The sub-figure (a)/(c) contain 10/100 clusters with different colors, where each cluster corresponds to one event query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Dense captioning performance of PDVC on different activity classes. Activity labels are from the ActivityNet1.3 dataset [?].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of predicted dense captions. Incorrect phases are underlined in red and the correct ones in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.5 0.7 0.9 avg 0.3 0.5 0.7 0.9 avg MFT [40] 46.18 29.76 15.54 5.77 24.31 86.34 68.79 38.30 12.19 51.41 33.01 SDVC [9] 93.41 76.40 42.40 10.10 55.58 96.71 77.73 44.84 10.99 57.57 56.56 PDVC light 88.78 71.74 45.70 17.45 55.92 96.83 78.01 41.05 14.69 57.65 56.77 PDVC 89.47 71.91 44.63 15.67 55.42 97.16 78.09 42.68 14.40 58.07 56.71 Table 1: Event localization on the ActivityNet Captions validation set. PDVC light 0.89 4.56 23.07 4.34 PDVC 0.80 4.74 22.71 4.42</figDesc><table><row><cell>Method</cell><cell>Recall 0.3 0Method Precision F1</cell><cell cols="2">Predicted proposals B4 M C SODA c</cell></row><row><cell></cell><cell>MT [31]</cell><cell>0.30 3.18 6.10</cell><cell>-</cell></row><row><cell></cell><cell>ECHR [34]</cell><cell>-3.82 -</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Dense captioning on YouCook2.</figDesc><table><row><cell>Method</cell><cell>Features</cell><cell cols="3">Ground-truth proposals B4 M C</cell><cell cols="3">Predicted proposals B4 M C SODA c</cell></row><row><cell>DCE [5]</cell><cell>C3D</cell><cell cols="5">1.60 8.88 25.12 0.17 5.69 12.43</cell><cell>-</cell></row><row><cell>TDA-CG [24]  *</cell><cell>C3D</cell><cell>-</cell><cell>9.69</cell><cell>-</cell><cell cols="2">1.31 5.86 7.99</cell><cell>-</cell></row><row><cell>DVC [7]</cell><cell>C3D</cell><cell cols="5">1.62 10.33 25.24 0.73 6.93 12.61</cell><cell>-</cell></row><row><cell>SDVC [9]</cell><cell>C3D</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-6.92</cell><cell>-</cell><cell>-</cell></row><row><cell>Efficient [37]</cell><cell>C3D</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">1.35 6.21 13.82</cell><cell>-</cell></row><row><cell>ECHR [34]</cell><cell>C3D</cell><cell cols="6">1.96 10.58 39.73 1.29 7.19 14.71 3.22</cell></row><row><cell>PDVC light</cell><cell>C3D</cell><cell cols="6">2.61 10.48 47.83 1.51 7.11 26.21 5.17</cell></row><row><cell>PDVC</cell><cell>C3D</cell><cell cols="6">2.64 10.54 47.26 1.65 7.50 25.87 5.26</cell></row><row><cell>MT [31]  *</cell><cell>TSN</cell><cell cols="5">2.71 11.16 47.71 1.15 4.98 9.25</cell><cell>-</cell></row><row><cell>PDVC</cell><cell>TSN</cell><cell cols="6">3.07 11.27 52.53 1.78 7.96 28.96 5.44</cell></row><row><cell cols="7">MDVC [35]  *  ? I3D+VGGish 1.98 11.07 42.67 1.01 6.86 7.77</cell><cell>-</cell></row><row><cell>BMT [36]  *  ?</cell><cell cols="6">I3D+VGGish 1.99 10.90 41.85 1.88 7.43 11.94</cell><cell>-</cell></row><row><cell>PDVC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? I3D+VGGish 3.12 11.26 53.65 1.96 8.08 28.59 5.42</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">Features B4</cell><cell>M</cell><cell>C</cell></row><row><cell cols="2">Ground-truth proposals</cell><cell></cell><cell></cell></row><row><cell>HSE [41]</cell><cell>V</cell><cell cols="3">9.84 13.78 18.78</cell></row><row><cell>MART [42]</cell><cell cols="4">V+F 10.33 15.68 23.42</cell></row><row><cell>VTrans [31]</cell><cell cols="4">V+F 9.75 15.64 22.16</cell></row><row><cell cols="5">Trans-XL [43] V+F 10.39 15.09 21.67</cell></row><row><cell>GVD [44]</cell><cell cols="4">V+F+O 11.04 15.71 21.95</cell></row><row><cell cols="5">GVDsup [44] V+F+O 11.30 16.41 22.94</cell></row><row><cell>AdvInf [33]</cell><cell cols="4">V+F+O 10.04 16.60 20.97</cell></row><row><cell>PDVC</cell><cell cols="4">V+F 11.80 15.93 27.27</cell></row><row><cell cols="2">Predicted proposals</cell><cell></cell><cell></cell></row><row><cell>MFT [40]</cell><cell></cell><cell></cell><cell></cell></row></table><note>Dense captioning on the ActivityNet Captions validation set. B4/M/C is short for BLEU4/METEOR/CIDEr.* indicates results re-evaluated by the same eval- uation toolkit.? means results with part of the dataset (9% videos missing).V+F 10.29 14.73 19.12 PDVC V+F 10.24 15.80 20.45</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Paragraph captioning on the Activi- tyNet Captions ae-val set [44]. V/F/O refers to visual/flow/object features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Inference speed. We report average inference time (secs/video) of 100 sampled videos with a single Tesla V100 GPU. 56.35 58.69 57.49 0.98 6.71 19.36 Ours (PDVC) loc.+cap. 3.03 55.42 58.07 56.71 1.24 7.03 21.91</figDesc><table><row><cell>Method</cell><cell>Loss</cell><cell>#p Rec. Pre. F1 B4 M</cell><cell>C</cell></row><row><cell>SST [1]</cell><cell>loc.</cell><cell cols="2">3.00 42.00 60.99 49.74 0.98 6.70 17.34</cell></row><row><cell>SST+ESGN [9]</cell><cell>loc.</cell><cell cols="2">2.79 53.80 61.37 57.33 1.09 6.80 19.67</cell></row><row><cell>Ours loc only</cell><cell>loc.</cell><cell>3.26</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Proposal quality with different loss types. Rec./Pre./F1 measures the localization performance, while B4/M/C measures dense captioning performance. #p is the number of proposals.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Ablation studies on the ActivityNet Captions validation set. Subfigure (b) and (c) are based on PDVC light.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">more complex captioning network (e.g., Hierarchical RNN<ref type="bibr" target="#b27">[27]</ref>) and extralong training time, which is opposite to the design philosophy of PDVC. Moreover, RL training tends to produce longer sentences with repeated phrase<ref type="bibr" target="#b23">[23]</ref>, lowering the coherence and readability of generated captions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epoch 1</head><p>6. Supplementary Materials 6.1. More Implementation Details Event proposal generation module based on merely captioning supervision. In Sec. 4.3, we make the following modifications to train an event proposal generation module without localization supervision: 1) We extend the 1D reference point to the 2D reference point p j = (p c j , p l j ), where p c j , p l j denote the center and the length of the reference point, respectively. 2) For each decoder layer, we fix the sampling keys in deformable attention as K = 4 evenly spaced positions over a specified interval from p c j ? 0.5p l j to p c j + 0.5p l j to stabilize the network training. 3) Without gIOU cost in bipartite matching, it is hard to accurately assign the target captions to event queries. We design the caption cost to mitigate this problem. Given any groundtruth caption S j ? = {w j ? t } M j ? t=1 and any event query features q j , we obtain the output probabilities {c cap jj ? t } M j ? t=1 predicted by the captioning head with teacher forcing, where M j ? denotes the caption length. The caption cost matrix is calculated by:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SST: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video captioning with attention-based LSTM and semantic consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2045" to="2055" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL Workshop Intrinsic Extrinsic Eval. Measures Mach. Transl. Summarization</title>
		<meeting>ACL Workshop Intrinsic Extrinsic Eval. Measures Mach. Transl. Summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jointly localizing and describing events for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2929" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Streamlined dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4594" to="4602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Annu. Meet. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="311" to="318" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Memory-attended recurrent network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8347" to="8356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sports video captioning via attentive motion representation and group relationship modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2019.2921655.1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol., early access</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Proc. Sys</title>
		<meeting>Adv. Neural Inf. . Sys</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. North Amer. Chapter Assoc. Comput. Linguistics-Human Language Technol</title>
		<meeting>North Amer. Chapter Assoc. Comput. Linguistics-Human Language Technol</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1494" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reconstruction network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7622" to="7631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Describing like humans: On diversity in image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4195" to="4203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bidirectional attentive fusion with context gating for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint event detection and description in continuous video streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanishka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. Applicat. Comput. Vis</title>
		<meeting>IEEE Winter Conf. Applicat. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="396" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical context encoding for events captioning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1288" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show, tell and summarize: Dense video captioning using visual cue aided sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2019.2936526.1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">early access</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7590" to="7598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Endto-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>2, 3, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video captioning with guidance of multimodal latent topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1838" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial inference for multi-sentence video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Event-centric hierarchical representation for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-modal dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, 2020</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A better use of audio-visual cues: Dense video captioning with bi-modal transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vis. Conf., 2020</title>
		<meeting>British Mach. Vis. Conf., 2020</meeting>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An efficient framework for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn</title>
		<meeting>Int. Conf. Learn</meeting>
		<imprint/>
	</monogr>
	<note>Represent., 2021. 3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sergey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Move forward and tell: A progressive generator of video descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Crossmodal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Grounded video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12299" to="12310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Soda: Story oriented dense video captioning evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis., 2020</title>
		<meeting>Eur. Conf. Comput. Vis., 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11499" to="11506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bmn: Boundarymatching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
