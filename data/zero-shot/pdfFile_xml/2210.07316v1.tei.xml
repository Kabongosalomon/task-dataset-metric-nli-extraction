<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MTEB: Massive Text Embedding Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Magne</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MTEB: Massive Text Embedding Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 56 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-theart results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://huggingface. co/spaces/mteb/leaderboard.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language embeddings power a variety of use cases from clustering and topic representation <ref type="bibr" target="#b0">(Aggarwal and Zhai, 2012;</ref><ref type="bibr">Angelov, 2020)</ref> to search systems and text mining <ref type="bibr">(Huang et al., 2020;</ref><ref type="bibr">Zhu et al., 2021;</ref><ref type="bibr">Nayak, 2019)</ref> to feature representations for downstream models <ref type="bibr">(Saharia et al., 2022;</ref><ref type="bibr">Borgeaud et al., 2022)</ref>. Using generative language models or cross-encoders for these applications is often intractable, as they may require exponentially more computations <ref type="bibr">(Reimers and Gurevych, 2019)</ref>.</p><p>However, the evaluation regime of current text embedding models rarely covers the breadth of * Most of the work done while at Hugging Face their possible use cases. For example, <ref type="bibr">Sim-CSE (Gao et al., 2021)</ref> or <ref type="bibr">SBERT (Reimers and Gurevych, 2019)</ref> solely evaluate on STS and classification tasks, leaving open questions about the transferability of the embedding models to search or clustering tasks. STS is known to poorly correlate with other real-world use cases <ref type="bibr">(Neelakantan et al., 2022;</ref><ref type="bibr">Wang et al., 2021)</ref>. Further, evaluating embedding methods on many tasks requires implementing multiple evaluation pipelines. Implementation details like pre-processing or hyperparameters may influence the results making it unclear whether performance improvements simply come from a favorable evaluation pipeline. This leads to the "blind" application of these models to new use cases in industry or requires incremental work to reevaluate them on different tasks.</p><p>The Massive Text Embedding Benchmark (MTEB) aims to provide clarity on how models perform on a variety of embedding tasks and thus serves as the gateway to finding universal text embeddings applicable to a variety of tasks. MTEB consists of 56 datasets covering 112 languages from 8 embedding tasks: Bitext mining, classification, clustering, pair classification, reranking, retrieval, STS and summarization. MTEB software is available open-source 1 enabling evaluation of any embedding model by adding less than 10 lines of code. Datasets and the MTEB leaderboard are available on the Hub 2 .</p><p>We evaluate over 30 different models on MTEB with additional speed and memory benchmarking to provide a holistic view of the state of text embedding models. We cover both models available opensource as well as models accessible via APIs, such as the OpenAI Embedding endpoint. We find there to be no single best solution, with different mod-els dominating different tasks. Our benchmarking sheds light on the weaknesses and strengths of individual models, such as <ref type="bibr">SimCSE's (Gao et al., 2021)</ref> low performance on clustering and retrieval despite its strong performance on STS. We hope our work makes selecting the right embedding model easier and simplifies future model comparisons.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Benchmarks</head><p>Yearly released SemEval datasets <ref type="bibr">(Agirre et al., 2012</ref><ref type="bibr">(Agirre et al., , 2013</ref><ref type="bibr">(Agirre et al., , 2014</ref><ref type="bibr">(Agirre et al., , 2015</ref><ref type="bibr">(Agirre et al., , 2016</ref> are commonly used as the go-to text embedding benchmark. SemEval datasets correspond to the task of semantic textual similarity (STS) requiring models to embed similar sentences with geometrically close embeddings. Due to the limited expressivity of a single SemEval dataset, SentEval (Conneau and Kiela, 2018) aggregates multiple STS datasets. SentEval focuses on fine-tuning classifiers on top of embeddings. It lacks tasks like retrieval or clustering, where embeddings are directly compared without additional classifiers. Further, the toolkit was proposed in 2018 and thus does not provide easy support for recent trends like text embeddings from transformers <ref type="bibr">(Reimers and Gurevych, 2019)</ref>. Due to the insufficiency of STS benchmarking, USEB <ref type="bibr">(Wang et al., 2021)</ref> was introduced consisting mostly of reranking tasks. Consequently, it does not cover tasks like retrieval or classification. Meanwhile, the recently released BEIR Benchmark <ref type="bibr">(Thakur et al., 2021)</ref> has become the standard for the evaluation of embeddings for zero-shot information retrieval.</p><p>MTEB unifies datasets from different embedding tasks into a common, accessible framework. MTEB incorporates SemEval datasets (STS11 -STS22) and BEIR alongside a variety of other datasets from various tasks to provide a holistic performance review of text embedding models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Embedding Models</head><p>Text embedding models like Glove <ref type="bibr">(Pennington et al., 2014)</ref> lack context awareness and are thus commonly labeled as Word Embedding Models. They consist of a layer mapping each input word to a vector often followed by an averaging layer to provide a final embedding invariant of input length. Transformers <ref type="bibr">(Vaswani et al., 2017)</ref> inject context awareness into language models via self-attention and form the foundation of most recent embedding models. <ref type="bibr">BERT (Devlin et al., 2018)</ref> uses the transformer architecture and performs large-scale self-supervised pre-training. The resulting model can directly be used to produce text embeddings via an averaging operation alike Glove. Building on InferSent <ref type="bibr">(Conneau et al., 2017)</ref>, <ref type="bibr">SBERT (Reimers and Gurevych, 2019)</ref> demonstrated it to be beneficial to perform additional fine-tuning of the transformer for competitive embedding performance. Most recent fine-tuned embedding models use a contrastive loss objective to perform supervised fine-tuning on positive and negative text pairs <ref type="bibr">(Gao et al., 2021;</ref><ref type="bibr">Wang et al., 2021;</ref><ref type="bibr">Ni et al., 2021b;</ref><ref type="bibr">Muennighoff, 2022)</ref>. Due to the large variety of available pre-trained transformers <ref type="bibr">(Wolf et al., 2020)</ref>, there is an at least equally large variety of potential text embedding to be explored. This leads to confusion about which model provides practitioners with the best performance for their embedding use case.</p><p>We benchmark both word embedding and transformer models on MTEB quantifying gains provided by often much slower context aware models.</p><p>3 The MTEB Benchmark 3.1 Desiderata MTEB is built on a set of desiderata: (a) Diversity: MTEB aims to provide an understanding of the usability of embedding models in various use cases. The benchmark comprises 8 different tasks, with up to 15 datasets each. Of the 56 total datasets in MTEB, 10 are multilingual, covering 112 different languages. Sentence-level and paragraph-level datasets are included to contrast performance on short and long texts. (b) Simplicity: MTEB provides a simple API for plugging in any model that given a list of texts can produce a vector for each list item with a consistent shape. This makes it possible to benchmark a diverse set of models. (c) Extensibility: New datasets for existing tasks can be benchmarked in MTEB via a single file that specifies the task and a HuggingFace dataset name where the data has been uploaded <ref type="bibr">(Lhoest et al., 2021)</ref>. New tasks require implementing a task interface for loading the data and an evaluator for benchmarking. We welcome dataset, task or metric contributions from the community via pull requests to continue the development of MTEB. (d) Reproducibility: Through versioning at a dataset and software level, we aim to make it easy to reproduce results in MTEB. JSON files corresponding to all results available in this paper have been made available together with the MTEB benchmark 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tasks and evaluation</head><p>Figure 1 provides an overview of tasks and datasets available in MTEB. Dataset statistics are available in <ref type="table" target="#tab_4">Table 2</ref>. The benchmark consists of the following 8 task types:</p><p>Bitext Mining Inputs are two sets of sentences from two different languages. For each sentence in the first set, the best match in the second set needs to be found. The matches are commonly translations. The provided model is used to embed each sentence and the closest pairs are found via cosine similarity. F1 serves as the main metric for bitext mining. Accuracy, precision and recall are also computed.</p><p>Classification A train and test set are embedded with the provided model. The train set embeddings are used to train a kNN-classifier, which is scored on the test set. The main metric is accuracy with average precision and f1 additionally provided.</p><p>Clustering Given a set of sentences or paragraphs, the goal is to group them into meaningful clusters. A k-means model is trained on the embedded texts and scored using v-measure <ref type="bibr">(Rosenberg and Hirschberg, 2007)</ref>. V-measure does not depend Pair Classification A pair of text inputs is provided and a label needs to be assigned. Labels are typically binary variables denoting duplicate or paraphrase pairs. The two texts are embedded and their distance is computed with various metrics (cosine similarity, dot product, euclidean distance, manhattan distance). Using the best binary threshold accuracy, average precision, f1, precision and recall are computed. The average precision score based on cosine similarity is the main metric.</p><p>Reranking Inputs are a query and a list of relevant and irrelevant reference texts. The aim is to rank the results according to their relevance to the query. The model is used to embed the references which are then compared to the query using cosine similarity. The resulting ranking is scored for each query and averaged across all queries. Metrics are mean MRR@k and MAP with the latter being the main metric.</p><p>Retrieval Each dataset consists of a corpus, queries and a mapping for each query to relevant documents from the corpus. The aim is to find these relevant documents. The provided model is used to embed all queries and all corpus documents and similarity scores are computed using cosine similarity. After ranking the corpus documents for each query based on the scores, nDCG@k, MRR@k, MAP@k, precision@k and recall@k are computed for several values of k. nDCG@10 serves as the main metric. MTEB reuses datasets and evaluation from BEIR <ref type="bibr">(Thakur et al., 2021)</ref>.</p><p>Semantic Textual Similarity (STS) Given a sentence pair the aim is to determine their similarity. Labels are continuous scores with higher numbers indicating more similar sentences. The provided model is used to embed the sentences and their similarity is computed using various distance metrics. Distances are benchmarked with ground truth similarities using Pearson and Spearman correlations. Spearman correlation based on cosine similarity serves as the main metric <ref type="bibr">(Reimers et al., 2016)</ref>.</p><p>Summarization A set of human-written and machine-generated summaries are provided. The aim is to score the machine summaries. The provided model is first used to embed all summaries. For each machine summary embedding, distances to all human summary embeddings are computed. The closest score (e.g. highest cosine similarity) is kept and used as the model's score of a single machine-generated summary. Pearson and Spearman correlations with ground truth human assessments of the machine-generated summaries are computed. Like for STS, Spearman correlation based on cosine similarity serves as the main metric <ref type="bibr">(Reimers et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Datasets</head><p>To further the diversity of MTEB, datasets of varying text lengths are included. All datasets are grouped into three categories:</p><p>Sentence to sentence (S2S) A sentence is compared with another sentence. An example of S2S are all current STS tasks in MTEB, where the similarity between two sentences is assessed.</p><p>Paragraph to paragraph (P2P) A paragraph is compared with another paragraph. MTEB imposes no limit on the input length, leaving it up to the models to truncate if necessary. Several clustering tasks are framed as both S2S and P2P tasks. The former only compare titles, while the latter include both title and content. For ArxivClustering, for example, abstracts are concatenated to the title in the P2P setting.</p><p>Sentence to paragraph (S2P) A few retrieval datasets are mixed in a S2P setting. Here a query is a single sentence, while documents are long paragraphs consisting of multiple sentences. Similarities across 56 MTEB datasets are visualized in <ref type="figure" target="#fig_1">Figure 2</ref>. Several datasets rely on the same corpora, such as ClimateFEVER and FEVER, resulting in a score of 1. Clusters of similar datasets can be seen among CQADupstack variations or STS datasets. S2S and P2P variations of the same dataset tend to also be similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Models</head><p>We focus on benchmarking models with state-ofthe-art results in various embedding tasks. This leads to a high representation of transformer-based approaches <ref type="bibr">(Vaswani et al., 2017)</ref>. We group models into self-supervised and supervised methods.</p><p>Self-supervised methods (a) Transformerbased <ref type="bibr">BERT (Devlin et al., 2018)</ref> is trained using self-supervised mask and sentence prediction tasks. By taking the mean across the sequence length (mean-pooling) the model can directly be used to produce text embeddings. <ref type="bibr">SimCSE-Unsup (Gao et al., 2021)</ref> uses BERT as a foundation and performs additional self-supervised training. (b) Non-transformer: <ref type="bibr">Komninos (Komninos and Manandhar, 2016)</ref> and <ref type="bibr">Glove (Pennington et al., 2014)</ref> are two word embedding models that directly map words to vectors. Hence, their embeddings lack context awareness, but provide significant speed-ups.</p><p>Supervised methods The original transformer model <ref type="bibr">(Vaswani et al., 2017)</ref> consists of an encoder and decoder network. Subsequent transformers often train only encoders like <ref type="bibr">BERT (Devlin et al., 2018)</ref> or decoders like <ref type="bibr">GPT (Radford et al., 2019)</ref>.</p><p>(a) Transformer encoder methods coCondenser (Gao and Callan, 2021), Contriever <ref type="bibr">(Izacard et al., 2021</ref><ref type="bibr">), LaBSE (Feng et al., 2020</ref> and <ref type="bibr">SimCSE-BERT-sup (Gao et al., 2021)</ref> are based on the pre-trained BERT model <ref type="bibr">(Devlin et al., 2018)</ref>. coCondenser and Contriever add a self-supervised stage prior to supervised fine-tuning for a total of three training stages. LaBSE uses BERT to perform additional pre-training on parallel data to produce a competitive bitext mining model. SPECTER (b) Transformer decoder methods SGPT Bi-Encoders (Muennighoff, 2022) perform contrastive fine-tuning of &lt;0.1% of pre-trained parameters using weighted-mean pooling. Similar to ST5 and GTR, SGPT-nli models are geared towards STS, while SGPT-msmarco models towards retrieval. SGPT-msmarco models embed queries and documents for retrieval with different special tokens to help the model distinguish their role. For nonretrieval tasks, we use its query representations. We benchmark publicly available SGPT models based on <ref type="bibr">GPT-NeoX (Andonian et al., 2021)</ref>, <ref type="bibr">GPT-J (Wang and Komatsuzaki, 2021)</ref> and BLOOM <ref type="bibr">(Workshop, 2022)</ref>. Alternatively, cpt-text (Neelakantan et al., 2022) passes pre-trained GPT decoders through a two-stage process using last token pooling to provide embeddings from decoders. We benchmark their models via the OpenAI Embeddings API 4 .</p><p>(c) Non-transformer LASER (Heffernan et al., 2022) is the only context aware non-transformer model we benchmark, relying on an LSTM (Hochreiter and Schmidhuber, 1997) instead. Similar to LaBSE, the model trains on parallel data and focuses on bitext mining applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>Based on the results in <ref type="table">Table 1</ref>, we observe that there is considerable variability between tasks. No model claims the state-of-the-art in all seven English tasks. There is even more variability in the results per dataset present in the appendix. Further, there remains a large gap between self-supervised and supervised methods. Self-supervised large language models have been able to close this gap in many natural language generation tasks <ref type="bibr">(Chowdhery et al., 2022)</ref>. However, they appear to still require supervised fine-tuning for competitive embedding performance.</p><p>We find that performance strongly correlates with model size, see <ref type="figure" target="#fig_3">Figure 3</ref>. A majority of MTEB tasks are dominated by multi-billion parameter models. However, these come at a significant <ref type="figure">Figure 4</ref>: Performance, speed, and size of produced embeddings (size of the circles) of different embedding models. Embedding sizes range from 1.2 kB (Glove / Komninos) to 16.4 kB (SGPT-5.8B) per example. Speed was benchmarked on STS15 using 1x Nvidia A100 80GB with CUDA 11.6. cost as we investigate in Section 4.3.</p><p>Classification ST5 models dominate the classification task across most datasets, as can be seen in detail in the full results in the appendix. ST5-XXL has the highest average performance, 3% ahead of the best non-ST5 model, Ada Similarity.</p><p>Clustering Despite being almost 50x smaller, the MPNet embedding model is on par with the ST5-XXL state-of-the-art on Clustering. This may be due to the large variety of datasets MPNet (and MiniLM) has been fine-tuned on. Clustering requires coherent distances between a large number of embeddings. Models like SimCSE-sup or SGPTnli, which are only fine-tuned on a single dataset, NLI, may produce incoherent embeddings when encountering topics unseen during fine-tuning. Relatedly, we find that the query embeddings of SGPTmsmarco and the Ada Search endpoint are competitive with SGPT-nli and the Ada Similarity endpoint, respectively. We refer to the appendix for Ada Search results. This could be due to the MS-MARCO dataset being significantly larger than NLI. Thus, while the OpenAI docs recommend using the similarity embeddings for clustering use cases 5 , the retrieval query embeddings may be the better choice in some cases.</p><p>Pair Classification GTR-XL and GTR-XXL have the strongest performance. Pair classification is closest to STS in its framing, yet models rank significantly differently on the two tasks. This highlights the importance of benchmarking on a diverse set of tasks to avoid blindly reusing a model for a different task.</p><p>Reranking MPNet and MiniLM models perform strongly on reranking tasks. On SciDocsRR (Cohan et al., 2020a) they perform far better than bigger models, which is likely due to parts of Sci-DocsRR being included in their training data. Our scale of experiments and that of model pre-training make controlling for data contamination challenging. Thus, we ignore overlap of MTEB datasets with model training datasets in MTEB scores. As long as enough datasets are averaged, we believe these effects to be insignificant.</p><p>Retrieval SGPT-5.8B-msmarco is the best embedding model on the BEIR subset in MTEB as well as on the full BEIR benchmark (Thakur  <ref type="bibr">, 2021;</ref><ref type="bibr">Muennighoff, 2022)</ref>. The even larger 7.1B SGPT model making use of BLOOM (Workshop, 2022) performs significantly weaker, which is likely due to the multilinguality of BLOOM. Models geared towards STS (SimCSE, ST5, SGPTnli) perform badly on retrieval tasks. Retrieval tasks are unique in that there are two distinct types of texts: Queries and documents ("asymmetric"), while other tasks only have a single type of text ("symmetric"). On the QuoraRetrieval dataset, which has been shown to be largely symmetric <ref type="bibr">(Muennighoff, 2022)</ref>, the playing field is more even with SGPT-5.8B-nli outperforming SGPT-5.8B-msmarco (See appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STS &amp; Summarization</head><p>Retrieval models (GTR, SGPT-msmarco) perform badly on STS, while ST5-XXL has the highest performance. This highlights the bifurcation of the field into separate embedding models for retrieval (asymmetric) and similarity (symmetric) use cases (Muennighoff, 2022).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficiency</head><p>We investigate the latency-performance trade-off of models in <ref type="figure">Figure 4</ref>. The graph allows for significant elimination of model candidates in the model selection process. It brings model selection down to three clusters:</p><p>Maximum speed Word Embedding models offer maximum speed with Glove taking the lead on both performance and speed, thus making the choice simple in this case.</p><p>Maximum performance If latency is less important than performance, the left-hand side of the graph offers a cluster of highly performant, but slow models. Depending on the task at hand, GTR-XXL, ST5-XXL or SGPT-5.8B may be the right choice, see Section 4.2. SGPT-5.8B comes with the additional caveat of its high-dimensional embeddings requiring more storage.</p><p>Speed and performance The fine-tuned MPNet and MiniLM models lead the middle cluster making the choice easy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multilinguality</head><p>MTEB comes with 10 multilingual datasets across bitext mining, classification and STS tasks. We investigate performance on these in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>Bitext Mining LaBSE (Feng et al., 2020) performs strongly across a wide array of languages in bitext mining. LASER2 provides additional models for rare languages, but we use the same default LASER2 for all languages to compare models one-to-one. Thus, the high variance of LASER2 performance may be resolved by using its other variants. MPNet, MiniLM and SGPT-BLOOM-7B1-msmarco perform poorly on languages they have not been pre-trained on, such as German or Japanese for the latter.</p><p>Classification &amp; STS On multilingual classification and STS, the multilingual MPNet provides the overall strongest performance. It outperforms the slightly faster multilingual MiniLM on almost all languages. Both models have been trained on the same languages, thus bringing decisionmaking down to performance vs speed. SGPT-BLOOM-7B1-msmarco provides state-of-the-art performance on languages like Hindi, Portuguese, Chinese or French, which the model has seen extensively during pre-training, but is not significantly ahead of the much cheaper MPNet. LASER2 performs consistently worse than other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion &amp; Future Work</head><p>In this work, we presented the Massive Text Embedding Benchmark (MTEB). Consisting of 8 text embedding tasks with up to 15 datasets each and covering 112 languages, MTEB aims to provide reliable embedding performance estimates. By opensourcing MTEB alongside a leaderboard, we provide a foundation for further pushing the state-ofthe-art of available text embeddings.</p><p>To introduce MTEB, we have conducted the most comprehensive benchmarking of text embeddings to date. Through the course of close to 5,000 experiments on over 30 different models, we have set up solid baselines for future research to build on. We found model performance on different tasks to vary strongly with no model claiming state-ofthe-art on all tasks. Our studies on scaling behavior, model efficiency and multi-linguality revealed various intricacies of models that should ease the decision-making process for future research or industry applications of text embeddings.</p><p>We welcome future dataset or metric contributions to the MTEB codebase 6 as well as additions to the MTEB leaderboard via our automatic submission format 7 .  <ref type="table" target="#tab_4">Table 2</ref> provides a summary along with statistics of all MTEB tasks. In the following, we give a brief description of each dataset included in MTEB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Clustering</head><p>ArxivClusteringS2S, ArxivClusteringP2P, BiorxivClusteringS2S, BiorxivClusteringP2P, MedrxivClusteringP2P, MedrxivClusteringS2S All these datasets were created for MTEB, using the public API from arXiv 8 and bioRxiv/medRxiv 9 . For S2S datasets, the input text is simply the title of the paper, while for P2P the input text is the concatenation of the title and the abstract.</p><p>The cluster labels are generated using categories given to the papers by humans. For bioRxiv and medRxiv this category is unique, but for arXiv multiple categories can be given to a single paper so we only use the first one. For bioRxiv and medRxiv there is only one level of category (e.g. biochemistry, genetics, microbiology, etc.) hence we only perform clustering based on that label. However for arXiv, there is a main category and secondary category: for example "cs.AI" meaning the main category is Computer Science and the sub-category is AI, math.AG means the main category is Mathematics and the sub-category is Algrebraic Geometry etc. Hence, we create three types of splits:</p><p>? Note that for any of those datasets, for every splits and every strategy, we select subsets of all the labels and then sample articles from those labels. This yields splits with a varying amount and size of clusters.</p><p>RedditClustering <ref type="formula">(</ref>  RedditClusteringP2P Dataset created for MTEB, using available data from Reddit posts 10 . The task consists of clustering the concatenation of 10 https://huggingface.co/datasets/ sentence-transformers/reddit-title-body title+post according to their subreddit. It contains 10 splits, with 10 and 100 clusters per split and 1 000 to 100 000 posts.</p><p>StackExchangeClustering (Geigle et al., 2021) Clustering of titles from 121 stackexchanges. Clus-tering of 25 splits, each with 10-50 classes, and each class with 100 -1000 sentences.</p><p>StackExchangeClusteringP2P Dataset created for MTEB, using available data from StackExchange posts 11 . The task consists of clustering the concatenation of title and post according to their subreddit. It contains 10 splits, with 10 to 100 clusters and 5 000 to 10 000 posts per split.</p><p>TwentyNewsgroupsClustering 12 Clustering of the 20 Newsgroups dataset, given titles of article the goal is to find the newsgroup (20 in total). Contains 10 splits, each with 20 classes, with each split containing between 1 000 and 10 000 titles</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Classification</head><p>AmazonCounterfactual (O'Neill et al., 2021) A collection of Amazon customer reviews annotated for counterfactual detection pair classification. For each review the label is either "counterfactual" or "not-counterfactual". This is a multilingual dataset with 4 available languages.</p><p>AmazonPolarity (McAuley and Leskovec, 2013) A collection of Amazon customer reviews annotated for polarity classification. For each review the label is either "positive" or "negative".</p><p>AmazonReviews (McAuley and Leskovec, 2013) A collection of Amazon reviews designed to aid research in multilingual text classification. For each review the label is the score given by the review between 0 and 4 (1-5 stars). This is a multilingual dataset with 6 available languages.</p><p>Banking77 (Casanueva et al., 2020) Dataset composed of online banking queries annotated with their corresponding intents. For each user query the label is an intent among 77 intents like 'activate_my_card', 'apple_pay_or_google_pay', 'atm_support', 'balance_not_updated_after_bank_transfer', etc. MassiveIntent (FitzGerald et al., 2022) A collection of Amazon Alexa virtual assistant utterances annotated with the associated intent. For each user utterance the label is an intent among 60 intents like 'play_music', 'iot_hue_lightup', 'alarm_set', 'general_joke', etc. This is a multilingual dataset with 51 available languages.</p><p>MassiveScenario <ref type="figure" target="#fig_1">(FitzGerald et al., 2022)</ref> A collection of Amazon Alexa virtual assistant utterances annotated with the associated intent. For each user utterance the label is a theme among 60 scenarios like 'music', 'weather', 'alarm', etc. This is a multilingual dataset with 51 available languages.</p><p>MTOPDomain / MTOPIntent Multilingual sentence datasets from the MTOP (Li et al., 2020) benchmark. We refer to their paper for details.</p><p>ToxicConversations Dataset from Kaggle competition 13 . Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not. A.6 Semantic Textual Similarity (STS) STS12, STS13, STS14, STS15, STS16, STS17, STS22, STSBenchmark <ref type="bibr">(Agirre et al., 2012</ref><ref type="bibr">(Agirre et al., , 2013</ref>  <ref type="bibr">16171819</ref> Original STS benchmark, with scores from 0 to 5. The selection of sentences includes text from image captions, news headlines and user forums. In total they contain between 1 000 and 20 000 sentences. STS12-STS16 and STSBenchmark are monolingual english benchmarks. STS17 and STS22 contain crosslingual pairs of sentences, where the goal is to assess the similarity of two sentences in different languages. STS17 has 11 language pairs (among Korean, <ref type="bibr">Arabic, English, French, German, Turkish, Spanish, Italian and Dutch)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Retrieval</head><p>We refer to the BEIR paper <ref type="bibr">(Thakur et al., 2021)</ref>, which contains description of each dataset. For MTEB, we include the following datasets: ArguAna, ClimateFEVER, CQADupstackRetrieval, DBPedia, FEVER, FiQA2018, Hot-potQA, MSMARCO, NFCorpus, NQ, Quo-raRetrieval, SCIDOCS, SciFact, Touche2020, TRECCOVID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Limitations of MTEB</head><p>While MTEB aims to be a diverse benchmark to provide holistic performance reviews, the benchmark has its limitations. We list them here:</p><p>1. Long Document Datasets MTEB covers multiple text lengths (S2S, P2P, S2P), but very long documents are still missing. The longest datasets in MTEB have a few hundred words, and longer text sizes could be relevant for use cases like retrieval.</p><p>2. Task imbalance Tasks in MTEB have a different amount of datasets with summarization consisting of only a single dataset. This means MTEB average scores, which are computed over all datasets, are biased towards tasks with many datasets, notably retrieval, classification and clustering. As MTEB grows, we hope to add more datasets to currently underrepresented tasks like summarization or pair classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multilingual</head><p>Retrieval &amp; Clustering MTEB contains multilingual classification, STS and bitext mining datasets. However, retrieval and clustering are English-only. SGPT-BLOOM-7B1-msmarco is geared towards multilingual retrieval datasets and due to the lack of thereof cannot be comprehensively benchmarked in MTEB. We aim to extend MTEB in the future and welcome any contributions to address this issue. <ref type="table" target="#tab_7">Tables 3-9</ref> provide examples for each dataset for each task. For retrieval datasets, we refer to the BEIR paper <ref type="bibr">(Thakur et al., 2021)</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Correlations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StackExchangeClusteringP2P</head><p>Google play services error DEBUG: Application is pausing, which disconnects the RTMP client. I am having this issue from past day with Google Play Services Unity. What happens is, when I install app directly ot device via Unity, the Google Play Services work fine but when I upload it as beta to play store console and install it via that then it starts to give " DEBUG: Application is pausing, which disconnects the RTMP client" error. I have a proper SHA1 key.</p><p>unity TwentyNewsgroupsClustering Commercial mining activities on the moon 14    Morales went on to win the 2005 presidential election with an absolute majority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tatoeba</head><p>Chi le ha detto che Tom l'ha fatto? Who told you that Tom did that?  https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 MiniLM-L12-multilingual https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 MPNet https://huggingface.co/sentence-transformers/all-mpnet-base-v2 MPNet-multilingual https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2 MiniLM-L12-multilingual https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 SGPT-125M-nli https://huggingface.co/Muennighoff/SGPT-125M-weightedmean-nli-bitfit SGPT-5.8B-nli https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-nli-bitfit SGPT-125M-msmarco https://huggingface.co/Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit SGPT-1.3B-msmarco https://huggingface.co/Muennighoff/SGPT-1.3B-weightedmean-msmarco-specb-bitfit SGPT-2.7B-msmarco https://huggingface.co/Muennighoff/SGPT-2.7B-weightedmean-msmarco-specb-bitfit SGPT-5.8B-msmarco https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit SGPT-BLOOM-7.1B-msmarco https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco SGPT-BLOOM-1.7B-nli https://huggingface.co/bigscience-data/sgpt-bloom-1b7-nli GTR-Base https://huggingface.co/sentence-transformers/gtr-t5-base GTR-Large https://huggingface.co/sentence-transformers/gtr-t5-large GTR-XL https://huggingface.co/sentence-transformers/gtr-t5-xl GTR-XXL https://huggingface.co/sentence-transformers/gtr-t5-xxl ST5-Base https://huggingface.co/sentence-transformers/sentence-t5-base ST5-Large https://huggingface.co/sentence-transformers/sentence-t5-large ST5-XL https://huggingface.co/sentence-transformers/sentence-t5-xl ST5-XXL https://huggingface.co/sentence-transformers/sentence-t5-xxl      </p><formula xml:id="formula_0">SPECTER LaBSE LASER2 MiniLM- MiniLM- MiniLM- MPNet MPNet- Ada SGPT-125M- SGPT-5.8B- SGPT-125M- SGPT-1.3B- SGPT-2.7B- SGPT-5.8B- SGPT- GTR- GTR- GTR- GTR- ST5- ST5- ST5- ST5- BERT- BERT- msmarco iever L6 L12- L12- multilingual</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of tasks and datasets in MTEB. Multilingual datasets are marked with a purple shade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Similarity of MTEB datasets. We use the best model on MTEB STS (ST5-XXL) to embed 100 samples for each dataset. Cosine similarities between the averaged embeddings are computed and visualized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(Cohan et al., 2020a) relies on the pre-trained SciBERT (Beltagy et al., 2019) variant instead and fine-tunes on citation graphs. GTR (Ni et al., 2021b) and ST5 (Ni et al., 2021a) are based on the encoder part of the T5 model (Raffel et al., 2020) and only differ in their fine-tuning datasets. After additional self-supervised training, ST5 does contrastive finetuning on NLI (Ni et al., 2021a; Gao et al., 2021) being geared towards STS tasks. Meanwhile, GTR fine-tunes on MSMARCO and focuses on retrieval tasks. MPNet and MiniLM correspond to finetuned embedding models (Reimers and Gurevych, 2019) of the pre-trained MPNet (Song et al., 2020) and MiniLM (Wang et al., 2020) models using diverse datasets to target any embedding use case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>MTEB performance scales with model size. The smallest SGPT variant underperforms similarsized GTR and ST5 variants. This may be due to the bias-only fine-tuning SGPT employs, which catches up with full fine-tuning only as model size and thus the number of bias parameters increases(Muennighoff,  2022).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>MTEB multilingual performance. Bitext mining is dominated by LaBSE, while classification and STS results are mixed. SGPT-BLOOM-7B1-msmarco tends to perform well on the languages BLOOM has been pretrained on, such as Chinese, French and Portuguese. et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Emotion (Saravia et al., 2018) Dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. Imdb (Maas et al., 2011) Large movie review dataset. For each review the label is either positive or negative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>TweetSentimentExtraction</head><label></label><figDesc>Dataset from Kaggle competition 14 . Sentiment classification of tweets as neutral, positive or negative. A.3 Pair Classification SprintDuplicateQuestions (Shah et al., 2018): Collection of questions from the Sprint community. The goal is to classify a pair of sentences as duplicates or not. TwitterSemEval2015 (Xu et al., 2015) Paraphrase-Pairs of Tweets from the SemEval 2015 workshop. The goal is to classify a pair of tweets as paraphrases or not. TwitterURLCorpus (Lan et al., 2017) Paraphrase-Pairs of Tweets. The goal is to classify a pair of tweets as paraphrases or not. A.4 Bitext Mining BUCC (Zweigenbaum et al., 2016, 2017, 2018) BUCC provides big set of sentences (? 10-70k each) for English, French, Russian, German and Chinese, along with associated pairs annotation. The annotated pairs here corresponds to a pairs of 13 https://www.kaggle.com/competitions/ jigsaw-unintended-bias-in-toxicity-classification/ overview 14 https://www.kaggle.com/competitions/ tweet-sentiment-extraction/overview translated sentences, i.e. a sentence and its translation in the other language. Tatoeba (Research) Tatoeba provides mediumsized sets of sentences (1000 sentences each), for 112 languages including low resources ones, along with associated pairs annotation. The annotated pairs here corresponds to a pair of translated sentences, i.e. a sentence and its translation in the other language. A.5 Reranking AskUbuntuDupQuestions 15 Questions from AskUbuntu with manual annotations marking pairs of questions as similar or dissimilar. MindSmall (Wu et al., 2020) Large-scale English Dataset for News Recommendation Research. Ranking news article titles given the title of a news article. The idea is to recommend other news from the one you are reading. SciDocsRR (Cohan et al., 2020b) Ranking of related scientific papers based on their title. StackOverflowDupQuestions (Liu et al., 2018) Stack Overflow Duplicate Questions Task for questions with the tags Java, JavaScript and Python, ranking questions as duplicates or not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>BIOSSES</head><label></label><figDesc>20 Contains 100 sentence pairs from the biomedical field. SICK-R (Agirre et al., 2014) Sentences Involving Compositional Knowledge (SICK) contains a large number of sentence pairs (10 0000) that are lexically, syntactically and semantically rich. A.7 Summarization SummEval (Fabbri et al., 2020) Summaries generated by recent summarization models trained on CNN or DailyMail alongside human annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6</head><label>6</label><figDesc>provides correlation heatmaps for model performance &amp; MTEB tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Class. Clust. PairClass. Rerank. Retr.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">STS Summ. Avg.</cell></row><row><cell>Num. Datasets (?)</cell><cell>12</cell><cell>11</cell><cell>3</cell><cell>4</cell><cell>15</cell><cell>10</cell><cell>1</cell><cell>56</cell></row><row><cell>Self-supervised methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Glove</cell><cell cols="2">57.29 27.73</cell><cell>70.92</cell><cell>43.29</cell><cell cols="4">21.62 61.85 28.87 41.97</cell></row><row><cell>Komninos</cell><cell cols="2">57.65 26.57</cell><cell>72.94</cell><cell>44.75</cell><cell cols="4">21.22 62.47 30.49 42.06</cell></row><row><cell>BERT</cell><cell cols="2">61.66 30.12</cell><cell>56.33</cell><cell>43.44</cell><cell cols="4">10.59 54.36 29.82 38.33</cell></row><row><cell>SimCSE-BERT-unsup</cell><cell cols="2">62.50 29.04</cell><cell>70.33</cell><cell>46.47</cell><cell cols="4">20.29 74.33 31.15 45.45</cell></row><row><cell>Supervised methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCSE-BERT-sup</cell><cell cols="2">67.32 33.43</cell><cell>73.68</cell><cell>47.54</cell><cell cols="4">21.82 79.12 23.31 48.72</cell></row><row><cell>coCondenser-msmarco</cell><cell cols="2">64.71 37.64</cell><cell>81.74</cell><cell>51.84</cell><cell cols="4">32.96 76.47 29.50 52.35</cell></row><row><cell>Contriever</cell><cell cols="2">66.68 41.10</cell><cell>82.53</cell><cell>53.14</cell><cell cols="4">41.88 76.51 30.36 56.00</cell></row><row><cell>SPECTER</cell><cell cols="2">52.37 34.06</cell><cell>61.37</cell><cell>48.10</cell><cell cols="4">15.88 61.02 27.66 40.28</cell></row><row><cell>LaBSE</cell><cell cols="2">62.71 29.55</cell><cell>78.87</cell><cell>48.42</cell><cell cols="4">18.99 70.80 31.05 45.21</cell></row><row><cell>LASER2</cell><cell cols="2">53.65 15.28</cell><cell>68.86</cell><cell>41.44</cell><cell cols="4">7.93 55.32 26.80 33.63</cell></row><row><cell>MiniLM-L6</cell><cell cols="2">63.06 42.35</cell><cell>82.37</cell><cell>58.04</cell><cell cols="4">41.95 78.90 30.81 56.26</cell></row><row><cell>MiniLM-L12</cell><cell cols="2">63.21 41.81</cell><cell>82.41</cell><cell>58.44</cell><cell cols="4">42.69 79.80 27.90 56.53</cell></row><row><cell>MiniLM-L12-multilingual</cell><cell cols="2">64.30 37.14</cell><cell>78.45</cell><cell>53.62</cell><cell cols="4">32.45 78.92 30.67 52.44</cell></row><row><cell>MPNet</cell><cell cols="2">65.07 43.69</cell><cell>83.04</cell><cell>59.36</cell><cell cols="4">43.81 80.28 27.49 57.78</cell></row><row><cell>MPNet-multilingual</cell><cell cols="2">67.91 38.40</cell><cell>80.81</cell><cell>53.80</cell><cell cols="4">35.34 80.73 31.57 54.71</cell></row><row><cell>Ada Similarity</cell><cell cols="2">70.44 37.52</cell><cell>76.86</cell><cell>49.02</cell><cell></cell><cell cols="2">78.60 26.94</cell><cell></cell></row><row><cell>SGPT-125M-nli</cell><cell cols="2">61.46 30.95</cell><cell>71.78</cell><cell>47.56</cell><cell cols="4">20.90 74.71 30.26 45.97</cell></row><row><cell>SGPT-5.8B-nli</cell><cell cols="2">70.14 36.98</cell><cell>77.03</cell><cell>52.33</cell><cell cols="4">32.34 80.53 30.38 53.74</cell></row><row><cell>SGPT-125M-msmarco</cell><cell cols="2">60.72 35.79</cell><cell>75.23</cell><cell>50.58</cell><cell cols="4">37.04 73.41 28.90 51.23</cell></row><row><cell>SGPT-1.3B-msmarco</cell><cell cols="2">66.52 39.92</cell><cell>79.58</cell><cell>54.00</cell><cell cols="4">44.49 75.74 25.44 56.11</cell></row><row><cell>SGPT-2.7B-msmarco</cell><cell cols="2">67.13 39.83</cell><cell>80.65</cell><cell>54.67</cell><cell cols="4">46.54 76.83 27.87 57.12</cell></row><row><cell>SGPT-5.8B-msmarco</cell><cell cols="2">68.13 40.35</cell><cell>82.00</cell><cell>56.56</cell><cell cols="4">50.25 78.10 24.75 58.81</cell></row><row><cell cols="3">SGPT-BLOOM-7.1B-msmarco 66.19 38.93</cell><cell>81.90</cell><cell>55.65</cell><cell cols="4">48.21 77.74 24.99 57.44</cell></row><row><cell>GTR-Base</cell><cell cols="2">65.25 38.63</cell><cell>83.85</cell><cell>54.23</cell><cell cols="4">44.67 77.07 29.67 56.19</cell></row><row><cell>GTR-Large</cell><cell cols="2">67.14 41.60</cell><cell>85.33</cell><cell>55.36</cell><cell cols="4">47.42 78.19 29.50 58.28</cell></row><row><cell>GTR-XL</cell><cell cols="2">67.11 41.51</cell><cell>86.13</cell><cell>55.96</cell><cell cols="4">47.96 77.80 30.21 58.42</cell></row><row><cell>GTR-XXL</cell><cell cols="2">67.41 42.42</cell><cell>86.12</cell><cell>56.65</cell><cell cols="4">48.48 78.38 30.64 58.97</cell></row><row><cell>ST5-Base</cell><cell cols="2">69.81 40.21</cell><cell>85.17</cell><cell>53.09</cell><cell cols="4">33.63 81.14 31.39 55.27</cell></row><row><cell>ST5-Large</cell><cell cols="2">72.31 41.65</cell><cell>84.97</cell><cell>54.00</cell><cell cols="4">36.71 81.83 29.64 57.06</cell></row><row><cell>ST5-XL</cell><cell cols="2">72.84 42.34</cell><cell>86.06</cell><cell>54.71</cell><cell cols="4">38.47 81.66 29.91 57.87</cell></row><row><cell>ST5-XXL</cell><cell cols="2">73.42 43.71</cell><cell>85.06</cell><cell>56.43</cell><cell cols="4">42.24 82.63 30.08 59.51</cell></row></table><note>Table 1: Average results per task per model on MTEB English subsets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Mihalcea, et al. 2015. Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability. In Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pages 252-263. Casanueva, Tadas Tem?inas, Daniela Gerz, Matthew Henderson, and Ivan Vuli?. 2020. Efficient intent detection with dual sentence encoders. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Luyu Gao and Jamie Callan. 2021. Unsupervised corpus aware language model pre-training for dense passage retrieval. arXiv preprint arXiv:2108.05540. Heffernan, Onur ?elebi, and Holger Schwenk. 2022. Bitext mining using distilled sentence representations for low-resource languages. arXiv preprint arXiv:2205.12654. Julian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics: Understanding rating dimensions with review text. RecSys '13, New York, NY, USA. Association for Computing Machinery. Facebook Research. Tatoeba multilingual test set. Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. pages 410-420. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487.Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021. Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning. arXiv preprint arXiv:2104.06979.</figDesc><table><row><cell>Niklas Muennighoff. 2022. embeddings for semantic search. arXiv preprint Sgpt: Gpt sentence arXiv:2202.08904. language processing: system demonstrations, pages 38-45. BigScience Workshop. 2022. Bloom (revision 4ab0472). Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. 2020. Mind: A I?igo Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. 2020a. Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S. Weld. 2020b. Specter: Document-level representation learning using citation-informed transformers. large-scale dataset for news recommendation. In Pandu Nayak. 2019. Understanding searches better Proceedings of the 58th Annual Meeting of the Asso-than ever before. ciation for Computational Linguistics, pages 3597-3606. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Wei Xu, Chris Callison-Burch, and William B Dolan. Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2015. Semeval-2015 task 1: Paraphrase and seman-2022. Text and code embeddings by contrastive pre-tic similarity in twitter (pit). In Proceedings of the training. arXiv preprint arXiv:2201.10005. 9th international workshop on semantic evaluation tavo Hern?ndez ?brego, Ji Ma, Vincent Y Zhao, Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-preprint arXiv:2108.08877. coders from pre-trained text-to-text models. arXiv Yang. 2021a. Sentence-t5: Scalable sentence en-stant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Jianmo Ni, Gustavo Hern?ndez ?brego, Noah Con-(SemEval 2015), pages 1-11.</cell><cell>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel M Cer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. Semeval-2014 task 10: Multilingual semantic textual similarity. In SemEval@ COLING, pages 81-91. Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez Agirre, Rada Mihalcea, Ger-man Rigau Claramunt, and Janyce Wiebe. 2016. Semeval-2016 task 1: Semantic textual similar-ity, monolingual and cross-lingual evaluation. In SemEval-2016. 10th International Workshop on Se-Kevin Sepp Hochreiter and J?rgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780. Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanab-Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, han, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-based retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD Interna-tional Conference on Knowledge Discovery &amp; Data Mining, pages 2553-2561. Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-textualized affect representations for emotion recog-nition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3687-3697, Brussels, Belgium. Association for Computational Linguistics. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118. Dependency based embeddings for sentence classi-fication tasks. In Proceedings of the 2016 confer-for Computational Linguistics. pages 1056-1063, Brussels, Belgium. Association Alexandros Komninos and Suresh Manandhar. 2016. Empirical Methods in Natural Language Processing, tion. In Proceedings of the 2018 Conference on ial domain adaptation for duplicate question detec-tore Romeo, and Preslav Nakov. 2018. Adversar-Darsh Shah, Tao Lei, Alessandro Moschitti, Salva-</cell></row><row><cell>Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021b. Large dual encoders are generalizable re-trievers. arXiv preprint arXiv:2112.07899.</cell><cell>ence of the North American chapter of the associa-Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-tion for computational linguistics: human language Yan Liu. 2020. Mpnet: Masked and permuted pre-technologies, pages 1490-1500. training for language understanding. Advances in</cell></row><row><cell>Motoko Kubota, and Danushka Bollegala. 2021. I James O'Neill, Polina Rozenshtein, Ryuichi Kiryo,</cell><cell>Neural Information Processing Systems, 33:16857-Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017. A continuously growing dataset of sentential para-16867.</cell></row><row><cell>wish i would have loved this one, but i didn't -a</cell><cell>phrases. In Proceedings of The 2017 Conference on Nandan Thakur, Nils Reimers, Andreas R?ckl?, Ab-</cell></row><row><cell>multilingual dataset for counterfactual detection in</cell><cell>Empirical Methods on Natural Language Process-hishek Srivastava, and Iryna Gurevych. 2021. Beir:</cell></row><row><cell>product reviews.</cell><cell>ing (EMNLP), pages 1235-1245. Association for A heterogenous benchmark for zero-shot evaluation</cell></row><row><cell></cell><cell>Computational Linguistics. of information retrieval models.</cell></row><row><cell>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word rep-resentation. In Proceedings of the 2014 conference on empirical methods in natural language process-ing (EMNLP), pages 1532-1543.</cell><cell>Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et al. 2021. Datasets: A commu-nity library for natural language processing. arXiv Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ?ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information process-ing systems, 30.</cell></row><row><cell>Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Lan-guage models are unsupervised multitask learners. OpenAI blog, 1(8):9. Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Swetha Ranganath, Laurie Crist, Misha Britan, Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wouter Leeuwis, Gokhan Tur, and Prem Natara-jan. 2022. Wei Li, Peter J Liu, et al. 2020. Exploring the limits Massive: A 1m-example multilin-of transfer learning with a unified text-to-text trans-gual natural language understanding dataset with 51 former. J. Mach. Learn. Res., 21(140):1-67. typologically-diverse languages. Nils Reimers, Philip Beyer, and Iryna Gurevych. 2016. Task-oriented intrinsic evaluation of semantic tex-</cell><cell>preprint arXiv:2109.02846. Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2020. Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/ kingoflolz/mesh-transformer-jax. Mtop: A comprehensive multilingual task-oriented semantic parsing benchmark. Xueqing Liu, Chi Wang, Yue Leng, and ChengXiang Zhai. 2018. Linkso: a dataset for learning to retrieve similar question answer pairs on software develop-ment forums. In Proceedings of the 4th ACM SIG-Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan SOFT International Workshop on NLP for Software Yang, and Ming Zhou. 2020. Minilm: Deep self-Engineering, pages 2-5. attention distillation for task-agnostic compression</cell></row><row><cell>tual similarity. In Proceedings of COLING 2016,</cell><cell>of pre-trained transformers. Advances in Neural In-</cell></row><row><cell>the 26th International Conference on Computational</cell><cell>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, formation Processing Systems, 33:5776-5788.</cell></row><row><cell>Linguistics: Technical Papers, pages 87-96.</cell><cell>Dan Huang, Andrew Y. Ng, and Christopher Potts.</cell></row><row><cell></cell><cell>2011. Learning word vectors for sentiment analy-</cell></row><row><cell></cell><cell>sis. In Proceedings of the 49th Annual Meeting of</cell></row><row><cell></cell><cell>the Association for Computational Linguistics: Hu-</cell></row><row><cell></cell><cell>man Language Technologies, pages 142-150, Port-</cell></row><row><cell></cell><cell>land, Oregon, USA. Association for Computational</cell></row><row><cell></cell><cell>Linguistics.</cell></row></table><note>6 https://github.com/ embeddings-benchmark/mteb7 https://huggingface.co/spaces/mteb/ leaderboardmantic Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p. 497-511. ACL (As- sociation for Computational Linguistics). Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi- lot on semantic textual similarity. In * SEM 2012: The First Joint Conference on Lexical and Compu- tational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385- 393. Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, and Weiwei Guo. 2013. * sem 2013 shared task: Semantic textual similarity. In Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main confer- ence and the shared task: semantic textual similar- ity, pages 32-43. Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Phil Wang, and Samuel Weinbach. 2021. GPT-NeoX: Large scale autoregressive language modeling in pytorch. Dimo Angelov. 2020. Top2vec: Distributed representa- tions of topics. arXiv preprint arXiv:2008.09470. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib- ert: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676. Sebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Milli- can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving from tril- lions of tokens. In International Conference on Ma- chine Learning, pages 2206-2240. PMLR.Alexis Conneau and Douwe Kiela. 2018. Senteval: An evaluation toolkit for universal sentence representa- tions. arXiv preprint arXiv:1803.05449. Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Alexander R. Fabbri, Wojciech Kry?ci?ski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2020. Summeval: Re-evaluating summarization evaluation. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2020. Language- agnostic bert sentence embedding. arXiv preprint arXiv:2007.01852.Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence em- beddings. arXiv preprint arXiv:2104.08821. Gregor Geigle, Nils Reimers, Andreas R?ckl?, and Iryna Gurevych. 2021. Tweac: Transformer with ex- tendable qa agent classifiers.Nils Reimers and Iryna Gurevych. 2019. Sentence- bert: Sentence embeddings using siamese bert- networks. arXiv preprint arXiv:1908.10084.Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R?mi Louf, Morgan Funtow- icz, et al. 2020. Transformers: State-of-the-art nat- ural language processing. In Proceedings of the 2020 conference on empirical methods in naturalJeffrey Zhu, Mingqin Li, Jason Li, and Cassandra Oduola. 2021. Bing delivers more contextualized search using quantized transformer inference on nvidia gpus in azure. Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2016. Towards preparation of the second bucc shared task: Detecting parallel sentences in compa- rable corpora. In Proceedings of the Ninth Workshop on Building and Using Comparable Corpora. Euro- pean Language Resources Association (ELRA), Por- toroz, Slovenia, pages 38-43. Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2017. Overview of the second bucc shared task: Spotting parallel sentences in comparable cor- pora. In Proceedings of the 10th Workshop on Build- ing and Using Comparable Corpora, pages 60-67. Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2018. Overview of the third bucc shared task: Spotting parallel sentences in comparable corpora. In Proceedings of 11th workshop on building and using comparable corpora, pages 39-42.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Tasks in MTEB</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10</head><label>10</label><figDesc>provides publicly available model checkpoints used for MTEB evaluation.Tables 11 until the end provide results on individual datasets of MTEB. The results are additionally available in json format on the Hub 21 and can be inspected on the leaderboard 22 .AmazonCounterfactualClassificationIn person it looks as though it would have cost a lot more. counterfactualAmazonPolarityClassification an absolute masterpiece I am quite sure any of you actually taking the time to read this have played the game at least once, and heard at least a few of the tracks here. And whether you were aware of it or not, Mitsuda's music contributed greatly to the... saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York's portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe...</figDesc><table><row><cell>F Additional results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Classification examples Influenza A viral RNA through IFI16 promotes pyroptotic cell death Programmed cell death pathways are triggered by various stresses or stimuli, including viral infections. The mechanism underlying the regulation of these pathways upon Influenza A virus IAV infection is not well characterized. We report that a cytosolic DNA sensor IFI16 is... Haemophagocytic lymphohistiocytosis (HLH) is rare, results in high mortality and is increasingly being diagnosed. Little is known about what is driving the apparent rise in the incidence of this disease. Using national linked electronic health data from hospital admissions and death certification cases of HLH that were diagnosed in England between 1/1/2003 and 31/12/2018 were identified using a previously validated approach. We calculated incidence...</figDesc><table><row><cell>Dataset</cell><cell>Text</cell><cell>Cluster</cell></row><row><cell></cell><cell></cell><cell>math</cell></row><row><cell>ArxivClusteringS2S</cell><cell>Vertical shift and simultaneous Diophantine approximation on polynomial curves</cell><cell>math</cell></row><row><cell>BiorxivClusteringP2P</cell><cell cols="2">Innate Immune sensing of immunology</cell></row><row><cell>BiorxivClusteringS2S</cell><cell>Association of CDH11 with ASD revealed by matched-gene co-expression analysis and mouse behavioral</cell><cell>neuroscience</cell></row><row><cell>MedrxivClusteringP2P</cell><cell cols="2">Temporal trends in the incidence of haemophagocytic lymphohistiocytosis: a nationwide cohort study from England 2003-2018. infectious diseases</cell></row><row><cell>MedrxivClusteringS2S</cell><cell>Current and Lifetime Somatic Symptom Burden Among Transition-aged Young Adults on the Autism Spectrum</cell><cell>psychiatry and clinical psychology</cell></row><row><cell>RedditClustering</cell><cell>Could anyone tell me what breed my bicolor kitten is?</cell><cell>r/cats</cell></row><row><cell>RedditClusteringP2P</cell><cell>Headaches after working out? Hey guys! I've been diagnosed with adhd since I was seven. I just recently got rediag-</cell><cell>r/ADHD</cell></row><row><cell></cell><cell>nosed (22f) and I've been out on a different medication, adderall I was normally taking vyvanse but because of cost and</cell><cell></cell></row><row><cell></cell><cell>no insurance adderall was more affordable. I've noticed that if I take adderall and workout...</cell><cell></cell></row><row><cell>StackExchangeClustering</cell><cell>Does this property characterize a space as Hausdorff?</cell><cell>math.stackexchange.com</cell></row></table><note>ArxivClusteringP2P Finite groups of rank two which do not involve Qd(p). Let p &gt; 3 be a prime. We show that if G is a finite group with p-rank equal to 2, then G involves Qd(p) if and only if G p -involves Qd(p). This allows us to use a version of Glauberman's ZJ-theorem to give a more direct construction of finite group actions on mod-p homotopy spheres. We give an example to illustrate that the above conclusion does not hold for p ? 3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Clustering examples</figDesc><table><row><cell>Dataset</cell><cell>Sentence 1</cell><cell>Sentence 2</cell><cell>Label</cell></row><row><cell>SprintDuplicateQuestions</cell><cell>Franklin U722 USB modem signal strength</cell><cell>How do I know if my Franklin U772 USB Modem has a</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>weak signal ?</cell><cell></cell></row><row><cell>TwitterSemEval2015</cell><cell>All the home alones watching 8 mile","All the home alones</cell><cell>The last rap battle in 8 Mile nevr gets old ahah</cell><cell>0</cell></row><row><cell></cell><cell>watching 8 mile</cell><cell></cell><cell></cell></row><row><cell>TwitterURLCorpus</cell><cell>How the metaphors we use to describe discovery affect men</cell><cell>Light Bulbs or Seeds ? How Metaphors for Ideas Influence</cell><cell>0</cell></row><row><cell></cell><cell>and women in the sciences</cell><cell>Judgments About Genius</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Pair classification examples. Labels are binary. probe of Giuliani associates is freed on bail Studies show these are the best and worst states for your retirement There are 14 cheap days to fly left in 2019: When are they and what deals can you score?</figDesc><table><row><cell>Dataset</cell><cell>Query</cell><cell>Positive</cell><cell>Negative</cell></row><row><cell>AskUbuntuDupQuestions</cell><cell>change the application icon theme but not changing the</cell><cell>change folder icons in ubuntu-mono-dark theme</cell><cell>change steam tray icon back to default</cell></row><row><cell></cell><cell>panel icons</cell><cell></cell><cell></cell></row><row><cell cols="2">MindSmallReranking Man accused in SciDocsRR Discovering social circles in ego networks</cell><cell>Benchmarks for testing community detection algorithms on</cell><cell>Improving www proxies performance with greedy-dual-</cell></row><row><cell></cell><cell></cell><cell>directed and weighted graphs with overlapping communi-</cell><cell>size-frequency caching policy</cell></row><row><cell></cell><cell></cell><cell>ties.</cell><cell></cell></row><row><cell>StackOverflowDupQuestions</cell><cell>Java launch error selection does not contain a main type</cell><cell>Error: Selection does not contain a main type</cell><cell>Selection Sort in Java</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Reranking examples</figDesc><table><row><cell>Dataset</cell><cell>Sentence 1</cell><cell>Sentence 2</cell><cell>Score</cell></row><row><cell>BIOSSES</cell><cell>It has recently been shown that Craf is essential for Kras</cell><cell>It has recently become evident that Craf is essential for the</cell><cell>4.0</cell></row><row><cell></cell><cell>G12D-induced NSCLC.</cell><cell>onset of Kras-driven non-small cell lung cancer.</cell><cell></cell></row><row><cell>SICK-R</cell><cell>A group of children is playing in the house and there is no</cell><cell>A group of kids is playing in a yard and an old man is stand-</cell><cell>3.2</cell></row><row><cell></cell><cell>man standing in the background</cell><cell>ing in the background</cell><cell></cell></row><row><cell>STS12</cell><cell>Nationally, the federal Centers for Disease Control and Pre-</cell><cell>There were 293 human cases of West Nile in Indiana in</cell><cell>1.7</cell></row><row><cell></cell><cell>vention recorded 4,156 cases of West Nile, including 284</cell><cell>2002, including 11 deaths statewide.</cell><cell></cell></row><row><cell></cell><cell>deaths.</cell><cell></cell><cell></cell></row><row><cell>STS13</cell><cell>this frame has to do with people ( the residents ) residing in</cell><cell>inhabit or live in ; be an inhabitant of ;</cell><cell>2.8</cell></row><row><cell></cell><cell>locations , sometimes with a co-resident .</cell><cell></cell><cell></cell></row><row><cell>STS14</cell><cell>then the captain was gone.</cell><cell>then the captain came back.</cell><cell>0.8</cell></row><row><cell>STS15</cell><cell>you 'll need to check the particular policies of each pub-</cell><cell>if you need to publish the book and you have found one</cell><cell>3.0</cell></row><row><cell></cell><cell>lisher to see what is allowed and what is not allowed.</cell><cell>publisher that allows it.</cell><cell></cell></row><row><cell>STS16</cell><cell>you do not need to worry.</cell><cell>you don 't have to worry.</cell><cell>5.0</cell></row><row><cell>STS17</cell><cell>La gente muestra su afecto el uno por el otro.</cell><cell>A women giving something to other lady.</cell><cell>1.4</cell></row><row><cell>STS22</cell><cell>El secretario general de la Asociaci?n Gremial de los Tra-</cell><cell>En di?logo con el servicio informativo de la Radio P?blica,</cell><cell>1</cell></row><row><cell></cell><cell>bajadores del Subte y Premetro de Metrodelegados, Beto</cell><cell>el ministro de Salud de la Naci?n, Gin?s Gonz?lez Garc?a,</cell><cell></cell></row><row><cell></cell><cell>Pianelli, dijo que el Gobierno porte?o debe convocar "in-</cell><cell>habl? sobre el avance del coronavirus en la Argentina y se</cell><cell></cell></row><row><cell></cell><cell>mediatamente" a licitaci?n para la compra de nuevos trenes</cell><cell>manifest? a favor de prorrogar la cuarentena obligatoria dis-</cell><cell></cell></row><row><cell></cell><cell>y retirar los que quedan en circulaci?n...</cell><cell>puesta por...</cell><cell></cell></row><row><cell>STSBenchmark</cell><cell>A man is playing the cello.</cell><cell>A man seated is playing the cello.</cell><cell>4.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>STS examples. Scores are continuous between 0 and 5 (included).</figDesc><table><row><cell>Dataset</cell><cell>First set sentence</cell><cell>Second set sentence</cell></row><row><cell>BUCC</cell><cell>Morales remporte l'?lection pr?sidentielle de 2005 ? la ma-</cell><cell></cell></row><row><cell></cell><cell>jorit? absolue.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Bitext mining examples</figDesc><table><row><cell>Dataset</cell><cell>Human Summary</cell><cell>Machine Summary</cell><cell>Relevance</cell></row><row><cell>SummEval</cell><cell>V. Stiviano must pay back $2.6 million in gifts from Donald</cell><cell>donald sterling , nba team last year . sterling 's wife sued</cell><cell>1.7</cell></row><row><cell></cell><cell>Sterling. Sterling's wife claimed the ex-Clippers used the</cell><cell>for $ 2.6 million in gifts . sterling says he is the former</cell><cell></cell></row><row><cell></cell><cell>couple's money for the gifts. The items included a Ferrari,</cell><cell>female companion who has lost the . sterling has ordered</cell><cell></cell></row><row><cell></cell><cell>two Bentleys and a Range Rover.</cell><cell>v. stiviano to pay back $ 2.6 m in gifts after his wife sued .</cell><cell></cell></row><row><cell></cell><cell></cell><cell>sterling also includes a $ 391 easter bunny costume , $ 299</cell><cell></cell></row><row><cell></cell><cell></cell><cell>and a $ 299 .</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9</head><label>9</label><figDesc>Pearson correlations across model &amp; task results. Left: Size variants of the same architecture show high correlations. Right: Performance on clustering &amp; reranking correlates strongest, while summarization and classification show weaker correlation with other tasks.</figDesc><table><row><cell>: Summarization example</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Publicly available model links used for evaluation</figDesc><table><row><cell>Dataset</cell><cell>Glove</cell><cell>Komninos</cell><cell>BERT</cell><cell>SimCSE-</cell><cell>SimCSE-</cell><cell>coCondenser-</cell><cell>Contr-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>All English results. The main score for each task is reported as described in Section 3.2.</figDesc><table><row><cell cols="7">Dataset Language LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco</cell></row><row><cell>BUCC</cell><cell>de-en</cell><cell>99.21</cell><cell>99.35</cell><cell>97.11</cell><cell>98.59</cell><cell>54.00</cell></row><row><cell>BUCC</cell><cell>fr-en</cell><cell>98.39</cell><cell>98.72</cell><cell>94.99</cell><cell>96.89</cell><cell>97.06</cell></row><row><cell>BUCC</cell><cell>ru-en</cell><cell>97.62</cell><cell>97.78</cell><cell>95.06</cell><cell>96.44</cell><cell>45.30</cell></row><row><cell>BUCC</cell><cell>zh-en</cell><cell>97.70</cell><cell>99.16</cell><cell>95.63</cell><cell>97.56</cell><cell>97.96</cell></row><row><cell>Tatoeba</cell><cell>sqi-eng</cell><cell>97.22</cell><cell>96.76</cell><cell>98.17</cell><cell>98.57</cell><cell>10.38</cell></row><row><cell>Tatoeba</cell><cell>fry-eng</cell><cell>42.07</cell><cell>89.31</cell><cell>31.13</cell><cell>43.54</cell><cell>24.62</cell></row><row><cell>Tatoeba</cell><cell>kur-eng</cell><cell>19.09</cell><cell>83.59</cell><cell>46.94</cell><cell>61.44</cell><cell>8.26</cell></row><row><cell>Tatoeba</cell><cell>tur-eng</cell><cell>98.03</cell><cell>98.00</cell><cell>95.08</cell><cell>96.17</cell><cell>6.15</cell></row><row><cell>Tatoeba</cell><cell>deu-eng</cell><cell>99.07</cell><cell>99.20</cell><cell>97.02</cell><cell>97.73</cell><cell>70.10</cell></row><row><cell>Tatoeba</cell><cell>nld-eng</cell><cell>95.35</cell><cell>96.07</cell><cell>94.58</cell><cell>95.50</cell><cell>29.74</cell></row><row><cell>Tatoeba</cell><cell>ron-eng</cell><cell>96.52</cell><cell>96.92</cell><cell>95.30</cell><cell>96.43</cell><cell>27.23</cell></row><row><cell>Tatoeba</cell><cell>ang-eng</cell><cell>25.22</cell><cell>59.28</cell><cell>10.24</cell><cell>16.72</cell><cell>28.76</cell></row><row><cell>Tatoeba</cell><cell>ido-eng</cell><cell>80.86</cell><cell>89.42</cell><cell>40.25</cell><cell>43.91</cell><cell>43.91</cell></row><row><cell>Tatoeba</cell><cell>jav-eng</cell><cell>9.95</cell><cell>79.77</cell><cell>17.04</cell><cell>23.39</cell><cell>15.02</cell></row><row><cell>Tatoeba</cell><cell>isl-eng</cell><cell>94.32</cell><cell>94.75</cell><cell>24.07</cell><cell>59.25</cell><cell>6.29</cell></row><row><cell>Tatoeba</cell><cell>slv-eng</cell><cell>95.40</cell><cell>96.03</cell><cell>96.92</cell><cell>97.08</cell><cell>10.14</cell></row><row><cell cols="2">Tatoeba cym-eng</cell><cell>5.85</cell><cell>92.00</cell><cell>13.25</cell><cell>22.31</cell><cell>6.97</cell></row><row><cell>Tatoeba</cell><cell>kaz-eng</cell><cell>53.30</cell><cell>87.49</cell><cell>34.89</cell><cell>61.49</cell><cell>3.32</cell></row><row><cell>Tatoeba</cell><cell>est-eng</cell><cell>96.43</cell><cell>96.55</cell><cell>97.33</cell><cell>98.40</cell><cell>4.76</cell></row><row><cell>Tatoeba</cell><cell>heb-eng</cell><cell>0.00</cell><cell>91.53</cell><cell>86.88</cell><cell>88.26</cell><cell>1.69</cell></row><row><cell>Tatoeba</cell><cell>gla-eng</cell><cell>1.52</cell><cell>85.66</cell><cell>3.61</cell><cell>4.72</cell><cell>2.09</cell></row><row><cell>Tatoeba</cell><cell>mar-eng</cell><cell>92.93</cell><cell>92.65</cell><cell>92.38</cell><cell>93.83</cell><cell>45.53</cell></row><row><cell>Tatoeba</cell><cell>lat-eng</cell><cell>64.81</cell><cell>80.07</cell><cell>19.47</cell><cell>24.25</cell><cell>28.76</cell></row><row><cell>Tatoeba</cell><cell>bel-eng</cell><cell>79.54</cell><cell>95.00</cell><cell>67.73</cell><cell>79.94</cell><cell>8.03</cell></row><row><cell>Tatoeba</cell><cell>pms-eng</cell><cell>36.23</cell><cell>64.57</cell><cell>30.70</cell><cell>34.19</cell><cell>31.94</cell></row><row><cell>Tatoeba</cell><cell>gle-eng</cell><cell>4.20</cell><cell>93.80</cell><cell>11.62</cell><cell>16.85</cell><cell>3.26</cell></row><row><cell>Tatoeba</cell><cell>pes-eng</cell><cell>93.13</cell><cell>94.70</cell><cell>92.59</cell><cell>93.47</cell><cell>12.13</cell></row><row><cell>Tatoeba</cell><cell>nob-eng</cell><cell>95.77</cell><cell>98.40</cell><cell>97.73</cell><cell>98.53</cell><cell>21.07</cell></row><row><cell>Tatoeba</cell><cell>bul-eng</cell><cell>93.57</cell><cell>94.58</cell><cell>92.65</cell><cell>93.52</cell><cell>20.09</cell></row><row><cell>Tatoeba</cell><cell>cbk-eng</cell><cell>77.17</cell><cell>79.44</cell><cell>55.37</cell><cell>58.68</cell><cell>64.63</cell></row><row><cell>Tatoeba</cell><cell>hun-eng</cell><cell>95.20</cell><cell>96.55</cell><cell>91.58</cell><cell>94.18</cell><cell>5.07</cell></row><row><cell>Tatoeba</cell><cell>uig-eng</cell><cell>56.49</cell><cell>92.40</cell><cell>24.39</cell><cell>48.35</cell><cell>1.27</cell></row><row><cell>Tatoeba</cell><cell>rus-eng</cell><cell>92.58</cell><cell>93.75</cell><cell>91.87</cell><cell>92.92</cell><cell>59.84</cell></row><row><cell>Tatoeba</cell><cell>spa-eng</cell><cell>97.33</cell><cell>98.40</cell><cell>95.42</cell><cell>97.00</cell><cell>94.48</cell></row><row><cell>Tatoeba</cell><cell>hye-eng</cell><cell>88.72</cell><cell>94.09</cell><cell>93.28</cell><cell>94.38</cell><cell>0.50</cell></row><row><cell>Tatoeba</cell><cell>tel-eng</cell><cell>96.72</cell><cell>97.86</cell><cell>36.40</cell><cell>79.73</cell><cell>64.62</cell></row><row><cell>Tatoeba</cell><cell>afr-eng</cell><cell>92.59</cell><cell>96.18</cell><cell>58.22</cell><cell>72.96</cell><cell>16.62</cell></row><row><cell cols="2">Tatoeba mon-eng</cell><cell>3.42</cell><cell>95.91</cell><cell>95.04</cell><cell>96.14</cell><cell>2.85</cell></row><row><cell>Tatoeba</cell><cell>arz-eng</cell><cell>66.16</cell><cell>76.00</cell><cell>51.26</cell><cell>55.69</cell><cell>70.66</cell></row><row><cell>Tatoeba</cell><cell>hrv-eng</cell><cell>96.72</cell><cell>96.95</cell><cell>95.98</cell><cell>97.00</cell><cell>12.79</cell></row><row><cell>Tatoeba</cell><cell>nov-eng</cell><cell>60.02</cell><cell>74.38</cell><cell>47.99</cell><cell>50.23</cell><cell>52.23</cell></row><row><cell>Tatoeba</cell><cell>gsw-eng</cell><cell>27.52</cell><cell>46.50</cell><cell>25.74</cell><cell>25.12</cell><cell>21.03</cell></row><row><cell>Tatoeba</cell><cell>nds-eng</cell><cell>77.13</cell><cell>79.42</cell><cell>32.16</cell><cell>38.88</cell><cell>23.92</cell></row><row><cell>Tatoeba</cell><cell>ukr-eng</cell><cell>93.52</cell><cell>93.97</cell><cell>92.82</cell><cell>92.67</cell><cell>22.06</cell></row><row><cell>Tatoeba</cell><cell>uzb-eng</cell><cell>23.20</cell><cell>84.23</cell><cell>17.14</cell><cell>23.19</cell><cell>4.71</cell></row><row><cell>Tatoeba</cell><cell>lit-eng</cell><cell>96.20</cell><cell>96.47</cell><cell>93.16</cell><cell>95.37</cell><cell>4.49</cell></row><row><cell>Tatoeba</cell><cell>ina-eng</cell><cell>93.93</cell><cell>95.37</cell><cell>79.13</cell><cell>84.32</cell><cell>73.67</cell></row><row><cell>Tatoeba</cell><cell>lfn-eng</cell><cell>63.39</cell><cell>67.54</cell><cell>47.02</cell><cell>49.56</cell><cell>44.85</cell></row><row><cell>Tatoeba</cell><cell>zsm-eng</cell><cell>95.41</cell><cell>95.62</cell><cell>95.31</cell><cell>95.80</cell><cell>79.95</cell></row><row><cell>Tatoeba</cell><cell>ita-eng</cell><cell>94.32</cell><cell>92.72</cell><cell>93.05</cell><cell>93.76</cell><cell>65.04</cell></row><row><cell cols="2">Tatoeba cmn-eng</cell><cell>85.62</cell><cell>95.10</cell><cell>94.93</cell><cell>95.83</cell><cell>91.45</cell></row><row><cell>Tatoeba</cell><cell>lvs-eng</cell><cell>95.33</cell><cell>95.88</cell><cell>97.87</cell><cell>97.53</cell><cell>6.55</cell></row><row><cell>Tatoeba</cell><cell>glg-eng</cell><cell>96.14</cell><cell>96.82</cell><cell>94.00</cell><cell>95.32</cell><cell>79.86</cell></row><row><cell>Tatoeba</cell><cell>ceb-eng</cell><cell>9.93</cell><cell>64.42</cell><cell>8.05</cell><cell>7.39</cell><cell>6.64</cell></row><row><cell>Tatoeba</cell><cell>bre-eng</cell><cell>31.2</cell><cell>15.07</cell><cell>5.56</cell><cell>6.42</cell><cell>4.67</cell></row><row><cell>Tatoeba</cell><cell>ben-eng</cell><cell>89.43</cell><cell>88.55</cell><cell>36.48</cell><cell>64.90</cell><cell>75.98</cell></row><row><cell>Tatoeba</cell><cell>swg-eng</cell><cell>33.10</cell><cell>59.36</cell><cell>26.31</cell><cell>22.80</cell><cell>16.89</cell></row><row><cell>Tatoeba</cell><cell>arq-eng</cell><cell>26.63</cell><cell>42.69</cell><cell>18.60</cell><cell>19.84</cell><cell>27.75</cell></row><row><cell>Tatoeba</cell><cell>kab-eng</cell><cell>65.88</cell><cell>4.31</cell><cell>1.16</cell><cell>1.41</cell><cell>1.69</cell></row><row><cell>Tatoeba</cell><cell>fra-eng</cell><cell>94.28</cell><cell>94.86</cell><cell>91.72</cell><cell>93.12</cell><cell>91.44</cell></row><row><cell>Tatoeba</cell><cell>por-eng</cell><cell>94.54</cell><cell>94.14</cell><cell>92.13</cell><cell>93.02</cell><cell>92.62</cell></row><row><cell>Tatoeba</cell><cell>tat-eng</cell><cell>34.74</cell><cell>85.92</cell><cell>10.25</cell><cell>10.89</cell><cell>3.59</cell></row><row><cell>Tatoeba</cell><cell>oci-eng</cell><cell>58.13</cell><cell>65.81</cell><cell>38.57</cell><cell>43.49</cell><cell>40.17</cell></row><row><cell>Tatoeba</cell><cell>pol-eng</cell><cell>97.32</cell><cell>97.22</cell><cell>94.28</cell><cell>96.95</cell><cell>14.09</cell></row><row><cell>Tatoeba</cell><cell>war-eng</cell><cell>8.25</cell><cell>60.29</cell><cell>7.25</cell><cell>7.42</cell><cell>10.38</cell></row><row><cell>Tatoeba</cell><cell>aze-eng</cell><cell>82.41</cell><cell>94.93</cell><cell>62.10</cell><cell>76.36</cell><cell>6.32</cell></row><row><cell>Tatoeba</cell><cell>vie-eng</cell><cell>96.73</cell><cell>97.20</cell><cell>95.12</cell><cell>97.23</cell><cell>94.20</cell></row><row><cell>Tatoeba</cell><cell>nno-eng</cell><cell>72.75</cell><cell>94.48</cell><cell>76.34</cell><cell>81.41</cell><cell>16.28</cell></row><row><cell>Tatoeba</cell><cell>cha-eng</cell><cell>14.86</cell><cell>31.77</cell><cell>15.98</cell><cell>12.59</cell><cell>23.26</cell></row><row><cell>Tatoeba</cell><cell>mhr-eng</cell><cell>6.86</cell><cell>15.74</cell><cell>6.89</cell><cell>7.57</cell><cell>1.56</cell></row><row><cell>Tatoeba</cell><cell>dan-eng</cell><cell>95.22</cell><cell>95.71</cell><cell>94.80</cell><cell>96.17</cell><cell>23.52</cell></row><row><cell>Tatoeba</cell><cell>ell-eng</cell><cell>96.20</cell><cell>95.35</cell><cell>95.43</cell><cell>94.93</cell><cell>5.34</cell></row><row><cell cols="2">Tatoeba amh-eng</cell><cell>80.82</cell><cell>91.47</cell><cell>36.21</cell><cell>53.49</cell><cell>0.03</cell></row><row><cell cols="2">Tatoeba pam-eng</cell><cell>3.24</cell><cell>10.73</cell><cell>5.41</cell><cell>5.39</cell><cell>5.85</cell></row><row><cell>Tatoeba</cell><cell>hsb-eng</cell><cell>45.75</cell><cell>67.11</cell><cell>36.10</cell><cell>44.32</cell><cell>9.68</cell></row><row><cell>Tatoeba</cell><cell>srp-eng</cell><cell>93.64</cell><cell>94.43</cell><cell>92.24</cell><cell>94.12</cell><cell>11.69</cell></row><row><cell>Tatoeba</cell><cell>epo-eng</cell><cell>96.61</cell><cell>98.20</cell><cell>41.73</cell><cell>55.12</cell><cell>26.20</cell></row><row><cell>Tatoeba</cell><cell>kzj-eng</cell><cell>4.46</cell><cell>11.33</cell><cell>6.24</cell><cell>5.88</cell><cell>5.17</cell></row><row><cell>Tatoeba</cell><cell>awa-eng</cell><cell>33.74</cell><cell>71.70</cell><cell>33.43</cell><cell>42.83</cell><cell>35.01</cell></row><row><cell>Tatoeba</cell><cell>fao-eng</cell><cell>57.04</cell><cell>87.40</cell><cell>27.51</cell><cell>38.24</cell><cell>12.61</cell></row><row><cell>Tatoeba</cell><cell>mal-eng</cell><cell>98.16</cell><cell>98.45</cell><cell>32.20</cell><cell>88.46</cell><cell>83.30</cell></row><row><cell>Tatoeba</cell><cell>ile-eng</cell><cell>87.88</cell><cell>85.58</cell><cell>57.71</cell><cell>60.36</cell><cell>59.59</cell></row><row><cell>Tatoeba</cell><cell>bos-eng</cell><cell>95.86</cell><cell>94.92</cell><cell>93.27</cell><cell>94.02</cell><cell>13.65</cell></row><row><cell>Tatoeba</cell><cell>cor-eng</cell><cell>4.45</cell><cell>10.11</cell><cell>3.42</cell><cell>3.53</cell><cell>2.83</cell></row><row><cell>Tatoeba</cell><cell>cat-eng</cell><cell>95.80</cell><cell>95.38</cell><cell>94.42</cell><cell>96.05</cell><cell>88.31</cell></row><row><cell>Tatoeba</cell><cell>eus-eng</cell><cell>93.32</cell><cell>95.01</cell><cell>23.18</cell><cell>31.33</cell><cell>53.38</cell></row><row><cell>Tatoeba</cell><cell>yue-eng</cell><cell>87.75</cell><cell>89.58</cell><cell>71.45</cell><cell>77.58</cell><cell>77.03</cell></row><row><cell>Tatoeba</cell><cell>swe-eng</cell><cell>95.31</cell><cell>95.63</cell><cell>94.42</cell><cell>95.45</cell><cell>19.53</cell></row><row><cell>Tatoeba</cell><cell>dtp-eng</cell><cell>7.39</cell><cell>10.85</cell><cell>5.69</cell><cell>5.03</cell><cell>3.41</cell></row><row><cell>Tatoeba</cell><cell>kat-eng</cell><cell>81.16</cell><cell>95.02</cell><cell>95.44</cell><cell>95.46</cell><cell>0.42</cell></row><row><cell>Tatoeba</cell><cell>jpn-eng</cell><cell>93.78</cell><cell>95.38</cell><cell>90.41</cell><cell>92.51</cell><cell>71.36</cell></row><row><cell>Tatoeba</cell><cell>csb-eng</cell><cell>27.03</cell><cell>52.57</cell><cell>21.56</cell><cell>23.73</cell><cell>10.03</cell></row><row><cell>Tatoeba</cell><cell>xho-eng</cell><cell>4.68</cell><cell>91.55</cell><cell>4.52</cell><cell>6.53</cell><cell>5.51</cell></row><row><cell>Tatoeba</cell><cell>orv-eng</cell><cell>23.24</cell><cell>38.93</cell><cell>15.10</cell><cell>23.77</cell><cell>5.79</cell></row><row><cell>Tatoeba</cell><cell>ind-eng</cell><cell>92.98</cell><cell>93.66</cell><cell>92.74</cell><cell>93.50</cell><cell>88.04</cell></row><row><cell>Tatoeba</cell><cell>tuk-eng</cell><cell>16.35</cell><cell>75.27</cell><cell>15.16</cell><cell>14.91</cell><cell>5.48</cell></row><row><cell cols="2">Tatoeba max-eng</cell><cell>36.96</cell><cell>63.26</cell><cell>45.25</cell><cell>48.77</cell><cell>36.14</cell></row><row><cell>Tatoeba</cell><cell>swh-eng</cell><cell>55.66</cell><cell>84.50</cell><cell>14.48</cell><cell>16.02</cell><cell>16.74</cell></row><row><cell>Tatoeba</cell><cell>hin-eng</cell><cell>95.32</cell><cell>96.87</cell><cell>97.62</cell><cell>97.75</cell><cell>85.23</cell></row><row><cell>Tatoeba</cell><cell>dsb-eng</cell><cell>42.34</cell><cell>64.81</cell><cell>33.43</cell><cell>36.85</cell><cell>8.78</cell></row><row><cell>Tatoeba</cell><cell>ber-eng</cell><cell>77.63</cell><cell>8.40</cell><cell>4.43</cell><cell>4.88</cell><cell>4.92</cell></row><row><cell>Tatoeba</cell><cell>tam-eng</cell><cell>87.32</cell><cell>89.0</cell><cell>24.64</cell><cell>73.60</cell><cell>72.76</cell></row><row><cell>Tatoeba</cell><cell>slk-eng</cell><cell>95.82</cell><cell>96.5</cell><cell>95.15</cell><cell>96.62</cell><cell>9.98</cell></row><row><cell>Tatoeba</cell><cell>tgl-eng</cell><cell>63.19</cell><cell>96.02</cell><cell>13.09</cell><cell>17.67</cell><cell>10.70</cell></row><row><cell>Tatoeba</cell><cell>ast-eng</cell><cell>76.35</cell><cell>90.68</cell><cell>62.17</cell><cell>70.08</cell><cell>71.13</cell></row><row><cell cols="2">Tatoeba mkd-eng</cell><cell>93.63</cell><cell>93.6</cell><cell>91.00</cell><cell>93.02</cell><cell>10.47</cell></row><row><cell cols="2">Tatoeba khm-eng</cell><cell>74.19</cell><cell>78.37</cell><cell>32.11</cell><cell>58.80</cell><cell>0.37</cell></row><row><cell>Tatoeba</cell><cell>ces-eng</cell><cell>95.52</cell><cell>96.68</cell><cell>95.12</cell><cell>95.73</cell><cell>9.55</cell></row><row><cell>Tatoeba</cell><cell>tzl-eng</cell><cell>36.56</cell><cell>58.88</cell><cell>25.46</cell><cell>34.21</cell><cell>27.82</cell></row><row><cell>Tatoeba</cell><cell>urd-eng</cell><cell>84.23</cell><cell>93.22</cell><cell>94.57</cell><cell>95.12</cell><cell>70.10</cell></row><row><cell>Tatoeba</cell><cell>ara-eng</cell><cell>90.14</cell><cell>88.80</cell><cell>87.93</cell><cell>90.19</cell><cell>85.37</cell></row><row><cell>Tatoeba</cell><cell>kor-eng</cell><cell>87.97</cell><cell>90.95</cell><cell>92.52</cell><cell>93.07</cell><cell>22.39</cell></row><row><cell>Tatoeba</cell><cell>yid-eng</cell><cell>2.49</cell><cell>88.79</cell><cell>14.38</cell><cell>30.73</cell><cell>0.16</cell></row><row><cell>Tatoeba</cell><cell>fin-eng</cell><cell>96.98</cell><cell>96.37</cell><cell>93.10</cell><cell>95.92</cell><cell>3.41</cell></row><row><cell>Tatoeba</cell><cell>tha-eng</cell><cell>96.38</cell><cell>96.14</cell><cell>96.72</cell><cell>95.99</cell><cell>2.22</cell></row><row><cell cols="2">Tatoeba wuu-eng</cell><cell>75.09</cell><cell>90.18</cell><cell>76.00</cell><cell>78.25</cell><cell>79.58</cell></row><row><cell>Average</cell><cell>mix</cell><cell>67.42</cell><cell>81.75</cell><cell>57.98</cell><cell>63.38</cell><cell>31.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>Multilingual bitext mining results. Scores are f1. Dataset Language LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco</figDesc><table><row><cell>AmazonCounterfactualClassification</cell><cell>de</cell><cell>67.82</cell><cell>73.17</cell><cell>68.35</cell><cell>69.95</cell><cell>61.35</cell></row><row><cell>AmazonCounterfactualClassification</cell><cell>ja</cell><cell>68.76</cell><cell>76.42</cell><cell>63.45</cell><cell>69.79</cell><cell>58.23</cell></row><row><cell>AmazonReviewsClassification</cell><cell>de</cell><cell>31.07</cell><cell>39.92</cell><cell>35.91</cell><cell>39.52</cell><cell>29.70</cell></row><row><cell>AmazonReviewsClassification</cell><cell>es</cell><cell>32.72</cell><cell>39.39</cell><cell>37.49</cell><cell>39.99</cell><cell>35.97</cell></row><row><cell>AmazonReviewsClassification</cell><cell>fr</cell><cell>31.12</cell><cell>38.52</cell><cell>35.30</cell><cell>39.00</cell><cell>35.92</cell></row><row><cell>AmazonReviewsClassification</cell><cell>ja</cell><cell>28.94</cell><cell>36.44</cell><cell>33.24</cell><cell>36.64</cell><cell>27.64</cell></row><row><cell>AmazonReviewsClassification</cell><cell>zh</cell><cell>30.89</cell><cell>36.45</cell><cell>35.26</cell><cell>37.74</cell><cell>32.63</cell></row><row><cell>MassiveIntentClassification</cell><cell>af</cell><cell>38.01</cell><cell>56.12</cell><cell>45.88</cell><cell>52.32</cell><cell>47.85</cell></row><row><cell>MassiveIntentClassification</cell><cell>am</cell><cell>12.70</cell><cell>55.71</cell><cell>36.75</cell><cell>41.55</cell><cell>33.30</cell></row><row><cell>MassiveIntentClassification</cell><cell>ar</cell><cell>37.16</cell><cell>50.86</cell><cell>45.14</cell><cell>51.43</cell><cell>59.25</cell></row><row><cell>MassiveIntentClassification</cell><cell>az</cell><cell>19.98</cell><cell>58.97</cell><cell>47.42</cell><cell>56.98</cell><cell>45.24</cell></row><row><cell>MassiveIntentClassification</cell><cell>bn</cell><cell>42.51</cell><cell>58.22</cell><cell>35.34</cell><cell>48.79</cell><cell>61.59</cell></row><row><cell>MassiveIntentClassification</cell><cell>cy</cell><cell>17.33</cell><cell>50.16</cell><cell>26.12</cell><cell>27.87</cell><cell>44.92</cell></row><row><cell>MassiveIntentClassification</cell><cell>da</cell><cell>45.61</cell><cell>58.25</cell><cell>57.73</cell><cell>62.77</cell><cell>51.23</cell></row><row><cell>MassiveIntentClassification</cell><cell>de</cell><cell>44.79</cell><cell>56.21</cell><cell>50.71</cell><cell>59.57</cell><cell>56.10</cell></row><row><cell>MassiveIntentClassification</cell><cell>el</cell><cell>46.71</cell><cell>57.03</cell><cell>58.70</cell><cell>62.62</cell><cell>46.13</cell></row><row><cell>MassiveIntentClassification</cell><cell>es</cell><cell>45.44</cell><cell>58.32</cell><cell>59.66</cell><cell>64.43</cell><cell>66.35</cell></row><row><cell>MassiveIntentClassification</cell><cell>fa</cell><cell>45.01</cell><cell>62.33</cell><cell>61.02</cell><cell>65.34</cell><cell>51.20</cell></row><row><cell>MassiveIntentClassification</cell><cell>fi</cell><cell>45.94</cell><cell>60.12</cell><cell>57.54</cell><cell>62.28</cell><cell>45.33</cell></row><row><cell>MassiveIntentClassification</cell><cell>fr</cell><cell>46.13</cell><cell>60.47</cell><cell>60.25</cell><cell>64.82</cell><cell>66.95</cell></row><row><cell>MassiveIntentClassification</cell><cell>he</cell><cell>42.55</cell><cell>56.55</cell><cell>52.51</cell><cell>58.21</cell><cell>43.18</cell></row><row><cell>MassiveIntentClassification</cell><cell>hi</cell><cell>40.20</cell><cell>59.40</cell><cell>58.37</cell><cell>62.77</cell><cell>63.54</cell></row><row><cell>MassiveIntentClassification</cell><cell>hu</cell><cell>42.77</cell><cell>59.52</cell><cell>60.41</cell><cell>63.87</cell><cell>44.73</cell></row><row><cell>MassiveIntentClassification</cell><cell>hy</cell><cell>28.07</cell><cell>56.20</cell><cell>51.60</cell><cell>57.74</cell><cell>38.13</cell></row><row><cell>MassiveIntentClassification</cell><cell>id</cell><cell>45.81</cell><cell>61.12</cell><cell>59.85</cell><cell>65.43</cell><cell>64.06</cell></row><row><cell>MassiveIntentClassification</cell><cell>is</cell><cell>39.86</cell><cell>54.90</cell><cell>30.83</cell><cell>37.05</cell><cell>44.35</cell></row><row><cell>MassiveIntentClassification</cell><cell>it</cell><cell>48.25</cell><cell>59.83</cell><cell>59.61</cell><cell>64.68</cell><cell>60.77</cell></row><row><cell>MassiveIntentClassification</cell><cell>ja</cell><cell>45.30</cell><cell>63.11</cell><cell>60.89</cell><cell>63.74</cell><cell>61.22</cell></row><row><cell>MassiveIntentClassification</cell><cell>jv</cell><cell>24.30</cell><cell>50.98</cell><cell>32.37</cell><cell>36.49</cell><cell>50.94</cell></row><row><cell>MassiveIntentClassification</cell><cell>ka</cell><cell>22.70</cell><cell>48.35</cell><cell>43.03</cell><cell>49.85</cell><cell>33.84</cell></row><row><cell>MassiveIntentClassification</cell><cell>km</cell><cell>22.48</cell><cell>48.55</cell><cell>40.04</cell><cell>45.47</cell><cell>37.34</cell></row><row><cell>MassiveIntentClassification</cell><cell>kn</cell><cell>4.32</cell><cell>56.24</cell><cell>40.98</cell><cell>50.63</cell><cell>53.54</cell></row><row><cell>MassiveIntentClassification</cell><cell>ko</cell><cell>44.26</cell><cell>60.99</cell><cell>50.30</cell><cell>61.82</cell><cell>53.36</cell></row><row><cell>MassiveIntentClassification</cell><cell>lv</cell><cell>39.75</cell><cell>57.10</cell><cell>54.68</cell><cell>61.29</cell><cell>46.50</cell></row><row><cell>MassiveIntentClassification</cell><cell>ml</cell><cell>41.33</cell><cell>57.91</cell><cell>42.41</cell><cell>54.34</cell><cell>58.27</cell></row><row><cell>MassiveIntentClassification</cell><cell>mn</cell><cell>16.20</cell><cell>58.50</cell><cell>51.77</cell><cell>56.59</cell><cell>40.28</cell></row><row><cell>MassiveIntentClassification</cell><cell>ms</cell><cell>43.23</cell><cell>58.60</cell><cell>54.76</cell><cell>60.70</cell><cell>59.65</cell></row><row><cell>MassiveIntentClassification</cell><cell>my</cell><cell>25.37</cell><cell>57.35</cell><cell>52.01</cell><cell>57.09</cell><cell>37.42</cell></row><row><cell>MassiveIntentClassification</cell><cell>nb</cell><cell>37.74</cell><cell>57.91</cell><cell>55.50</cell><cell>62.60</cell><cell>49.41</cell></row><row><cell>MassiveIntentClassification</cell><cell>nl</cell><cell>45.00</cell><cell>59.37</cell><cell>59.51</cell><cell>63.57</cell><cell>52.09</cell></row><row><cell>MassiveIntentClassification</cell><cell>pl</cell><cell>44.99</cell><cell>59.71</cell><cell>59.43</cell><cell>64.30</cell><cell>50.48</cell></row><row><cell>MassiveIntentClassification</cell><cell>pt</cell><cell>48.55</cell><cell>60.16</cell><cell>61.27</cell><cell>64.89</cell><cell>66.69</cell></row><row><cell>MassiveIntentClassification</cell><cell>ro</cell><cell>44.30</cell><cell>57.92</cell><cell>58.39</cell><cell>62.80</cell><cell>50.53</cell></row><row><cell>MassiveIntentClassification</cell><cell>ru</cell><cell>44.29</cell><cell>60.67</cell><cell>59.04</cell><cell>63.26</cell><cell>58.32</cell></row><row><cell>MassiveIntentClassification</cell><cell>sl</cell><cell>44.72</cell><cell>59.37</cell><cell>57.36</cell><cell>63.51</cell><cell>47.74</cell></row><row><cell>MassiveIntentClassification</cell><cell>sq</cell><cell>46.12</cell><cell>58.03</cell><cell>56.59</cell><cell>62.49</cell><cell>48.94</cell></row><row><cell>MassiveIntentClassification</cell><cell>sv</cell><cell>45.95</cell><cell>59.66</cell><cell>59.43</cell><cell>64.73</cell><cell>50.79</cell></row><row><cell>MassiveIntentClassification</cell><cell>sw</cell><cell>31.89</cell><cell>51.62</cell><cell>29.57</cell><cell>31.95</cell><cell>49.81</cell></row><row><cell>MassiveIntentClassification</cell><cell>ta</cell><cell>29.63</cell><cell>55.04</cell><cell>36.77</cell><cell>50.17</cell><cell>56.40</cell></row><row><cell>MassiveIntentClassification</cell><cell>te</cell><cell>36.03</cell><cell>58.32</cell><cell>40.72</cell><cell>52.82</cell><cell>54.71</cell></row><row><cell>MassiveIntentClassification</cell><cell>th</cell><cell>43.39</cell><cell>56.58</cell><cell>58.97</cell><cell>61.11</cell><cell>44.43</cell></row><row><cell>MassiveIntentClassification</cell><cell>tl</cell><cell>29.73</cell><cell>55.28</cell><cell>33.67</cell><cell>38.83</cell><cell>50.21</cell></row><row><cell>MassiveIntentClassification</cell><cell>tr</cell><cell>43.93</cell><cell>60.91</cell><cell>59.90</cell><cell>64.54</cell><cell>46.56</cell></row><row><cell>MassiveIntentClassification</cell><cell>ur</cell><cell>26.11</cell><cell>56.70</cell><cell>52.80</cell><cell>56.37</cell><cell>56.75</cell></row><row><cell>MassiveIntentClassification</cell><cell>vi</cell><cell>44.33</cell><cell>56.67</cell><cell>56.61</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Multilingual classification results. Scores are accuracy.</figDesc><table><row><cell cols="8">Dataset Language Komninos LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco</cell></row><row><cell>STS17</cell><cell>ko-ko</cell><cell>2.54</cell><cell>70.52</cell><cell>71.32</cell><cell>77.03</cell><cell>83.41</cell><cell>66.89</cell></row><row><cell>STS17</cell><cell>ar-ar</cell><cell>13.78</cell><cell>67.47</cell><cell>69.07</cell><cell>79.16</cell><cell>79.10</cell><cell>76.42</cell></row><row><cell>STS17</cell><cell>en-ar</cell><cell>9.08</cell><cell>65.05</cell><cell>74.51</cell><cell>81.22</cell><cell>80.85</cell><cell>78.07</cell></row><row><cell>STS17</cell><cell>en-de</cell><cell>-3.11</cell><cell>66.66</cell><cell>73.85</cell><cell>84.22</cell><cell>83.28</cell><cell>59.10</cell></row><row><cell>STS17</cell><cell>en-tr</cell><cell>-0.45</cell><cell>70.05</cell><cell>72.07</cell><cell>76.74</cell><cell>74.90</cell><cell>11.80</cell></row><row><cell>STS17</cell><cell>es-en</cell><cell>-8.18</cell><cell>55.30</cell><cell>65.71</cell><cell>84.44</cell><cell>86.11</cell><cell>78.22</cell></row><row><cell>STS17</cell><cell>es-es</cell><cell>48.23</cell><cell>79.67</cell><cell>80.83</cell><cell>85.56</cell><cell>85.14</cell><cell>86.00</cell></row><row><cell>STS17</cell><cell>fr-en</cell><cell>5.81</cell><cell>70.82</cell><cell>76.98</cell><cell>76.59</cell><cell>81.17</cell><cell>80.46</cell></row><row><cell>STS17</cell><cell>it-en</cell><cell>3.64</cell><cell>70.98</cell><cell>76.99</cell><cell>82.35</cell><cell>84.24</cell><cell>51.58</cell></row><row><cell>STS17</cell><cell>nl-en</cell><cell>-0.44</cell><cell>68.12</cell><cell>75.22</cell><cell>81.71</cell><cell>82.51</cell><cell>45.85</cell></row><row><cell>STS22</cell><cell>de</cell><cell>33.04</cell><cell>25.69</cell><cell>48.58</cell><cell>44.64</cell><cell>46.70</cell><cell>30.05</cell></row><row><cell>STS22</cell><cell>es</cell><cell>48.53</cell><cell>54.92</cell><cell>63.18</cell><cell>56.56</cell><cell>59.91</cell><cell>65.41</cell></row><row><cell>STS22</cell><cell>pl</cell><cell>12.47</cell><cell>18.34</cell><cell>39.30</cell><cell>33.74</cell><cell>33.65</cell><cell>31.13</cell></row><row><cell>STS22</cell><cell>tr</cell><cell>47.38</cell><cell>36.97</cell><cell>58.15</cell><cell>53.39</cell><cell>56.30</cell><cell>47.14</cell></row><row><cell>STS22</cell><cell>ar</cell><cell>32.42</cell><cell>42.57</cell><cell>57.67</cell><cell>46.2</cell><cell>52.19</cell><cell>58.67</cell></row><row><cell>STS22</cell><cell>ru</cell><cell>19.44</cell><cell>39.24</cell><cell>57.49</cell><cell>57.08</cell><cell>58.74</cell><cell>43.36</cell></row><row><cell>STS22</cell><cell>zh</cell><cell>4.78</cell><cell>49.41</cell><cell>63.02</cell><cell>58.75</cell><cell>61.75</cell><cell>66.78</cell></row><row><cell>STS22</cell><cell>fr</cell><cell>49.43</cell><cell>58.61</cell><cell>77.95</cell><cell>70.55</cell><cell>74.30</cell><cell>80.38</cell></row><row><cell>STS22</cell><cell>de-en</cell><cell>28.65</cell><cell>32.35</cell><cell>50.14</cell><cell>52.65</cell><cell>50.81</cell><cell>51.16</cell></row><row><cell>STS22</cell><cell>es-en</cell><cell>26.97</cell><cell>54.34</cell><cell>71.86</cell><cell>67.33</cell><cell>70.26</cell><cell>75.06</cell></row><row><cell>STS22</cell><cell>it</cell><cell>57.77</cell><cell>60.31</cell><cell>72.22</cell><cell>55.22</cell><cell>60.65</cell><cell>65.65</cell></row><row><cell>STS22</cell><cell>pl-en</cell><cell>45.55</cell><cell>53.63</cell><cell>69.41</cell><cell>69.02</cell><cell>73.07</cell><cell>53.31</cell></row><row><cell>STS22</cell><cell>zh-en</cell><cell>14.05</cell><cell>46.19</cell><cell>64.02</cell><cell>65.71</cell><cell>67.96</cell><cell>68.45</cell></row><row><cell>STS22</cell><cell>es-it</cell><cell>41.10</cell><cell>42.21</cell><cell>69.69</cell><cell>47.67</cell><cell>53.70</cell><cell>65.50</cell></row><row><cell>STS22</cell><cell>de-fr</cell><cell>14.77</cell><cell>37.41</cell><cell>53.28</cell><cell>51.73</cell><cell>62.34</cell><cell>53.28</cell></row><row><cell>STS22</cell><cell>de-pl</cell><cell>11.21</cell><cell>15.67</cell><cell>58.69</cell><cell>44.22</cell><cell>40.53</cell><cell>43.05</cell></row><row><cell>STS22</cell><cell>fr-pl</cell><cell>39.44</cell><cell>39.44</cell><cell>61.98</cell><cell>50.71</cell><cell>84.52</cell><cell>28.17</cell></row><row><cell cols="2">Average mix</cell><cell>22.14</cell><cell>51.55</cell><cell>65.67</cell><cell>64.23</cell><cell>67.71</cell><cell>57.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>Multilingual STS Results. Scores are Spearman correlations of cosine similarities.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ embeddings-benchmark/mteb 2 https://huggingface.co/spaces/mteb/ leaderboard</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://huggingface.co/datasets/mteb/ results on the cluster label, thus the permutation of labels does not affect the score.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://beta.openai.com/docs/guides/ embeddings</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://beta.openai.com/docs/guides/ embeddings/similarity-embeddings</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://arxiv.org/help/api/ 9 https://api.biorxiv.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://huggingface.co/datasets/ flax-sentence-embeddings/stackexchange_ title_body_jsonl 12 https://scikit-learn.org/0.19/ datasets/twenty_newsgroups.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">https://tabilab.cmpe.boun.edu.tr/ BIOSSES/DataSet.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Douwe Kiela and Teven Le Scao for feedback and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of text clustering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining text data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="77" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rada BIOSSES</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page">93</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
