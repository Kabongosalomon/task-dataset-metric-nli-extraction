<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Transformation-Invariant Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Monnier</surname></persName>
							<email>tom.monnier@enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
							<email>thibault.groueix@enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
							<email>mathieu.aubry@enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Transformation-Invariant Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in image clustering typically focus on learning better deep representations. In contrast, we present an orthogonal approach that does not rely on abstract features but instead learns to predict transformations and performs clustering directly in pixel space. This learning process naturally fits in the gradient-based training of K-means and Gaussian mixture model, without requiring any additional loss or hyper-parameters. It leads us to two new deep transformation-invariant clustering frameworks, which jointly learn prototypes and transformations. More specifically, we use deep learning modules that enable us to resolve invariance to spatial, color and morphological transformations. Our approach is conceptually simple and comes with several advantages, including the possibility to easily adapt the desired invariance to the task and a strong interpretability of both cluster centers and assignments to clusters. We demonstrate that our novel approach yields competitive and highly promising results on standard image clustering benchmarks. Finally, we showcase its robustness and the advantages of its improved interpretability by visualizing clustering results over real photograph collections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gathering collections of images on a topic of interest is getting easier every day: simple tools can aggregate data from social media, web search, or specialized websites and filter it using hashtags, GPS coordinates, or semantic labels. However, identifying visual trends in such image collections remains difficult and usually involves manually organizing images or designing an ad hoc algorithm. Our goal in this paper is to design a clustering method which can be applied to such image collections, output a visual representation for each cluster and show how it relates to every associated image.</p><p>Directly comparing image pixels to decide if they belong to the same cluster leads to poor results because they are strongly impacted by factors irrelevant to clustering, such as exact viewpoint or lighting. Approaches to obtain clusters invariant to these transformations can be broadly classified into two groups. A first set of methods extracts invariant features and performs clustering in feature space. The features can be manually designed, but most state-of-the-art methods learn them directly from data. This is challenging because images are high-dimensional and learning relevant invariances thus requires huge amounts of data. For this reason, while recent approaches perform well on simple datasets like MNIST, they still struggle with real images. Another limitation of these approaches is that learned features are hard to interpret and visualize, making clustering results difficult to analyze. A second set of approaches, following the seminal work of Frey and Jojic on transformation-invariant clustering <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, uses explicit transformation models to align images before comparing them. These approaches have several potential advantages: (i) they enable direct control of the invariances to consider; (ii) because they do not need to discover invariances, they are potentially less data-hungry; (iii) since images are explicitly aligned, clustering process and results can easily be visualized. However, transformation-invariant approaches require solving a difficult joint optimization problem. In practice, they are thus often limited to small datasets and simple transformations, such as affine 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.  <ref type="figure">Figure 1</ref>: Overview. (a) Given a sample x i and prototypes c 1 and c 2 , standard clustering such as K-means assigns the sample to the closest prototype. Our DTI clustering first aligns prototypes to the sample using a family of parametric transformations -here rotations -then picks the prototype whose alignment yields the smallest distance. (b) We predict alignment with deep learning. Given an image x i , each parameter predictor f k predicts parameters for a sequence of transformations -here affine T aff ?aff , morphological T mor ?mor , and thin plate spline T tps ?tps -to align prototype c k to x i . (c) Examples of interpretable prototypes discovered from large images sets (15k each) associated to hashtags in Instagram using our DTI clustering with 40 clusters. Each cluster contains from 200 to 800 images.</p><p>transformations, and to the best of our knowledge they have never been evaluated on large standard image clustering datasets.</p><p>In this paper, we propose a deep transformation-invariant (DTI) framework that enables to perform transformation-invariant clustering at scale and uses complex transformations. Our main insight is to jointly learn deep alignment and clustering parameters with a single loss, relying on the gradient-based adaptations of K-means <ref type="bibr" target="#b37">[38]</ref> and GMM optimization <ref type="bibr" target="#b8">[9]</ref>. Not only is predicting transformations more computationally efficient than optimizing them, but it enables us to use complex color, thin plate spline and morphological transformations without any specific regularization. Because it is pixel-based, our deep transformation-invariant clustering is also easy to interpret: cluster centers and image alignments can be visualized to understand assignments. Despite its apparent simplicity, we demonstrate that our DTI clustering framework leads to results on par with the most recent feature learning approaches on standard benchmarks. We also show it is capable of discovering meaningful modes in real photograph collections, which we see as an important step to bridge the gap between theoretically well-grounded clustering approaches and semi-automatic tools relying on hand-designed features for exploring image collections, such as AverageExplorer <ref type="bibr" target="#b51">[52]</ref> or ShadowDraw <ref type="bibr" target="#b31">[32]</ref>.</p><p>We first briefly discuss related works in Section 2. Section 3 then presents our DTI framework ( <ref type="figure">Fig. 1a</ref>). Section 4 introduces our deep transformation modules and architecture ( <ref type="figure">Fig. 1b</ref>) and discuss training details. Finally, Section 5 presents and analyzes our results ( <ref type="figure">Fig. 1c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>In this paper we present: -a deep transformation-invariant clustering approach that jointly learns to cluster and align images, -a deep image transformation module to learn spatial alignment, color modifications and for the first time morphological transformations, -an experimental evaluation showing that our approach is competitive on standard image clustering benchmarks, improving over state-of-the-art on Fashion-MNIST and SVHN, and provides highly interpretable qualitative results even on challenging web image collections.</p><p>Code, data, models as well as more visual results are available on our project webpage 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Most recent approaches to image clustering focus on learning deep image representations, or features, on which clustering can be performed. Common strategies include autoencoders <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, contrastive approaches <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b43">44]</ref>, GANs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b40">41]</ref> and mutual information based strategies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>. Especially related to our work is <ref type="bibr" target="#b27">[28]</ref> which leverages the idea of capsule <ref type="bibr" target="#b19">[20]</ref> to learn equivariant image features, in a similar fashion of equivariant models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref>. However, our method aims at being invariant to transformations but not at learning a representation. Another type of approach is to align images in pixel space using a relevant family of transformations, such as translations, rotations, or affine transformations to obtain more meaningful pixel distances before clustering them. Frey and Jojic first introduced transformation-invariant clustering <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> by integrating pixel permutations as a discrete latent variable within an Expectation Maximization (EM) <ref type="bibr" target="#b8">[9]</ref> procedure for a mixture of Gaussians. Their approach was however limited to a finite set of discrete transformations. Congealing generalized the idea to continuous parametric transformations, and in particular affine transformations, initially by using entropy minimization <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b29">30]</ref>. A later version using least square costs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> demonstrated the relation of this approach to the classical Lukas-Kanade image alignment algorithm <ref type="bibr" target="#b36">[37]</ref>. In its classical version, congealing only enables to align all dataset images together, but the idea was extended to clustering <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b33">34]</ref>, for example using a Bayesian model <ref type="bibr" target="#b38">[39]</ref>, or in a spectral clustering framework <ref type="bibr" target="#b33">[34]</ref>. These works typically formulate difficult joint optimization problems and solve them by alternating between clustering and transformation optimization for each sample. They are thus limited to relatively small datasets and to the best of our knowledge were never compared to modern deep approaches on large benchmarks. Deep learning was recently used to scale the idea of congealing for global alignment of a single class of images <ref type="bibr" target="#b0">[1]</ref> or time series <ref type="bibr" target="#b45">[46]</ref>. Both works build on the idea of Spatial Transformer Networks <ref type="bibr" target="#b22">[23]</ref> (STN) that spatial transformation are differentiable and can be learned by deep networks. We also build upon STN, but go beyond single-class alignment to jointly perform clustering. Additionally, we extend the idea to color and morphological transformations. We believe our work is the first to use deep learning to perform clustering in pixel space by explicitly aligning images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Transformation-Invariant clustering</head><p>In this section, we first discuss a generic formulation of our deep transformation-invariant clustering approach, then derive two algorithms based on K-means <ref type="bibr" target="#b37">[38]</ref> and Gaussian mixture model <ref type="bibr" target="#b8">[9]</ref>. Notation: In all the rest of the paper, we use the notation a 1:n to refer to the set {a 1 , . . . , a n }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DTI framework</head><p>Contrary to most recent image clustering methods which rely on feature learning, we propose to perform clustering in pixel space by making the clustering invariant to a family of transformations. We consider N image samples x 1:N and aim at grouping them in K clusters using a prototype method. More specifically, each cluster k is defined by a prototype c k , which can also be seen as an image, and prototypes are optimized to minimize a loss L which typically evaluates how well they represent the samples. We further assume that L can be written as a sum of a loss l computed over each sample:</p><formula xml:id="formula_0">L(c 1:K ) = N i=1 l(x i , {c 1 , . . . , c K }).<label>(1)</label></formula><p>Once the problem is solved, each sample x i will be associated to the closest prototype.</p><p>Our key assumption is that in addition to the data, we have access to a group of parametric transformations {T ? , ? ? B} to which we want to make the clustering invariant. For example, one can consider ? ? R 6 and T ? the 2D affine transformation parametrized by ?. Other transformations are discussed in Section 4.1. Instead of finding clusters by minimizing the loss of Equation 1, one can minimize the following transformation-invariant loss:</p><formula xml:id="formula_1">L TI (c 1:K ) = N i=1 min ? 1:K l(x i , {T ?1 (c 1 ), . . . , T ? K (c K )}).<label>(2)</label></formula><p>In this equation, the minimum over ? 1:K is taken for each sample independently. This loss is invariant to transformations of the prototypes (see proof in Appendix B). Also note there is not a single optimum since the loss is the same if any prototype c k is replaced by T ? (c k ) for any ? ? B. If necessary, for example for visualization purposes, this ambiguity can easily be resolved by adding a small regularization on the transformations. The optimization problem associated to L TI is of course difficult. A natural approach, which we use as baseline (noted TI), is to alternatively minimize over transformations and clustering parameters. We show that performing such optimization using a gradient descent can already lead to improved results over standard clustering but is computationally expensive. We experimentally show it is faster and actually better to instead learn K (deep) predictors f 1:K for each prototype, which aim at associating to each sample x i the transformation parameters f 1:K (x i ) minimizing the loss, i.e. to minimize the following loss:</p><formula xml:id="formula_2">L DTI (c 1:K , f 1:K ) = N i=1 l(x i , {T f1(xi) (c 1 ), . . . , T f K (xi) (c K )}),<label>(3)</label></formula><p>where predictors f 1:K are now shared for all samples. We found that using deep parameters predictors not only enables more efficient training but also leads to better clustering results especially with more complex transformations. Indeed, the structure and optimization of the predictors naturally regularize the parameters for each sample, without requiring any specific regularization loss, especially in the case of high numbers N of samples and transformation parameters.</p><p>In the next section we present concrete losses and algorithms. We then describe differentiable modules for relevant transformations and discuss parameter predictor architecture as well as training in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Application to K-means and GMM</head><p>K-means. The goal of K-means algorithm <ref type="bibr" target="#b37">[38]</ref> is to find a set of prototypes c 1:K such that the average Euclidean distance between each sample and the closest prototype is minimized. Following the reasoning of Section 3.1, the loss optimized in K-means can be transformed into a transformationinvariant loss:</p><formula xml:id="formula_3">L DTI K-means (c 1:K , f 1:K ) = N i=1 min k x i ? T f k (xi) (c k ) 2 .<label>(4)</label></formula><p>Following batch gradient-based trainings <ref type="bibr" target="#b2">[3]</ref> of K-means, we can then simply jointly minimize L DTI K-means over prototypes c 1:K and deep transformation parameter predictors f 1:K using a batch gradient descent algorithm. In practice, we initialize prototypes c 1:K with random samples and predictors f 1:K such that ?k, ?x, T f k (x) = Id.</p><p>Gaussian mixture model. We now consider that data are observations of a mixture of K multivariate normal random variables X 1:K , i.e. X = k ? k,? X k where ? is the Kronecker function and ? ? {1, . . . , K} is a random variable defined by P (? = k) = ? k , with ?k, ? k &gt; 0 and k ? k = 1.</p><p>We write ? k and ? k the mean and covariance of X k and G( . ; ? k , ? k ) associated probability density function. The transformation-invariant negative log-likelihood can then be written:</p><formula xml:id="formula_4">L DTI GMM (? 1:K , ? 1:K , ? 1:K , f 1:K ) = ? N i=1 log K k=1 ? k G x i ; T f k (xi) (? k ), T * fk(xi) (? k ) ,<label>(5)</label></formula><p>where T * is slightly modified version of T . Indeed, T may include transformations that one can apply to the covariance, such as spatial transformations, and other that would not make sense, such as additive color transformations. We jointly minimize L DTI GMM over Gaussian parameters, mixing probabilities, and deep transformation parameters f 1:K using a batch gradient-based EM procedure similar to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref> and detailed in Algorithm 1. In practice, we assume that pixels are independent resulting in diagonal covariance matrices.</p><p>In such gradient-based procedures, two constraints have to be enforced, namely the positivity and normalization of mixing probabilities ? k and the non-negativeness of the diagonal covariance terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Deep Transformation-Invariant Gaussian Mixture Model</head><p>Input: data X, number of clusters K, transformation T Output: cluster assignations, Gaussian parameters ? 1:K , ? 1:K , deep predictors f 1:K Initialization: ? 1:K with random samples, ? 1:K = 0.5, ? 1:K = 1 and ?k, ?x, T f k (x) = Id while not converged do i. sample a batch of data points x 1:N ii. compute mixing probabilities:</p><formula xml:id="formula_5">? 1:K = softmax(? 1:K ) iii. compute per-sample Gaussian transformed parameters: ?k, ?i,? ki = T f k (xi) (? k ) and? ki = T * f k (xi) (? k ) + diag(? 2 min ) iv. compute responsibilities: ?k, ?i, ? ki = ? k G(xi ;? ki ,? ki ) j ?j G(xi ;?ji,?ji) (E-step) v. minimize expected negative log-likelihood w.r.t to {? 1:K , ? 1:K , ? 1:K , f 1:K }: E[L DTI GMM ] = ? N i=1 K k=1 ? ki log G(x i ;? ki ,? ki ) + log(? k ) (M-step) end</formula><p>For the mixing probabilities constraints, we adopt the approach used in <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b13">[14]</ref> which optimize mixing parameters ? k used to compute the probabilities ? k using a softmax instead of directly optimizing ? k , which we write ? 1:K = softmax(? 1:K ). For the variance non-negativeness, we introduce a fixed minimal variance value ? 2 min which is added to the variances when evaluating the probability density function. This approach is different from the one in <ref type="bibr" target="#b13">[14]</ref> which instead use clipping, because we found training with clipped values was harder. In practice, we take ? min = 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning image transformations 4.1 Architecture and transformation modules</head><p>We consider a set of prototypes c 1:K we would like to transform to match a given sample x. To do so, we propose to learn for each prototype c k , a separate deep predictor which predicts transformation parameters ?. We propose to model the family of transformations T ? as a sequence of M parametric transformations such that, writing ? = (? 1 , . . . , ? M ),</p><formula xml:id="formula_6">T ? = T M ? M ? . . . ? T 1 ? 1 .</formula><p>In the following, we describe the architecture of transformation parameter predictors f 1:K , as well as each family of parametric transformation modules we use. <ref type="figure">Figure 1b</ref> shows our learned transformation process on a MNIST example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters prediction network.</head><p>For all experiments, we use the same parameter predictor network architecture composed of a shared ResNet <ref type="bibr" target="#b18">[19]</ref> backbone truncated after the global average pooling, followed by K ? M Multi-Layer Perceptrons (MLPs), one for each prototype and each transformation module. For the ResNet backbone, we use ResNet-20 for images smaller than 64 ? 64 and ResNet-18 otherwise. Each MLP has the same architecture, with two hidden layers of 128 units.</p><p>Spatial transformer module. To model spatial transformations of the prototypes, we follow the spatial transformers developed by Jaderberg et al. <ref type="bibr" target="#b22">[23]</ref>. The key idea is to model spatial transformations as a differentiable image sampling of the input using a deformed sampling grid. We use affine T aff ? , projective T proj ? and thin plate spline T tps ? <ref type="bibr" target="#b1">[2]</ref> transformations which respectively correspond to 6, 8 and 16 (a 4x4 grid of control points) parameters.</p><p>Color transformation module. We model color transformation with a channel-wise diagonal affine transformation on the full image, which we write T col ? . It has 2 parameters for greyscale images and 6 parameters for colored images. We first used a full affine transformation with 12 parameters, however the network was able to hide several patterns in the different color channels of a single prototype (Appendix C.4). Note that a similar transformation was theoretically introduced in capsules <ref type="bibr" target="#b27">[28]</ref>, but with the different goal of obtaining a color-invariant feature representation. Deep feature-based approaches often handle color images with a pre-processing step such as Sobel filtering <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. We believe the way we align colors of the prototypes to obtain color invariance in pixel space is novel, and it enables us to directly work with colored images without using any pre-processing or specific invariant features.</p><p>Morphological transformation module. We introduce a new transformation module to learn morphological operations <ref type="bibr" target="#b15">[16]</ref> such as dilation and erosion. We consider a greyscale image x ? R D of size U ? V = D, we write x[u, v] the value of the pixel (u, v) for u ? {1, . . . , U } and v ? {1, . . . , V }. Given a 2D region A, the dilation of x by A,</p><formula xml:id="formula_7">D A (x) ? R D , is defined by D A (x)[u, v] = max (u ,v )?A x[u + u , v + v ] and its erosion by A, E A (x) ? R D , is defined by E A (x)[u, v] = min (u ,v )?A x[u + u , v + v ].</formula><p>Directly learning the region A which parametrizes these transformations is challenging, we thus propose to learn parameters (?, a) for the following soft version of these transformations:</p><formula xml:id="formula_8">T mor (?,a) (x)[u, v] = (u ,v )?W x[u + u , v + v ] ? a[u + u , v + v ] ? e ?x[u+u ,v+v ] (u ,v )?W a[u + u , v + v ] ? e ?x[u+u ,v+v ] ,<label>(6)</label></formula><p>where W is a fixed set of 2D positions, ? is a softmax (positive values) or softmin (negative values) parameter and a is a set of parameters with values between 0 and 1 defined for every position (u , v ) ? W . Parameters a can be interpreted as an image, or as a soft version of the region A used for morphological operations. Note that if a[u , v ] = 1 {(u ,v )?A} , when ? ? +? (resp. ??), it successfully emulates D A (resp. E A ). In practice, we use a grid of integer positions around the origin of size 7 ? 7 for W . Note that since morphological transformations do not form a group, transformation-invariant denomination is slightly abusive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>We found that two key elements were critical to obtain good results: empty cluster reassignment and curriculum learning. We then discuss further implementation details and computational cost.</p><p>Empty cluster reassignment. Similar to <ref type="bibr" target="#b3">[4]</ref>, we adopt an empty cluster reassignment strategy during our clustering optimization. We reinitialize both prototype and deep predictor of "tiny" clusters using the parameters of the largest cluster with a small added noise. In practice, the size of balanced clusters being N/K, we define "tiny" as less than 20% of N/K.</p><p>Curriculum learning. Learning to predict transformations is a hard task, especially when the number of parameters is high. To ease learning, we thus adopt a curriculum learning strategy by gradually adding more complex transformation modules to the training. Given a target sequence of transformations to learn, we first train our model without any transformation -or equivalently with an identity module -then iteratively add subsequent modules once convergence has been reached. We found this is especially important when modeling local deformations with complex transformations with many parameters, such as TPS and morphological transformations. Intuitively, prototypes should first be coarsely aligned before attempting to refine the alignment with more complex transformations.</p><p>Implementation details. Both clustering parameters and parameter prediction networks are learned jointly and end-to-end using Adam optimizer <ref type="bibr" target="#b26">[27]</ref> with a 10 ?6 weight decay on the neural network parameters. We sequentially add transformation modules at a constant learning rate of 0.001 then divide the learning rate by 10 after convergence -corresponding to different numbers of epochs depending on the dataset characteristics -and train for a few more epochs with the smaller learning rate. We use a batch size of 64 for real photograph collections and 128 otherwise.</p><p>Computational cost. Training DTI K-means or DTI GMM on MNIST takes approximately 50 minutes on a single Nvidia GeForce RTX 2080 Ti GPU and full dataset inference takes 30 seconds. We found it to be much faster than directly optimizing transformation parameters (TI clustering) for which convergence took more than 10 hours of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we first analyze our approach and compare it to state-of-the-art, then showcase its interest for image collection analysis and visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis and comparisons</head><p>Similar to previous work on image clustering, we evaluate our approach with global classification accuracy (ACC), where a cluster-to-class mapping is computed using the Hungarian algorithm <ref type="bibr" target="#b28">[29]</ref>,    and Normalized Mutual Information (NMI). Datasets and corresponding transformation modules we used are described in Appendix A.</p><formula xml:id="formula_9">- - - - - - - - - 55.3 ? IIC [24] 5 avg 98.4 - - - - - - - - - - 5 minLoss 99.2 - - - - - - - - - -</formula><p>Comparison on standard benchmarks. In <ref type="table" target="#tab_0">Table 1</ref>, we report our results on standard image clustering benchmarks, i.e. digit datasets (MNIST <ref type="bibr" target="#b30">[31]</ref>, USPS <ref type="bibr" target="#b16">[17]</ref>), a clothing dataset (Fashion-MNIST <ref type="bibr" target="#b46">[47]</ref>) and a face dataset (FRGC <ref type="bibr" target="#b42">[43]</ref>). We also report results for SVHN <ref type="bibr" target="#b41">[42]</ref> where concurrent methods use pre-processing to remove color bias. In the table, we separate representation-based from pixel-based methods and mark results using data augmentation or manually selected features as input. Note that our results depend on initialization, we provide detailed statistics in Appendix C.1. Our DTI clustering is fully unsupervised and does not require any data augmentation, ad hoc features, nor any hyper-parameter while performing clustering directly in pixel space. We report average performances and performances of the minimal loss run which we found to correlate well with high performances (Appendix C.2). Because this non-trivial criterion allows to automatically select a run in a fully unsupervised way, we argue it can be compared to average results from competing methods which don't provide such criterion. First, DTI clustering achieves competitive results on all datasets, in particular improving state-ofthe-art by a significant margin on SVHN and Fashion-MNIST. For SVHN, we first found that the <ref type="table">Table 2</ref>: Augmented and specific datasets. Clustering accuracy (%) with standard deviation for methods applied on raw images (no pre-processing). We used 10 runs for our method and 5 for the baselines.  prototypes quality was harmed by digits on the side of the image. To pay more attention to the center digit, we weighted the clustering loss by a Gaussian weight (? = 7). It led to better prototypes and allowed us to improve over all concurrent methods by a large margin. Compared to representationbased methods, our pixel-based clustering is highly interpretable. <ref type="figure" target="#fig_2">Figure 2a</ref> shows standard GMM prototypes and our prototypes learned with DTI GMM which appear to be much sharper than standard ones. This directly stems from the quality of the learned transformations, visualized in <ref type="figure" target="#fig_2">Figure 2b</ref>. Our transformation modules can successfully align the prototype, adapt the thickness and apply local elastic deformations. More alignment results are available on our project webpage.</p><p>Augmented and specific datasets. DTI clustering also works on small, colored and misaligned datasets. In <ref type="table">Table 2</ref>, we highlight these strengths on specifics datasets generated from MNIST: MNIST-1k is a 1000 images subset, MNIST-color is obtained by randomly selecting a color for the foreground and background and affNIST-test 2 is the result of random affine transformations. We used an online implementation 3 for VaDE <ref type="bibr" target="#b24">[25]</ref> and official ones for IMSAT <ref type="bibr" target="#b21">[22]</ref> and IIC <ref type="bibr" target="#b23">[24]</ref> to obtain baselines. Our results show that the performances of DTI clustering is barely affected by spatial and color transformations, while baseline performances drop on affNIST-test and are almost chance on MNIST-color. <ref type="figure" target="#fig_2">Figure 2a</ref> shows the quality and interpretability of our cluster centers on affNIST-test and MNIST-color. DTI clustering also seems more data-efficient than the baselines we tested.</p><p>Ablation on MNIST. In <ref type="table" target="#tab_3">Table 3</ref>, we conduct an ablation study on MNIST of our full model trained following Section 4.2 with affine, morphological and TPS transformations. We first explore the effect of transformation modules. Their order is not crucial, as shown by similar minLoss performances, but can greatly affect the stability of the training, as can be seen in the average results. Each module contributes to the final performance, affine transformations being the most important. We then validate our training strategy showing that both empty cluster reassignment and curriculum learning for the different modules are necessary. Finally, we directly optimize the loss of Equation 2 (TI clustering) by optimizing the transformation parameters for each sample at each iteration of the batch clustering algorithm, without using our parameter predictors. With rich transformations which have many parameters, such as TPS and morphological ones, this approach fails completely. Using only affine transformations, we obtain results clearly superior to standard clustering, but worse than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Application to web images</head><p>One of the main interest of our DTI clustering is that it allows to discover trends in real image collections. All images are resized and center cropped to 128?128. The selection of the number of clusters is a difficult problem and is discussed in Appendix C.3.</p><p>In <ref type="figure">Figure 1c</ref>, we show examples of prototypes discovered in very large unfiltered sets (15k each) of Instagram images associated to different hashtags 4 using DTI GMM applied with 40 clusters. While many images are noise and are associated to prototypes which are not easily interpretable, we show prototypes where iconic photos and poses can be clearly identified. To the best of our knowledge, we believe we are the first to demonstrate this type of results from raw social network image collections. Comparable results in AverageExplorer <ref type="bibr" target="#b51">[52]</ref>, e.g. on Santa images, could be obtained using ad hoc features and user interactions, while our results are produced fully automatically. <ref type="figure">Figure 3</ref> shows qualitative clustering results on MegaDepth <ref type="bibr" target="#b34">[35]</ref> and WikiPaintings <ref type="bibr" target="#b25">[26]</ref>. Similar to our results on image clustering benchmarks, our learned prototypes are more relevant and accurate than the ones obtained from standard clustering. Note that some of our prototypes are very sharp: they typically correspond to sets of photographs between which we can accurately model deformations, e.g. scenes that are mostly planar, with little perspective effects. On the contrary, more unique photographs and photographs with strong 3D effects that we cannot model will be associated to less interpretable and blurrier prototypes, such as the ones in the last two columns of <ref type="figure">Figure 3b</ref>. In <ref type="figure">Figure 3b</ref>, in addition to the prototypes discovered, we show examples of images contained in each cluster as well as the aligned prototype. Even for such complex images, the simple combination of our color and spatial modules manages to model real image transformations like illumination variations and viewpoint changes. More web image clustering results are shown on our project webpage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced an efficient deep transformation-invariant clustering approach in raw input space.</p><p>Our key insight is the online optimization of a single clustering objective over clustering parameters and deep image transformation modules. We demonstrate competitive results on standard image clustering benchmarks, including improvements over state-of-the-art on SVHN and Fashion-MNIST. We also demonstrate promising results for real photograph collection clustering and visualization. Finally, note that our DTI clustering framework is not specific to images and can be extended to other types of data as long as appropriate transformation modules are designed beforehand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The impact of clustering mainly depends on the data it is applied on. For instance, adding structure in user data can raise ethical concerns when users are assimilated to their cluster, and receive targeted advertisement and newsfeed. However, this is not specific to our method and can be said of any clustering algorithm. Also note that while our clustering can be applied for example to data from social media, the visual interpretation of the clusters it returns via the cluster centers respects privacy much better than showing specific examples from each cluster.</p><p>Because our method provides highly interpretable results, it might bring increased understanding of clustering algorithm results for the broader public, which we think may be a significant positive impact.  <ref type="bibr" target="#b30">[31]</ref> 10,000 10 1?28?28 aff-morpho-tps USPS <ref type="bibr" target="#b16">[17]</ref> 9,298 10 1?16?16 col-aff-tps Fashion-MNIST <ref type="bibr" target="#b46">[47]</ref> 70,000 10 1?28?28 col-aff-tps FRGC <ref type="bibr" target="#b42">[43]</ref> 2,462 20 3?32?32 col-aff-tps SVHN <ref type="bibr" target="#b41">[42]</ref> 99,289 + unlabeled extra 10 3?28?28 col-proj</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset descriptions</head><p>Augmented  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Transformation invariance</head><p>We consider N image samples x 1:N , K prototypes c 1:K and a group of parametric transformations {T ? , ? ? B}. For ? 1 , ? 2 ? B, we write ? 1 ? 2 ? B the element such that T ?1?2 = T ?1 ? T ?2 . We have, for any ? 1 , . . . , ? K ? B:</p><formula xml:id="formula_10">L TI ({c 1 , . . . , c K }) = L TI ({T ?1 (c 1 ), . . . , T ? K (c K )}).</formula><p>Indeed:</p><formula xml:id="formula_11">L TI ({T ?1 (c 1 ), . . . , T ? K (c K )}) = N i=1 min {?1,...,? K }?B K l(x i , {T ?1 ? T ?1 (c 1 ), . . . , T ? K ? T ? K (c K )}) = N i=1 min {?1,...,? K }?B K l(x i , {T ?1?1 (c 1 ), . . . , T ? K ? K (c K )}) = N i=1 min {? 1 ,...,? K }?B K l(x i , {T ? 1 (c 1 ), . . . , T ? K (c K )}) = L TI ({c 1 , . . . , c K }),</formula><p>using the variable change ? k = ? k ? k , which is possible because for any ? ? B, ?B = B as we assumed to have a group of transformations.</p><p>In some specific cases, the loss is also invariant to the samples, in particular when the loss l is invariant to joint transformation of the prototype and the samples, i.e. for any ? ? B, l(x i , {c 1 , . . . , c K } = l(T ? (x i ), {T ? (c 1 ), . . . , T ? (c K )}. This is the case for example for K-means with a group of isometric transformations (e.g. rigid transformations), and it is also the case for GMM with the group of affine transformations applied to both the mean and covariance mixture parameters.</p><p>Note that we also tried to transform the samples to match the prototypes, which would lead to an invariance to sample transformation. However, a trivial solution to corresponding optimization problem is to learn "empty" prototypes and transformations of the samples into empty images. For examples, for the MNIST case with affine transformations, we observed that completely black prototypes were learned and any digit was transformed into a black image. Although a regularization term could have prevented such behaviour, we argue that keeping raw samples as target and transforming the prototypes is simpler and effective.  <ref type="figure">Figure 4</ref>: Accuracy/loss correlation. We report loss and accuracy for DTI Kmeans on MNIST-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Statistics on standard clustering benchmarks</head><p>Similar to standard K-means and GMM, there is a variation in performances depending on the random initialization. We experimentally found that: (i) runs seem to be mainly grouped into distinct modes, each corresponding to roughly the same clustering quality; (ii) a run with a low loss usually leads to high clustering performances. We launched 100 runs on MNIST-test dataset and plot the loss with respect to the accuracy for each run in <ref type="figure">Figure 4</ref>. Except 2 outliers for the 100 runs, the runs with lower loss correspond to the runs with better performances. This is verified in most of our experiments, where the minLoss criterion clearly improves over the average performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Effect of the number of clusters K</head><p>Similar to many clustering methods, the selection of the number of clusters is a challenge. We investigated if a purely quantitative analysis could be applied to select K. In <ref type="figure" target="#fig_4">Figure 5a</ref>, we plot the loss of DTI-Kmeans as a function of the number of clusters for MNIST-test (left) and Notre-Dame (right). For MNIST-test, it is clear an elbow method could be applied to select the good number of clusters. For Notre-Dame, the quantitative analysis is not as conclusive but in this case, the correct number of clusters is not clearly defined. In practice, we did not find the qualitative results on internet photo collections to be very sensitive to this choice, as shown in <ref type="figure" target="#fig_4">Figure 5b</ref> where learned prototypes are mostly consistent across the different clustering results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Constraining color transformation</head><p>While evaluating our approaches on real photograph collections, we experimentally observed that a full affine color transformation module (12 parameters) was too flexible and as a result, prototypes were able to learn different patterns hidden in each color channel. In <ref type="figure" target="#fig_5">Figure 6</ref>, we show each R, G and B channel as a greyscale image for two prototypes learned using a full affine color transformation module. One can see that a second pattern is hidden in particular in the green channels. To avoid this effect, we restricted the color transformation module to be a diagonal affine transformation corresponding to 6 parameters in total. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2006.11132v2 [cs.CV] 27 Oct 2020 (a) Classical versus Deep Transformation-Invariant clustering (b) Deep transformation module T f k (c) Prototypes learned from unfiltered Instagram images associated to different hashtags</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Prototypes learned for different datasets (b) Transformations predicted for all prototypes for 4 MNIST images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative results. (a) compares prototypes learned from GMM and our DTI GMM, (b) shows transformed prototypes given query samples from MNIST and highlight the closest prototype.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )Figure 3 :</head><label>a3</label><figDesc>Full sets of prototypes discovered with GMM and DTI GMM (b) Examples of cluster centers and aligned images with DTI GMM (20 clusters) Qualitative results on real photographs. (a) Clustering results from photographs of different locations in [35] (1,089 Sacre Coeur top-left, 1,688 Trevi fountain top-right, 2,625 Notre-Dame bottom-left) and 980 Baroque portraits from [26] (bottom-right). (b) Clustering results from 1,892 Florence cathedral images from [35]. Top row shows learned prototypes while the three bottom rows show examples of images from each cluster and aligned prototypes. These clusters contain respectively 44, 154, 134, 64, 71, 133, 85 and 64 images. The left six examples are successful clusters while the two right clusters are relative failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Loss w.r.t varying numbers of clusters for MNIST-test (left) and Notre-Dame (right) (b) Prototypes learned on Notre-Dame for different numbers of clusters Effect of K. (a) We report the loss of DTI K-means for different numbers of clusters. For MNIST-test (left), the loss is averaged over 5 runs and for Notre-Dame (right), the loss corresponds to a single run. (b) We show the prototypes learned on Notre-Dame for even numbers of clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Learned prototypes and RGB decomposition. Two examples of learned prototypes (first column) on Florence cathedral collection from<ref type="bibr" target="#b34">[35]</ref> using a full color transformation module (12 parameters). The 3 right columns correspond to R, G and B channels rescaled between 0 and 1. Note how the green channel is used to hide a completely different pattern from the other 2 channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons. We report ACC and NMI in % on standard clustering benchmarks. Symbols mark methods that use data augmentation ( ) and manually selected features as input ( ? for pretrained features from best VaDE run, ? for GIST features, ? for Sobel filters) and are thus not directly comparable. For SVHN, we also report our results with our Gaussian weighted loss ( ). Eval column refers to the aggregate used: best run (max), average (avg) or run with minimal loss (minLoss).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">MNIST</cell><cell cols="2">MNIST-test</cell><cell cols="2">USPS</cell><cell cols="2">F-MNIST</cell><cell>FRGC</cell><cell></cell><cell>SVHN</cell></row><row><cell>Method</cell><cell cols="2">Runs Eval</cell><cell cols="10">ACC NMI ACC NMI ACC NMI ACC NMI ACC NMI</cell><cell>ACC</cell></row><row><cell cols="3">Clustering on a learned feature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DEC [48, 50]</cell><cell>9</cell><cell>max</cell><cell cols="10">86.3 83.4 85.6 83.0 76.2 76.7 51.8 54.6 37.8 50.5</cell><cell>-</cell></row><row><cell>InfoGAN [6, 41]</cell><cell>5</cell><cell>max</cell><cell cols="2">89.0 86.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">61.0 59.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VaDE [25, 50]</cell><cell>10</cell><cell>max</cell><cell cols="2">94.5 87.6</cell><cell>-</cell><cell>-</cell><cell cols="4">56.6 51.2 57.8 63.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ClusterGAN [41] 5</cell><cell>max</cell><cell cols="2">95.0 89.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">63.0 64.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>JULE [49]</cell><cell>3</cell><cell>avg</cell><cell cols="10">96.4 91.3 96.1 91.5 95.0 91.3 56.3 60.8 46.1 57.4</cell><cell>-</cell></row><row><cell>DEPICT [10]</cell><cell>5</cell><cell>avg</cell><cell cols="10">96.5 91.7 96.3 91.5 96.4 92.7 39.2 39.2 47.0 61.0</cell><cell>-</cell></row><row><cell>DSCDAN [50]</cell><cell>10</cell><cell>avg</cell><cell cols="8">97.8 94.1 98.0 94.6 86.9 85.7 66.2 64.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="11">Clustering on a learned feature with data augmentation and/or ad hoc data representation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SpectralNet [44]</cell><cell>5</cell><cell>avg</cell><cell cols="3">97.1  ? 92.4  ? -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IMSAT [22]</cell><cell>12</cell><cell>avg</cell><cell>98.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.3  ?</cell></row><row><cell>ADC [18]</cell><cell>20</cell><cell>avg</cell><cell>98.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-43.7</cell><cell>-</cell><cell>38.6</cell></row><row><cell>SCAE [28]</cell><cell>5</cell><cell>avg</cell><cell>98.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>98.0 95.1 87.3 89.0 68.2 66.3 41.6 51.1 39.5 / 63.3</figDesc><table><row><cell cols="2">Clustering on pixel values</cell><cell></cell><cell></cell><cell></cell></row><row><cell>K-means [38]</cell><cell>10</cell><cell>avg</cell><cell>54.8 50.2 55.9 51.2 65.3 61.2 54.1 51.4 22.7 26.5</cell><cell>12.2</cell></row><row><cell>GMM [9]</cell><cell>10</cell><cell>avg</cell><cell>54.2 51.7 55.6 54.7 66.0 60.9 49.7 51.2 24.2 27.9</cell><cell>11.6</cell></row><row><cell>DTI K-means</cell><cell>10</cell><cell>avg</cell><cell cols="2">97.3 94.0 96.6 94.6 86.4 88.2 61.2 63.7 39.6 48.7 36.4 / 44.5</cell></row><row><cell></cell><cell cols="4">10 minLoss 97.2 93.8 98.0 95.3 89.8 89.5 57.4 64.1 41.1 49.7 39.6 / 62.6</cell></row><row><cell>DTI GMM</cell><cell>10</cell><cell>avg</cell><cell cols="2">95.9 93.2 97.8 94.7 84.5 87.2 59.6 62.2 40.1 48.9 36.7 / 57.4</cell></row><row><cell></cell><cell cols="3">10 minLoss 97.1 93.7</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on MNIST. Clustering accuracy (%) over 10 runs.</figDesc><table><row><cell>Method</cell><cell>Avg</cell><cell>MinLoss</cell></row><row><cell>DTI clustering (aff-morpho-tps)</cell><cell>97.3</cell><cell>97.2</cell></row><row><cell>ordering: aff-tps-morpho</cell><cell>95.5</cell><cell>96.9</cell></row><row><cell>ordering: morpho-aff-tps</cell><cell>27.5</cell><cell>97.0</cell></row><row><cell>w/o morphological</cell><cell>94.8</cell><cell>95.8</cell></row><row><cell>w/o thin plate spline</cell><cell>90.0</cell><cell>82.5</cell></row><row><cell>w/o affine</cell><cell>85.1</cell><cell>96.8</cell></row><row><cell>affine only</cell><cell>90.1</cell><cell>90.5</cell></row><row><cell>w/o empty cluster reassignment</cell><cell>80.9</cell><cell>78.6</cell></row><row><cell>w/o curriculum learning</cell><cell>83.9</cell><cell>78.9</cell></row><row><cell>TI clustering (aff-morpho-tps, 1 run)</cell><cell>26.3</cell><cell>26.3</cell></row><row><cell>TI clustering (affine only)</cell><cell>73.0</cell><cell>73.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Datasets and transformation sequences used</figDesc><table><row><cell>Dataset</cell><cell>Samples</cell><cell cols="3">Classes Dimension Transformation sequence</cell></row><row><cell>Standard</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MNIST [31]</cell><cell>70,000</cell><cell>10</cell><cell>1?28?28</cell><cell>aff-morpho-tps</cell></row><row><cell>MNIST-test</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>summarizes dataset characteristics as well as the transformation sequences used. Datasets are: -MNIST and MNIST-test [31] which respectively correspond to full and test subset of MNIST dataset. They depict binary white handwritten digits centered over a black background. -USPS [17] is a handwritten digit dataset from USPS composed of greyscale images. -Fashion-MNIST [47] is a 10-class clothing dataset composed of greyscale images of cloth over black background. Classes are: T-shirt, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, ankle boot. -FRGC [43] is a colored face dataset. We use a subset of this dataset introduced in [49], where 20 subjects are selected and each image is cropped and resized to a constant size of 32?32. -SVHN [42] is composed of digits extracted from house numbers cropped from Google Street View images. Following standard practice for clustering, we use both labeled samples (99,289) and unlabeled extra samples (~530k) for training and evaluate on the labeled subset only. -affNIST-test is the test split of affNIST (https://www.cs.toronto.edu/ tijmen/affNIST/) an augmented dataset of MNIST where random affine transformations are applied. -MNIST-1k: we randomly sampled 1,000 images from the test split of MNIST. -MNIST-color: we augmented MNIST with random colors for background and foreground.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Detailed results. We report statistics of our results on standard clustering benchmarks. For SVHN, we also report results with our Gaussian weighted loss ( ). 93.8 84.9 90.4 83.2 87.1 57.4 63.2 35.9 43.9 34.5 / 37.0 10 median 97.3 94.0 97.9 95.1 85.0 87.4 61.9 63.3 40.2 49.3 35.8 / 39.6 10 max 97.5 94.2 98.0 95.3 96.4 92.0 63.3 64.2 41.1 51.4 39.6 / 62.6 10 minLoss 97.2 93.8 98.0 95.3 89.8 89.5 57.4 64.1 41.1 49.7 39.6 / 62.6 89.1 97.7 94.4 82.0 86.3 56.1 59.7 38.4 46.8 34.0 / 49.9 10 median 97.1 93.7 97.8 94.7 84.3 87.1 57.2 60.9 39.6 49.1 36.4 / 57.4 10 max 97.3 93.9 98.0 95.1 87.3 89.0 68.2 66.3 41.9 51.1 39.5 / 64.6 10 minLoss 97.1 93.7 98.0 95.1 87.3 89.0 68.2 66.3 41.6 51.1 39.5 / 63.3C.2 Accuracy and loss correlation</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>MNIST MNIST-test</cell><cell>USPS</cell><cell>F-MNIST</cell><cell>FRGC</cell><cell></cell><cell cols="2">SVHN</cell></row><row><cell>Method</cell><cell>Runs</cell><cell>Stat</cell><cell cols="5">ACC NMI ACC NMI ACC NMI ACC NMI ACC NMI</cell><cell cols="2">ACC</cell></row><row><cell cols="2">DTI K-means 10</cell><cell>avg</cell><cell cols="7">97.3 94.0 96.6 94.6 86.4 88.2 61.2 63.7 39.6 48.7 36.4 / 44.5</cell></row><row><cell></cell><cell>10</cell><cell>std</cell><cell cols="7">0.1 0.1 4.1 1.5 4.1 1.6 2.0 0.3 1.7 2.2 1.9 / 9.6</cell></row><row><cell cols="10">10 97.1 DTI GMM min 10 avg 95.9 93.2 97.8 94.7 84.5 87.2 59.6 62.2 40.1 48.9 36.7 / 57.4</cell></row><row><cell></cell><cell>10</cell><cell>std</cell><cell cols="7">3.9 1.5 0.1 0.2 2.0 0.8 4.7 2.4 1.4 1.5 2.3 / 5.1</cell></row><row><cell></cell><cell>10</cell><cell>min</cell><cell cols="3">6.4 6.5 6.9 7.0 84.7 0.75 6.6 6.7 6.8 Loss (x 1e3)</cell><cell>0.80</cell><cell>0.85</cell><cell>ACC</cell><cell>0.90</cell><cell>0.95</cell><cell>1.00</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://imagine.enpc.fr/~monniert/DTIClustering/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.cs.toronto.edu/~tijmen/affNIST/ 3 https://github.com/GuHongyang/VaDE-pytorch 4 https://github.com/arc298/instagram-scraper was used to scrape photographs</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by ANR project EnHerit ANR-17-CE23-0008, project Rapid Tabasco, gifts from Adobe and HPC resources from GENCI-IDRIS (Grant 2020-AD011011697). We thank Bryan Russell, Vladimir Kim, Matthew Fisher, Fran?ois Darmon, Simon Roburin, David Picard, Micha?l Ramamonjisoa, Vincent Lepetit, Elliot Vincent, Jean Ponce, William Peebles and Alexei Efros for inspiring discussions and valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jointly Aligning Millions of Images with Deep Penalised Reconstruction Congealing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Annunziata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Principal Warps: Thin-Plate Splines and the Decomposition of Deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bookstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convergence Properties of the K-Means Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Clustering for Unsupervised Learning of Visual Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep Adaptive Image Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Least Squares Congealing for Unsupervised Alignment of Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Least-Squares Congealing for Large Numbers of Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum Likelihood from Incomplete Data via the EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating Mixture Models of Images and Inferring Spatial Transformations Using the EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast, large-scale transformation-invariant clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Transformation-Invariant Clustering Using the EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gepperth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pf?lb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09379</idno>
		<title level="m">Gradient-based training of Gaussian Mixture Models in High-Dimensional Spaces</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural Expectation Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Sternberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Analysis Using Mathematical Morphology. TPAMI</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Associative Deep Clustering: Training a Classification Network with no Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transforming Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matrix Manifold Optimization for Gaussian Mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning Discrete Representations via Information Maximizing Self-Augmented Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Invariant Information Clustering for Unsupervised Image Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Winnemoeller</surname></persName>
		</author>
		<title level="m">Recognizing Image Style. In BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Stacked Capsule Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistic Quarterly</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<title level="m">Data Driven Image Models through Continuous Joint Alignment. TPAMI</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradient-Based Learning Applied to Document Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ShadowDraw: Real-Time User Guidance for Freehand Drawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transformation invariant subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MegaDepth: Learning Single-View Depth Prediction from Internet Photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simultaneous Alignment and Clustering for an Image Ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Wheeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An Iterative Image Registration Technique with an Application to Stereo Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Berkeley Symposium on Mathematical Statistics and Probability</title>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learned-Miller. Unsupervised Joint Alignment and Clustering using Bayesian Nonparametrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning from One Example Through Shared Densities on Transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ClusterGAN : Latent Space Clustering in Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reading Digits in Natural Images with Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Overview of the Face Recognition Grand Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SpectralNet: Spectral Clustering Using Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Equivariant Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Diffeomorphic Temporal Alignment Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A S</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Skafte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shriki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<title level="m">Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised Deep Embedding for Clustering Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Joint Unsupervised Learning of Deep Representations and Image Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep Spectral Clustering Using Dual Autoencoder Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep Adversarial Subspace Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">AverageExplorer: Interactive Exploration and Alignment of Visual Data Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
