<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Link Prediction Based on Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Washington University in St. Louis</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
							<email>chen@cse.wustl.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Washington University</orgName>
								<address>
									<settlement>St. Louis</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Link Prediction Based on Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a "heuristic" that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel ?-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the ?-decaying theory, we propose a new method to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems. USAir [44] is a network of US Air lines with 332 nodes and 2,126 edges. The average node degree is 12.81. NS [45] is a collaboration network of researchers in network science with 1,589 nodes and 2,742 edges. The average node degree is 3.45. PB [46] is a network of US political blogs with 1,222 nodes and 16,714 edges. The average node degree is 27.36. Yeast [47] is a protein-protein interaction network in yeast with 2,375 nodes and 11,693 edges. The average node degree is 9.85. C.ele [48] is a neural network of C. elegans with 297 nodes and 2,148 edges. The average node degree is 14.46. Power [48] is an electrical grid of western US with 4,941 nodes and 6,594 edges. The average node degree is 2.67. Router [49] is a router-level Internet with 5,022 nodes and 6,258 edges. The average node degree is 2.49. E.coli [50] is a pairwise reaction network of metabolites in E. coli with 1,805 nodes and 14,660 edges. The average node degree is 12.55.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Link prediction is to predict whether two nodes in a network are likely to have a link <ref type="bibr" target="#b0">[1]</ref>. Given the ubiquitous existence of networks, it has many applications such as friend recommendation <ref type="bibr" target="#b1">[2]</ref>, movie recommendation <ref type="bibr" target="#b2">[3]</ref>, knowledge graph completion <ref type="bibr" target="#b3">[4]</ref>, and metabolic network reconstruction <ref type="bibr" target="#b4">[5]</ref>.</p><p>One class of simple yet effective approaches for link prediction is called heuristic methods. Heuristic methods compute some heuristic node similarity scores as the likelihood of links <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. Existing heuristics can be categorized based on the maximum hop of neighbors needed to calculate the score. For example, common neighbors (CN) and preferential attachment (PA) <ref type="bibr" target="#b6">[7]</ref> are first-order heuristics, since they only involve the one-hop neighbors of two target nodes. Adamic-Adar (AA) and resource allocation (RA) <ref type="bibr" target="#b7">[8]</ref> are second-order heuristics, as they are calculated from up to two-hop neighborhood of the target nodes. We define h-order heuristics to be those heuristics which require knowing up to h-hop neighborhood of the target nodes. There are also some high-order heuristics which require knowing the entire network. Examples include Katz, rooted PageRank (PR) <ref type="bibr" target="#b8">[9]</ref>, and SimRank (SR) <ref type="bibr" target="#b9">[10]</ref>. <ref type="table" target="#tab_2">Table 3</ref> in Appendix A summarizes eight popular heuristics.</p><p>Although working well in practice, heuristic methods have strong assumptions on when links may exist. For example, the common neighbor heuristic assumes that two nodes are more likely to connect if they have many common neighbors. This assumption may be correct in social networks, but is shown to fail in protein-protein interaction (PPI) networks -two proteins sharing many common neighbors are actually less likely to interact <ref type="bibr" target="#b10">[11]</ref>.  <ref type="figure">Figure 1</ref>: The SEAL framework. For each target link, SEAL extracts a local enclosing subgraph around it, and uses a GNN to learn general graph structure features for link prediction. Note that the heuristics listed inside the box are just for illustration -the learned features may be completely different from existing heuristics.</p><p>In fact, the heuristics belong to a more generic class, namely graph structure features. Graph structure features are those features located inside the observed node and edge structures of the network, which can be calculated directly from the graph. Since heuristics can be viewed as predefined graph structure features, a natural idea is to automatically learn such features from the network. Zhang and Chen <ref type="bibr" target="#b11">[12]</ref> first studied this problem. They extract local enclosing subgraphs around links as the training data, and use a fully-connected neural network to learn which enclosing subgraphs correspond to link existence. Their method called Weisfeiler-Lehman Neural Machine (WLNM) has achieved state-of-the-art link prediction performance. The enclosing subgraph for a node pair (x, y) is the subgraph induced from the network by the union of x and y's neighbors up to h hops. <ref type="figure">Figure 1</ref> illustrates the 1-hop enclosing subgraphs for (A, B) and (C, D). These enclosing subgraphs are very informative for link prediction -all first-order heuristics such as common neighbors can be directly calculated from the 1-hop enclosing subgraphs.</p><p>However, it is shown that high-order heuristics such as rooted PageRank and Katz often have much better performance than first and second-order ones <ref type="bibr" target="#b5">[6]</ref>. To effectively learn good high-order features, it seems that we need a very large hop number h so that the enclosing subgraph becomes the entire network. This results in unaffordable time and memory consumption for most practical networks. But do we really need such a large h to learn high-order heuristics?</p><p>Fortunately, as our first contribution, we show that we do not necessarily need a very large h to learn high-order graph structure features. We dive into the inherent mechanisms of link prediction heuristics, and find that most high-order heuristics can be unified by a ?-decaying theory. We prove that, under mild conditions, any ?-decaying heuristic can be effectively approximated from an h-hop enclosing subgraph, where the approximation error decreases at least exponentially with h. This means that we can safely use even a small h to learn good high-order features. It also implies that the "effective order" of these high-order heuristics is not that high.</p><p>Based on our theoretical results, we propose a novel link prediction framework, SEAL, to learn general graph structure features from local enclosing subgraphs. SEAL fixes multiple drawbacks of WLNM. First, a graph neural network (GNN) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> is used to replace the fully-connected neural network in WLNM, which enables better graph feature learning ability. Second, SEAL permits learning from not only subgraph structures, but also latent and explicit node features, thus absorbing multiple types of information. We empirically verified its much improved performance.</p><p>Our contributions are summarized as follows. 1) We present a new theory for learning link prediction heuristics, justifying learning from local subgraphs instead of entire networks. 2) We propose SEAL, a novel link prediction framework based on GNN (illustrated in <ref type="figure">Figure 1</ref>). SEAL outperforms all heuristic methods, latent feature methods, and recent network embedding methods by large margins. SEAL also outperforms the previous state-of-the-art method, WLNM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Notations Let G = (V, E) be an undirected graph, where V is the set of vertices and E ? V ? V is the set of observed links. Its adjacency matrix is A, where A i,j = 1 if (i, j) ? E and A i,j = 0 otherwise. For any nodes x, y ? V , let ?(x) be the 1-hop neighbors of x, and d(x, y) be the shortest path distance between x and y. A walk w = v 0 , ? ? ? , v k is a sequence of nodes with</p><formula xml:id="formula_0">(v i , v i+1 ) ? E.</formula><p>We use | v 0 , ? ? ? , v k | to denote the length of the walk w, which is k here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent features and explicit features</head><p>Besides graph structure features, latent features and explicit features are also studied for link prediction. Latent feature methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> factorize some matrix representations of the network to learn a low-dimensional latent representation/embedding for each node. Examples include matrix factorization <ref type="bibr" target="#b2">[3]</ref> and stochastic block model <ref type="bibr" target="#b17">[18]</ref> etc. Recently, a number of network embedding techniques have been proposed, such as DeepWalk <ref type="bibr" target="#b18">[19]</ref>, LINE <ref type="bibr" target="#b20">[21]</ref> and node2vec <ref type="bibr" target="#b19">[20]</ref>, which are also latent feature methods since they implicitly factorize some matrices too <ref type="bibr" target="#b21">[22]</ref>. Explicit features are often available in the form of node attributes, describing all kinds of side information about individual nodes. It is shown that combining graph structure features with latent features and explicit features can improve the performance <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Graph neural networks Graph neural network (GNN) is a new type of neural network for learning over graphs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>). Here, we only briefly introduce the components of a GNN since this paper is not about GNN innovations but is a novel application of GNN. A GNN usually consists of 1) graph convolution layers which extract local substructure features for individual nodes, and 2) a graph aggregation layer which aggregates node-level features into a graph-level feature vector. Many graph convolution layers can be unified into a message passing framework <ref type="bibr" target="#b26">[27]</ref>.</p><p>Supervised heuristic learning There are some previous attempts to learn supervised heuristics for link prediction. The closest work to ours is the Weisfeiler-Lehman Neural Machine (WLNM) <ref type="bibr" target="#b11">[12]</ref>, which also learns from local subgraphs. However, WLNM has several drawbacks. Firstly, WLNM trains a fully-connected neural network on the subgraphs' adjacency matrices. Since fullyconnected neural networks only accept fixed-size tensors as input, WLNM requires truncating different subgraphs to the same size, which may lose much structural information. Secondly, due to the limitation of adjacency matrix representations, WLNM cannot learn from latent or explicit features. Thirdly, theoretical justifications are also missing. We include more discussion on WLNM in Appendix D. Another related line of research is to train a supervised learning model on different heuristics' combination. For example, the path ranking algorithm <ref type="bibr" target="#b27">[28]</ref> trains logistic regression on different path types' probabilities to predict relations in knowledge graphs. Nickel et al. <ref type="bibr" target="#b22">[23]</ref> propose to incorporate heuristic features into tensor factorization models. However, these models still rely on predefined heuristics -they cannot learn general graph structure features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A theory for unifying link prediction heuristics</head><p>In this section, we aim to understand deeper the mechanisms behind various link prediction heuristics, and thus motivating the idea of learning heuristics from local subgraphs. Due to the large number of graph learning techniques, note that we are not concerned with the generalization error of a particular method, but focus on the information reserved in the subgraphs for calculating existing heuristics. Definition 1. (Enclosing subgraph) For a graph G = (V, E), given two nodes x, y ? V , the h-hop enclosing subgraph for (x, y) is the subgraph G h x,y induced from G by the set of nodes</p><formula xml:id="formula_1">{ i | d(i, x) ? h or d(i, y) ? h }.</formula><p>The enclosing subgraph describes the "h-hop surrounding environment" of (x, y). Since G h</p><p>x,y contains all h-hop neighbors of x and y, we naturally have the following theorem. Theorem 1. Any h-order heuristic for (x, y) can be accurately calculated from G h x,y . For example, a 2-hop enclosing subgraph will contain all the information needed to calculate any first and second-order heuristics. However, although first and second-order heuristics are well covered by local enclosing subgraphs, an extremely large h seems to be still needed for learning high-order heuristics. Surprisingly, our following analysis shows that learning high-order heuristics is also feasible with a small h. We support this first by defining the ?-decaying heuristic. We will show that under certain conditions, a ?-decaying heuristic can be very well approximated from the h-hop enclosing subgraph. Moreover, we will show that almost all well-known high-order heuristics can be unified into this ?-decaying heuristic framework. Definition 2. (?-decaying heuristic) A ?-decaying heuristic for (x, y) has the following form:</p><formula xml:id="formula_2">H(x, y) = ? ? l=1 ? l f (x, y, l),<label>(1)</label></formula><p>where ? is a decaying factor between 0 and 1, ? is a positive constant or a positive function of ? that is upper bounded by a constant, f is a nonnegative function of x, y, l under the the given network.</p><p>Next, we will show that under certain conditions, a ?-decaying heuristic can be approximated from an h-hop enclosing subgraph, and the approximation error decreases at least exponentially with h.</p><formula xml:id="formula_3">Theorem 2. Given a ?-decaying heuristic H(x, y) = ? ? l=1 ? l f (x, y, l), if f (x, y, l) satisfies: ? (property 1) f (x, y, l) ? ? l where ? &lt; 1 ? ; and ? (property 2) f (x, y, l) is calculable from G h</formula><p>x,y for l = 1, 2, ? ? ? , g(h), where g(h) = ah+b with a, b ? N and a &gt; 0, then H(x, y) can be approximated from G h x,y and the approximation error decreases at least exponentially with h.</p><p>Proof. We can approximate such a ?-decaying heuristic by summing over its first g(h) terms.</p><formula xml:id="formula_4">H(x, y) := ? g(h) l=1 ? l f (x, y, l).<label>(2)</label></formula><p>The approximation error can be bounded as follows.</p><formula xml:id="formula_5">|H(x, y) ? H(x, y)| = ? ? l=g(h)+1 ? l f (x, y, l) ? ? ? l=ah+b+1 ? l ? l = ?(??) ah+b+1 (1 ? ??) ?1 .</formula><p>In practice, a small ?? and a large a lead to a faster decreasing speed. Next we will prove that three popular high-order heuristics: Katz, rooted PageRank and SimRank, are all ?-decaying heuristics which satisfy the properties in Theorem 2. First, we need the following lemma. Lemma 1. Any walk between x and y with length l ? 2h + 1 is included in G h x,y . Proof. Given any walk w = x, v 1 , ? ? ? , v l?1 , y with length l, we will show that every node</p><formula xml:id="formula_6">v i is included in G h x,y . Consider any v i . Assume d(v i , x) ? h + 1 and d(v i , y) ? h + 1. Then, 2h + 1 ? l = | x, v 1 , ? ? ? , v i | + | v i , ? ? ? , v l?1 , y | ? d(v i , x) + d(v i , y) ? 2h + 2, a contradiction. Thus, d(v i , x) ? h or d(v i , y) ? h. By the definition of G h</formula><p>x,y , v i must be included in G h x,y . Next we will analyze Katz, rooted PageRank and SimRank one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Katz index</head><p>The Katz index <ref type="bibr" target="#b28">[29]</ref> for (x, y) is defined as</p><formula xml:id="formula_7">Katz x,y = ? l=1 ? l |walks l (x, y)| = ? l=1 ? l [A l ] x,y ,<label>(3)</label></formula><p>where walks l (x, y) is the set of length-l walks between x and y, and A l is the l th power of the adjacency matrix of the network. Katz index sums over the collection of all walks between x and y where a walk of length l is damped by ? l (0 &lt; ? &lt; 1), giving more weight to shorter walks.</p><p>Katz index is directly defined in the form of a ?-decaying heuristic with ? = 1, ? = ?, and f (x, y, l) = |walks l (x, y)|. According to Lemma 1, |walks l (x, y)| is calculable from G h x,y for l ? 2h + 1, thus property 2 in Theorem 2 is satisfied. Now we show when property 1 is satisfied. Proposition 1. For any nodes i, j, [A l ] i,j is bounded by d l , where d is the maximum node degree of the network.</p><p>Proof. We prove it by induction. When l = 1, A i,j ? d for any (i, j). Thus the base case is correct. Now, assume by induction that [A l ] i,j ? d l for any (i, j), we have</p><formula xml:id="formula_8">[A l+1 ] i,j = |V | k=1 [A l ] i,k A k,j ? d l |V | k=1 A k,j ? d l d = d l+1 .</formula><p>Taking ? = d, we can see that whenever d &lt; 1/?, the Katz index will satisfy property 1 in Theorem 2. In practice, the damping factor ? is often set to very small values like 5E-4 <ref type="bibr" target="#b0">[1]</ref>, which implies that Katz can be very well approximated from the h-hop enclosing subgraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PageRank</head><p>The rooted PageRank for node x calculates the stationary distribution of a random walker starting at x, who iteratively moves to a random neighbor of its current position with probability ? or returns to x with probability 1 ? ?. Let ? x denote the stationary distribution vector. Let [? x ] i denote the probability that the random walker is at node i under the stationary distribution.</p><p>Let P be the transition matrix with P i,j = 1 |?(vj )| if (i, j) ? E and P i,j = 0 otherwise. Let e x be a vector with the x th element being 1 and others being 0. The stationary distribution satisfies</p><formula xml:id="formula_9">? x = ?P ? x + (1 ? ?)e x .<label>(4)</label></formula><p>When used for link prediction, the score for (x, y) is given by [? x ] y (or [? x ] y + [? y ] x for symmetry).</p><p>To show that rooted PageRank is a ?-decaying heuristic, we introduce the inverse P-distance theory <ref type="bibr" target="#b29">[30]</ref>, which states that [? x ] y can be equivalently written as follows:</p><formula xml:id="formula_10">[? x ] y = (1 ? ?) w:x y P [w]? len(w) ,<label>(5)</label></formula><p>where the summation is taken over all walks w starting at x and ending at y (possibly touching x and y multiple times).</p><formula xml:id="formula_11">For a walk w = v 0 , v 1 , ? ? ? , v k , len(w) := | v 0 , v 1 , ? ? ? , v k | is the length of the walk. The term P [w] is defined as k?1 i=0</formula><p>1 |?(vi)| , which can be interpreted as the probability of traveling w. Now we have the following theorem. Theorem 3. The rooted PageRank heuristic is a ?-decaying heuristic which satisfies the properties in Theorem 2.</p><p>Proof. We first write [? x ] y in the following form.</p><formula xml:id="formula_12">[? x ] y = (1 ? ?) ? l=1 w:x y len(w)=l P [w]? l .<label>(6)</label></formula><p>Defining</p><formula xml:id="formula_13">f (x, y, l) := w:x y len(w)=l P [w] leads to the form of a ?-decaying heuristic. Note that f (x, y, l)</formula><p>is the probability that a random walker starting at x stops at y with exactly l steps, which satisfies z?V f (x, z, l) = 1. Thus, f (x, y, l) ? 1 &lt; 1 ? (property 1). According to Lemma 1, f (x, y, l) is also calculable from G h x,y for l ? 2h + 1 (property 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SimRank</head><p>The SimRank score <ref type="bibr" target="#b9">[10]</ref> is motivated by the intuition that two nodes are similar if their neighbors are also similar. It is defined in the following recursive way: if x = y, then s(x, y) := 1; otherwise,</p><formula xml:id="formula_14">s(x, y) := ? a??(x) b??(y) s(a, b) |?(x)| ? |?(y)|<label>(7)</label></formula><p>where ? is a constant between 0 and 1. According to <ref type="bibr" target="#b9">[10]</ref>, SimRank has an equivalent definition:</p><formula xml:id="formula_15">s(x, y) = w:(x,y) (z,z) P [w]? len(w) ,<label>(8)</label></formula><p>where w : (x, y) (z, z) denotes all simultaneous walks such that one walk starts at x, the other walk starts at y, and they first meet at any vertex z. For a simultaneous walk w = (v 0 , u 0 ), ? ? ? , (v k , u k ) , len(w) = k is the length of the walk. The term P [w] is similarly defined as</p><formula xml:id="formula_16">k?1 i=0</formula><p>1 |?(vi)||?(ui)| , describing the probability of this walk. Now we have the following theorem. Proof. We write s(x, y) as follows.</p><formula xml:id="formula_17">s(x, y) = ? l=1 w:(x,y) (z,z) len(w)=l P [w]? l ,<label>(9)</label></formula><p>Defining f (x, y, l) := w:(x,y) (z,z)</p><formula xml:id="formula_18">len(w)=l P [w] reveals that SimRank is a ?-decaying heuristic. Note that f (x, y, l) ? 1 &lt; 1 ? . It is easy to see that f (x, y, l) is also calculable from G h x,y for l ? h.</formula><p>Discussion There exist several other high-order heuristics based on path counting or random walk <ref type="bibr" target="#b5">[6]</ref> which can be as well incorporated into the ?-decaying heuristic framework. We omit the analysis here. Our results reveal that most high-order heuristics inherently share the same ?-decaying heuristic form, and thus can be effectively approximated from an h-hop enclosing subgraph with exponentially smaller approximation error. We believe the ubiquity of ?-decaying heuristics is not by accidentit implies that a successful link prediction heuristic is better to put exponentially smaller weight on structures far away from the target, as remote parts of the network intuitively make little contribution to link existence. Our results build the foundation for learning heuristics from local subgraphs, as they imply that local enclosing subgraphs already contain enough information to learn good graph structure features for link prediction which is much desired considering learning from the entire network is often infeasible. To summarize, from the small enclosing subgraphs extracted around links, we are able to accurately calculate first and second-order heuristics, and approximate a wide range of high-order heuristics with small errors. Therefore, given adequate feature learning ability of the model used, learning from such enclosing subgraphs is expected to achieve performance at least as good as a wide range of heuristics. There is some related work which empirically verifies that local methods can often estimate PageRank and SimRank well <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Another related theoretical work <ref type="bibr" target="#b32">[33]</ref> establishes a condition of h to achieve some fixed approximation error for ordinary PageRank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SEAL: An implemetation of the theory using GNN</head><p>In this section, we describe our SEAL framework for link prediction. SEAL does not restrict the learned features to be in some particular forms such as ?-decaying heuristics, but instead learns general graph structure features for link prediction. It contains three steps: 1) enclosing subgraph extraction, 2) node information matrix construction, and 3) GNN learning. Given a network, we aim to learn automatically a "heuristic" that best explains the link formations. Motivated by the theoretical results, this function takes local enclosing subgraphs around links as input, and output how likely the links exist. To learn such a function, we train a graph neural network (GNN) over the enclosing subgraphs. Thus, the first step in SEAL is to extract enclosing subgraphs for a set of sampled positive links (observed) and a set of sampled negative links (unobserved) to construct the training data.</p><p>A GNN typically takes (A, X) as input, where A (with slight abuse of notation) is the adjacency matrix of the input enclosing subgraph, X is the node information matrix each row of which corresponds to a node's feature vector. The second step in SEAL is to construct the node information matrix X for each enclosing subgraph. This step is crucial for training a successful GNN link prediction model. In the following, we discuss this key step. The node information matrix X in SEAL has three components: structural node labels, node embeddings and node attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Node labeling</head><p>The first component in X is each node's structural label. A node labeling is function f l : V ? N which assigns an integer label f l (i) to every node i in the enclosing subgraph. The purpose is to use different labels to mark nodes' different roles in an enclosing subgraph: 1) The center nodes x and y are the target nodes between which the link is located. 2) Nodes with different relative positions to the center have different structural importance to the link. A proper node labeling should mark such differences. If we do not mark such differences, GNNs will not be able to tell where are the target nodes between which a link existence should be predicted, and lose structural information.</p><p>Our node labeling method is derived from the following criteria: 1) The two target nodes x and y always have the distinctive label "1". 2) Nodes i and j have the same label if d(i, x) = d(j, x) and d(i, y) = d(j, y). The second criterion is because, intuitively, a node i's topological position within an enclosing subgraph can be described by its radius with respect to the two center nodes, namely (d(i, x), d(i, y)). Thus, we let nodes on the same orbit have the same label, so that the node labels can reflect nodes' relative positions and structural importance within subgraphs.</p><p>Based on the above criteria, we propose a Double-Radius Node Labeling (DRNL) as follows. First, assign label 1 to x and y. Then, for any node i with (d(i, x), d(i, y)) = (1, 1), assign label f l (i) = 2.</p><p>Nodes with radius (1, 2) or (2, 1) get label 3. Nodes with radius (1, 3) or (3, 1) get 4. Nodes with (2, 2) get 5. Nodes with <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4)</ref> or (4, 1) get 6. Nodes with (2, 3) or (3, 2) get 7. So on and so forth. In other words, we iteratively assign larger labels to nodes with a larger radius w.r.t. both center nodes, where the label f l (i) and the double-radius (d(i, x), d(i, y)) satisfy</p><formula xml:id="formula_19">1) if d(i, x) + d(i, y) = d(j, x) + d(j, y), then d(i, x) + d(i, y) &lt; d(j, x) + d(j, y) ? f l (i) &lt; f l (j); 2) if d(i, x) + d(i, y) = d(j, x) + d(j, y), then d(i, x)d(i, y) &lt; d(j, x)d(j, y) ? f l (i) &lt; f l (j).</formula><p>One advantage of DRNL is that it has a perfect hashing function</p><formula xml:id="formula_20">f l (i) = 1 + min(d x , d y ) + (d/2)[(d/2) + (d%2) ? 1], (10) where d x := d(i, x), d y := d(i, y), d := d x + d y , (d/2)</formula><p>and (d%2) are the integer quotient and remainder of d divided by 2, respectively. This perfect hashing allows fast closed-form computations.</p><p>For nodes with d(i, x) = ? or d(i, y) = ?, we give them a null label 0. Note that DRNL is not the only possible way of node labeling, but we empirically verified its better performance than no labeling and other naive labelings. We discuss more about node labeling in Appendix B. After getting the labels, we use their one-hot encoding vectors to construct X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Incorporating latent and explicit features</head><p>Other than the structural node labels, the node information matrix X also provides an opportunity to include latent and explicit features. By concatenating each node's embedding/attribute vector to its corresponding row in X, we can make SEAL simultaneously learn from all three types of features.</p><p>Generating the node embeddings for SEAL is nontrivial. Suppose we are given the observed network G = (V, E), a set of sampled positive training links E p ? E, and a set of sampled negative training links E n with E n ? E = ?. If we directly generate node embeddings on G, the node embeddings will record the link existence information of the training links (since E p ? E). We observed that GNNs can quickly find out such link existence information and optimize by only fitting this part of information. This results in bad generalization performance in our experiments. Our trick is to temporally add E n into E, and generate the embeddings on G = (V, E ? E n ). This way, the positive and negative training links will have the same link existence information recorded in the embeddings, so that GNN cannot classify links by only fitting this part of information. We empirically verified the much improved performance of this trick to SEAL. We name this trick negative injection.</p><p>We name our proposed framework SEAL (learning from Subgraphs, Embeddings and Attributes for Link prediction), emphasizing its ability to jointly learn from three types of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head><p>We conduct extensive experiments to evaluate SEAL. Our results show that SEAL is a superb and robust framework for link prediction, achieving unprecedentedly strong performance on various networks. We use AUC and average precision (AP) as evaluation metrics. We run all experiments for 10 times and report the average AUC results and standard deviations. We leave the the AP and time results in Appendix F. SEAL is flexible with what GNN or node embeddings to use. Thus, we choose a recent architecture DGCNN <ref type="bibr" target="#b16">[17]</ref> as the default GNN, and node2vec <ref type="bibr" target="#b19">[20]</ref> as the default embeddings. The code and data are available at https://github.com/muhanzhang/SEAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>The eight datasets used are: USAir, NS, PB, Yeast, C.ele, Power, Router, and E.coli (please see Appendix C for details). We randomly remove 10% existing links from each dataset as positive testing data. Following a standard manner of learning-based link prediction, we randomly sample the same number of nonexistent links (unconnected node pairs) as negative testing data. We use the remaining 90% existing links as well as the same number of additionally sampled nonexistent links to construct the training data.</p><p>Comparison to heuristic methods We first compare SEAL with methods that only use graph structure features. We include eight popular heuristics (shown in Appendix A, <ref type="table" target="#tab_2">Table 3</ref>): common neighbors (CN), Jaccard, preferential attachment (PA), Adamic-Adar (AA), resource allocation (RA), Katz, PageRank (PR), and SimRank (SR). We additionally include Ensemble (ENS) which trains a logistic regression classifier on the eight heuristic scores. We also include two heuristic learning methods: Weisfeiler-Lehman graph kernel (WLK) <ref type="bibr" target="#b33">[34]</ref> and WLNM <ref type="bibr" target="#b11">[12]</ref>, which also learn from (truncated) enclosing subgraphs. We omit path ranking methods <ref type="bibr" target="#b27">[28]</ref> as well as other recent methods which are specifically designed for knowledge graphs or recommender systems <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35]</ref>. As all the baselines only use graph structure features, we restrict SEAL to not include any latent or explicit features. In SEAL, the hop number h is an important hyperparameter. Here, we select h only from {1, 2}, since on one hand we empirically verified that the performance typically does not increase after h ? 3, which validates our theoretical results that the most useful information is within local structures. On the other hand, even h = 3 sometimes results in very large subgraphs if a hub node is included. This raises the idea of sampling nodes in subgraphs, which we leave to future work. The selection principle is very simple: If the second-order heuristic AA outperforms the first-order heuristic CN on 10% validation data, then we choose h = 2; otherwise we choose h = 1. For datasets PB and E.coli, we consistently use h = 1 to fit into the memory. We include more details about the baselines and hyperparameters in Appendix D.  <ref type="table" target="#tab_0">Table 1</ref> shows the results. Firstly, we observe that methods which learn from enclosing subgraphs (WLK, WLNM and SEAL) generally perform much better than predefined heuristics. This indicates that the learned "heuristics" are better at capturing the network properties than manually designed ones. Among learning-based methods, SEAL has the best performance, demonstrating GNN's superior graph feature learning ability over graph kernels and fully-connected neural networks. From the results on Power and Router, we can see that although existing heuristics perform similarly to random guess, learning-based methods still maintain high performance. This suggests that we can even discover new "heuristics" for networks where no existing heuristics work. Comparison to latent feature methods Next we compare SEAL with six state-of-the-art latent feature methods: matrix factorization (MF), stochastic block model (SBM) <ref type="bibr" target="#b17">[18]</ref>, node2vec (N2V) <ref type="bibr" target="#b19">[20]</ref>, LINE <ref type="bibr" target="#b20">[21]</ref>, spectral clustering (SPC), and variational graph auto-encoder (VGAE) <ref type="bibr" target="#b35">[36]</ref>. Among them, VGAE uses a GNN too. Please note the difference between VGAE and SEAL: VGAE uses a node-level GNN to learn node embeddings that best reconstruct the network, while SEAL uses a graph-level GNN to classify enclosing subgraphs. Therefore, VGAE still belongs to latent feature methods. For SEAL, we additionally include the 128-dimensional node2vec embeddings in the node information matrix X. Since the datasets do not have node attributes, explicit features are not included. <ref type="table" target="#tab_1">Table 2</ref> shows the results. As we can see, SEAL shows significant improvement over latent feature methods. One reason is that SEAL learns from both graph structures and latent features simultaneously, thus augmenting those methods that only use latent features. We observe that SEAL with node2vec embeddings outperforms pure node2vec by large margins. This implies that network embeddings alone may not be able to capture the most useful link prediction information located in the local structures. It is also interesting that compared to SEAL without node2vec embeddings <ref type="table" target="#tab_0">(Table 1)</ref>, joint learning does not always improve the performance. More experiments and discussion are included in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Learning link prediction heuristics automatically is a new field. In this paper, we presented theoretical justifications for learning from local enclosing subgraphs. In particular, we proposed a ?-decaying theory to unify a wide range of high-order heuristics and prove their approximability from local subgraphs. Motivated by the theory, we proposed a novel link prediction framework, SEAL, to simultaneously learn from local enclosing subgraphs, embeddings and attributes based on graph neural networks. Experimentally we showed that SEAL achieved unprecedentedly strong performance by comparing to various heuristics, latent feature methods, and network embedding algorithms. We hope SEAL can not only inspire link prediction research, but also open up new directions for other relational machine learning problems such as knowledge graph completion and recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A More about the three types of features for link prediction</head><p>In this section, we discuss more about the difference among the three types commonly used features for link prediction: graph structure features, latent features, and explicit features.</p><p>Graph structure features locate inside the observed node and edge structures of the network, which can be directly observed and computed. Link prediction heuristics belong to graph structure features. We show eight popular heuristics in <ref type="table" target="#tab_2">Table 3</ref>. In addition to link prediction heuristics, node centrality scores (degree, closeness, betweenness, PageRank, eigenvector, hubs etc.), graphlets, network motifs etc. all belong to graph structure features. Although effective in many domains, these predefined graph structure features are handcrafted -they only capture a small set of structure patterns, lacking the ability to express general structure patterns underlying different networks. Considering deep neural networks' success in feature learning, a natural question to ask is whether we can automatically learn such features, no longer relying on predefined ones.</p><p>Graph structure features are inductive, meaning that these features are not associated with a particular node or network. For example, the common neighbor heuristic between any pair of nodes x and y is consistently calculated by counting the number of their common one-hop neighbors, invariant to where x and y are located. Thus, graph structure features are transferrable to new nodes and new networks. This is in contrast to latent features, which are often transductive -the changing of network structure will require a complete retraining to get the latent features again. high Notes: ?(x) denotes the neighbor set of vertex x. ? &lt; 1 is a damping factor. |walks l (x, y)| counts the number of length-l walks between x and y.</p><p>[?x]y is the stationary distribution probability of y under the random walk from x with restart, see <ref type="bibr" target="#b8">[9]</ref>. SimRank score is a recursive definition. We exclude those heuristics which are simple variants of the above or are proven to be meaningless for large graphs (e.g., commute time <ref type="bibr" target="#b36">[37]</ref>).</p><p>Latent features are latent properties or representations of nodes, often obtained by factorizing a specific matrix derived from a network, such as the adjacency matrix or the Laplacian matrix. Through factorization, a low-dimensional embedding is learned for each node. Latent features focus more on global properties and long range effects, because the network's matrix is treated as a whole during factorization. Latent features cannot capture structural similarities between nodes <ref type="bibr" target="#b37">[38]</ref>, and usually need an extremely large dimension to express some simple heuristics <ref type="bibr" target="#b22">[23]</ref>. Latent features are also transductive. They cannot be transferred to new nodes or new networks. They are also less interpretable than graph structure features.</p><p>Network embedding methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> have gained great popularity recently. They learn low-dimensional representations for nodes too. Recently, it is shown that network embedding methods (including DeepWalk <ref type="bibr" target="#b18">[19]</ref>, LINE <ref type="bibr" target="#b20">[21]</ref>, and node2vec <ref type="bibr" target="#b19">[20]</ref>) implicitly factorize some matrix representation of a network <ref type="bibr" target="#b21">[22]</ref>. For example, DeepWalk approximately factorizes</p><formula xml:id="formula_21">log(vol(G)( 1 T T r=1 (D ?1 A) r )D ?1 ) ? log(b),</formula><p>where A is the adjacency matrix of the network G, D is the diagonal degree matrix, T is skip-gram's window size, and b is the number of negative samples. For LINE and node2vec, there also exist such matrices. Since network embedding methods also factorize matrix representations of networks, we may regard them as learning more expressive latent features through factorizing some more informative matrices.</p><p>Explicit features are often given by continuous or discrete node attribute vectors. In principle, any side information about the network other than its structure can be seen as explicit features. For example, in citation networks, word distributions are explicit features of document nodes. In social networks, a user's profile information is also explicit feature (however, their friendship information belongs to graph structure features).</p><p>These three types of features are largely orthogonal to each other. Many papers have considered using them together for link prediction <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> to improve the performance of single-feature-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More discussion about node labeling</head><p>The necessity of structural node labels for enclosing subgraphs is because, unlike ordinary graphs, enclosing subgraphs intrinsically have a directionality. The center of an enclosing subgraph are two nodes x and y between which the target link is located. Outward from the center, other nodes have larger and larger distance to x and y. Node labeling is to mark such structural differences thus providing additional structural information to facilitate GNN training.</p><p>When designing a node labeling for enclosing subgraphs, we always want to ensure that the target nodes x and y have a distinct label so that GNN can distinguish the target link to predict from other edges. Secondly, we want the node labels to reflect nodes' relative positions in their enclosing subgraph. This relative position can be intuitively described by a node i's double-radius with respect to x and y, i.e., (d(i, x), d(i, y)).</p><p>We restate our Double-Radius Node Labeling (DRNL) algorithm here. First, assign label 1 to x and y. Then, for any node i with (d(i, x), d(i, y)) = (1, 1), assign label f l (i) = 2. Nodes with double-radius (1, 2) or (2, 1) get label 3. Nodes with double-radius (1, 3) or (3, 1) get 4. Nodes with (2, 2) get 5. Nodes with <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4)</ref> or (4, 1) get 6. Nodes with (2, 3) or (3, 2) get 7. So on and so forth. Our DRNL not only satisfies the above criteria, but also attains the additional benefits that for nodes i and j:</p><formula xml:id="formula_22">1) if d(i, x) + d(i, y) = d(j, x) + d(j, y), then d(i, x) + d(i, y) &lt; d(j, x) + d(j, y) ? f l (i) &lt; f l (j); 2) if d(i, x) + d(i, y) = d(j, x) + d(j, y), then d(i, x)d(i, y) &lt; d(j, x)d(j, y) ? f l (i) &lt; f l (j).</formula><p>That is, the magnitude of node labels also reflects their distance to the center. Nodes with smaller arithmetic mean distance to the target nodes get smaller labels. If two nodes have the same arithmetic mean distance, the node with a smaller geometric mean distance to the target nodes gets a smaller label. Note that these additional benefits will not be available under one-hot encoding of node labels, since the magnitude information will be lost after one-hot encoding. However, such a labeling is potentially useful when node labels are directly used for training, or used to rank the nodes. Furthermore, our node labeling has a perfect hashing (10) which allows closed-form computation.</p><p>We present a lookup table for DRNL and an example labeled subgraph in <ref type="figure" target="#fig_2">Figure 2</ref>. Note that when calculating d(i, x), we temporally remove y from the subgraph, and vice versa. This is because we aim to use the pure distance between i and x without the influence of y. If we do not remove y, d(i, x) will be upper bounded by d(i, y) + d(x, y), obscuring the "true distance" between i and x.</p><p>Our node labeling algorithm is different from the Weisfeiler-Lehman algorithm used in WLNM <ref type="bibr" target="#b11">[12]</ref>. In WLNM, node labeling is for defining a node order in adjacency matrices -the labels are not really input to machine learning models. To rank nodes with least ties, the node labels should be as fine as possible in WLNM. In comparison, the node labels in SEAL need not be very fine, as their purpose is for indicating nodes' different roles within the enclosing subgraph, not for ranking nodes. In addition, node labels in SEAL are encoded into node information matrices and input to machine learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional details about baselines</head><p>Hyperparameters of heuristic and latent feature methods Most hyperparameters are inherited from the original paper of each method. For Katz, we set the damping factor ? to 0.001. For PageRank, we set the damping factor ? to 0.85. For SimRank, we set ? to 0.8. For stochastic block model (SBM), we use the implementation of <ref type="bibr" target="#b50">[51]</ref> using a latent group number 12. For matrix factorization (MF), we use the libFM <ref type="bibr" target="#b51">[52]</ref> software with the default parameters. For node2vec, LINE, and spectral clustering, we first generate 128-dimensional embeddings from the observed networks with default parameters of each software. Then, we use the Hadamard product of two nodes' embeddings as a link's embedding as suggested in <ref type="bibr" target="#b19">[20]</ref>, and train a logistic regression model with Liblinear <ref type="bibr" target="#b52">[53]</ref> using automatic hyperparameter selection. For VGAE, we use its default setting.</p><p>WLNM Weisfeiler-Lehman Neural Machine (WLNM) <ref type="bibr" target="#b11">[12]</ref> is a recent link prediction method that learns general graph structure features. It achieves state-of-the-art performance on various networks, outperforming all handcrafted heuristics. WLNM has three steps: enclosing subgraph extraction, subgraph pattern encoding, and neural network training. In the enclosing subgraph extraction step: for each node pair (x, y), WLNM iteratively extracts x and y's one-hop neighbors, two-hop neighbors, and so on, until the enclosing subgraph has more than K vertices, where K is a user-defined integer. In the subgraph pattern encoding step, WLNM uses the Weisfeiler-Lehman algorithm to define an order for nodes within each enclosing subgraph, so that the neural network can read different subgraphs' nodes in a consistent order and learn meaningful patterns. To unify the sizes of the enclosing subgraphs, after getting the vertex order, the last few vertices are deleted so that all the truncated enclosing subgraphs have the same size K. These truncated enclosing subgraphs are reordered and their fixed-size adjacency matrices are fed into the fully-connected neural network to train a link prediction model. Due to the truncation, WLNM cannot consistently learn from each link's full h-hop neighborhood. The loss of structural information limits WLNM's performance and restrict it from learning complete h-order graph structure features. Following <ref type="bibr" target="#b11">[12]</ref>, we use K = 10 (the best performing K) in our experiments. WLK Weisfeiler-Lehman graph kernel (WLK) <ref type="bibr" target="#b33">[34]</ref> is a state-of-the-art graph kernel. Graph kernels make kernel machines feasible for graph classification by defining some positive semidefinite graph similarity scores. Most graph kernels measure graph similarity by decomposing graphs into small substructures and adding up the pair-wise similarities between these components. Common types of substructures include walks <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>, subgraphs <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>, paths <ref type="bibr" target="#b57">[58]</ref>, and subtrees <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b58">59]</ref>. WLK is based on counting common rooted subtrees between two graphs. In our experiments, we train a SVM on the WL kernel matrix. We feed the same enclosing subgraphs as in SEAL to WLK. We search the subtree depth from {0, 1, 2, 3, 4, 5} on 10% validation links. WLK does not support continuous node information, but supports integer node labels. Thus, we feed the same structural node labels from (10) to WLK too.</p><p>We compare the characteristics of different link prediction methods in <ref type="table" target="#tab_3">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Configuration details of SEAL</head><p>In the experiments, we use Deep Graph Convolutional Neural Network (DGCNN) <ref type="bibr" target="#b16">[17]</ref> as the default GNN engine of SEAL. DGCNN is a recent GNN architecture for graph classification. It has consistently good performance on various benchmark datasets with a single network architecture (avoid hyperparameter tweaking). DGCNN is equipped with propagation-based graph convolution layers and a novel graph aggregation layer, called SortPooling. We illustrate the overall architecture of DGCNN in <ref type="figure" target="#fig_3">Figure 3</ref>. Given the adjacency matrix A ? {1, 0} n?n and the node information matrix X ? R n?c of an enclosing subgraph, DGCNN uses the following graph convolution layer:</p><formula xml:id="formula_23">Z = f (D ?1? XW ),<label>(11)</label></formula><p>where? = A + I,D is a diagonal degree matrix withD i,i = j? i,j , W ? R c?c is a matrix of trainable graph convolution parameters, f is an element-wise nonlinear activation function, and Z ? R n?c are the new node states. The mechanism behind <ref type="bibr" target="#b10">(11)</ref> is that the initial node states X are first applied a linear transformation by multiplying W , and then propagated to neighboring nodes through the propagation matrixD ?1? . After graph convolution, the i th row of Z becomes:</p><formula xml:id="formula_24">Z i = f 1 |?(i)| + 1 [X i W + j??(i) X j W ] ,<label>(12)</label></formula><p>which summarizes the node information as well as the first-order structure pattern from i's neighbors. DGCNN stacks multiple graph convolution layers <ref type="bibr" target="#b10">(11)</ref> and concatenates each layer's node states as the final node states, in order to extract multi-hop node features.</p><p>A graph aggregation layer constructs a graph-level feature vector from individual nodes' final states, which is used for graph classification. The most widely used aggregation operation is summing, i.e., nodes' final states after graph convolutions are summed up as the graph's representation. However, the averaging effect of summing might lose much individual nodes' information as well as the topological information of the graph. DGCNN uses a novel SortPooling layer, which sorts the final node states according to the last graph convolution layer's output to achieve an isomorphism invariant node ordering <ref type="bibr" target="#b16">[17]</ref>. A max-k pooling operation is then used to unify the sizes of the sorted representations of different graphs, which enables training a traditional 1-D CNN on the node sequence.</p><p>We use the default setting of DGCNN, i.e., four graph convolution layers as in (11) with 32,32,32,1 channels, a SortPooling layer (with k such that 60% graphs have nodes less than k), two 1-D convolution layers (16 and 32 output channels) and a dense layer (128 neurons), see <ref type="bibr" target="#b16">[17]</ref>. We train DGCNN on enclosing subgraphs for 50 epochs, and select the model with the smallest loss on the 10% validation data to predict the testing links.</p><p>Note that, in any positive training link's enclosing subgraph, we should always remove the edge between the two target nodes before feeding it into a graph classification model. This is because this edge will contain the link existence information, which is not available in any testing link's enclosing subgraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional results</head><p>In this section, we show the additional experimental results. We first use 90% observed links as training links and 10% as testing links following the main paper's experiments. The average precision (AP) comparison results with heuristic methods are shown in <ref type="table" target="#tab_4">Table 5</ref>. The AP comparison results with latent feature methods are shown in <ref type="table" target="#tab_5">Table 6</ref>. We can see that our proposed SEAL shows great performance improvement over all baselines in both AUC and AP.  To evaluate SEAL's scalability, we show its single-GPU inference time performance in <ref type="table" target="#tab_6">Table 7</ref>. As we can see, SEAL has good scalability. For networks with over 1E7 potential links, SEAL took less than an hour to make all the predictions. One possible way to further scale SEAL to social networks with millions of users is to first use some simple heuristics such as common neighbors to filter out most unlikely links and then use SEAL to make further recommendations. Another way is to restrict the candidate friend recommendations to be those who are at most 2 or 3 hops away from the target user, which will vastly reduce the number of candidate links to infer for each user and thus further increase the scalability. Next, we redo the comparisons under 50%-50% train/test split. We randomly remove 50% existing links as positive testing links and use the remaining 50% existing links as positive training links. The same number of negative training and testing links are sampled from the nonexistent links as well.</p><p>The AUC results are shown in <ref type="table" target="#tab_7">Table 8</ref> and 9. The AP results are shown in <ref type="table" target="#tab_0">Table 10</ref> and 11.</p><p>The results are consistent with the 90%-10% split setting. As we can see, SEAL is still the best among all methods in general. The performance gains over heuristic methods are even larger compared to the 90%-10% split. This indicates that SEAL is able to learn good heuristics even when the network is very incomplete. SEAL also shows more clear advantages over WLNM. On the other hand, we observe that VGAE becomes a strong baseline when network is sparser by achieving the best AUC results on 3 out of 8 datasets. It is thus interesting to study whether replacing the node2vec embeddings in SEAL with the VGAE embeddings can further improve the performance. We leave it to future work.     <ref type="table" target="#tab_0">Table 12</ref> shows the results. As we can see, SEAL consistently outperforms all embedding methods. Especially on the last three networks, SEAL (with node2vec embeddings) outperforms pure node2vec by large margins. These results indicate that in many cases, embedding methods alone cannot capture the most useful link prediction information, while effectively combining the power of different types of features results in much better performance. SEAL also consistently outperforms WLNM. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 4 .</head><label>4</label><figDesc>SimRank is a ?-decaying heuristic which satisfies the properties in Theorem 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Double-Radius Node Labeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The DGCNN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with heuristic methods (AUC). USAir 93.80?1.22 89.79?1.61 88.84?1.45 95.06?1.03 95.77?0.92 92.88?1.42 94.67?1.08 78.89?2.31 88.96?1.44 96.63?0.73 95.95?1.10 96.62?0.72 NS 94.42?0.95 94.43?0.93 68.65?2.03 94.45?0.93 94.45?0.93 94.85?1.10 94.89?1.08 94.79?1.08 97.64?0.25 98.57?0.51 98.61?0.49 98.85?0.47 PB 92.04?0.35 87.41?0.39 90.14?0.45 92.36?0.34 92.46?0.37 92.92?0.35 93.54?0.41 77.08?0.80 90.15?0.45 93.83?0.59 93.49?0.47 94.72?0.46 Yeast 89.37?0.61 89.32?0.60 82.20?1.02 89.43?0.62 89.45?0.62 92.24?0.61 92.76?0.55 91.49?0.57 82.36?1.02 95.86?0.54 95.62?0.52 97.91?0.52 C.ele 85.13?1.61 80.19?1.64 74.79?2.04 86.95?1.40 87.49?1.41 86.34?1.89 90.32?1.49 77.07?2.00 74.94?2.04 89.72?1.67 86.18?1.72 90.30?1.35 Power 58.80?0.88 58.79?0.88 44.33?1.02 58.79?0.88 58.79?0.88 65.39?1.59 66.00?1.59 76.15?1.06 79.52?1.78 82.41?3.43 84.76?0.98 87.61?1.57 Router 56.43?0.52 56.40?0.52 47.58?1.47 56.43?0.51 56.43?0.51 38.62?1.35 38.76?1.39 37.40?1.27 47.58?1.48 87.42?2.08 94.41?0.88 96.38?1.45 E.coli 93.71?0.39 81.31?0.61 91.82?0.58 95.36?0.34 95.95?0.35 93.50?0.44 95.57?0.44 62.49?1.43 91.89?0.58 96.94?0.29 97.21?0.27 97.64?0.22</figDesc><table><row><cell>Data</cell><cell>CN</cell><cell>Jaccard</cell><cell>PA</cell><cell>AA</cell><cell>RA</cell><cell>Katz</cell><cell>PR</cell><cell>SR</cell><cell>ENS</cell><cell>WLK</cell><cell>WLNM</cell><cell>SEAL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with latent feature methods (AUC). 08?0.80 94.85?1.14 91.44?1.78 81.47?10.71 74.22?3.11 89.28?1.99 97.09?0.70 NS 74.55?4.34 92.30?2.26 91.52?1.28 80.63?1.90 89.94?2.39 94.04?1.64 97.71?0.93 PB 94.30?0.53 93.90?0.42 85.79?0.78 76.95?2.76 83.96?0.86 90.70?0.53 95.01?0.34 Yeast 90.28?0.69 91.41?0.60 93.67?0.46 87.45?3.33 93.25?0.40 93.88?0.21 97.20?0.64 C.ele 85.90?1.74 86.48?2.60 84.11?1.27 69.21?3.14 51.90?2.57 81.80?2.18 89.54?2.04 Power 50.63?1.10 66.57?2.05 76.22?0.92 55.63?1.47 91.78?0.61 71.20?1.65 84.18?1.82 Router 78.03?1.63 85.65?1.93 65.46?0.86 67.15?2.10 68.79?2.42 61.51?1.22 95.68?1.22 E.coli 93.76?0.56 93.82?0.41 90.82?1.49 82.38?2.19 94.92?0.32 90.81?0.63 97.22?0.28</figDesc><table><row><cell>Data</cell><cell>MF</cell><cell>SBM</cell><cell>N2V</cell><cell>LINE</cell><cell>SPC</cell><cell>VGAE</cell><cell>SEAL</cell></row><row><cell>USAir 94.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Popular heuristics for link prediction, see<ref type="bibr" target="#b0">[1]</ref> for details.</figDesc><table><row><cell>Name</cell><cell>Formula</cell><cell></cell><cell>Order</cell></row><row><cell>common neighbors</cell><cell>|?(x) ? ?(y)|</cell><cell></cell><cell>first</cell></row><row><cell>Jaccard</cell><cell>|?(x)??(y)| |?(x)??(y)|</cell><cell></cell><cell>first</cell></row><row><cell cols="2">preferential attachment |?(x)| ? |?(y)|</cell><cell></cell><cell>first</cell></row><row><cell>Adamic-Adar</cell><cell>z??(x)??(y)</cell><cell>1 log |?(z)|</cell><cell>second</cell></row><row><cell>resource allocation</cell><cell>z??(x)??(y)</cell><cell>1 |?(z)|</cell><cell>second</cell></row><row><cell>Katz</cell><cell cols="3">? l=1 ? l |walks l (x, y)| high</cell></row><row><cell>PageRank</cell><cell>[?x]y + [?y]x</cell><cell></cell><cell>high</cell></row><row><cell>SimRank</cell><cell cols="2">? a??(x) b??(y) score(a,b) |?(x)|?|?(y)|</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different link prediction methods</figDesc><table><row><cell></cell><cell>Heuristics</cell><cell>Latent features</cell><cell cols="3">WLK WLNM SEAL</cell></row><row><cell>Graph structure features</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Learn from full h-hop</cell><cell>No</cell><cell>n/a</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Latent/explicit features</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Model</cell><cell>n/a</cell><cell cols="2">LR/inner product SVM</cell><cell>NN</cell><cell>GNN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison with heuristic methods (AP), 90% training links. 45?1.19 87.54?2.07 91.22?1.28 95.36?1.00 96.27?0.79 94.07?1.18 95.08?1.16 69.24?2.61 91.33?1.27 96.82?0.84 95.95?1.13 96.80?0.55 NS 94.39?0.96 94.44?0.93 72.85?1.88 94.46?0.93 94.46?0.93 95.05?1.08 95.11?1.04 94.98?1.02 97.68?0.36 98.79?0.40 98.81?0.49 99.06?0.37 PB 91.47?0.45 84.78?0.71 89.33?0.72 92.36?0.46 92.37?0.57 93.07?0.46 92.97?0.77 64.33?0.95 89.35?0.71 93.34?0.89 92.69?0.64 94.31?0.56 Yeast 89.34?0.62 89.15?0.67 85.36?0.85 89.53?0.63 89.55?0.63 95.23?0.39 95.47?0.43 93.42?0.64 85.54?0.85 96.82?0.35 96.40?0.38 98.33?0.37 C.ele 82.62?1.51 77.06?2.55 75.49?1.86 86.46?1.43 87.10?1.53 85.93?1.69 89.56?1.57 68.61?2.31 75.69?1.86 88.96?2.06 85.08?2.05 89.48?1.85 Power 58.77?0.88 58.77?0.89 51.93?1.16 58.76?0.89 58.76?0.90 79.82?0.91 80.56?0.91 77.02?0.93 83.63?1.37 83.02?3.19 87.16?0.77 89.55?1.29 Router 56.39?0.53 55.84?0.80 69.03?0.95 56.50?0.51 56.51?0.50 64.52?0.81 64.91?0.85 58.82?1.12 69.25?0.96 86.59?2.23 93.53?1.09 96.23?1.71 E.coli 93.49?0.38 82.42?0.59 94.04?0.33 96.05?0.25 96.72?0.25 94.83?0.30 96.41?0.33 55.01?0.86 94.11?0.33 97.25?0.42 97.50?0.23 98.03?0.20</figDesc><table><row><cell>Data</cell><cell>CN</cell><cell>Jaccard</cell><cell>PA</cell><cell>AA</cell><cell>RA</cell><cell>Katz</cell><cell>PR</cell><cell>SR</cell><cell>ENS</cell><cell>WLK</cell><cell>WLNM</cell><cell>SEAL</cell></row><row><cell>USAir 93.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison with latent feature methods (AP), 90% training links. USAir 94.36?0.79 95.08?1.10 89.71?2.97 79.70?11.76 78.07?2.92 89.27?1.29 97.13?0.80 NS 78.41?3.85 92.13?2.36 94.28?0.91 85.17?1.65 90.83?2.16 95.83?1.04 98.12?0.77 PB 93.56?0.71 93.35?0.52 84.79?1.03 78.82?2.71 86.57?0.61 90.38?0.72 94.55?0.43 Yeast 92.01?0.47 92.73?0.44 94.90?0.38 90.55?2.39 94.63?0.56 95.19?0.36 97.95?0.35 C.ele 83.63?2.09 84.66?2.95 83.12?1.90 67.51?2.72 62.07?2.40 78.32?3.49 88.81?2.32 Power 53.50?1.22 65.48?1.85 81.49?0.86 56.66?1.43 91.00?0.58 75.91?1.56 86.69?1.50 Router 82.59?1.38 84.67?1.89 68.66?1.49 71.92?1.53 73.53?1.47 70.36?0.85 95.66?1.23 E.coli 95.59?0.31 95.30?0.27 90.87?1.48 86.45?1.82 96.08?0.37 92.77?0.65 97.83?0.20</figDesc><table><row><cell>Data</cell><cell>MF</cell><cell>SBM</cell><cell>N2V</cell><cell>LINE</cell><cell>SPC</cell><cell>VGAE</cell><cell>SEAL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Inference time of SEAL.</figDesc><table><row><cell></cell><cell>USAir</cell><cell>NS</cell><cell>PB</cell><cell>Yeast</cell><cell>C.ele</cell><cell>Power</cell><cell>Router</cell><cell>E.coli</cell></row><row><cell>Number of potential links</cell><cell cols="8">5.49E+04 1.26E+06 7.46E+05 2.82E+06 4.40E+04 1.22E+07 1.26E+07 1.39E+06</cell></row><row><cell>Inference time per link (s)</cell><cell cols="8">6.05E-04 2.55E-04 2.04E-04 3.96E-04 4.13E-04 1.35E-04 2.13E-04 2.40E-04</cell></row><row><cell>Inference time for all potential links (s)</cell><cell>31</cell><cell>321</cell><cell>146</cell><cell>1106</cell><cell>16</cell><cell>1640</cell><cell>2681</cell><cell>328</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparison with heuristic methods (AUC), 50% training links. USAir 87.93?0.43 84.82?0.52 87.59?0.50 88.61?0.40 88.73?0.39 88.91?0.51 90.57?0.62 81.09?0.59 87.71?0.50 91.93?0.71 91.42?0.95 93.23?1.46 NS 77.13?0.75 77.12?0.75 65.87?0.83 77.13?0.75 77.13?0.75 82.30?0.93 82.32?0.94 81.60?0.87 87.19?1.04 87.27?1.71 87.61?1.63 90.88?1.18 PB 86.74?0.17 83.40?0.24 89.52?0.19 87.06?0.17 87.01?0.18 91.25?0.22 92.23?0.21 81.82?0.43 89.54?0.19 92.54?0.33 90.93?0.23 93.75?0.18 Yeast 82.59?0.28 82.52?0.28 81.61?0.39 82.63?0.27 82.62?0.27 88.87?0.28 89.35?0.29 88.50?0.26 81.84?0.38 91.15?0.35 92.22?0.32 93.90?0.54 C.ele 72.29?0.82 69.75?0.86 73.81?0.97 73.37?0.80 73.42?0.82 79.99?0.59 84.95?0.58 76.05?0.80 74.11?0.96 83.29?0.89 75.72?1.33 81.16?1.52 Power 53.38?0.22 53.38?0.22 46.79?0.69 53.38?0.22 53.38?0.22 57.34?0.51 57.34?0.52 56.16?0.45 62.70?0.95 63.44?1.29 64.09?0.76 65.84?1.10 Router 52.93?0.28 52.93?0.28 55.06?0.44 52.94?0.28 52.94?0.28 54.39?0.38 54.44?0.38 54.38?0.42 55.06?0.44 71.25?4.37 86.10?0.52 86.64?1.58 E.coli 86.55?0.57 81.70?0.42 90.80?0.40 87.66?0.56 87.81?0.56 89.81?0.46 92.96?0.43 73.70?0.53 90.88?0.40 92.38?0.46 92.81?0.30 94.18?0.41</figDesc><table><row><cell>Data</cell><cell>CN</cell><cell>Jaccard</cell><cell>PA</cell><cell>AA</cell><cell>RA</cell><cell>Katz</cell><cell>PR</cell><cell>SR</cell><cell>ENS</cell><cell>WLK</cell><cell>WLNM</cell><cell>SEAL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Comparison with latent feature methods (AUC), 50% training links. USAir 91.28?0.71 91.68?0.66 84.63?1.58 72.51?12.19 65.42?3.41 90.09?0.94 93.36?0.67 NS 62.95?1.03 81.91?1.55 80.29?1.20 65.96?1.60 79.63?1.34 93.38?1.07 87.73?1.08 PB 93.27?0.16 92.96?0.20 79.29?0.67 75.53?1.78 78.06?1.00 90.57?0.69 93.79?0.25 Yeast 84.99?0.49 88.32?0.38 90.18?0.17 79.44?7.90 89.73?0.28 93.51?0.41 93.30?0.51 C.ele 78.49?1.73 81.83?1.44 75.53?1.23 59.46?7.08 47.30?0.91 81.51?1.69 82.33?2.31 Power 50.53?0.60 57.53?0.76 55.40?0.84 53.44?1.83 56.51?0.94 70.34?0.84 61.88?1.31 Router 77.49?0.64 74.66?1.52 62.45?0.81 62.43?3.10 53.87?1.33 62.91?0.95 85.08?1.53 E.coli 91.75?0.33 90.60?0.58 84.73?0.81 74.50?11.10 92.00?0.50 91.27?0.42 94.17?0.36</figDesc><table><row><cell>Data</cell><cell>MF</cell><cell>SBM</cell><cell>N2V</cell><cell>LINE</cell><cell>SPC</cell><cell>VGAE</cell><cell>SEAL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Comparison with heuristic methods (AP), 50% training links. USAir 87.60?0.45 80.35?1.26 90.29?0.45 89.39?0.39 89.54?0.36 91.29?0.36 91.93?0.50 73.04?0.84 90.47?0.45 93.34?0.51 92.54?0.81 94.11?1.08 NS 77.11?0.74 77.10?0.75 68.56?0.71 77.14?0.74 77.14?0.75 82.69?0.88 82.73?0.90 81.86?0.88 86.77?0.88 89.97?1.02 90.10?1.11 92.21?0.97 PB 85.90?0.16 78.59?0.43 88.83?0.25 87.24?0.18 87.05?0.21 91.54?0.16 91.92?0.25 70.78?0.69 88.87?0.25 92.34?0.34 91.01?0.20 93.42?0.19 Yeast 82.55?0.27 82.16?0.39 84.45?0.34 82.68?0.27 82.66?0.27 92.22?0.21 92.54?0.23 90.98?0.30 84.77?0.34 93.55?0.46 93.93?0.20 95.32?0.38 C.ele 69.82?0.74 64.04?1.02 74.20?0.65 73.40?0.77 73.33?0.96 79.94?0.79 84.15?0.86 68.45?1.17 74.62?0.64 83.20?0.90 76.12?1.08 81.01?1.51 Power 53.37?0.22 53.35?0.24 51.44?0.59 53.37?0.23 53.37?0.23 57.63?0.52 57.61?0.56 56.19?0.49 61.81?0.71 63.97?1.81 66.43?0.85 68.14?1.02 Router 52.91?0.27 52.71?0.23 65.20?0.42 52.94?0.27 52.93?0.27 60.87?0.26 61.01?0.30 58.27?0.51 65.38?0.42 75.49?3.43 86.12?0.68 87.79?1.71 E.coli 86.42?0.54 78.71?0.40 93.25?0.26 89.01?0.49 89.21?0.48 91.93?0.35 94.68?0.28 63.05?0.48 93.35?0.27 94.51?0.32 94.47?0.21 95.58?0.28</figDesc><table><row><cell>Data</cell><cell>CN</cell><cell>Jaccard</cell><cell>PA</cell><cell>AA</cell><cell>RA</cell><cell>Katz</cell><cell>PR</cell><cell>SR</cell><cell>ENS</cell><cell>WLK</cell><cell>WLNM</cell><cell>SEAL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Comparison with latent feature methods (AP), 50% training links.</figDesc><table><row><cell>Data</cell><cell>MF</cell><cell>SBM</cell><cell>N2V</cell><cell>LINE</cell><cell>SPC</cell><cell>VGAE</cell><cell>SEAL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Comparison with network embedding methods (AUC and standard deviation, OOM: out of memory). 18?0.40 84.64?0.03 87.00?0.14 OOM 99.19?0.03 99.40?0.14 Facebook 99.05?0.07 89.63?0.06 98.59?0.11 98.21?0.22 99.24?0.03 99.40?0.08 BlogCatalog 85.97?1.56 90.92?2.05 96.74?0.31 OOM 96.55?0.08 98.10?0.60 Wikipedia 76.59?2.06 74.44?0.66 99.54?0.04 89.74?0.18 99.05?0.03 99.63?0.05 PPI 70.31?0.79 72.82?1.53 92.27?0.22 85.86?0.43 88.79?0.38 93.52?0.37</figDesc><table><row><cell></cell><cell>N2V</cell><cell>LINE</cell><cell>SPC</cell><cell>VGAE</cell><cell>WLNM</cell><cell>SEAL</cell></row><row><cell>arXiv</cell><cell>96.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work is supported in part by the III-1526012 and SCH-1622678 grants from the National Science Foundation and grant 1R21HS024581 from the National Institute of Health.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Friends and neighbors on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="230" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boostgapfill: Improving the fidelity of metabolic network reconstructions through integrated constraint and pattern-based methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolutola</forename><surname>Oyetunde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Link prediction in complex networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1150" to="1170" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-L?szl?</forename><surname>Barab?si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?ka</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting missing links via local information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Cheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="630" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reprint of: The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer networks</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3825" to="3833" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simrank: a measure of structural-context similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="538" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Network-based prediction of protein interactions. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Istv?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Kov?cs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerstin</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Spirohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadie</forename><surname>Pollis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Schlabach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dae-Kyum</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishka</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">275529</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman neural machine for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mixed membership stochastic blockmodels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1981" to="2014" />
			<date type="published" when="2008-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02971</idno>
		<title level="m">Network embedding as matrix factorization: Unifyingdeepwalk, line, pte, and node2vec</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reducing the rank in relational factorization models by including observable patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Leveraging node attributes for incomplete relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4072" to="4081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new status index derived from sociometric analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scaling personalized web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on World Wide Web</title>
		<meeting>the 12th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>Acm</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Local methods for estimating pagerank values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Suel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth ACM international conference on Information and knowledge management</title>
		<meeting>the thirteenth ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Local methods for estimating simrank score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhe</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Conference (APWEB), 2010 12th International Asia-Pacific</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Local approximation of pagerank and reverse pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziv</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Yossef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Tal</forename><surname>Mashiach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM conference on Information and knowledge management</title>
		<meeting>the 17th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3700" to="3710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Variational graph auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Getting lost in space: Large sample analysis of the resistance distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnes</forename><surname>Ulrike V Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Radl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2622" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning node representations from structural identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Prune: Preserving proximity and global ranking for network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-An</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Chi</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi-Yen</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shou-De</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5263" to="5272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5125" to="5136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Batagelj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Mrvar</surname></persName>
		</author>
		<ptr target="http://vlado.fmf.uni-lj.si/pub/networks/data/" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Finding community structure in networks using the eigenvectors of matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36104</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mapping the us political blogosphere: Are conservative bloggers more prominent?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ackland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BlogTalk Downunder 2005 Conference, Sydney. BlogTalk Downunder 2005 Conference</title>
		<meeting><address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Comparative assessment of large-scale data sets of protein-protein interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Christian Von Mering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berend</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Snel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">417</biblScope>
			<biblScope unit="issue">6887</biblScope>
			<biblScope unit="page" from="399" to="403" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Collective dynamics of &apos;small-world&apos;networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">6684</biblScope>
			<biblScope unit="page" from="440" to="442" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Measuring isp topologies with rocketfuel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratul</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wetherall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on networking</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="16" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond link prediction: Predicting hyperlinks in adjacency space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shali</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4430" to="4437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning latent block structure in weighted networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Aicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><forename type="middle">Z</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="248" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Halting in random walk kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahito</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1639" to="1647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast neighborhood subgraph pairwise distance kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>De Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning</title>
		<meeting>the 26th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Subgraph matching kernels for attributed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML-12)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML-12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, Fifth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">{SNAP Datasets}:{Stanford} large network dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Krevl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Social computing data repository at asu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://socialcomputing.asu.edu" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Biogrid: a general repository for interaction datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobby-Joe</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Reguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorrie</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Tyers</surname></persName>
		</author>
		<idno>92.33?0.90 92.79?0.44 82.51?2.08 71.75?11.85 70.18?2.16 89.86?1.23 94.15?0.54 NS 66.62?0.89 84.14?1.18 86.01?0.87 71.53?0.97 81.16?1.26 95.31?0.80 90.42?0.79 PB 92.53?0.33 92.64?0.17 77.21?0.97 78.72?1.24 81.30?0.84 90.57?0.79 93.40?0.33 Yeast 87.28?0.57 90.65?0.24 92.45?0.23 83.06?9.70 92.07?0.27 94.71?0.25 94.83?0.38</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="535" to="539" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">For Facebook and arXiv, all remained links are used as positive training data. For PPI, BlogCatalog and Wikipedia, we sample 10,000 remained links as positive training data. We compare SEAL (h = 1, 10 training epochs) with node2vec, LINE, SPC, VGAE, and WLNM (K = 10). For node2vec, we use the parameters</title>
	</analytic>
	<monogr>
		<title level="m">50% of random links are removed and used as testing data, while keeping the remaining network connected</title>
		<imprint/>
	</monogr>
	<note>provided in [20] if available. For SEAL and VGAE, the node attributes are used since only these two methods support explicit features</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
