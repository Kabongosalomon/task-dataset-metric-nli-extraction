<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A sequence-to-sequence approach for document-level relation extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Giorgi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Terrence Donnelly Centre for Cellular &amp; Biomolecular Research</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Vector Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">D</forename><surname>Bader</surname></persName>
							<email>gary.bader@mail.utoronto.cabowang@vectorinstitute.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Molecular Genetics</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Terrence Donnelly Centre for Cellular &amp; Biomolecular Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Laboratory Medicine and Pathobiology</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Vector Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A sequence-to-sequence approach for document-level relation extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>6 The Lunenfeld-Tanenbaum Research Institute, Sinai Health System 7 Princess Margaret Cancer Centre, University Health Network 8 Peter Munk Cardiac Center, University Health Network Corresponding author ? Equal contribution</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the fact that many relations cross the sentence boundary, there has been increasing interest in document-level relation extraction (DocRE). DocRE requires integrating information within and across sentences, capturing complex interactions between mentions of entities. Most existing methods are pipelinebased, requiring entities as input. However, jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps. In this paper, we develop a sequence-tosequence approach, seq2rel, that can learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) end-toend, replacing a pipeline of task-specific components. Using a simple strategy we call entity hinting, we compare our approach to existing pipeline-based methods on several popular biomedical datasets, in some cases exceeding their performance. We also report the first end-to-end results on these datasets for future comparison. Finally, we demonstrate that, under our model, an end-to-end approach outperforms a pipeline-based approach. Our code, data and trained models are available at https: /</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>PubMed, the largest repository of biomedical literature, contains over 30 million publications and is adding more than two papers per minute. Accurate, automated text mining and natural language processing (NLP) methods are needed to maximize discovery and extract structured information from this massive volume of text. An important step in this process is relation extraction (RE), the task of identifying groups of entities within some text that participate in a semantic relationship. In the domain of biomedicine, relations of interest include chemical-induced disease, protein-protein interactions, and gene-disease associations.</p><p>Many methods have been proposed for RE, ranging from rule-based to machine learning-based <ref type="bibr">(Zhou et al., 2014;</ref><ref type="bibr">Liu et al., 2016)</ref>. Most of this work has focused on intra-sentence binary RE, where pairs of entities within a sentence are classified as belonging to a particular relation (or none). These methods often ignore commonly occurring complexities like nested or discontinuous entities, coreferent mentions (words or phrases in the text that refer to the same entity), inter-sentence and n-ary relations (see <ref type="figure" target="#fig_0">Figure 1</ref> for examples). The decision not to model these phenomena is a strong assumption. In <ref type="bibr">GENIA (Kim et al., 2003)</ref>, a corpus of PubMed articles labelled with around 100,000 biomedical entities, ?17% of all entities are nested within another entity. Discontinuous entities are particularly common in clinical text, where ?10% of mentions in popular benchmark corpora are discontinuous . In the CDR corpus <ref type="bibr">(Li et al., 2016b)</ref>, which comprises 1500 PubMed articles annotated for chemical-induced disease relations, ?30% of all relations are inter-sentence. Some relations, like drug-gene-mutation interactions, are difficult to model with binary <ref type="bibr">RE (Zhou et al., 2014)</ref>.</p><p>In response to some of these shortcomings, there has been a growing interest in document-level RE <ref type="bibr">(DocRE)</ref>. DocRE aims to model inter-sentence re- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexities Example Comment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discontinuous mentions</head><p>Induction by paracetamol of bladder and liver tumours. Discontinuous mention of bladder tumours.</p><p>paracetamol @DRUG@ bladder tumours @DISEASE@ @CID@ paracetamol @DRUG@ liver tumours @DISEASE@ @CID@</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coreferent mentions</head><p>Proto-oncogene HER2 (also known as erbB-2 or neu) plays an important role in the carcinogenesis and the prognosis of breast cancer.</p><p>Two coreferent mentions of HER2.</p><p>her2 ; erbb-2 ; neu @GENE@ breast cancer @DISEASE@ @GDA@ n-ary, intersentence</p><p>The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10. All patients were treated with gefitinib and showed a partial response.</p><p>Ternary DGM relationship crosses a sentence boundary.</p><p>gefitinib @DRUG@ egfr @GENE@ l858e @MUTATION@ @DGM@ lations between coreferent mentions of entities in a document. A popular approach involves graphbased methods, which have the advantage of naturally modelling inter-sentence relations <ref type="bibr" target="#b2">(Peng et al., 2017;</ref><ref type="bibr" target="#b5">Song et al., 2018;</ref><ref type="bibr">Christopoulou et al., 2019;</ref><ref type="bibr">Nan et al., 2020;</ref><ref type="bibr">Minh Tran et al., 2020)</ref>. However, like all pipeline-based approaches, these methods assume that the entities within the text are known.</p><p>As previous work has demonstrated, and as we show in ?5.2, jointly learning to extract entities and relations can improve performance <ref type="bibr">(Miwa and Sasaki, 2014;</ref><ref type="bibr">Miwa and Bansal, 2016;</ref><ref type="bibr">Gupta et al., 2016;</ref><ref type="bibr">Li et al., 2016a</ref><ref type="bibr">Li et al., , 2017</ref><ref type="bibr">Nguyen and Verspoor, 2019a;</ref><ref type="bibr" target="#b22">Yu et al., 2020)</ref> and may be more efficient due to shared parameters and training steps. Existing end-to-end methods typically combine taskspecific components for entity detection, coreference resolution, and relation extraction that are trained jointly. Most approaches are restricted to intra-sentence <ref type="bibr">RE (Bekoulis et al., 2018;</ref><ref type="bibr">Luan et al., 2018;</ref><ref type="bibr" target="#b0">Nguyen and Verspoor, 2019b;</ref><ref type="bibr" target="#b13">Wadden et al., 2019;</ref><ref type="bibr">Giorgi et al., 2019)</ref> and have only recently been extended to <ref type="bibr">DocRE (Eberts and Ulges, 2021)</ref>. However, they still focus on binary relations. Ideally, DocRE methods would be capable of modelling the complexities mentioned above without strictly requiring entities to be known.</p><p>A less popular end-to-end approach is to frame RE as a generative task with sequence-to-sequence (seq2seq) learning . This framing simplifies RE by removing the need for task-specific components and explicit negative training examples, i.e. pairs of entities that do not express a relation. If the information to extract is appropriately linearized to a string, seq2seq methods are flexible enough to model all complexities discussed thus far. However, existing work stops short, focusing on intra-sentence binary relations <ref type="bibr" target="#b25">(Zeng et al., 2018;</ref><ref type="bibr">Nayak and Ng, 2020;</ref>. In this paper, we extend work on seq2seq methods for RE to the document level, with several important contributions:</p><p>? We propose a novel linearization schema that can handle complexities overlooked by previous seq2seq approaches, like coreferent mentions and n-ary relations ( ?3.1).</p><p>? Using this linearization schema, we demonstrate that a seq2seq approach is able to learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) jointly, and report the first end-to-end results on several popular biomedical datasets ( ?5.1).</p><p>? We devise a simple strategy, referred to as "entity hinting" ( ?3.3), to compare our model to existing pipeline-based approaches, in some cases exceeding their performance ( ?5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task definition: document-level relation extraction</head><p>Given a source document of S tokens, a model must extract all tuples corresponding to a relation, R, expressed between the entities, E in the document, (E 1 , ..., E n , R) where n is the number of participating entities, or arity, of the relation. Each entity E i is represented as the set of its coreferent mentions {e i j } in the document, which are often expressed as aliases, abbreviations or acronyms. All entities appearing in a tuple have at least one mention in the document. The mentions that express a given relation are not necessarily contained within <ref type="figure">Figure 2</ref>: A sequence-to-sequence model for document-level relation extraction. Special tokens are generated by the decoder. Entity mentions are copied from the input via a copy mechanism (not shown). Decoding is initiated by a @START@ token and terminated when the model generates the @END@ token. Attention connections shown only for the second timestep to reduce clutter. CID: chemical-induced disease. the same sentence. Commonly, E is assumed to be known and provided as input to a model. We will refer to these methods as "pipeline-based". In this paper, we are primarily concerned with the situation where E is not given and must be predicted by a model, which we will refer to as "end-to-end".</p><p>3 Our approach: seq2rel</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linearization</head><p>To use seq2seq learning for RE, the information to be extracted must be linearized to a string. This linearization should be expressive enough to model the complexities of entity and relation extraction without being overly verbose. We propose the following schema, illustrated with an example: X: Variants in the estrogen receptor alpha (ESR1) gene and its mRNA contribute to risk for schizophrenia. Y : estrogen receptor alpha ; ESR1 @GENE@ schizophrenia @DISEASE@ @GDA@</p><p>The input text X, expresses a gene-disease association (GDA) between ESR1 and schizophrenia. In the corresponding target string Y , each relation begins with its constituent entities. A semicolon separates coreferent mentions (;), and entities are terminated with a special token denoting their type (e.g. @GENE@). Similarly, relations are terminated with a special token denoting their type (e.g. @GDA@). Two or more entities can be included before the special relation token to support n-ary extraction. Entities can be ordered if they serve specific roles as head or tail of a relation. For each document, multiple relations can be included in the target string. Entities may be nested or discontinuous in the input text. In <ref type="figure" target="#fig_0">Figure 1</ref>, we provide examples of how this schema can be used to model various complexities, like coreferent entity mentions and n-ary relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>The model follows a canonical seq2seq setup. An encoder maps each token in the input to a contextual embedding. An autoregressive decoder generates an output, token-by-token, attending to the outputs of the encoder at each timestep ( <ref type="figure">Figure 2</ref>). Decoding proceeds until a special "end-of-sequence" token (@END@) is generated, or a maximum number of tokens have been generated. Formally, X is the source sequence of length S, which is some text we would like to extract relations from. Y is the corresponding target sequence of length T , a linearization of the relations contained in the source. We model the conditional probability</p><formula xml:id="formula_0">p(Y |X) = T t=1 p(y t |X, y &lt;t )<label>(1)</label></formula><p>During training, we optimize over the model parameters ? the sequence cross-entropy loss</p><formula xml:id="formula_1">(?) = ? T t=1 log p(y t |X, y &lt;t ; ?)<label>(2)</label></formula><p>maximizing the log-likelihood of the training data. <ref type="bibr">1</ref> The main problems with this setup for RE are: 1) The model might "hallucinate" by generating entity mentions that do not appear in the source text. 2) It may generate a target string that does not follow the linearization schema and therefore cannot be parsed. 3) The loss function is permutationsensitive, enforcing an unnecessary decoding order. To address 1) we use two modifications: a restricted target vocabulary ( ?3.2.1) and a copy mechanism ( ?3.2.2). To address 2) we experiment with several constraints applied during decoding ( ?3.2.3). Finally, to address 3) we sort relations according to their order of appearance in the source text ( ?3.2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Restricted target vocabulary</head><p>To prevent the model from "hallucinating" (generating entity mentions that do not appear in the source text), the target vocabulary is restricted to the set of special tokens needed to model entities and relations (e.g. ; and @DRUG@). All other tokens must be copied from the input using a copy mechanism (see ?3.2.2). The embeddings of these special tokens are initialized randomly and learned jointly with the rest of the model's parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Copy mechanism</head><p>To enable copying of input tokens during decoding, we use a copying mechanism (Gu et al., 2016a). The mechanism works by effectively extending the target vocabulary with the tokens in the source sequence X, allowing the model to "copy" these tokens into the output sequence, Y . Our use of the copy mechanism is similar to previous seq2seqbased approaches for RE <ref type="bibr" target="#b25">(Zeng et al., 2018</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Constrained decoding</head><p>We experimented with several constraints applied to the decoder during test time to reduce the likelihood of generating syntactically invalid target strings (strings that do not follow the linearization schema). These constraints are applied by setting the predicted probabilities of invalid tokens to a tiny value at each timestep. The full set of constraints is depicted in Appendix A. In practice, we found that a trained model rarely generates invalid target strings, so these constraints have little effect on final performance (see ?5.3). We elected not to apply them in the rest of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Sorting relations</head><p>The relations to extract from a given document are inherently unordered. However, the sequence crossentropy loss (Equation 2) is permutation-sensitive with respect to the predicted tokens. During training, this enforces an unnecessary decoding order and may make the model prone to overfit frequent token combinations in the training set <ref type="bibr" target="#b12">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b20">Yang et al., 2019)</ref>. To partially mitigate this, we sort relations within the target strings according to their order of appearance in the source text, providing the model with a consistent decoding order. The position of a relation is determined by the first occurring mention of its head entity. The position of a mention is determined by the sum of its start and end character offsets. In the case of ties, we then sort by the first mention of its tail entity (and so on for n-ary relations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entity hinting</head><p>Although the proposed model can jointly extract entities and relations from unannotated text, most existing DocRE methods provide the entities as input. Therefore, to more fairly compare to existing methods, we also provide entities as input, using a simple strategy that we will refer to as "entity hinting". This involves prepending entities to the source text as they appear in the target string. Taking the example from ?3.1, entity hints would be added as follows:</p><p>X: estrogen receptor alpha ; ESR1 @GENE@ schizophrenia @DISEASE@ @SEP@ Variants in the estrogen receptor alpha (ESR1) gene and its mRNA contribute to risk for schizophrenia.</p><p>where the special @SEP@ token demarcates the end of the entity hint. <ref type="bibr">2</ref> We experimented with the common approach of inserting marker tokens before and after each entity mention (Zhou and Chen, 2021) but found this to perform worse. Our approach adds fewer extra tokens to the source text and provides a location for the copy mechanism to focus, i.e. tokens left of @SEP@. In our experiments, we use entity hinting when comparing to methods that provide ground truth entity annotations as input ( ?5.1.1). In ?5.2, we use entity hinting to compare pipeline-based and end-to-end approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup 4.1 Datasets</head><p>We evaluate our approach on several biomedical, DocRE datasets. We also include one nonbiomedical dataset, DocRED. In Appendix B, we list relevant details about their annotations.</p><p>CDR (Li et al., 2016b) The BioCreative V CDR task corpus is manually annotated for chemicals, diseases and chemical-induced disease (CID) relations. It contains the titles and abstracts of 1500 PubMed articles and is split into equally sized train, validation and test sets. Given the relatively small size of the training set, we follow <ref type="bibr">Christopoulou et al. (2019)</ref> and others by first tuning the model on the validation set and then training on the combination of the train and validation sets before evaluating on the test set. Similar to prior work, we filter negative relations with disease entities that are hypernyms of a corresponding true relations disease entity within the same abstract (see Appendix C).</p><p>GDA <ref type="bibr" target="#b18">(Wu et al., 2019)</ref> The gene-disease association corpus contains 30,192 titles and abstracts from PubMed articles that have been automatically labelled for genes, diseases and gene-disease associations via distant supervision. The test set is comprised of 1000 of these examples. Following <ref type="bibr">Christopoulou et al. (2019)</ref> and others, we hold out a random 20% of the remaining abstracts as a validation set and use the rest for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DGM (Jia et al., 2019)</head><p>The drug-gene-mutation corpus contains 4606 PubMed articles that have been automatically labelled for drugs, genes, mutations and ternary drug-gene-mutation relationships via distant supervision. The dataset is available in three variants: sentence, paragraph, and documentlength text. We train and evaluate our model on the paragraph-length inputs. Since the test set does not contain relation annotations on the paragraph level, we report results on the validation set. We hold out a random 20% of training examples to form a new validation set for tuning.</p><p>DocRED <ref type="bibr" target="#b21">(Yao et al., 2019</ref>) DocRED includes over 5000 human-annotated documents from Wikipedia. There are six entity and 96 relation types, with ?40% of relations crossing the sentence boundary. We use the same split as previous end-to-end methods (Eberts and Ulges, 2021), which has 3,008 documents in the training set, 300 in the validation set and 700 in the test set 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We evaluate our model using the micro F1-score by extracting relation tuples from the decoder's output (see Appendix D). Similar to prior work, we use a "strict" criteria. A predicted relation is considered 3 https://github.com/lavis-nlp/jerex correct if the relation type and its entities match a ground truth relation. An entity is considered correct if the entity type and its mentions match a ground truth entity. However, since the aim of DocRE is to extract relations at the entity-level (as opposed to the mention-level), we also report performance using a relaxed criterion (denoted "relaxed"), where predicted entities are considered correct if more than 50% of their mentions match a ground truth entity (see Appendix E).</p><p>Existing methods that evaluate on CDR, GDA and DGM use the ground truth entity annotations as input. This makes it difficult to directly compare with our end-to-end approach, which takes only the raw text as input. To make the comparison fairer, we use entity hinting ( ?3.3) so that our model has access to the ground truth entity annotations. We also report the performance of our method in the end-to-end setting on these corpora to facilitate future comparison. To compare to existing end-toend approaches, we use DocRED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation, training and hyperparameters</head><p>Implementation We implemented our model in PyTorch <ref type="bibr" target="#b1">(Paszke et al., 2017)</ref> using AllenNLP <ref type="bibr">(Gardner et al., 2018)</ref>. As encoder, we use a pretrained transformer, implemented in the Transformers library <ref type="bibr" target="#b17">(Wolf et al., 2020)</ref>, which is fine-tuned during training. When training and evaluating on biomedical corpora, we use PubMedBERT (Gu et al., 2020), and BERT BASE (Devlin et al., 2019) otherwise. In both cases, we use the default hyperparameters of the pretrained model. As decoder, we use a single-layer LSTM (Hochreiter and Schmidhuber, 1997) with randomly initialized weights. We use multi-head attention <ref type="bibr" target="#b10">(Vaswani et al., 2017)</ref> as the cross-attention mechanism between encoder and decoder. Select hyperparameters were tuned on the validation sets, see Appendix F for details.</p><p>Training All parameters are trained jointly using the AdamW optimizer <ref type="bibr">(Loshchilov and Hutter, 2019)</ref>. Before training, we re-initialize the top L layers of the pretrained transformer encoder, which has been shown to improve performance and stability during fine-tuning <ref type="bibr">(Zhang et al., 2021b)</ref>. During training, the learning rate is linearly increased for the first 10% of training steps and linearly decayed to zero afterward. Gradients are scaled to a vector norm of 1.0 before backpropagating. During each forward propagation, the hidden state of the LSTM decoder is initialized with the mean of token embeddings output by the encoder. The decoder is regularized by applying dropout <ref type="bibr" target="#b6">(Srivastava et al., 2014)</ref> with probability 0.1 to its inputs, and Drop-Connect <ref type="bibr" target="#b14">(Wan et al., 2013)</ref> with probability 0.5 to the hidden-to-hidden weights. As is common, we use teacher forcing, feeding previous ground truth inputs to the decoder when predicting the next token in the sequence. During test time, we generate the output using beam search <ref type="bibr">(Graves, 2012)</ref>. Beams are ranked by mean token log probability after applying a length penalty. 4 Models were trained and evaluated on a single NVIDIA Tesla V100. 5 5 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison to existing methods</head><p>In the following sections, we compare our model to existing DocRE methods on several benchmark corpora. We compare to existing pipeline-based methods ( ?5.1.1), including n-ary methods ( ?5.1.2), and end-to-end methods ( ?5.1.3). Details about these methods are provided in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Existing pipeline-based methods</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we use entity hinting to compare our method to existing pipeline-based methods on CDR and GDA. We also report end-to-end performance, which is not comparable to existing pipeline-based methods but will facilitate future comparisons. The large performance improvement when using entity hinting (+27-29%) confirms that the model exploits the entity annotations. The fact that relaxed entity matching makes a large difference in the end-to-end setting (+12-15%) suggests that a significant portion of the model's mistakes occur during coreference resolution. Although our method is designed for end-to-end RE, we find that it outperforms existing pipeline-based methods when using entity hinting on GDA. Our method is competitive with existing methods when using entity hinting on the CDR corpus but ultimately underperforms state-of-the-art results. Given that GDA is 46X larger, we speculated that our method might be underperforming in the low-data regime.</p><p>To determine if this is a contributing factor, we artificially reduce the size of the CDR and GDA training sets and plot the performance as a curve <ref type="figure" target="#fig_1">(Figure 3</ref>). In all cases besides GDA with entity hinting, performance increases monotonically with dataset size. There is no obvious plateau on CDR even when using all 500 training examples. Together, these results suggest that our seq2seq based approach can outperform existing pipeline-based methods when there are sufficient training examples but underperforms relative to existing methods in the low-data regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">n-ary relation extraction</head><p>In <ref type="table" target="#tab_1">Table 2</ref> we compare to existing n-ary methods on the DGM corpus. With entity hinting, our method significantly outperforms the existing method. The difference in encoders partially explains this large performance gap. Where Jia et al.</p><p>(2019) use a BiLSTM that is trained from scratch, we use PubMedBERT, a much larger model that has been pretrained on abstracts and full-text ar- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">End-to-end methods</head><p>In <ref type="table" target="#tab_2">Table 3</ref> we compare to an existing end-to-end approach on DocRED, JEREX (Eberts and Ulges, 2021). To make the comparison fair, we use the same pretrained encoder (BERT BASE ). We find that although our model is arguably simpler (JEREX contains four task-specific sub-components, each with its own loss) it only slightly underperforms JEREX, mainly due to recall. We speculate that one reason for this is a large number of relations per document, which leads to longer target strings and, therefore, more decoding steps. The median length of the target strings in DocRED, using our linearization, is 110, whereas the next largest is 19 in GDA. Improving the decoder's ability to process long sequences, e.g. switching the LSTM for a transformer or modifying the linearization schema to produce shorter target strings, may improve recall and close the gap with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pipeline vs. End-to-end</head><p>In ?5.1.1 and ?5.1.2, we provide gold-standard entity annotations from each corpus as input to  our model via entity hinting (referred to as "gold" hints from here on, see ?3.3). This allowed us to compare to existing methods that also provide these annotations as input. However, gold-standard entity annotations are (almost) never available in real-world settings, such as large-scale extraction on PubMed. In this setting, there are two strategies: pipeline-based, where independent systems perform entity and relation extraction, and end-toend, where a single model performs both tasks. To compare these approaches under our model, we perform evaluations where a named entity recognition (NER) system is used to determine entity hints (referred to as "silver" hints from here on) and when no entity hints are provided (end-to-end). 8 However, this alone does not create a true pipeline, as our model can recover from both false negatives and false positives in the NER step. To mimic error propagation in the pipeline setting, we filter any entity mention predicted by our model that was not predicted by the NER system. In <ref type="table" target="#tab_3">Table 4</ref>, we present the results of all four settings (gold and silver entity hints, pipeline and end-to-end) on CDR.</p><p>We find that using gold entity hints significantly outperforms all other settings. This is expected, as the gold-standard entity annotations are highquality labels produced by domain experts. Using silver hints significantly drops performance, likely due to a combination of false positive and false negatives from the NER step. In the pipeline setting, where there is no recovery from false negatives, performance falls by another 15%. The end-to-end setting significantly outperforms the pipeline setting (due to a large boost in recall) and performs comparably to using silver hints. Together, our results suggest that performance reported using gold-standard entity annotations may be overly optimistic and corroborates previous work demonstrating the benefits of jointly learning entity and relation extraction <ref type="bibr">(Miwa and Sasaki, 2014;</ref><ref type="bibr">Miwa and Bansal, 2016;</ref><ref type="bibr">Gupta et al., 2016;</ref><ref type="bibr">Li et al., 2016a</ref><ref type="bibr">Li et al., , 2017</ref><ref type="bibr">Nguyen and Verspoor, 2019a;</ref><ref type="bibr" target="#b22">Yu et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation</head><p>In <ref type="table" target="#tab_4">Table 5</ref>, we present the results of an ablation study. We perform the analysis twice, once on the biomedical corpus CDR and once on the general domain corpus DocRED. Unsurprisingly, we find that fine-tuning a pretrained encoder greatly impacts performance. Training the same encoder from scratch (-pretraining) reduces performance by ?30%. Using the pretrained weights without fine-tuning (-fine-tuning) drops performance by 15.6-18.1%. Restricting the target vocabulary (vocab restriction, see ?3.2.1) has a small positive impact, boosting performance by 1.1%-2.3%. Deliberately ordering the relations within each target string (-sorting relations, see ?3.2.4) has a large positive impact, boosting performance by 5.6%-14.7%. This effect is larger on DocRED, likely because it has more relations per document on average than CDR, so ordering becomes more impor-tant. Finally, adding constraints to the decoding process (+ constrained decoding) has little impact on performance, suggesting that a trained model rarely generates invalid target strings (see ?3.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Related work</head><p>Seq2seq learning for RE has been explored in prior work. CopyRE <ref type="bibr" target="#b25">(Zeng et al., 2018)</ref> uses an encoder-decoder architecture with a copy mechanism, similar to our approach, but is restricted to intra-sentence relations. Additionally, because CopyRE's decoding proceeds for exactly three timesteps per relation, the model is limited to generating binary relations between single token entities. The ability to decode multi-token entities was addressed in follow-up work, CopyMTL . A similar approach was published concurrently but was again limited to intra-sentence binary relations (Nayak and Ng, 2020). Most recently, GenerativeRE (Cao and Ananiadou, 2021) proposed a novel copy mechanism to improve performance on multi-token entities. None of these approaches deal with the complexities of DocRE, where many relations cross the sentence boundary, and coreference resolution is critical. <ref type="bibr">9</ref> More generally, our paper is related to a recently proposed "text-to-text" framework <ref type="bibr" target="#b4">(Raffel et al., 2020)</ref>. In this framework, a task is formulated so that the inputs and outputs are both text strings, enabling the use of the same model, loss function and even hyperparameters across many seq2seq, classification and regression tasks. This framework has recently been applied to biomedical literature to perform named entity recognition, relation extraction (binary, intra-sentence), natural language inference, and question answering <ref type="bibr">(Phan et al., 2021)</ref>. Our work can be seen as an attempt to formulate the task of DocRE within this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Limitations and future work</head><p>Permutation-sensitive loss Our approach adopts the sequence cross-entropy loss (Equation 2), which is sensitive to the order of predicted tokens, enforcing an unnecessary decoding order on the inherently unordered relations. To partially mitigate this problem, we order relations within the target string according to order of appearance in the source text, providing the model with a consistent decoding order that can be learned (see <ref type="bibr">?3.2.4, ?5.3)</ref>. Previous work has addressed this issue with various strategies, including reinforcement learning <ref type="bibr" target="#b24">(Zeng et al., 2019)</ref>, unordered-multi-tree decoders , and non-autoregressive decoders <ref type="bibr" target="#b7">(Sui et al., 2020)</ref>. However, these works are limited to binary intra-sentence relation extraction, and their suitability for DocRE has not been explored. A promising future direction would be to modify our approach such that the arbitrary order of relations is not enforced during training.</p><p>Input length restriction Due to the pretrained encoder's input size limit (512 tokens), our experiments are conducted on paragraph-length text. Our model could be extended to full documents by swapping its encoder with any of the recently proposed "efficient transformers" <ref type="bibr" target="#b9">(Tay et al., 2021)</ref>. Future work could evaluate such a model's ability to extract relations from full scientific papers.</p><p>Pretraining the decoder In our model, the encoder is pretrained, while the decoder is trained from scratch. Several recent works, such as T5 <ref type="bibr" target="#b4">(Raffel et al., 2020)</ref> and <ref type="bibr">BART (Lewis et al., 2020)</ref>, have proposed pretraining strategies for entire encoder-decoder architectures, which can be fine-tuned on downstream tasks. An interesting future direction would be to fine-tune such a model on DocRE using our linearization schema.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we extend generative, seq2seq methods for relation extraction to the document level. We propose a novel linearization schema that can handle complexities overlooked by previous seq2seq approaches, like coreferent mentions and n-ary relations. We compare our approach to existing pipeline-based and end-to-end methods on several benchmark corpora, in some cases exceeding their performance. In future work, we hope to extend our method to full scientific papers and develop strategies to improve performance in the low-data regime and in cases where there are many relations per document. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Constrained decoding</head><p>In <ref type="figure">Figure 4</ref>, we illustrate the rules used to constrain decoding. At each timestep t, given the prediction of the previous timestep t ? 1, the predicted class probabilities of tokens that would generate a syntactically invalid target string are set to a tiny value.</p><p>In practice, we found that a model rarely generates invalid target strings, so these constraints have little effect on final performance (see ?3.2.3 and ?5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details about dataset annotations</head><p>In <ref type="table" target="#tab_7">Table 6</ref>, we list which complexities (e.g. nested &amp; discontinuous mentions, n-ary relations) are contained within each dataset used in our evaluations. We also report the fraction of relations in the test set that are inter-sentence. We consider a relation intra-sentence if any sentence in the document contains at least one mention of each entity in the relation, and inter-sentence otherwise. This produces an estimate that matches previously reported numbers for CDR (?30%). In <ref type="bibr" target="#b21">Yao et al. (2019)</ref>, the fraction of inter-sentence relations in DocRED is reported as ?40.7%. We can reproduce this value if we consider relations intra-sentence when all mentions of an entity exist within a single sentence and inter-sentence otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hypernym filtering</head><p>The CDR dataset is annotated for chemical-induced disease (CID) relationships between the most specific chemical and disease mentions in an abstract. Take the following example from the corpus: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Parsing the models output</head><p>At test time, our model autoregressively generates an output, token-by-token, using beam search decoding (see ?3.2). In order to extract the predicted relations from this output, we apply the following steps. First, predicted token ids are converted to a string. We use the decode() 11 method of the HuggingFace Transformers tokenizer <ref type="bibr" target="#b17">(Wolf et al., 2020)</ref> to do this. For example, after calling decode() on the predicted token ids, this string might look like:</p><p>monoamine oxidase b ; maob @GENE@ parkinson's <ref type="figure">Figure 4</ref>: A diagram depicting syntactically valid predictions during decoding at each timestep t. The log probabilities of all other possible predictions are set to a tiny value to prevent the model from producing a syntactically invalid target string. BOS is the special beginning-of-sequence token, COPY denotes any token copied from the source text, and COREF is the special token used to separate coreferent mentions (i.e. ;). ENTITY is any special entity token (e.g. @GENE@) and RELATION any special relation token (e.g. @GDA@ for gene-disease association).n ents denotes the number of entities predicted by the current timestep and n ents the expected arity of the relation. The special end-of-sequence token (not shown) is always considered valid and its log probability is never modified.  <ref type="bibr" target="#b18">(Wu et al., 2019)</ref> 15.6 DGM <ref type="bibr">(Jia et al., 2019)</ref> 63.5 DocRED <ref type="bibr" target="#b21">(Yao et al., 2019)</ref> 12.5* disease ; pd @DISEASE@ @GDA@</p><p>We then use regular expressions to extract any relations from this string that match our linearization schema (see ?3.1), which produces a dictionary of nested lists, keyed by relation class: Finally, we apply some normalization steps to the entity mentions. Namely, we strip leading and trailing white space characters, sort entity mentions lexicographically (as their order is not important), and remove duplicate mentions. Similarly, we remove duplicate relations. These steps are applied to both target and model output strings. The F1-score can then be computed by tallying true positives, false positives and false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Relaxed entity matching</head><p>The aim of DocRE is to extract relations at the entity-level. However, it is common to evaluate these methods with a "strict" matching criteria, where a predicted entity P is considered correct if and only if all its mentions exactly match a corresponding gold entities mentions, i.e. P = G. This penalizes model predictions that miss even a single coreferent mention, but are otherwise correct. A relaxed criteria, proposed in prior work <ref type="bibr">(Jain et al., 2020)</ref> considers P to match G if more than 50% of P's mentions belong to G, that is |P ? G| |P| &gt; 0.5</p><p>In this paper, alongside the strict criteria, we report performance using this relaxed entity matching strategy, denoted "relaxed".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Hyperparameters</head><p>In <ref type="table" target="#tab_8">Table 7</ref>, we list the hyperparameter values used during evaluation on each corpus, with and without entity hinting. Select hyperparameters were tuned </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Baselines</head><p>This section contains detailed descriptions of all methods we compare to in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Pipeline-based methods</head><p>These methods are pipeline-based, assuming the entities are provided as input. Many of them construct a document-level graph using dependency parsing, heuristics, or structured attention and then update node and edge representations using propagation.</p><p>? <ref type="bibr">Christopoulou et al. (2019)</ref> propose EoG, an edge-orientated graph neural model. The nodes of the graph are constructed from mentions, entities, and sentences. Edges between nodes are initially constructed using heuristics. An iterative algorithm is then used to generate edges between nodes in the graph. Finally, a classification layer takes the representation of entity-to-entity edges as input to determine whether those entities express a relation or</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of complexities in entity and relation extraction and the proposed linearization schema to model them. CID: chemical-induced disease. GDA: gene-disease association. DGM: drug-gene-mutation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Effect of training set size on performance. Performance reported as the median micro F1-score obtained over five runs with different random seeds on the CDR and GDA validation sets, with and without entity hinting. Error bands correspond to the standard deviation over the five runs. The absolute number of training examples are displayed for each corpus. Some labels are excluded to reduce clutter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison to existing pipeline-based methods. Performance reported as micro-precision, recall and F1-scores (%) on the CDR and GDA test sets. Results below the horizontal line are not comparable to existing methods. Bold: best scores.</figDesc><table><row><cell></cell><cell></cell><cell>CDR</cell><cell></cell><cell></cell><cell>GDA</cell><cell></cell></row><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Christopoulou et al. (2019)</cell><cell cols="3">62.1 65.2 63.6</cell><cell>-</cell><cell>-</cell><cell>81.5</cell></row><row><cell>Nan et al. (2020)</cell><cell>-</cell><cell>-</cell><cell>64.8</cell><cell>-</cell><cell>-</cell><cell>82.2</cell></row><row><cell>Minh Tran et al. (2020)</cell><cell>-</cell><cell>-</cell><cell>66.1</cell><cell>-</cell><cell>-</cell><cell>82.8</cell></row><row><cell>Lai and Lu (2021)</cell><cell cols="3">64.9 67.1 66.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Xu et al. (2021)</cell><cell>-</cell><cell>-</cell><cell>68.7</cell><cell>-</cell><cell>-</cell><cell>83.7</cell></row><row><cell>Zhou et al. (2021)</cell><cell>-</cell><cell>-</cell><cell>69.4</cell><cell>-</cell><cell>-</cell><cell>83.9</cell></row><row><cell>seq2rel (entity hinting)</cell><cell cols="6">68.2 66.2 67.2 84.4 85.3 84.9</cell></row><row><cell cols="7">seq2rel (entity hinting, relaxed) 68.2 66.2 67.2 84.5 85.4 85.0</cell></row><row><cell>seq2rel (end-to-end)</cell><cell cols="6">43.5 37.5 40.2 55.0 55.4 55.2</cell></row><row><cell>seq2rel (end-to-end, relaxed)</cell><cell cols="6">56.6 48.8 52.4 70.3 70.8 70.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison to existing n-ary methods. Performance reported as micro-precision, recall and F1scores (%) on the DGM validation set. Results below the horizontal line are not comparable to existing methods. Bold: best scores. ? Jia et al. 2019 do not report results on the validation set, so we re-run their paragraphlevel model.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Jia et al. (2019)  ?</cell><cell cols="3">62.9 76.2 68.9</cell></row><row><cell>seq2rel (entity hinting)</cell><cell cols="3">84.0 84.8 84.4</cell></row><row><cell cols="4">seq2rel (entity hinting, relaxed) 84.1 84.9 84.5</cell></row><row><cell>seq2rel (end-to-end)</cell><cell cols="3">68.9 65.9 67.4</cell></row><row><cell>seq2rel (end-to-end, relaxed)</cell><cell cols="3">78.3 74.9 76.6</cell></row><row><cell>ticles from PubMedCentral.</cell><cell></cell><cell></cell><cell></cell></row></table><note>6 However, this does not completely account for the improvement in performance, as recent work that has replaced the BiLSTM encoder of (Jia et al., 2019) with Pub- MedBERT found that it improves performance by approximately 2-4% on the task of drug-gene- mutation prediction (Zhang et al., 2021a).7 Our results on the DGM corpus suggest that our lin- earization schema effectively models n-ary rela- tions without requiring changes to the model archi- tecture or training procedure.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison to existing end-to-end methods. Performance reported as micro-precision, recall and F1scores (%) on the DocRED test set. Results below the horizontal line are not comparable to existing methods. Bold: best scores.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="4">JEREX (Eberts and Ulges, 2021) 42.8 38.2 40.4</cell></row><row><cell>seq2rel (end-to-end)</cell><cell cols="3">44.0 33.8 38.2</cell></row><row><cell>seq2rel (end-to-end, relaxed)</cell><cell cols="3">53.7 41.3 46.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of pipeline-based and end-to-end approaches. Gold hints use gold-standard entity annotations to insert entity hints in the source text. Silver hints use the entity annotations provided by PubTator.</figDesc><table><row><cell></cell><cell></cell><cell>Strict</cell><cell></cell><cell></cell><cell>Relaxed</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Gold hints</cell><cell cols="6">68.2 66.2 67.2 68.2 66.2 67.2</cell></row><row><cell cols="7">Silver hints 42.4 37.3 39.7 53.0 46.7 49.7</cell></row><row><cell>Pipeline</cell><cell cols="6">45.0 16.9 24.6 62.5 23.5 34.1</cell></row><row><cell cols="7">End-to-end 43.5 37.5 40.2 56.6 48.8 52.4</cell></row></table><note>Pipeline is identical to silver entity hints, except that we filter out entity mentions predicted by our model that PubTator does not predict. The end-to-end model only has access to the unannotated source text as input. Performance reported as micro-precision, recall and F1- scores (%) on the CDR test set, with strict and relaxed entity matching criteria. Bold: best scores.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study results. Performance reported as the micro-precision, recall and F1-scores (%) on the CDR and DocRED validation sets. ?: difference to the complete models F1-score. Bold: best scores.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CDR</cell><cell></cell><cell></cell><cell cols="2">DocRED</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>?</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>?</cell></row><row><cell>seq2rel (end-to-end)</cell><cell cols="3">41.0 35.1 37.8</cell><cell>-</cell><cell cols="3">46.9 36.1 40.8</cell><cell>-</cell></row><row><cell>-pretraining</cell><cell>9.4</cell><cell>6.9</cell><cell>8.0</cell><cell cols="3">-29.8 18.5 7.7</cell><cell cols="2">10.8 -30.0</cell></row><row><cell>-fine-tuning</cell><cell cols="8">24.3 20.5 22.2 -15.6 42.4 15.5 22.7 -18.1</cell></row><row><cell>-vocab restriction</cell><cell cols="4">39.6 32.2 35.5 -2.3</cell><cell cols="4">45.2 35.5 39.7 -1.1</cell></row><row><cell>-sorting relations</cell><cell cols="4">36.1 29.2 32.3 -5.6</cell><cell cols="4">52.9 17.4 26.2 -14.7</cell></row><row><cell cols="9">+ constrained decoding 40.8 35.6 38.0 +0.2 46.8 35.9 40.6 -0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Evaluation datasets used in this paper with details about their annotations. Inter-sentence relations (%) are the fraction of relations in the test set that cross sentence boundaries. We consider a relation intra-sentence if any sentence in the document contains at least one mention of each entity in the relation, and inter-sentence otherwise. *This differs from the estimate in Yao et al. (2019), see Appendix B.</figDesc><table><row><cell>Corpus</cell><cell>Nested Mentions? Discontinuous Mentions? Coreferent mentions? n-ary relations? Inter-sentence relations (%)</cell></row><row><cell>CDR (Li et al., 2016b)</cell><cell>29.8</cell></row><row><cell>GDA</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameter values used for each corpus. Hyperparameters values when using entity hinting, if they differ from the values used without entity hinting, are shown in parentheses. Tuned indicates whether or not the hyperparameters were tuned on the validation sets.Akiba et al., 2019). The tuning process selects the best hyperparameters according to the validation set micro F1-score using the TPE (Tree-structured Parzen Estimator) algorithm(Bergstra et al., 2011). 12  During tuning, we use greedy decoding (i.e. beam size of one). Once optimal hyperparameters are found, we tune the beam size (bs) and length penalty (?) using a grid search over the values bs = {2...10}, with a step size of 1, and ? = {0.2...2.0}, with a step size of 0.2.</figDesc><table><row><cell>Hyperparameter</cell><cell>Tuned?</cell><cell>CDR</cell><cell>GDA</cell><cell>DGM</cell><cell>DocRED</cell></row><row><cell>Batch size</cell><cell></cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell>Training epochs</cell><cell></cell><cell>130 (70)</cell><cell>30 (25)</cell><cell>30 (45)</cell><cell>50</cell></row><row><cell>Encoder learning rate</cell><cell></cell><cell>2e-5</cell><cell>2e-5</cell><cell>2e-5</cell><cell>2e-5</cell></row><row><cell>Encoder weight decay</cell><cell></cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Encoder re-initialized top L layers</cell><cell></cell><cell>1</cell><cell>1 (2)</cell><cell>1</cell><cell>1</cell></row><row><cell>Decoder learning rate</cell><cell></cell><cell cols="3">1.21e-4 (1.13e-4) 5e-4 (4e-4) 8e-4 (1.5e-5)</cell><cell>7.8e-5</cell></row><row><cell>Decoder input dropout</cell><cell></cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Decoder hidden-to-hidden weights dropout</cell><cell></cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>Target embedding size</cell><cell></cell><cell>256</cell><cell>256</cell><cell>256</cell><cell>256</cell></row><row><cell>No. heads in multi-head cross-attention</cell><cell></cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell></row><row><cell>Beam size</cell><cell></cell><cell>3 (2)</cell><cell>4 (1)</cell><cell>3 (2)</cell><cell>8</cell></row><row><cell>Length penalty</cell><cell></cell><cell>1.4 (0.2)</cell><cell>0.8 (1.0)</cell><cell>0.2 (0.8)</cell><cell>1.4</cell></row><row><cell>Max decoding steps</cell><cell></cell><cell>128</cell><cell>96</cell><cell>96</cell><cell>400</cell></row><row><cell>using Optuna (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See ?4.3 for details about the encoder and decoder.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Some pretrained models have their own separator token which can be used in place of @SEP@, e.g. BERT uses [SEP].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://docs.allennlp. org/main/api/nn/beam_search/ #lengthnormalizedsequencelogprobabilityscorer 5 https://www.nvidia.com/en-us/data-center/ v100/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://www.ncbi.nlm.nih.gov/pmc/ 7 The authors have not released code at the time of writing, so we were unable to evaluate this model on the DGM validation set in order to compare with our method directly.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Specifically, we use PubTator<ref type="bibr" target="#b16">(Wei et al., 2013)</ref>. PubTator provides up-to-date entity annotations for PubMed using stateof-the-art machine learning systems.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Concurrent to our work, REBEL (Huguet Cabot and Navigli, 2021) also extends seq2seq methods to document-level RE, achieving strong performance on DocRED. However, the method was not evaluated on n-ary relations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://meshb.nlm.nih.gov 11 https://huggingface.co/docs/transformers/ main_classes/tokenizer#transformers. PreTrainedTokenizerBase.decode</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">https://optuna.readthedocs.io/en/stable/ reference/generated/optuna.samplers.TPESampler.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was enabled in part by support provided by Compute Ontario (www.computeontario.ca), Compute Canada (www.computecanada.ca) and the CIFAR AI Chairs Program and partially funded by the US National Institutes of Health (NIH) [U41 HG006623, U41 HG003751).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>html not. We compare to EoG in the pipeline-based setting on the CDR and GDA corpora.</p><p>? <ref type="bibr">Nan et al. (2020)</ref> propose LSR (Latent Structure Refinement). A "node constructor" encodes each sentence of an input document and outputs contextual representations. Representations that correspond to mentions and tokens on the shortest dependency path in a sentence are extracted as nodes. A "dynamic reasoner" is then applied to induce a document-level graph based on the extracted nodes. The classifier uses the final representations of nodes for relation classification. We compare to LSR in the pipeline-based setting on the CDR and GDA corpora.</p><p>? Lai and Lu (2021) propose BERT-GT, which combines BERT with a graph transformer. Both BERT and the graph transformer accept the document text as input, but the graph transformer requires the neighbouring positions for each token, and the self-attention mechanism is replaced with a neighbour-attention mechanism. The hidden states of the two transformers are aggregated before classification. We compare to BERT-GT in the pipeline-based setting on the CDR and GDA corpora.</p><p>? Minh <ref type="bibr">Tran et al. (2020)</ref> propose EoGANE (EoG model Augmented with Node Representations), which extends the edge-orientated model proposed by <ref type="bibr">Christopoulou et al. (2019)</ref> to include explicit node representations which are used during relation classification. We compare to EoGANE in the pipeline-based setting on the CDR and GDA corpora.</p><p>? SSAN <ref type="bibr" target="#b19">(Xu et al., 2021)</ref> propose SSAN (Structured Self-Attention Network), which inherits the architecture of the transformer encoder <ref type="bibr" target="#b10">(Vaswani et al., 2017)</ref> but adds a novel structured self-attention mechanism to model the coreference and co-occurrence dependencies between an entities mentions. We compare to SSAN in the pipeline-based setting on the CDR and GDA corpora.</p><p>? Zhou et al. <ref type="formula">(2021)</ref> propose ALTOP (Adaptive Thresholding and Localized cOntext Pooling), which extends BERT with two modifications. Adaptive thresholding, which learns an optimal threshold to apply to the relation classifier. Localized context pooling, which uses the pretrained self-attention layers of BERT to create an entity embedding from its mentions and their context. We compare to ALTOP in the pipeline-based setting on the CDR and GDA corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 n-ary relation extraction</head><p>These methods are explicitly designed for the extraction of n-ary relations, where n &gt; 2.</p><p>? Jia et al. (2019) propose a multiscale neural architecture, which combines representations learned over text spans of varying scales and for various sub-relations. We compare to <ref type="bibr">Jia et al. (2019)</ref> in the pipeline-based setting on the n-ary DGM corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 End-to-end methods</head><p>These methods are capable of performing the subtasks of DocRE in an end-to-end fashion with only the document text as input.</p><p>? Eberts and Ulges (2021) propose JEREX, which extends BERT with four task-specific components that use BERTs outputs to perform entity mention localization, coreference resolution, entity classification, and relation classification. They present two versions of their relation classifier, denoted "global relation classifier" (GRC) and "multi-instance relation classifier" (MRC). We compare to JEREX-MRC in the end-to-end setting on the DocRED corpus.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Endto-end neural relation extraction using deep biaffine attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph LSTMs. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00049</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Long N Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Anibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaurya</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chanana</surname></persName>
		</author>
		<title level="m">Erol Bahadroglu, Alec Peltekian, and Gr?goire Altan-Bonnet. 2021. Scifive: a text-to-text transformer model for biomedical literature</title>
		<imprint/>
	</monogr>
	<note>ArXiv preprint, abs/2106.03598</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">N-ary relation extraction using graphstate LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1246</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2235" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Joint entity and relation extraction with set prediction networks. ArXiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2011.01675</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long range arena : A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1080</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="872" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discontinuous named entity recognition as maximal clique discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.63</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="764" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pubtator: a web-based text mining tool for assisting biocuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">W1</biblScope>
			<biblScope unit="page" from="518" to="522" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Renet: A deep learning approach for extracting gene-disease associations from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hing-Fung</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wah</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research in Computational Molecular Biology</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A deep reinforced sequence-toset model for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5252" to="5258" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel decomposition strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI 2020</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2282" to="2289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Copymtl: Copy mechanism for joint extraction of entities and relations with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9507" to="9514" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning the extraction order of multiple relational facts in a sentence with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1035</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Ranran Haoran Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Xuemo Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<publisher>Daisuke</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
