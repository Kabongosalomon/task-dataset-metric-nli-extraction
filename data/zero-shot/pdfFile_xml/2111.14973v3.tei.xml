<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTIPATH++: EFFICIENT INFORMATION FUSION AND TRAJECTORY AGGREGATION FOR BEHAVIOR PREDICTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Hefny</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avikalp</forename><surname>Srivastava</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><forename type="middle">S</forename><surname>Refaat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigamaa</forename><surname>Nayakanti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Cornman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Douillard</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Pang</forename><surname>Lam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
						</author>
						<title level="a" type="main">MULTIPATH++: EFFICIENT INFORMATION FUSION AND TRAJECTORY AGGREGATION FOR BEHAVIOR PREDICTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting the future behavior of road users is one of the most challenging and important problems in autonomous driving. Applying deep learning to this problem requires fusing heterogeneous world state in the form of rich perception signals and map information, and inferring highly multimodal distributions over possible futures. In this paper, we present MultiPath++, a future prediction model that achieves state-of-the-art performance on popular benchmarks. MultiPath++ improves the MultiPath architecture [45] by revisiting many design choices. The first key design difference is a departure from dense image-based encoding of the input world state in favor of a sparse encoding of heterogeneous scene elements: MultiPath++ consumes compact and efficient polylines to describe road features, and raw agent state information directly (e.g., position, velocity, acceleration). We propose a context-aware fusion of these elements and develop a reusable multi-context gating fusion component. Second, we reconsider the choice of pre-defined, static anchors, and develop a way to learn latent anchor embeddings end-to-end in the model. Lastly, we explore ensembling and output aggregation techniques-common in other ML domains-and find effective variants for our probabilistic multimodal output representation. We perform an extensive ablation on these design choices, and show that our proposed model achieves state-of-the-art performance on the Argoverse Motion Forecasting Competition [12] and the Waymo Open Dataset Motion Prediction Challenge <ref type="bibr" target="#b17">[18]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling and predicting the future behavior of human agents is a fundamental problem in many real-world robotics domains. For example, accurately forecasting the future state of other vehicles, cyclists and pedestrians is critical for safe, comfortable, and human-like autonomous driving. However, behavior prediction in an autonomous vehicle (AV) driving setting poses a number of unique modeling challenges: 1) Multimodal output space: The problem is inherently stochastic; it is impossible to truly know the future state of the environment. This is exacerbated by the fact that other agents' intentions are not observable, and leads to a highly multimodal distribution over possible outcomes (e.g., a car could turn left or right at an intersection). Effective models must be able to represent such a rich output space with high precision and recall matching the underlying distribution.</p><p>2) Heterogenous, interrelated input space: The driving environment representation can contain a highly heterogeneous mix of static and dynamic inputs: road network information (lane geometry and connectivity, stop lines, crosswalks), traffic light state information, and motion history of agents. Driving situations are often highly interactive, and can involve many agents at once (e.g. negotiating a 4-way stop with crosswalks). This requires careful modeling choices, as explicitly modeling joint future distributions over multiple agents is exponential in the number of agents. Effective models must capture not only the interactions between the agents in the scene, but also the relationships between the road elements and the behavior of agents given the road context.</p><p>The novel challenges and high impact of this problem have naturally garnered much interest in recent years. There has been a rich body of work on how to model agents' futures, their interactions, and the environment. However, there is little consensus to date on the best modeling choices for each component, and in popular benchmark challenge datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b52">53]</ref>, there is a surprisingly diverse set of solutions to this problem; for details see Section 2 and <ref type="table">Table 2</ref>.</p><p>The MultiPath framework <ref type="bibr" target="#b44">[45]</ref> addresses the multimodal output space challenge above by modeling the highly multimodal output distributions via a Gaussian Mixture Model. It handles a common issue of mode collapse during learning by using static trajectory anchors, an external input to the model. This practical solution gives practitioners a straightforward way of ensuring diversity and an extra level of modeler control via the design of such anchors. The choice of a GMM representation proved to be an extremely popular, appearing in many works-see <ref type="table">Table 2</ref>, "Trajectory Distribution", where "Weighted set" is a special case of GMMs where only means and mixture weights are modeled.</p><p>The MultiPath input representation and backbone draws heavily upon the computer vision literature. By rasterizing all world state in a top-down orthographic view, MultiPath and others <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref> leverage powerful, established CNN architectures like ResNet <ref type="bibr" target="#b24">[25]</ref>, which offer solutions to the heterogeneous interrelated input space: the heterogeneous world state is mapped to a common pixel format, and interactions occur via local information sharing via convolution operations. While convenient and established, there are downsides to such rasterization: <ref type="bibr" target="#b0">(1)</ref> There is an uneasy trade-off between resolution of the spatial grid, field of view, and compute requirements. <ref type="bibr" target="#b1">(2)</ref> Rasterizing is a form of manual feature engineering, and some features may be inherently difficult to represent in such a framework (e.g. radial velocity). <ref type="bibr">(</ref>3) It is difficult to capture long range interactions via convolutions with small receptive fields. <ref type="bibr" target="#b3">(4)</ref> The information content is spatially very sparse, making a dense representation a potentially computationally wasteful choice.</p><p>In this paper, we introduce MultiPath++, which builds upon MultiPath, taking its output GMM representation and concept of anchors, but reconsidering how to represent and combine highly heterogeneous world state inputs and model interactions between state elements. MultiPath++ introduces a number of key upgrades:</p><p>? We eschew the rasterization-and-CNN approach in favor of modeling sparse world state objects more directly from their compact state description. We represent road elements as polylines, agent history as a sequence of physical state encoded with RNNs, and agent interactions as RNNs over the state of neighbors relative to each ego-agent. These choices avoid lossy rasterization in favor of raw, continuous state, and result in compute complexity that scales with the number of scene elements rather than the size of a spatial grid. Long-range dependencies are effectively and efficiently modeled in our representation.</p><p>? Capturing relationships between road elements and agents is critical, and we find that encoding each element independently does not perform as well as modeling their interactions (e.g. , when the road encoding is aware of relevant agents, and vice versa). To address this, we propose a novel form of context awareness we call multi-context gating (MCG), in which sets of elements have access to a summary context vector upon which encodings are conditioned. MCG is implemented as a generic neural network component that is applied throughout our model. MCG can be viewed as an efficient form of cross-attention, whose efficiency/quality trade-off depends on the size of the context vector.</p><p>? We also explore improvements in trajectory modeling, comparing representations based on kinematic controls, and/or polynomials as a function of continuous future time. We further demonstrate a way to learn latent representations of anchors and show they outperform the original static anchors of MultiPath, while simplifying model creation to a single-step process.</p><p>? Finally, we find significant additional gains on public benchmarks by applying ensembling techniques to our models. Unlike models with static anchors, the latent anchors of a MultiPath++ ensemble are not in direct correspondence. Furthermore, a lot of popular behavior prediction benchmarks have introduced metrics such as miss-rate (MR) and mean Average Precision (mAP), which require the ability to model diverse outcomes with few trajectories and differ from pure trajectory distance error capturing the average agent behavior. With the above in mind, we formulate the problem of ensembling the results of several models as one of greedy iterative clustering, which maximizes a probabilistic objective using the popular Expectation Maximization algorithm <ref type="bibr" target="#b2">[3]</ref>.</p><p>As of November 1, 2021, MultiPath++ ranks 1 st on the Waymo Open Motion Dataset leaderboard 2 , 4 th on the Argoverse Motion Forecasting. Competition <ref type="bibr" target="#b2">3</ref> We offer MultiPath++ as a reference set of design choices, empirically validated via ablation studies, that can be adopted, further studied and extended by the behavior modeling community.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We focus on architectural design choices for behavior prediction in driving environments-what representations to use to encode road information, agent motion, agent interactions, output trajectories, and output distributions. <ref type="table">Table 2</ref> is a summary of past work, which we go over here with additional context.</p><p>For road encoding, there is a dichotomy of representations. The raster approach encodes the world as a stack of images, from a top-down orthographic (or "bird's-eye") view. Rasterizing the world state has the benefit of simplicity-all the various types of input information (road configuration, agent state history, spatial relationships) are unified via rendering as a multi-channel image, enabling one to leverage powerful off-the-shelf Convolutional Neural Network (CNN) techniques. However, this one-size-fits-all approach has significant downsides: difficulty in modeling long-range interactions, constrained field of view, and difficulty in representing continuous physical states. As an alternative, the polyline approach describes curves (e.g., lanes, crosswalks, boundaries) as piecewise linear segments. This is a significantly more compact form due to the sparse nature of road networks. Previous works typically process a set-of-polylines description of the world in a per-agent, agent-centric coordinate system. LaneGCN <ref type="bibr" target="#b31">[32]</ref> stands apart by treating road lanes as nodes in a graph neural network, leveraging road network connectivity structure.</p><p>To model motion history, one popular choice is to encode the sequence of past observed states via a recurrent net (GRU, LSTM) or temporal (1D) convolution. As an alternative, in the raster framework, the state sequence is typically rendered as a stack of binary mask images depicting agent oriented bounding boxes, or rendered in the same image, with the corresponding time information rendered separately <ref type="bibr" target="#b38">[39]</ref>.</p><p>To model agent interactions, one must deal with a dynamic set of neighboring agents around each modeled agent. This is typically done by aggregating neighbor motion history with a permutation-invariant set operator: pooling or soft attention. Notably, Precog <ref type="bibr" target="#b40">[41]</ref> jointly rolls out agent policies in a step-wise simulation. Raster approaches rely on convolution over the 2D spatial grid to implicitly capture interactions; long-term interactions are dependent on the network receptive fields.</p><p>Agent trajectory decoding choices are similar to choices for encoding motion history, with the exception of methods that do lookup on a fixed or learned trajectory database <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>The most popular output trajectory representation is a sequence of states (or state differences). A few works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42]</ref> instead model Newton's laws of motion in a discrete time-step aggregation capturing Verlet integration. Other works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b43">44]</ref> explicitly model controls which parameterize a kinematically-feasible model for vehicles and bicycles. With any of these representations, the spacetime trajectory can be intrinsically represented as a sequence of sample points or a continuous polynomial representation <ref type="bibr" target="#b4">[5]</ref>. In our experimental results, we explore the effectiveness of states and kinematic controls, with and without an underlying polynomial basis. Notably unique are (1) HOME <ref type="bibr" target="#b21">[22]</ref> and GOHOME <ref type="bibr" target="#b20">[21]</ref> which first predict a heatmap, and then decode trajectories after sampling, and (2) MP3 <ref type="bibr" target="#b10">[11]</ref> and NMP <ref type="bibr" target="#b51">[52]</ref> which learn a cost function evaluator of trajectories, and the trajectories are enumerated heuristically rather than generated by a learned model.</p><p>Nearly all work assumes an independent, per-agent output space, in which agent interactions cannot be explicitly captured. A few works are notable in describing joint interactions as output, either in an asymmetric <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">47]</ref> or symmetric way <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>The choice of output trajectory distribution has ramifications on downstream applications. An intrinsic property of the driving setting is that a vehicle or a pedestrian can follow one of a diverse set of possible trajectories. It is thus essential to capture the multimodal nature of the problem. Gaussian Mixture Models (GMMs) are a popular choice for this purpose due to their compact parameterized form; mode collapse is addressed through training tricks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">50]</ref> or the use of trajectory anchors <ref type="bibr" target="#b44">[45]</ref>. Other approaches model a discrete distribution over a set of trajectories (learned or fixed a priori) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54]</ref>, or via a collection of trajectory samples drawn from a latent distribution and decoded by the model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41</ref>].</p><p>3 Model Architecture <ref type="figure" target="#fig_0">Figure 1</ref> depicts the proposed MultiPath++ model architecture, which on a high level is similar to that of MultiPath <ref type="bibr" target="#b44">[45]</ref>; the model consists of an encoding step and a predictor head which conditions on anchors and outputs a Gaussian Mixture Model (GMM) <ref type="bibr" target="#b2">[3]</ref> distribution for the possible agent position at each future time step.</p><p>MultiPath used a common, top-down image based representation for all input modalities (e.g., agents' tracked state, road network information), and a CNN encoder. In contrast, MultiPath++ has encoders processing each input modality and converting it to a compact and sparse representation; the different modality encodings are later fused using a multi-context gating (MCG) mechanism.</p><p>Learned anchor embeddings  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Representation</head><p>MultiPath++ makes predictions based on the following input modalities:</p><p>? Agent state history: a state sequence describing the agent trajectory for a fixed number of past steps. In the Waymo Open Motion dataset <ref type="bibr" target="#b17">[18]</ref>, this state information includes position, velocity, 3D bounding box size, heading angle and object type; for Argoverse <ref type="bibr" target="#b11">[12]</ref> only position information is provided. The state is transformed to an agent-centric coordinate system, such that the most recent agent pose is located at the origin and heading east. <ref type="bibr" target="#b3">4</ref> ? Road network: Road network elements such as lane lines, cross walks, and stop lines are often represented as parametric curves like clothoids <ref type="bibr" target="#b51">[52]</ref>, which can be sampled to produce point collections that are easily stored in multi-dimensional array format, as is done in many public datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>. We further summarize this information by approximating point sequences for each road element as a set of piecewise linear segments, or polylines, similar to <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>? Agent interactions: For each modeled agent, we consider all neighboring agents. For each neighboring agent, we extract features in the modeled agent's coordinate frame, such as relative orientation, distance, history and speed.</p><p>? AV-relative features: Similar to the interaction features, we extract features of the autonomous vehicle / sensing vehicle (AV) relative to each other agent. We model the AV separately from the other agents. We hypothesize this is a helpful distinction for the model because: (a) The AV is the center of sensors' field of view. Tracking errors due to distance and occlusion are relative to this center. (b) The behavior of the AV can be unlike the other road users, which to a good approximation can be assumed to all be humans.</p><p>Details on how these features are encoded and fused are described next. These steps comprise the "Encoder" block of <ref type="figure" target="#fig_0">Figure 1</ref>, whose output is an encoding per agent, in each agent's coordinate frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi Context Gating for fusing modalities</head><p>In this section we focus on how to combine the different input modality encodings in an effective way. Other works use a common rasterized format <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b51">52]</ref>, a simple concatenation of encodings <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref>, or employ attention <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref>. We propose an efficient mechanism for fusing information we term multi-context gating (MCG), and use MCG blocks throughout the MultiPath++ architecture.</p><p>Given a set of elements s 1:N and an input context vector c, a CG block assigns an output s 1:N to each element in the set, and computes an output context vector c . The output does not depend on the ordering of input elements. Mathematically, let CG(?, ?) be the function implemented by the CG block, and ? be any permutation operation on a sequence of n elements. The following equations hold for CG:</p><formula xml:id="formula_0">(s 1:n , c ) = CG(s 1:n , c) (s * 1:n , c * ) = CG(?(s 1:n ), c)<label>(1)</label></formula><p>which imply that we have</p><formula xml:id="formula_1">c * = c (permutation-invariance) s * 1:n = ?(s 1:n ) (permutation-equivariance).</formula><p>The size of the set n can vary across calls to CG(?, ?).</p><p>CG's set function properties-permutation invariance/equivariance and ability to process arbitrarily sized sets-are naturally motivated by the need to encode a variable, unordered set of road network elements and agent relationships. A number of set functions have been proposed in the literature such as DeepSets <ref type="bibr" target="#b50">[51]</ref>, PointNet <ref type="bibr" target="#b37">[38]</ref> and SetTransformers <ref type="bibr" target="#b28">[29]</ref>. A single CG block is implemented vias</p><formula xml:id="formula_2">i = MLP(s i ) (2) c = MLP(c) (3) s i =s i c (4) c = Pool(s 1:n ),<label>(5)</label></formula><p>where denotes element-wise product and Pool is a permutation-invariant pooling layer such as max or average pooling. These operations are illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. In the absence of an input context, we simply setc to an all-ones vector in the first context gating block. Note that both s i and c depend on all inputs. It can be shown that c is permutation-invariant w.r.t the input embeddings. It can also be shown that s i are permutation-equivariant.</p><p>We stack multiple CG blocks by incorporating running-average skip-connections, as is done residual networks <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_3">s k 1:n = 1 k k j=1 s j 1:n (6) c k = 1 k k j=1 c j (7) s k+1 1:n , c k+1 = CG s k 1:n ,c k .<label>(8)</label></formula><p>We denote such multi-layer CG blocks as MCG N (?, ?) for a stack of N CG blocks.</p><p>Comparison with attention. Attention is a popular mechanism in domains such as NLP <ref type="bibr" target="#b47">[48]</ref> and computer vision <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>, in which the encoding for each element of a set is updated via a combination of encodings of all other elements. For a set of size n, this intrinsically requires O(n 2 ) operations. In models of human behavior in driving scenarios, self attention has been employed to update encodings for, e.g. , road lanes, by attending to neighboring lanes, or to update encodings per agent based on the other agents in the scene. Cross attention has also been used to condition one input type (e.g. agent encodings) on another (e.g. road lanes) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>. Without loss of generality, if there are n agents and m road elements, this cross attention scales as O(nm) to aggregate road information for each agent.</p><p>CG can be viewed as an approximation to cross-attention. Rather than each of n elements attending to all m elements of the latter set, CG summarizes the latter set with the single context vector c, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Thus the dimensionality of c needs to be great enough to capture all the useful information contained in the original m encodings. If the dimensionality of elements is d, and the dimensionality of c is d c , then if d c = md, CG can be reduced to some form of cross-attention by setting c = Concat({c j } m j=1 ). When d c &lt; md, we are trading off the representational power of full cross-attention with computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoders</head><p>In this section we detail the specific encoders shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Agent history encoding. The agent history encoding is obtained by concatenating the output of three sources:</p><p>1. A LSTM on the history features from H time steps ago to the present time: (x t , y t ) t=?H:0 . 2. A LSTM on the difference in the history features (x t ? x t?1 , y t ? y t?1 ) t=?H+1:0 . 3. MCG blocks applied to the set of history elements. Each element in the set consists of a historical position and time offset in seconds relative to the present time. The context input here is an all-ones vector with an identity context MLP. Additionally we also encode the history frame id as a one hot vector to further disambiguate the history steps.</p><p>We denote the final embedding, which concatenates these three state history encodings, as ? (state) .</p><p>Agent interaction encoding. For each modeled agent, we build an interaction encoding by considering each neighboring agent ?'s past state observations:</p><formula xml:id="formula_4">{x ? ?H , . . . , x ? ?1 , x ? 0 }.</formula><p>We transform ?'s state into the modeled agent's coordinate frame, and embed it with a LSTM to obtain an embedding ? (interaction) ? . Note this is similar to the ego-agent history embedding but instead applied to the relative coordinates of another agent.</p><p>By doing this for n neighboring agents we obtain a set of interaction embeddings ? (interaction) i=1:n . We fuse neighbor information with stacked MCG blocks as follows</p><formula xml:id="formula_5">(? (interaction) 1:n , ? (state) ) = MCG N (? (interaction) 1:n , [? (state) , ? (interaction) AV ])<label>(9)</label></formula><p>where the second argument is the input context vector to MCG, which in this case is a concatenation of the modeled agent's history embedding, and the AV's interaction embedding. In this way we emphasize the AV's representation as a unique entity in the context for all interactions; see Section 3.1 for motivation.</p><p>Road network encoding. We use the polyline road element representation discussed in Section 3.1 as input. Each line segment is parameterized by its start point, end point and the road element semantic type ? (e.g. , Crosswalk, SolidDoubleYellow, etc). For each agent of interest, we transform the closest P = 128 polylines into their frame of reference and call the transformed segment p = (a, b). Let r be the closest point from the agent to the segment, and a ? be the unit tangent vector at a on the original curve. Then we represent the agent's spatial relationship to the segment via the vector [||r|| 2 , r/||r|| 2 , (b ? a)/||b ? a|| 2 , ||b ? a|| 2 , ||b ? r|| 2 , a ? , OneHotEncoding(?)]. These feature vectors are each processed with a shared MLP, resulting in a set of agent-specific embeddings per road segment, which we denote by ? (road) 1:P . We then fuse road element embeddings with the agent history embedding ? (state) using stacked MCG blocks (?</p><formula xml:id="formula_6">(road) 1:P , ? (state) ) = MCG N (? (road) 1:P , ? (state) )<label>(10)</label></formula><p>and thus enrich the road embeddings with dynamic state information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Output representation</head><p>MultiPath++ predicts a distribution of future behavior parameterized as a Gaussian Mixture Model (GMM), as is done in MultiPath <ref type="bibr" target="#b44">[45]</ref> and other works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>. For efficient long-term prediction, the distribution is conditionally independent over time steps across mixture components, thus each mode at each time step is represented as a Gaussian distribution over (x, y) with a mean ? t ? R 2 and covariance ? t ? R 2?2 . The M mode likelihoods p 1:M are tied over time. MAP inference per mode is equivalent to taking the sequence ? 1:T as state waypoints defining a possible future trajectory for the agent. The full output distribution is</p><formula xml:id="formula_7">p(s) = M i=1 p i T t=1 N (s t ? ? t i , ? t i )<label>(11)</label></formula><p>where s = s 1:T represents a trajectory; s t ? R 2 .</p><p>The classification head of <ref type="figure" target="#fig_0">Figure 1</ref> predicts the p i as a softmax distribution over mixture components. The regression head outputs the parameters of the Gaussians ? and ? for M modes and T time steps.</p><p>Training objective. We follow the original MultiPath approach and maximize the likelihood of the groundtruth trajectory under our model's predicted distribution. We make a hard-assignment labeling of a "correct" mixture component by choosing the one with the smallest Euclidean distance to the groundtruth trajectory.</p><p>The average log loss over the entire training set is optimized using Adam. We use an initial learning rate of 0.0002 and a batch size of 64, with decay rate of 0.5 every 2 epochs. The final model is chosen after training for 800, 000 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Prediction architecture with learned anchor embeddings</head><p>The goal of the Predictor module <ref type="figure" target="#fig_0">(Figure 1)</ref> is to predict the parameters of the GMM described in Section 3.4, namely M trajectories, with likelihoods and uncertainties around each waypoint.</p><p>In applications related to future prediction, capturing the highly uncertain and multimodal set of outcomes is a key challenge and the focus of much work <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref>. One of MultiPath's key innovations was to use a static set of anchor trajectories as pre-defined modes that applied to all scenes. One major downside to this is that most modes are not a good fit to any particular scene, thus requiring a large amount modes to be considered, with most obtaining a low-likelihood and getting discarded. Another downside is the added complexity and effort stemming from a 2-phase learning process (first estimating the modes from data, then training the network).</p><p>In this work, we learn anchor embeddings as part of the overall model training. We interpret these embeddings e 1:M as anchors in latent space, and construct our architecture to have a one-to-one correspondence with these embeddings and the output trajectory modes of our GMM. The vectors e 1:M are trainable model parameters that are independent of the input. This has connections to Detection Transformers (DETR) <ref type="bibr" target="#b7">[8]</ref> which propose a way to learn anchors rather than hand-design them for object detection. This is also similar in spirit to MANTRA <ref type="bibr" target="#b32">[33]</ref>, a trajectory prediction network, which has an explicit learned memory network which consists of a database of embeddings that can be retrieved and decoded into trajectories.</p><p>We concatenate the embeddings ? (interaction) , ? (road) and ? (state) obtained from the output of the respective MCG blocks to obtain a fixed-length feature vector ? (combined) for each modeled agent. We then use this as context in stacked MCG blocks that operate on the set of anchor embeddings e 1:M , with a final MLP that predicts all parameters of the output GMM:</p><formula xml:id="formula_8">(?, ? x , ? y , ? xy , log q) 1:M = MLP(MCG(e 1:M , ? (combined) )),</formula><p>where ? is formed from (? x , ? y , ? xy ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Internal Trajectory Representation</head><p>We model the future position and heading of agents, along with agent-relative longitudinal and lateral Gaussian uncertainties. We parameterize the trajectory using x(t), y(t), ?(t), ? lng (t), ? lat (t)-position, heading, and standard deviation for longitudinal and lateral uncertainty.</p><p>The most popular approach in the literature is to directly predict a sequence of such states at uniform time-discretization.</p><p>Here we also consider two non-mutually exclusive variants.</p><p>1. We can represent functions over time as polynomials, which add an inductive bias that ensures a smooth trajectory. It gives us a compact, interpretable representation of each predicted signal.</p><p>2. Instead of directly predicting {x(t), y(t), ?(t)}, we can predict the underlying kinematic control signals, which can then be integrated to evaluate the output state. In this work, we experiment with predicting the acceleration a(t) and heading change rate?(t) and integrating them to recover the trajectory as follows:</p><formula xml:id="formula_9">v(t) = v(0) + t 0 a(? )d? ?(t) = ?(0) + t 0? (? )d? x(t) = x(0) + t 0 v(? ) cos(?(? ))d? y(t) = y(0) + t 0 v(? ) sin(?(? ))d?.</formula><p>These representations add inductive bias encouraging natural and realistic trajectories that are based on realistic kinematics and consistent with the current state of the predicted agent. For the polynomial representation, it is also possible to specify a soft constraint by regularizing the polynomial's constant term, which determines the shift of the predicted signal from its current value. Algorithm 1 demonstrates the conversion from control signals to output positions. Note that this operation is differentiable, permitting end-to-end optimization. It is a numerical approximation of Equation 12 with additional technical considerations: (1) When computing the next position (x(t), y(t)), we use the midpoint approximation of the speed and heading?(t) ? ?(t ? ? t ) +?(t ? ? t ) ? (? t /2). (2) Given vehicle dimensions, we cap the heading change rate to match a predetermined maximum feasible curvature. (3) These equations are applied to the rear-axle of the vehicle rather than the center position. We use the rear-end position of the vehicle as an approximation of the rear-axle position.</p><p>Note that Algorithm 1 can be viewed as a special type of recurrent network, without learned parameters. This decoding stage then mirrors other works which use a learned RNN (LSTM or GRU cells) to decode an embedding vector into a trajectory <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref>. In our case, the recurrent network state consists of x(t), y(t), v(t) and ?(t), and the input consists of?(t) and a(t). Encoding an inductive bias derived from kinematic modeling spares the network the need to explicitly learn these properties makes the predicted state compact. This promotes data efficiency and generalization power, but can be more sensitive to perception errors in the current state estimate.</p><p>Data: a(t) and?(t) at t = 0, 0.1, 0.2, . . . , 10.0, x(0), y(0), ?(0), v(0) Result: x(t), y(t) and ?(t) at t = 0.1, 0.2, . . . , 10.0 ?t = 0.1 for t = 0.1, 0.2, . . . , </p><formula xml:id="formula_10">10.0 d? v ? v(t ? ?t) + a(t) ? (?t/2) ?cap ? CapCurvature(?(t ? ?t)) ? ? ?(t ? ?t) +?cap ? (?t/2) x(t) ? x(t ? ?t) +? cos(?)?t y(t) ? y(t ? ?t) +? sin(?)?t ?(t) ? ?(t ? 1) +?cap?t v(t) ? ?(t ? 1) + a(t ? ?t)?t end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ensembling predictor heads via bootstrap aggregation</head><p>Ensembling is a powerful and popular technique in many machine learning applications. For example, ensembling is a critical technique for getting the best performance on ImageNet <ref type="bibr" target="#b24">[25]</ref>. By combining multiple models which are to some degree complementary, we can enjoy the benefits of a higher capacity model with lower statistical variance.</p><p>We specifically apply bootstrap aggregation (bagging) <ref type="bibr" target="#b18">[19]</ref> to our predictor heads by training E such heads together. To encourage models learning complementary information, the weights of the E heads are initialized randomly, and an example is used to update the weights of each head with a 50% probability.</p><p>Unlike scalar regression or classification, it is not obvious how to combine output from different heads in our case-each is a Gaussian Mixture Model, with no correspondence of mixture components across ensemble heads. Furthermore, we consider allowing each predictor head to predict a richer output distribution with more modes L &gt; M ; where M is fixed as a requirement for the task (and is used in benchmark metrics calculations).</p><p>Let ? denote the union of the predictions from all heads</p><formula xml:id="formula_11">? = {(? 1 , ? 1 , q 1 ), . . . , (? M , ? M , q M )},<label>(12)</label></formula><p>where M = L ? E, and the mode likelihoods p are divided by the number of heads E so that they sum up to 1. Then we pose the ensemble combination task as one of converting ? to a more compact GMM? with M modes:</p><formula xml:id="formula_12">? = (? 1 ,? 1 ,q 1 ), . . . , (? M ,? M ,q M ) ,<label>(13)</label></formula><p>while requiring that? best approximates ?. In this section we describe the aggregation algorithm we use. Theoretical motivations and derivation can be found in Appendix A.</p><p>We find fit? to ? using an iterative clustering algorithm, like Expectation-Maximization <ref type="bibr" target="#b2">[3]</ref>, but with hard assignment of cluster membership. This setting lends itself to efficient implementation in a compute graph, and allows us to train this step end-to-end as a final layer in our deep network.</p><p>We start by selecting M cluster centroids from ? 1:M in a greedy fashion. The selection criteria is to maximize the probability that a centroid sampled from ? lies within ? distance from at least one selected centroid:</p><formula xml:id="formula_13">? 1:M = argmax ? 1:M M i=1 q i max ??? 1:M I ( ? i ?? 2 ? ? )<label>(14)</label></formula><p>This is a criterion that explicitly optimizes trajectory diversity, which is a fit for metrics such as miss rate, mAP and minADE, as defined in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>. Other criteria could also be used depending on the metric of interest. It is interesting to relate this criteria to the ensembling and sampling method employed by GOHOME <ref type="bibr" target="#b20">[21]</ref>. In that work, they output an intermediate spatial heatmap representation, which is amenable to ensemble aggregation. Then they greedily sample end-points in a similar fashion.</p><p>Since jointly optimizing? 1:M is hard, we select each ? i greedily for i = 1, . . . , M according t?</p><formula xml:id="formula_14">? i = argmax ?i M i=1 q i max ???1:i I ( ? i ?? 2 ? ? )<label>(15)</label></formula><p>which differs in that the outer argmax is done iteratively over? i rather than jointly? 1:M .</p><p>Starting with the selected centroids, We iteratively update the parameters of? using an expectation-maximizationstyle <ref type="bibr" target="#b15">[16]</ref> algorithm, where each iteration consists of the following updates</p><formula xml:id="formula_15">q h ? M i=1 q i p(h|? i ;?) (16) ? h ? 1 q h M i=1 q i p(h|? i ;?)x (17) ? h ? 1 q h M i=1 q i p(h|? i ;?) ? i + (? i ?? h )(? i ?? h ) T ,<label>(18)</label></formula><p>where p(h|x;?) is the posterior probability that a given sample x is sampled from the h th component of the mixture model specified by?, which can be computed as</p><formula xml:id="formula_16">p(h|x;?) =q h N (x ?? h ,? h ) M k=1q k N (x ?? k ,? k )<label>(19)</label></formula><p>5 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The Waymo Open Motion Dataset (WOMD) <ref type="bibr" target="#b17">[18]</ref> consists of 1.1M examples time-windowed from 103K 20s scenarios. The dataset is derived from real-world driving in urban and suburban environments. Each example for training and inference consists of 1 second of history state and 8 seconds of future, which we resample at 5Hz. The object-agent state contains attributes such as position, agent dimensions, velocity and acceleration vectors, orientation, angular velocity, and turn signal state. The long (8s) time horizon in this dataset tests the model's ability to capture a large field of view and scale to an output space of trajectories, which in theory grows exponentially with time.</p><p>The Argoverse dataset <ref type="bibr" target="#b11">[12]</ref> consists of 333K scenarios containing trajectory histories, context agents, and lane centerline inputs for motion prediction. The trajectories are sampled at 10Hz, with 2 seconds of past history and a 3-second future prediction horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Metrics</head><p>We compare models using competition specific metrics associated with various datasets 5 , Specifically, we report the following metrics.</p><p>minDE t k (Minimum Distance Error): The minimum distance, over the top k most-likely trajectories, between a predicted trajectory and the ground truth trajectory at time t. minADE k (Minimum Average Distance Error): Similar to minDE t k , but the distance is calculated as an average over all timesteps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MR t k @d (Miss Rate):</head><p>Measures the rate at which minFDE t k exceeds d meters. Note that WOMD leaderboard uses a different definition <ref type="bibr" target="#b17">[18]</ref>. mAP: For each set of predicted trajectories, we have at most one positive -the one closest to the ground truth and which is within ? distance from the ground truth. The other predicted trajectories are reported as misses. From this, we can compute precision and recall at various thresholds. Following WOMD metrics definition <ref type="bibr" target="#b17">[18]</ref> the agents future trajectories are partitioned into behavior buckets, and an area under the precision-recall curve is computed using the possible true positive and false positives per agent, giving us Average Precision per behavior bucket. The total mAP value is a mean over the AP's for each behavior bucket.</p><p>Overlap rate: The fraction of times the most likely trajectory prediction of any agent overlaps with a real future trajectory of another agent (see <ref type="bibr" target="#b17">[18]</ref> for details).  <ref type="table">Table 2</ref>: Comparison with select state-of-the-art methods on Argoverse leaderboard.</p><p>k is the number of trajectories.</p><p>d is the maximum distance for no miss, and t is the trajectory time duration. ? Rank on the public leaderboard https://eval.ai/web/challenges/challenge-page/454/leaderboard/ 1279 as of November 12, 2021, which is sorted by brier-minDE.</p><p>TRI: (Turning Radius Infeasibility) We compute the turning radius along the predicted trajectories using two approaches: one that uses the predicted yaw output from the model (TRI-h), and the other that doesn't require yaw predictions and instead uses the circumradius constituting three consecutive waypoints (TRI-c). If the radius is less than a certain threshold ? , it is treated as a violation. We set this threshold as the approximate minimum turning radius threshold for a midsize sedan, ? = 3.5m. Note that a model that simply predicts a constant heading can achieve a TRI-h rate of zero, hence we also compute inconsistencies between turning radius suggested by the coordinates and the predicted headings (TRI-hc). TRI-hc inconsistency is true when the difference in heading based on circumradius from waypoints and predicted headings is greater than 0.05 radians at any time step in a trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MultiPath baseline</head><p>As our work evolved from MultiPath, we include a reference MultiPath model where the input and backbone are faithful to the original paper <ref type="bibr" target="#b44">[45]</ref> for a point of comparison, with a few minor differences. Specifically, we use a top-down rendering of the scene as before, but now employ a splat rendering <ref type="bibr" target="#b54">[55]</ref> approach for rasterization, in which we sample points uniformly from scene elements and do an orthographic projection. This is a simpler, sparse form of rendering, which doesn't employ anti-aliasing, but is efficient and straightforward to implement in TensorFlow and run as part of the model compute graph on hardware accelerators (GPU/TPU).</p><p>As in the original paper, we use a grid of 400 ? 400 cells, with grid cell physical dimension of 0.2m ? 0.2m, thus a total field-of-view of 80m centered around the AV sensing vehicle in WOMD, with a ResNet18 backbone <ref type="bibr" target="#b24">[25]</ref>. We use 128 static anchors obtained via k-means, which are shared among all agent types (vehicles, pedestrians, cyslists) for simplicity. <ref type="figure" target="#fig_0">Figure 10</ref> illustrates this model's inputs and architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">External benchmark results</head><p>On Argoverse, MultiPath++ achieves top-5 performance on most metrics ( <ref type="table">Table 2</ref>). Our technique is ranked 1 st on all metrics on Waymo Open Motion Dataset <ref type="bibr" target="#b17">[18]</ref>  <ref type="table">(Table 3</ref>).</p><p>The tested model is based on the best configuration in <ref type="table" target="#tab_6">Table 4</ref>, where the outputs from multiple ensemble heads are aggregated as described in Section 4.</p><p>On WOMD, we also see that the original MultiPath model, even with the refinement of learned anchors and ensembling, is outperformed by more recent methods. It is interesting to note that MultiPath is the best performing top-down scene-centric model employing a CNN; every known method which outranks it uses sparse representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Qualitative Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study</head><p>In this section we evaluate our design choices through an ablation study. <ref type="table" target="#tab_6">Table 4</ref> summarizes ablation results. In the following subsections we discuss how our architecture choices affect the model performance.   <ref type="table">Table 3</ref>:</p><p>Comparison with published state-of-the-art methods on WOMD public leaderboard.</p><p>k is the number of trajectories and t is the trajectory time horizon. ? Rank on the public leaderboard https://waymo.com/open/challenges/2021/motion-prediction/ as of November 12, 2021, which is sorted by mAP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Set Functions</head><p>Recall that MultiPath++ uses two types of set functions. Invariant set functions are used to encode a set of elements (e.g. agents, roadgraph segments) into a single feature vector. Equivariant set functions are used to convert the set of learned anchors, together with the encoded feature vector as a context, into a corresponding set of trajectories with likelihoods.</p><p>We use multi-context gating to represent both types of functions. We experimented with other representations of set functions:</p><p>? MLP+MaxPool: In this experiment, we replace the multi-context gating (MCG) road network encoder with a MLP+MaxPool applied on points rather than polylines, inspired by PointNet <ref type="bibr" target="#b37">[38]</ref>. We use a 5 layer deep MLP and RELU activations. ? Equivariant DeepSet <ref type="bibr" target="#b50">[51]</ref>: The equivariant set function is represented as a series of blocks, each involving an element-wise transformation followed by pooling to compute the context. Unlike MCG, it does not use gating (pointwise multiplication) between set elements and the context vector. Instead, a linear transformation of the context is added to each element. We use a DeepSet of 5 blocks in the predictor. ? Transformers <ref type="bibr" target="#b28">[29]</ref>: We replace the gating mechanism (element-wise multiplication) on polylines with selfattention. For decoding, we used cross attention where the queries are the learned embeddings and the keys are the various encoder features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Trajectory representation</head><p>As mentioned in Section 3.6, we experiment with predicting polynomial coefficients for the trajectory, as well predicting kinematic control signals (acceleration and heading change rate). We found that polynomial representations hurt performance, counter to conclusions made in PLOP <ref type="bibr" target="#b3">[4]</ref>, where they demonstrated improvements over the then state of the art on PRECOG <ref type="bibr" target="#b42">[43]</ref> and nuScenes <ref type="bibr" target="#b6">[7]</ref> using polynomials to represent output trajectories. Furthermore, in the PLOP datasets, we need to predict 4s into the future which is much shorter than our prediction horizon of 10s. For such short futures, polynomial representations are more suitable. In our case, we do not see much gains from using the polynomial representation, possibly due to the larger dataset size and longer-term prediction horizon.</p><p>The controls-based output works better in distance metrics than a polynomial representations, which suggests it is a more beneficial and domain-specific form of inductive bias. Overall, our results suggest that the simple sequence of raw coordinates trajectory representation works best for distance-based metrics. However, these unconstrained representations have a non-trivial rate of kinematic infeasibility (TRI-x metrics in <ref type="table" target="#tab_6">Table 4</ref>). Kinematic feasibility and consistency between headings and positions is crucial in practice when such behavior models are used for planning and controls of a real-world robot, an issue that is not captured by public benchmark metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.3">Ensembling</head><p>We explore ensembling, producing an over-complete set of trajectories that is then summarized using the aggregation proposed in Section 4, as well as their combination. The number of ensembles is denoted by E and the number of trajecctories per ensemble is denoted by L. Finally we aggregate the E ? L trajectories to M = 6 which is the required number of trajectories for the WOMD submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.4">Anchor representation</head><p>We explore learned and kmeans based anchor representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Discussion</head><p>First, we remark that MultiPath++ is a significant improvement over its predecessor MultiPath, as seen in <ref type="table" target="#tab_6">Tables 3  and 4</ref>. As discussed in this paper, they differ in many design dimensions, the primary being the change from a dense top-down raster representation to a sparse, element-based representation with agent-centric coordinate systems. Other design choices are validated in isolation in the following discussion.</p><p>We find that MLP+MaxPool performs the worst among all set function variants as expected due to limited capacity. DeepSet is able to outperform MLP+MaxPool. Also increasing the depth of the MCG gives consistently better results owing to effective increase in capacity and flow of information across skip connections. We get the best performance by increasing the depth of the MCG to 5 layers.</p><p>We find that learning anchors ("Learned anchors") is more effective than using a set of anchors obtained a priori via k-means. This runs counter to the original findings in the MultiPath paper <ref type="bibr" target="#b44">[45]</ref> that anchor-free models suffer from mode  denotes the reference configuration: road encoding, state history encoding and interaction encoding as described in Section 3. "n/a" denotes a model that does not predict heading. collapse. The difference could possibly be due to the richer and more structured inputs, improved model architecture, and larger batch sizes in MultiPath++. We leave more detailed ablations on this issue between the two approaches to future work. t We compare the baseline of directly outputting a single head with 6 trajectories (E = 1, L = 6), to training 5 ensemble heads (E = 5, L = 6). We see that ensembling significantly improves most metrics, and particularly minDE, for which this combination is best. We also train a model with a single head that outputs 64 trajectories, followed by our aggregation method that reduces them to 6 (E = 1, L = 64). Compared to our initial baseline, this model significantly improves M R and AU C that require diverse predictions, but regresses the average trajectory distance metrics minDE, and even minADE a little bit. This suggests that the different metrics pose different solution requirements. As expected, our aggregation criterion is well suited to preserving diversity, while straight-up ensembling is better at capturing the average distribution. Finally, our experiment (E = 5, L = 64) with more ensemble heads and more predictions per ensemble combines the strengths of both techniques, obtaining a strictly superior performance in all metrics compared to the baseline.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Conclusion</head><p>We proposed a novel behavior prediction system, MultiPath++, by carefully considering choices for input representation and encoding, fusing encodings, and representing the output distribution. We demonstrated state-of-the-art results on popular benchmarks for behavior prediction. Furthermore, we surveyed existing methods, analyzed our approach empirically, and provided practical insights for the research community. In particular, we showed the importance of sparse encoding, efficient fusion methods, control-based methods, and learned anchors. Finally, we provided a practical guide for various tricks used for training and inference to improve robustness, increase diversity, handle missing data, and ensure fast convergence during training. expectations (integrations) in the above EM updates are hard to compute in closed form. Instead we employ the approximation for any function g(x) In other words, we assume that the posterior probability of any output cluster only depends on the mean of the overcomplete cluster centroid inside the expectation. This approximation is reasonable since most samples drawn from the distribution would be concentrated around the mean. Furthermore as we increase the number of cluster centroids in the overcomplete representation, the variance within each overcomplete cluster centroid becomes smaller yielding more focus around the mean. The set of updates can now be solved in closed form as follows:</p><formula xml:id="formula_17">q h ? M i=1 q i p(h|? i ;?) ? h ? 1 q h M i=1 q i p(h|? i ;?)x ? h ? 1 q h M i=1 q i p(h|? i ;?) ? i + (? i ?? h )(? i ?? h ) T</formula><p>Since EM is a local optimization method, careful initialization of GMM parameters is important. Our initialization criterion of GMM centroids is to maximize the probability that future point lies within ? distance from at least one centroid:? 1:M = argmax</p><formula xml:id="formula_18">? 1:M M i=1 q i max ??? 1:M I ( ? i ?? 2 ? ? )<label>(33)</label></formula><p>Unfortunately, directly optimizing (33) is NP-hard. So instead, we select an M -sized subset of ? 1:M in a greedy fashion to maximize (33) 6 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Multipath baseline system diagram</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>MultiPath++ Model Architecture. MCG denotes Multi-Context Gating, described in Section 3. Blocks in red highlight portions of the model with learned parameters. Dotted inputs to the MCG denotes context features. Each of the encoder MCG outputs aggregated embeddings (one per agent) as shown by dotted arrows. On the other hand, the predictor MCG outputs one embedding per trajectory per agent</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left: Context gating (CG) block diagram. Right: 3 CG blocks stacked together, with running-average skip-connections (shown as components labeled "?"). See Section 3.2 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A comparison of the element relationship graph for cross-attention and CG. In cross-attention, each element s i aggregates information from c 1:m . In CG, c 1:m summarized with a single context vector c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Integrating control-signal to positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>shows examples of Multipath++ on WOMD scenes. Figure 5 shows examples of Multipath++ on Argoverse scenes. These examples show the ability of MultiPath++ to handle different road layouts and agent interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Examples of MultiPath++ predictions for 8 seconds in WOMD scenes. Hue indicates time horizon while transparency indicates predicted probability. Rectangles indicate vehicles while small squares indicate pedestrians. (a): A four-way intersection involving multiple interactions. For example, car A is predicted to yield for car B. (b &amp; c): Narrow road interaction. Car A is predicted to yield for car B and then nudge around the parking car, designated by the arrow. (d &amp; e): Interaction between two vehicles at an intersection where can A is predicted to yield for car B or make a right-turn behind. After car B passes, car A can make a left turn. Also, note the bimodal prediction of the pedestrian that is located at the corner. (f and g): Predictions in a parking lot and atypical roadgraph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Examples of MultiPath++ predictions for 8 seconds in Argoverse scenes, showing the ability to follow different lane geometries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results showing the effect of control-based trajectory representation. Top row: MultiPath++ with state-based trajectories. Bottom row: MultiPath++ with control-based trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Example of improved diversity due to aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Illustration of weeding out unrealistic trajectories after aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Illustration of improved lane diversity: The model is able to predict the agent going to multiple lanes after aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>E</head><label></label><figDesc>x?N (x;?,?) g(x)p(h|x;?) ? E x?N (x;?,?) g(x)p(h|?;?) = p(h|?;?)E x?N (x;?,?) [g(x)] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 visualizesFigure 10 :</head><label>1010</label><figDesc>the splat rendering rasterization of input points, and the backbone and output architecture. See Section 5.3 for details. Original MultiPath architecture, with details described in Section 5.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Method Road Enc. Motion Enc.</figDesc><table><row><cell></cell><cell></cell><cell>Interactions</cell><cell cols="2">Decoder Output</cell><cell>Trajectory Distribution</cell></row><row><cell>Jean [34] -</cell><cell>LSTM</cell><cell>attention</cell><cell>LSTM</cell><cell>states</cell><cell>GMM</cell></row><row><cell>TNT [54] polyline</cell><cell>polyline</cell><cell cols="2">maxpool, attention MLP</cell><cell>states</cell><cell>Weighted set</cell></row><row><cell>LaneGCN [32] GNN</cell><cell>1D conv</cell><cell>GNN</cell><cell>MLP</cell><cell>states</cell><cell>Weighted set</cell></row><row><cell>WIMP [28] polyline</cell><cell>LSTM</cell><cell>GNN+attention</cell><cell>LSTM</cell><cell>states</cell><cell>GMM</cell></row><row><cell>VectorNet [20] polyline</cell><cell>polyline</cell><cell cols="2">maxpool, attention MLP</cell><cell>states</cell><cell>Single traj.</cell></row><row><cell>SceneTransformer [35] polyline</cell><cell>attention</cell><cell>attention</cell><cell cols="2">attention states</cell><cell>Weighted set</cell></row><row><cell>GOHOME [21] GNN</cell><cell cols="2">1D conv + GRU GNN</cell><cell>MLP</cell><cell>states</cell><cell>heatmap</cell></row><row><cell>MP3 [11] raster</cell><cell>raster</cell><cell>conv</cell><cell>conv</cell><cell cols="2">cost function Weighted samples</cell></row><row><cell>CoverNet [37] raster</cell><cell>raster</cell><cell>conv</cell><cell>lookup</cell><cell>states</cell><cell>GMM w/ dynamic anchors</cell></row><row><cell>DESIRE [30] raster</cell><cell>GRU</cell><cell>spatial pooling</cell><cell>GRU</cell><cell>states</cell><cell>Samples</cell></row><row><cell>RoadRules [27] raster</cell><cell>raster</cell><cell>conv</cell><cell>LSTM</cell><cell>states</cell><cell>GMM</cell></row><row><cell>SocialLSTM [1] -</cell><cell>LSTM</cell><cell>spatial pooling</cell><cell>LSTM</cell><cell>states</cell><cell>Samples</cell></row><row><cell>SocialGan [24] -</cell><cell>LSTM</cell><cell>maxpool</cell><cell>LSTM</cell><cell>states</cell><cell>Samples</cell></row><row><cell>MFP [46] raster</cell><cell>GRU</cell><cell>RNNs+attention</cell><cell>GRU</cell><cell>states</cell><cell>Samples</cell></row><row><cell>MANTRA [33] raster</cell><cell>GRU</cell><cell>-</cell><cell>GRU</cell><cell>states</cell><cell>Samples</cell></row><row><cell>PRANK [2] raster</cell><cell>raster</cell><cell>conv</cell><cell>lookup</cell><cell>states</cell><cell>Weighted set</cell></row><row><cell>IntentNet [10] raster</cell><cell>raster</cell><cell>conv</cell><cell>conv</cell><cell>states</cell><cell>Single traj.</cell></row><row><cell>SpaGNN [9] raster</cell><cell>raster</cell><cell>GNN</cell><cell>MLP</cell><cell>state</cell><cell>Single traj.</cell></row><row><cell>Multimodal [14] raster</cell><cell>raster</cell><cell>conv</cell><cell>conv</cell><cell>states</cell><cell>Weighted set</cell></row><row><cell>PLOP [5] raster</cell><cell>LSTM</cell><cell>conv</cell><cell>MLP</cell><cell>state poly</cell><cell>GMM</cell></row><row><cell>Precog [41] raster</cell><cell>GRU</cell><cell>multi-agent sim.</cell><cell>GRU</cell><cell>motion</cell><cell>Samples</cell></row><row><cell>R2P2 [40] raster</cell><cell>GRU</cell><cell>-</cell><cell>GRU</cell><cell>motion</cell><cell>Samples</cell></row><row><cell>HYU_ACE [36] raster</cell><cell>LSTM</cell><cell>attn</cell><cell>LSTM</cell><cell>motion</cell><cell>Samples</cell></row><row><cell>Trajectron++[44] raster</cell><cell>LSTM</cell><cell>RNNs+attention</cell><cell>GRU</cell><cell>controls</cell><cell>GMM</cell></row><row><cell>DKM [13] raster</cell><cell>raster</cell><cell>conv</cell><cell>conv</cell><cell>controls</cell><cell>Weighted set</cell></row><row><cell>MultiPath [45] raster</cell><cell>raster</cell><cell>conv</cell><cell>MLP</cell><cell>states</cell><cell>GMM w/ static anchors</cell></row><row><cell>MultiPath++ polyline</cell><cell>LSTM</cell><cell>RNNs+maxpool</cell><cell>MLP</cell><cell>control poly</cell><cell>GMM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>, motion history</cell></row></table><note>A survey of recent work in behavior prediction, categorized by choice of road encoding</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Input Output Encoder Concat Predictor</head><label></label><figDesc></figDesc><table><row><cell>Agent state history</cell><cell>Agent history encoder</cell><cell>History MCG encoder</cell><cell>Classification head</cell><cell>Trajectory likelihoods</cell></row><row><cell>AV state history</cell><cell>Interaction encoder</cell><cell>Interaction MCG encoder</cell><cell>MCG predictor</cell><cell>Gaussian mixture model</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Regression</cell><cell>Trajectory means</cell></row><row><cell></cell><cell></cell><cell>Concat</cell><cell>head</cell><cell>and covariances</cell></row><row><cell>Roadgraph polylines</cell><cell>Polyline encoder</cell><cell>Roadgraph MCG encoder</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Argoverse leaderboard (k = 6, d = 2m, t = 3s) Rank</figDesc><table><row><cell></cell><cell></cell><cell cols="2">? brier-minDE minFDE</cell><cell>MR</cell><cell>minADE</cell></row><row><cell>LaneGCN [32]</cell><cell>50</cell><cell>2.059</cell><cell>1.364</cell><cell>0.163</cell><cell>0.868</cell></row><row><cell>DenseTNT [23]</cell><cell>23</cell><cell>1.976</cell><cell>1.282</cell><cell>0.126</cell><cell>0.882</cell></row><row><cell>HOME + GOHOME [21]</cell><cell>10</cell><cell>1.860</cell><cell>1.292</cell><cell>0.085</cell><cell>0.890</cell></row><row><cell>TPCN++ [49]</cell><cell>5</cell><cell>1.796</cell><cell>1.168</cell><cell>0.116</cell><cell>0.780</cell></row><row><cell>MultiPath++ (ours)</cell><cell>4</cell><cell>1.793</cell><cell>1.214</cell><cell>0.132</cell><cell>0.790</cell></row><row><cell>QCraft Blue Team</cell><cell>1</cell><cell>1.757</cell><cell>1.214</cell><cell>0.114</cell><cell>0.801</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Model ablation on WOMD validatation set. Metrics are parameterized by k = 6, t = 8s, and d = 2m.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://waymo.com/open/challenges/2021/motion-prediction/ 3 https://eval.ai/web/challenges/challenge-page/454/leaderboard/1279</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Since the explicit heading is missing in Argoverse data, we use the last two time steps to get the current orientation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For each dataset, we report the results of our model against published results of publicly available models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Note that this subset selection problem is submodular, which means that a greedy method is guranteed to achieve at least 1 ? 1/e of the optimal subset value.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details and Derivation of Aggregation Algorithm</head><p>By having an overcomplete trajectory representation that is later aggregated into a fixed small number of trajectories, we attempt to address two kinds of uncertainties in the data:</p><p>? Aleatoric uncertainty: This is a natural variation in the data. For example an agent can either take a left or right turn or change lanes, etc given the same context information. This level of ambiguity cannot be resolved by increasing the model capacity, but rather the model needs to predict calibrated probabilities for these outcomes.</p><p>Despite the theoretical possibility of modeling these variations using a small number of output trajectories directly, there are several challenges in learning. Some examples include mode collapse and failure to model these variations due to limited model capacity. Training the model to produce an overcomplete representation forces the model to output a diverse distribution of trajectories and could make it more resistant to mode collapse. Following this up with greedy iterative trajectory aggregation enhances diversity in the final output.</p><p>? Epistemic uncertainty: This is the variation across model outputs, which typically indicates the model's failure to capture certain aspects of the scene or input features. Such variations could occur if some models are poorly trained or haven't seen a particular slice of the data. By doing model ensembling, we attempt to reduce this uncertainty.</p><p>For ease of exposition, we assume each to trajectory to be composed of a single time point; the same computations are applied to each time step in a future sequence. The output ? is a Gaussian mixture model (GMM) distribution with M modes on the future position:</p><p>We formulate the aggregation as obtaining an M -mode GMM? which minimizes the KL-divergence D KL (?||?). This is equivalent to maximizing the expected log likelihood p(x, y;?) of a sample point (x, y) drawn from the overcomplete distribution ?:</p><p>Assuming the overcomplete distribution approximates the real distribution, this is roughly equivalent to fitting the compact distribution to the real data, but with the added benefits described above. Directly maximizing <ref type="formula">(21)</ref> is intractable.</p><p>Denoting the hidden variable h to be a mixture in the compact representation, we may write: </p><p>Thus</p><p>The right hand side is called the Q function in the EM algorithm. Maximizing the Q function with respect to? ensures that the likelihood increases at least as much when we update the parameters to? . Noting that p(h, x;? ) = p(h;? )p(x;? |h) =q h N (x ?? h ,? h ) and factoring out the terms independent of? , we find the update that maximizes the lower bound to b?</p><p>The second equation follows from the fact that the overcomplete distribution ? is a mixture of M Gaussians. The updates can be solved as follows.</p><p>where p(h|x;?) is the posterior probability that a given sample x is sampled from the h th component of the mixture model specified by? (here we use the previous estimate for?). This can be computed as:</p><p>Notice the resemblence with standard GMM, except where p(x; ? i , ? i ) is a dirac delta function in the standard setting (since the input data in standard GMM is a set of points instead of a distribution). Unlike standard GMM, these</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social LSTM: Human Trajectory Prediction in Crowded Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Alexandre &amp;quot;alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuriy</forename><surname>Biktairov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Stebelev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Rudenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleh</forename><surname>Shliazhko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Yangel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12007</idno>
		<title level="m">Prank: motion prediction based on ranking</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Plop: Probabilistic polynomial objects trajectory planning for autonomous driving. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Buhet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Perrotton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Buhet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plop</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08744</idno>
		<title level="m">Probabilistic polynomial objects trajectory planning for autonomous driving</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11027</idno>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spagnn: Spatially-aware graph neural networks for relational behavior forecasting from sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cole</forename><surname>Gulino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Intentnet: Learning to predict intention from raw sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mp3: A unified model to map, perceive, predict and plan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14403" to="14412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Fang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep kinematic models for kinematically feasible vehicle trajectory predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Chieh</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Han</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10563" to="10569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal trajectory predictions for autonomous driving using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Chieh</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Han</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Kuo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04803</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabeek</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoey</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<title level="m">Jonathon Shlens, and Dragomir Anguelov. Large scale interactive motion forecasting for autonomous driving : The waymo open motion dataset</title>
		<imprint>
			<publisher>Alexander McCauley</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: Data mining, inference, and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jerome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>springer open</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VectorNet: Encoding hd maps and agent dynamics from vectorized representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gohome: Graph-oriented heatmap output for future motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Tsishkou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Moutarde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01827</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">HOME: heatmap output for future motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Tsishkou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Moutarde</surname></persName>
		</author>
		<idno>abs/2105.10968</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Densetnt: Waymo open dataset motion prediction challenge 1st place solution. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Social GAN: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent attention networks for structured online maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrinidhi</forename><surname>Kowshika Lakshmikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3417" to="3426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rules of the road: Predicting driving behavior with a convolutional model of semantic interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">What-if motion prediction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhesh</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DESIRE: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The garden of forking paths: Towards multi-future trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning lane graph representations for motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13732</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mantra: Memory augmented networks for multiple trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Becattini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7143" to="7152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-head attention for joint multi-modal vehicle motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Zoghby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Sandou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beauvois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Scene transformer: A unified multi-task model for behavior prediction and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Tien Lewis</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno>abs/2106.08417</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyubok</forename><surname>Seong Hyeon Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Jadhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03212</idno>
		<title level="m">Diverse and admissible trajectory forecasting through multimodal context understanding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Phan-Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><forename type="middle">Corina</forename><surname>Grigore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freddy</forename><forename type="middle">A</forename><surname>Boulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Wolff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10298</idno>
		<title level="m">CoverNet: Multimodal behavior prediction using trajectory sets</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Agent prioritization for autonomous navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Refaat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2060" to="2067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">R2P2: A reparameterized pushforward policy for diverse, precise generative path forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">PRECOG: Prediction conditioned on goals in visual multi-agent settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Precog: Prediction conditioned on goals in visual multi-agent settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">PRECOG: prediction conditioned on goals in visual multi-agent settings. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Trajectron++: Multi-agent generative trajectory forecasting with heterogeneous data for control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03093</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MultiPATH: Multiple probabilistic anchor trajectory hypotheses for behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Robot Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple futures prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Yichuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Identifying driver interactions via conditional behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><forename type="middle">I</forename><surname>Tolstaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlton</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation, ICRA 2021</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06-05" />
			<biblScope unit="page" from="3473" to="3479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tpcn: Temporal point cloud networks for motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11318" to="11327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Diverse trajectory forecasting with determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. End-to-end interpretable neural motion planner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubrey</forename><surname>Clausse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>K?mmerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>K?nigshof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03088</idno>
		<title level="m">Arnaud de La Fortelle, and Masayoshi Tomizuka. INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08294</idno>
		<title level="m">Target-driven trajectory prediction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Jeroen Van Baar, and Markus Gross. Surface splatting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="371" to="378" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
