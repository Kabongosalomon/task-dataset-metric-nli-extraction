<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative and Discriminative Voxel Modeling with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Physical Sciences Heriot-Watt University Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
							<email>t.lim@hw.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Physical Sciences Heriot-Watt University Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
							<email>j.m.ritchie@hw.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Physical Sciences Heriot-Watt University Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Weston</surname></persName>
							<email>nick.weston@renishaw.com</email>
							<affiliation key="aff1">
								<address>
									<addrLine>Renishaw plc Research Ave, North Edinburgh</addrLine>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generative and Discriminative Voxel Modeling with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification.</p><p>Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5% relative improvement in the state of the art for object classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Voxel-Based Variational Autoencoders</head><p>Interpolating between binary grids permits no obvious mathematical interpretation; if we wish to learn how voxellated objects relate to one another, we require a model capable of reasoning in an</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D data offer computer vision systems a rich view of the world, but also pose a unique set of challenges, particularly in applications where understanding the surrounding environment is critical. In particular, data such as a point cloud extracted from an RGB-D image or a polygonal mesh are not guaranteed to be arranged in a regular grid, making them unsuitable for use with high-performance machine learning algorithms such as Convolutional Neural Networks (ConvNets). Deep ConvNets are currently used in state-of-the-art systems for a number of tasks in computer vision, and to date, the three most recent systems to achieve state-of-the-art performance in 3D object recognition on the ModelNet40 <ref type="bibr" target="#b0">[1]</ref> benchmark have made use of 2D ConvNets pre-trained on ImageNet <ref type="bibr" target="#b1">[2]</ref>[3] <ref type="bibr" target="#b3">[4]</ref> and evaluated using multiple rendered object views.</p><p>Voxel models, wherein object shape is represented as a binary occupancy grid, provide a representation suitable for use with ConvNets, but present a number of difficulties. The addition of a third spatial dimension in the regular grid comes with a corresponding computational cost, and the curse of dimensionality is a central issue, limiting the available resolution of the voxel grid. Low resolution grids make it difficult to differentiate between similar shapes,and toss some of the texture information available in 2D renderings of equivalent dimesnionality. Shallow 3D ConvNets have been evaluated on the ModelNet benchmark <ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b5">[6]</ref>, but are generally outperformed by multi-view 2D ConvNets.</p><p>Despite these challenges, we posit that deep ConvNets are viable for use in modeling voxel-based 3D objects for both generative and discriminative tasks. In this work, we present deep ConvNet architectures for both generative and discriminative voxel modeling, and explore issues specific to voxel-based representations. Our generative methods display high fidelity shape interpolation, and our discriminative methods outperform the current state of the art by a relative 51.5% and 53.2% on the ModelNet40 and ModelNet10 benchmarks. abstract feature space that captures the salient factors of variation. For this work, we select the Variational Autoencoder (VAE) <ref type="bibr" target="#b6">[7]</ref>, a probabilistic framework that learns both an inference network to map from an input space to a set of descriptive latent variables, and a generative network that maps from the latent space back to the input space. By training a network to infer the latent variables which describe the underlying factors of variation between objects, we gain the ability to smoothly transition between objects by interpolating between each object's latent description and reconstructing using the decoder network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>Our model, implemented in Theano <ref type="bibr" target="#b7">[8]</ref> with Lasagne, 1 comprises an encoder network, the latent layer, and a decoder network, as displayed in <ref type="figure" target="#fig_0">Figure 1</ref>. The encoder network consists of 4 convolutional layers and a fully connected layer, followed by a linear projection from the fully connected layer to the latent layer. The decoder network has an identical, but inverted, architecture, and its weights are not tied to the encoder's. Each convolutional layer has a bank of 3x3x3 filters, starting with 8 filters in the layer furthest from the latents and doubling at each subsequent layer.</p><p>All layers use the exponential linear unit <ref type="bibr" target="#b8">[9]</ref> nonlinearity, with the exception of the final layer, which uses a sigmoid nonlinearity. The output of each element of the final layer can be interpreted as the predicted probability that a voxel is present at a given location. Downsampling in the encoder network is accomplished via strided convolutions (as opposed to pooling) in every second layer. Upsampling in the decoder network is accomplished via fractionally strided convolutions, implemented as the gradient of an equivalent strided convolution <ref type="bibr" target="#b9">[10]</ref>, in every second layer.</p><p>The network is initialized with Glorot Initialization <ref type="bibr" target="#b10">[11]</ref>, and all but the output layer are Batch Normalized <ref type="bibr" target="#b11">[12]</ref>. The variance and mean parameters of the latent layer are individually Batch Normalized, such that the output of the latent layer during training is still stochastic under the VAE parameterization trick.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Loss Function</head><p>The loss function consists of the KL divergence prior on the latents, L2 weight regularization, and the reconstruction error, for which we use a specialized form of Binary Cross-Entropy (BCE). The standard BCE loss is:</p><formula xml:id="formula_0">L = ?t log(o) ? (1 ? t) log(1 ? o)</formula><p>Where t is the target value in {0,1} and o is the output of the network in (0,1) at each output element. The derivative of the BCE with respect to o severely diminishes as o approaches t, which can result in vanishing gradients during training. Additionally, the standard BCE weights false positives and false negatives equally; because over 95% of the voxel grid in the training data is empty, the network can confidently plunge into a local optimum of the standard BCE by outputting all negatives.</p><p>We make two key modifications to the BCE to improve training. First, we change the range of the target and output to {-1,2} and [0.1,1),respectively. This change increases the magnitude of the loss gradient throughout the domain of o, reducing the probability of vanishing gradients. Second, we add a hyperparameter ? which weights the relative importance of false positives against false negatives: During training, we set ? to 0.97, strongly penalizing false negatives while reducing the penalty for false positives. Setting ? too high results in noisy reconstructions, while setting ? too low results in reconstructions which neglect salient object details and structure.</p><formula xml:id="formula_1">L = ?? t log(o) ? (1 ? ?) (1 ? t) log(1 ? o)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>The model is trained using stochastic gradient descent with Nesterov momentum <ref type="bibr" target="#b12">[13]</ref> for 100 epochs, or until the reconstruction error on a held-out validation set bottoms out. The learning rate is set to 0.0001 for the first epoch, then increased to 0.001. The data is augmented by adding random translations and horizontal flips to each training example, as in <ref type="bibr" target="#b4">[5]</ref>, then training on one noisy and one uncorrupted copy of each instance, randomly shuffled. By training the network to reconstruct both corrupted and uncorrupted data, we force it to learn invariance to small structural variations.</p><p>We first validate our modification to the BCE by comparing the validation errors of two identically initialized networks, one trained with the standard BCE, and one trained with our modification. The reconstruction error is plotted against training epochs in <ref type="figure" target="#fig_1">Figure 2</ref>(a). Interestingly, we note that the validation error is lower than the training error for this particular training run. We experiment with a number of different model architectures and training regimes before converging on the final method detailed above. In particular, we experiment with augmenting the training objective by adding a 10-unit fully connected softmax layer for classification in parallel with latent estimation, as well as a denoising objective, neither of which result in any observable performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">User Interface</head><p>We present a graphical user interface modeled after <ref type="bibr" target="#b13">[14]</ref>. The interface, implemented using VTK <ref type="bibr" target="#b14">[15]</ref>, allows the user to drag a center object that interpolates between up to four different objects, and supports class-unconditional random shape generation. The interpolant endpoint models are randomly selected from the ModelNet10 test set at runtime, and both inference and reconstruction run in real time on a laptop with a GT730m graphics card. <ref type="figure" target="#fig_1">Figure 2</ref>(b) shows a screenshot of the interface, and a video of the interface in action is available online. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Voxel-Based Convnets for Classification</head><p>Voxel-based ConvNets were first applied to 3D object recognition in Voxnet <ref type="bibr" target="#b4">[5]</ref>, a shallow volumetric ConvNet architecture, and were also used in ORION <ref type="bibr" target="#b5">[6]</ref>, wherein the classification task was augmented with an orientation estimation task. A recent extension, FusionNets <ref type="bibr" target="#b3">[4]</ref>, combines ConvNets trained on voxellated models and pre-trained ConvNets fine-tuned on rendered object views. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>Our model is designed in line with approaches used for high performance 2D ConvNets for object classification. Key to our approach is the use of Inception-style modules <ref type="bibr" target="#b15">[16]</ref>, Batch Normalization <ref type="bibr" target="#b11">[12]</ref>, Residual connections with pre-activation [17] <ref type="bibr" target="#b17">[18]</ref> and stochastic network depth <ref type="bibr" target="#b18">[19]</ref>. In contrast to previous 3D ConvNet approaches which used shallow networks, we train networks with up to 45 layers to take advantage of the increased expressivity that comes with model depth. Compared to FusionNets <ref type="bibr" target="#b3">[4]</ref>, our model requires significantly fewer parameters (18M as opposed to 118M for a full FusionNet) and fewer object views (12 or 24, compared to 60). Code to train and test our models is publicly available. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Voxception</head><p>After initial tests with vanilla ConvNets, we adopted a simple Inception-style architecture. The intuition behind the design was to maximize the number of possible "pathways" for information to propagate through the network, while still maintaining simplicity and efficiency.</p><p>For non-downsampling layers <ref type="figure" target="#fig_2">(Figure 3</ref>, left) , we concatenate equal numbers of 1x1x1 and 3x3x3 filters, allowing the network to choose between taking a weighted average of the featuremaps in the previous layer (i.e. by heavily weighting the 1x1x1 convolutions) or focusing on spatial relationships (i.e. by heavily weighting the 3x3x3 filters). For downsampling layers <ref type="figure" target="#fig_2">(Figure 3</ref>, center), we stack 3x3x3 convolutions with strided pooling operations, using both max and average pooling, and concatenate those features with strided 3x3x3 and 1x1x1 convolutions. Our intent is for the downsampling layers to let the network learn the best relative weighting of the various downsampling methods, maximizing propagation of information while still producing a more compact representation.</p><p>Our final model is nine layers deep, with four Voxception blocks and three Voxception Downsample blocks, followed by two fully connected layers and a softmax nonlinearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Voxception-ResNet</head><p>The Voxception-ResNet (VRN) architecture is based on the ResNet architecture <ref type="bibr" target="#b16">[17]</ref>, but concatenates both the ResNet Bottleneck Block and the standard ResNet block into a single Inception <ref type="bibr" target="#b15">[16]</ref>-style block <ref type="figure" target="#fig_2">(Figure 3</ref>, right). To improve parameter efficiency, the early layers in each path of the block have half as many filters as the final layer. Downsampling is accomplished through Voxception-Downsample blocks, which we do not change in our ResNet model. We change the order of application of rectifying nonlinearities and Batch Normalization to obtain pre-activation blocks <ref type="bibr" target="#b17">[18]</ref>. Finally, we stochastically drop the non-residual paths of blocks <ref type="bibr" target="#b18">[19]</ref>, where the keep probability is linearly decreased from 1.0 in the first layer to 0.25 in the final VRN layer, and used as a weighting value instead of a drop probability at test time.</p><p>Our best-performing architecture is shown in <ref type="figure" target="#fig_3">Figure 4</ref>, and consists of an initial convolutional layer, four main units, each containing three stacked VRN blocks and a Voxception-Downsample block, a final convolution with a residual connection and keep probability of 0.5, then a global pooling layer and two fully-connected layers. The number of filters begins at 32, and is doubled at each downsampling block. The deepest path through the network is 45 layers (going through the 3-layer </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Augmentation and Training</head><p>Unless otherwise specified, all models were initialized using Orthogonal Initialization <ref type="bibr" target="#b19">[20]</ref>, Batch Normalized <ref type="bibr" target="#b11">[12]</ref>, and trained using Nesterov Momentum <ref type="bibr" target="#b12">[13]</ref> with a momentum value of 0.9. Other than the final softmax nonlinearity, Exponential Linear Units <ref type="bibr" target="#b8">[9]</ref> are used as activations throughout the network. We change the binary voxel range from {0,1} to {-1,5} to encourage the network to pay more attention to positive entries.</p><p>We experiment with multiple learning rate decay schemes and find that dividing the learning rate by a factor of 2 every time the validation loss bottoms out to be more effective than annealing at a constant rate after a set number of minibatches or epochs. Initial hyperparameter studies were validated using a held-out, class-balanced tenth of the training set, but for final evaluation the annealing schedule was fixed and the entire training set was used.</p><p>During each epoch, we train on two copies of each example, where one of the copies is randomly flipped about a horizontal axis and/or translated, as was done in VoxNet <ref type="bibr" target="#b4">[5]</ref>. We use a fixed random seed scheme to ensure that different training runs make use of identically augmented datasets. We use two versions of the training set, one with 12 rotations of each instance, and one with 24 rotations of each instance. For our best performing models, we warm up the network by training for twelve epochs on the 12-rotation training set, then anneal the learning rate and fine-tune on the 24-rotation training set. During testing, we measured predictions on a single view and averaged predictions across 24 rotated copies of each instance. We found this data augmentation to be essential for training deeper networks, especially Voxception-Resnet. We produce a simple ensemble by summing predictions from five VRN models and one Voxception model. A single training epoch, using a batch size of 50 and the full 24-rotation augmented dataset, takes around 6 hours on a single Titan X, and most models require around 6 days of training to converge.</p><p>We also experiment with treating the different rotations as separate channels for a single instance (analagous to RGB channels in a 2D image) but found that training a model to look at a single orientation and averaging predictions across rotated versions of an instance yielded better performance. We suspect that this is because, without otherwise changing the model architecture, the rotationchannel network must still pass information through equivalent representational bottlenecks, as opposed to being able to separately evaluate each rotation as individual instances. Additionally, we experimented with a variety of values for the range of the binary voxel grid, including an adaptive method wherein the numerical value of positive entries is equal to the 100 times the percentage of the grid occupied by the object, but did not find these methods to significantly alter performance.</p><p>We experimented with stacking 3x1x1,1x3x1,1x1x3 blocks in place of 3x3x3 convolutions, but found that this did not noticeably affect performance or training time. We tried to use the Adam and Adamax <ref type="bibr" target="#b20">[21]</ref> optimizers; although using these optimization schemes caused training error to bottom out very quickly, the validation error did not improve with the training error, suggesting that the model was rapidly overfitting to the training set.</p><p>We also found that aggressively downsampling early in the network by replacing the initial convolution with a strided convolution enabled us to train significantly deeper models, but that such networks underperformed their 45 layer counterparts. We theorize that placing an early representational bottleneck causes the network to toss features that it might otherwise learn to keep or propagate to deeper layers, placing an upper bound on performance, or that we simply did not provide sufficient data to train networks of such depth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Voxel-based VAE</head><p>The reconstruction accuracy of our fully-trained VAE, evaluated on the ModelNet10 test set, is displayed in <ref type="table" target="#tab_1">Table 1</ref>. The model attains a 99.39% true positive and 92.36% true negative reconstruction accuracy on the ModelNet10 test set, indicating that it learns to reconstruct with high fidelity, but tends to slightly overestimate the probability of a voxel being present. Reconstruction and interpolation examples are displayed in <ref type="figure" target="#fig_4">Figure 5</ref> alongside class-unconditional random samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Object Classification</head><p>The accuracy of our discriminative models is evaluated and compared against competing approaches in   The network achieves passable reconstruction accuracy, and learns to smoothly interpolate between arbitrary, previously unseen shapes. The network is additionally capable of generating random shapes with consistent structure, indicating that the learned latent space is successful in disentangling the factors of structural variation, though these new shapes.</p><p>The network performs well for dense objects, particularly thick dense objects such as sofas and toilets, but occasionally struggles to reconstruct objects with long, thin members, such as tables or chairs. We suspect these features are too small to activate in the receptive field of the appropriate latents, and are lost in favor of denser features which weigh more heavily in the loss function, and suggest imposing an additional "local" reconstruction function that measures reconstruction accuracy in subsets of the voxel grid, such that the network learns to reconstruct features regardless of how small they are relative to the entire object.</p><p>The network also struggles to reproduce crisp edges, preferring to output smooth, rounded edges. We posit that this is analogous to the way in which a vanilla 2D VAE will tend to output images with blurry edges rather than crisp edges to avoid overconfidently making incorrect predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Interpolation</head><p>The system is capable of smoothly interpolating between reconstructions, indicating that it learns a representation which captures the underlying factors of structural variation. For example, when interpolating between two objects of the same class but slightly different orientation, the model will make only minimal changes in the output during interpolation, rather than completely deconstructing and reconstructing the output (i.e. passing through the origin of the latent space).</p><p>When interpolating between drastically different objects, the interface exhibits a "flowing water" effect, wherein preexisting voxels will appear to smoothly shift between shapes, rather than appearing at random, as can be seen in <ref type="figure" target="#fig_4">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Sampling</head><p>Samples generated by our model are shown in the last row of <ref type="figure" target="#fig_4">Figure 5</ref>. Our samples consistently bear a semblance of structure, with few to no free-floating voxels, suggesting that the decoder network has learned to maintain output voxel connectivity regardless of the latent configuration. The major limitation of the VAE is that its generated samples do not, however, resemble real objects. We hypothesize that training a deeper, more expressive model on the ModelNet40 dataset, and augmenting the latent vector with a class-conditional vector, would enable the generation of objects which clearly belong to a particular class, but leave such an investigation to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Object Classification</head><p>Our VRN model takes advantage of the increased expressivity associated with its substantially increased depth to achieve significant improvements on the ModelNet classification benchmark.</p><p>We found that averaging predictions across rotations was critical to achieving top performance, but that even taking predictions from a single view resulted in passable 88.98% ModelNet40 accuracy, and even an ensemble of 2 models predicting on a single view achieves over 91% accuracy. We also found that ensembling with a mix of predictions averaged over 12 and 24 rotations resulted in even higher test performance on ModelNet40 (95.78%), but we suspect that this result is not general, and do not claim it with our main results.</p><p>Our best single model is competitive with, but does not outperform the previous state of the art on ModelNet10, ORION <ref type="bibr" target="#b5">[6]</ref>, which augments the classification task with a rotation estimation task. We suspect that our deeper models do not perform quite as well when trained on the smaller subset due to a lack of data, with only 3991 training instances in ModelNet10 compared to 9843 instances in ModelNet40. Additionally, ORION <ref type="bibr" target="#b5">[6]</ref> incorporates class-specific priors to determine precisely which rotations to train on, which we eschew in favor of generality.</p><p>Our (by no means novel) hypothesis that the upper bound on model depth is dependent on the amount of available data is consistent with our observations that significant data augmentation was required to train our 45-layer model. We also note that our highest performance on ModelNet10 came from models trained on ModelNet40, despite ModelNet40 not containing any additional ModelNet10 instances. This is interesting from a transfer learning perspective: by learning to distinguish between a wider variety of classes, the model learns to better discriminate between a given subset of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Suggestions for Future Work</head><p>We believe that there still remains plenty of low-hanging fruit to be gained by investigating deep ConvNets for 3D object classification, and provide several suggestions for improvement:</p><p>? Refine the voxel grid beyond 32x32x32 to increase spatial resolution and improve the available detail. ? Change the voxel grid to be real-valued based on the percentage of space occupied by the instance at each grid element. This could also be used with the VAE to allow it to represent more complex shapes by fitting a corner-connected polyhedron with volume equal to the predicted occupancy percentage. ? Experiment with more data augmentation, such as rotating about random axes (or just adding more rotations about the central axis), random crops, or random rescaling. ? Try out different Voxception architectures, downsampling methods, and activation functions.</p><p>Our experiments focused primarily on high-level network architecture, and there are likely more effective ways to compose a Voxception-ResNet block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a voxel-based Variational Autoencoder and a graphical user interface for exploring the latent space of 3D generative models, along with a voxel-based deep convolutional neural network for classification. Our methods take into account challenges specific to voxel representations, and demonstrate the viability of voxel representations in discriminative tasks by improving the state of the art on the ModelNet classification task by large margins.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>VAE Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of training regimes (a), user interface (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Voxception and Voxception-Resnet Blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Voxception-ResNet 45 Layer Architecture. DS are Voxception-Downsample blocks. section of the VRN block), and the shallowest path (assuming all droppable non-residual paths are dropped) is 8 layers deep.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Voxel-Based VAE Reconstructions, Interpolations, and Samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Our best single VRN model obtains 91.33% accuracy on the ModelNet40 test set, and 93.61% accuracy on the ModelNet10 test set, which are respectively better than any previous published results and competitive with the current state of the art. Our best ensemble of VRN models obtains state-of-the art 95.54% and 97.14% accuracy on both ModelNet subsets, improving the state of the art by a relative 51.5% and 53.2%, respectively. Our best VRN model obtains 88.98% accuracy when tested using a single view of the object. Our best ensemble of models trained solely on ModelNet10 obtains 94.71% accuracy, which is also better than any previously published result;</figDesc><table><row><cell>we report our ModelNet10 accuracy for models trained on ModelNet40 for consistency with previous</cell></row><row><cell>results[2][3][4].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Reconstruction Results.</figDesc><table><row><cell></cell><cell cols="2">Predicted: Predicted:</cell></row><row><cell></cell><cell cols="2">Positive Negative</cell></row><row><cell cols="2">Actual: Positive 99.39%</cell><cell>0.61%</cell></row><row><cell>Actual: Negative</cell><cell>7.64%</cell><cell>92.36%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification Results.</figDesc><table><row><cell>Model</cell><cell cols="2">ModelNet40 ModelNet10</cell></row><row><cell>Voxnet</cell><cell>83.00%</cell><cell>92.00%</cell></row><row><cell>MVCNN</cell><cell>90.10%</cell><cell>-</cell></row><row><cell>Pairwise</cell><cell>90.70%</cell><cell>92.80%</cell></row><row><cell>FusionNets</cell><cell>90.80%</cell><cell>93.11%</cell></row><row><cell>ORION</cell><cell>-</cell><cell>93.80%</cell></row><row><cell>Voxception</cell><cell>90.56%</cell><cell>93.28%</cell></row><row><cell>VRN (One-View)</cell><cell>88.98%</cell><cell>-</cell></row><row><cell>VRN</cell><cell>91.33%</cell><cell>93.61%</cell></row><row><cell>VRN Ensemble</cell><cell>95.54%</cell><cell>97.14%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Lasagne/Lasagne</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.youtube.com/watch?v=LtpU1yBStlU</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/ajbrock/Generative-and-Discriminative-Voxel-Modeling</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was made possible by grants and support from Renishaw plc and the Edinburgh Centre For Robotics. The work presented herein is also partially funded under the European H2020 Programme BEACONING project, Grant Agreement nr. 687676.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pairwise decomposition of image sequences for active multi-view recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05695</idno>
		<title level="m">Fusionnet: 3d object classification using multiple data representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03351</idno>
		<title level="m">Orientation-boosted voxel nets for 3d object recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Theano: A python framework for fast computation of mathematical expressions</title>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
	<note>The Theano Development Team</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A guide to convolution arithmetic for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07285</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS 2010</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Procedural modeling using autoencoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Asente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Visualization Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lorenson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Kitware, 4 edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09382</idno>
		<title level="m">Deep networks with stochastic depth</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
