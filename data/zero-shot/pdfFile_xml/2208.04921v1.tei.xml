<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TSRFormer: Table Structure Recognition with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Sun</surname></persName>
							<email>sunzheng2019@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chixiang</forename><surname>Ma</surname></persName>
							<email>chixiangma@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingze</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><forename type="middle">Jiao</forename><surname>Tong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wang</surname></persName>
							<email>wangjiawei@mail.ustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
							<email>lsun@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Huo</surname></persName>
							<email>qianghuo@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Aisa</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences &amp; CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">University of Science and Technology of China Hefei</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">University of Science and Technology of China Hefei</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TSRFormer: Table Structure Recognition with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>table structure recognition</term>
					<term>separation line regression</term>
					<term>two-stage DETR</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage DETR based separator prediction approach, dubbed Separator REgression TRansformer (SepRETR), to predict separation lines from table images directly. To make the two-stage DETR framework work efficiently and effectively for the separation line prediction task, we propose two improvements: 1) A prior-enhanced matching strategy to solve the slow convergence issue of DETR; 2) A new cross attention module to sample features from a high-resolution convolutional feature map directly so that high localization accuracy is achieved with low computational cost. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet and WTW. Furthermore, we have validated the robustness of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Tables offer a means to efficiently represent and communicate structured data in many scenarios like scientific publications, financial statements, invoices, web pages, etc. Due to the trend of digital transformation, automatic table structure recognition (TSR) has become an important research topic in document understanding and attracted the attention of many researchers. TSR aims to recognize the cellular structures of tables from table images by extracting the coordinates of cell boxes and row/column spanning information. This task is very challenging since tables may have complex structures, diverse styles and contents, and become geometrically distorted or even curved during an image capturing process.</p><p>Recently, deep learning based TSR methods, e.g., <ref type="bibr">[23, 26, 31, 34-36, 39, 42, 43, 45, 50, 54]</ref>, have made impressive progress towards recognizing undistorted tables with complex structures and diverse styles. However, these methods except Cycle-CenterNet <ref type="bibr" target="#b25">[26]</ref> cannot be directly applied to geometrically distorted or even curved tables, which appear often in camera-captured images. Although Cycle-CenterNet <ref type="bibr" target="#b25">[26]</ref> proposed an effective approach to parsing the structures of distorted bordered tables in wild complex scenes and achieved promising results on their WTW <ref type="bibr" target="#b25">[26]</ref> dataset, this work didn't take borderless tables into account. Thus, the more challenging problem of recognizing the structures of various geometrically distorted tables still lacks investigation.</p><p>In this paper, we propose a new TSR approach, called TSRFormer, to robustly recognizing the structures of both bordered and borderless distorted tables. TSRFormer contains two effective components: 1) A two-stage DETR <ref type="bibr" target="#b56">[56]</ref> based separator regression module to directly predict linear and curvilinear row/column separation lines from input table images; 2) A relation network based cell merging module to recover spanning cells by merging adjacent cells generated by intersecting row and column separators. Unlike previous split-and-merge based approaches (e.g., <ref type="bibr" target="#b44">[45]</ref>), we formulate separation line prediction as a line regression problem instead of an image segmentation problem and propose a new separator prediction approach, dubbed Separator REgression TRansformer (SepRETR), to predict separation lines from table images directly. In this way, our approach can get rid of heuristic mask-to-line modules and become more robust to distorted tables. Specifically, SepRETR predicts one reference point for each row/column separator first, then takes the features of these reference points as object queries and feeds them into a DETR <ref type="bibr" target="#b0">[1]</ref> decoder to regress the coordinates of their corresponding separation lines directly. To make the two-stage DETR framework work efficiently and effectively for the separation line prediction task, we propose two improvements further: 1) A prior-enhanced matching strategy to solve the slow convergence issue of DETR; 2) A new cross attention module to sample features from a high-resolution convolutional feature map directly so that high localization accuracy is achieved with low computational cost. With these new techniques, our TSRFormer has achieved state-of-the-art performance on several public TSR benchmarks, including SciTSR <ref type="bibr" target="#b1">[2]</ref>, PubTabNet <ref type="bibr" target="#b55">[55]</ref> and WTW <ref type="bibr" target="#b25">[26]</ref>. Furthermore, we have demonstrated the robustness of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Table Structure Recognition</head><p>Early TSR methods were mainly based on handcrafted features and heuristic rules (e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>), so they could only deal with simple table structures or specific data formats, such as PDF files. Later, some statistical machine learning based methods (e.g, <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b46">47]</ref>) were proposed to reduce the dependence on heuristic rules. However, these methods still made strong assumptions about table layouts and relied on handcrafted features, which limited their generalization ability. In recent years, many deep learning based approaches have emerged and outperformed these traditional methods significantly in terms of both accuracy and capability. These approaches can be roughly divided into three categories: row/column extraction based methods, image-to-markup generation based methods and bottom-up methods.</p><p>Row/column extraction based methods. These approaches leverage object detection or semantic segmentation methods to detect entire rows and columns first, then intersect them to form a grid of cells. DeepDeSRT <ref type="bibr" target="#b38">[39]</ref> first applied an FCN-based semantic segmentation method <ref type="bibr" target="#b24">[25]</ref> to table structure extraction. TableNet <ref type="bibr" target="#b30">[31]</ref> proposed an end-to-end FCN-based model to simultaneously detect tables and recognize table structures. However, these vanilla FCN based TSR methods are not robust to tables containing large blank spaces due to limited receptive fields. To alleviate this problem, methods like <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref> tried different context enhancement techniques, e.g., pooling features along rows and columns of pixels on some intermediate feature maps of FCN models or using sequential models like bi-directional gated recurrent unit networks (GRU), to improve row/column segmentation accuracy. Another group of approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref> treated TSR as an object detection problem and used some object detection methods to directly detect the bounding boxes of rows and columns. Among these methods, only SPLERGE <ref type="bibr" target="#b44">[45]</ref> can deal with spanning cells, which proposed to add a simple cell merging module after a row/column extraction module to recover spanning cells by merging adjacent cells. Later, several works were proposed to further improve the cell merging module. TGRNet <ref type="bibr" target="#b49">[50]</ref> designed a network to jointly predict the spatial locations and spanning information of table cells. SEM <ref type="bibr" target="#b53">[53]</ref> fused the features of each cell from both vision and text modalities. Raja et al. <ref type="bibr" target="#b36">[37]</ref> improved this "split-and-merge" paradigm by targeting row, column and cell detection as object detection tasks and forming rectilinear associations through a graph-based formulation for generating row/column spanning information. Different from this two-stage paradigm, Zou et al. <ref type="bibr" target="#b57">[57]</ref> proposed a one-stage approach to predicting the real row and column separators to handle spanning cells. Although these methods have achieved impressive performance on some previous benchmarks, e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b55">55]</ref>, they cannot handle distorted tables because they rely on an assumption that tables are axis-aligned. Our previous work, RobusTabNet <ref type="bibr" target="#b27">[28]</ref>, proposed a new split-and-merge based method by incorporating a spatial CNN module <ref type="bibr" target="#b31">[32]</ref> into an image segmentation based split module to improve its robustness to distorted tables, which makes this new TSR approach able to recognize distorted tables robustly to some extend. However, the performance of this approach is affected by a heuristic mask-to-line module, which struggles with some low-quality separator masks predicted by the split module.</p><p>Image-to-markup generation based methods. This type of methods treat TSR as an image-to-markup generation problem and adopt existing image-to-markup models to directly convert each source table image into target presentational markup that fully describes its structure and cell contents. Prior arts tried different image-to-markup models to to convert table images into LaTeX symbols <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref> or HTML sequences <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b55">55]</ref>. These methods rely on a large amount of data to train their models and still struggle with big and complex tables <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b55">55]</ref>.</p><p>Bottom-up methods. Bottom-up methods can be further categorized into two groups. The first group <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> treats words or cell contents as nodes in a graph and uses graph neural networks to predict whether each sampled node pair is in a same cell, row or column. However, the cell contents used by these methods are not directly available when the inputs are table images. To bypass this problem, the second group of methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b54">54]</ref> detects the bounding boxes of table cells or cell contents directly and uses different methods to group them into rows and columns. After cell detection, methods like <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b54">54]</ref> used heuristic rules to cluster detected cells into rows and columns. CascadeTabNet <ref type="bibr" target="#b32">[33]</ref> recovered cell relations based on some rules for borderless tables while intersected detected separation lines to extract the grid of bordered tables. TabStruct-Net <ref type="bibr" target="#b35">[36]</ref> proposed an end-to-end network to detect cells and predict cell relations jointly. FLAG-Net <ref type="bibr" target="#b22">[23]</ref> predicted adjacency relationships between detected word bounding boxes rather than cells. However, these approaches fail to handle tables containing a large number of empty cells or distorted/curved tables. Cycle-CenterNet <ref type="bibr" target="#b25">[26]</ref> detected the vertices and center points of cells simultaneously, and grouped the cells into tabular objects by learning the common vertices. This method can handle curved bordered tables in wild scenes, but does not take borderless tables into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DETR and Its Variants</head><p>DETR [1] is a novel Transformer-based <ref type="bibr" target="#b45">[46]</ref> object detection algorithm, which introduced the concept of object query and set prediction loss to object detection. These novel attributes make DETR get rid of many hand-designed components in previous CNN-based object detectors like anchor design and non-maximum suppression (NMS). However, DETR has its own issues: 1) Slow training convergence; 2) Unclear physical meaning of object queries; 3) Hard to leverage high-resolution feature maps due to high computational complexity. Deformable DETR <ref type="bibr" target="#b56">[56]</ref> proposed several effective techniques to address these issues: 1) Formulating queries as 2D anchor points; 2) Designing a deformable attention module that only attends to certain sampling points around a reference point to efficiently leverage multi-scale feature maps; 3) Proposing a two-stage DETR framework and an iterative bounding box refinement algorithm to further improve accuracy. Inspired by the concept of reference point in Deformable DETR, some follow-up works attempted to address the slow convergence issue by giving spatial priors to the object query. For instance, Conditional DETR <ref type="bibr" target="#b28">[29]</ref> divided the cross-attention weights into two parts, i.e., content attention weights and spatial attention weights, and proposed a conditional spatial query to make each cross attention head in each decoder layer focus on a different part of an object. Anchor DETR <ref type="bibr" target="#b47">[48]</ref> generated object queries from 2D anchor points directly. DAB-DETR <ref type="bibr" target="#b23">[24]</ref> proposed to use 4D anchor box coordinates to represent queries and dynamically update boxes in each decoder layer. SMCA <ref type="bibr" target="#b4">[5]</ref> first predicted a reference 4D box for each query and then directly generated its related spatial cross attention weights with a Gaussian prior in the transformer decoder. Inspired by two-stage Deformable DETR, Efficient DETR [51] took top-K scored proposals output from the first dense prediction stage and their encoder features as the reference boxes and object queries, respectively. Different from the above works, TSP <ref type="bibr" target="#b43">[44]</ref> discarded the whole DETR decoder and proposed an encoder-only DETR. Recently, DN-DETR <ref type="bibr" target="#b16">[17]</ref> pointed out that the bipartite matching algorithm used in Hungarian loss is another reason for slow convergence and proposed a denoising based training method to speed up DETR convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>As depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>, TSRFormer contains two key components: 1) A SepRETR based split module to predict all row and column separation lines from each input table image; 2) A relation network based cell merging module to recover spanning cells. These two modules are attached to a shared convolutional feature map 2 generated by a ResNet-FPN backbone <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SepRETR based Split Module</head><p>In the split module, two parallel branches are attached to the shared feature map 2 to predict row and column separators, respectively. Each branch comprises three modules: (1) A feature enhancement module to generate a context-enhanced feature map; (2) A SepRETR based separation line prediction module; (3) An auxiliary separation line segmentation module. In subsequent sections, we will take the row separation line prediction branch as an example to introduce the details of these three modules.</p><p>Feature enhancement. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we add a 3 ? 3 convolutional layer and three repeated down-sampling blocks, each composed of a sequence of a 1?2 max-pooling layer, a 3?3 convolutional layer and a ReLU activation function, after 2 sequentially to generate a down-sampled feature map ? 2 ? 4 ? 32 ? first. Then, following <ref type="bibr" target="#b27">[28]</ref>, two cascaded spatial CNN (SCNN) <ref type="bibr" target="#b31">[32]</ref> modules are attached to ? 2 to enhance its feature representation ability further by propagating contextual information across the whole feature map in rightward and leftward directions. Take the rightward direction as an example, the SCNN module splits ? 2 into 32 slices along the width direction and propagates the information slice by slice from left to right. For each slice, it is first sent to a convolutional layer with the kernel size of 9 ? 1 and then merged with the next slice by element-wise addition. With the help of SCNN modules, each pixel in the output context-enhanced feature map can leverage the structural information from both sides for better representation ability.</p><p>SepRETR based separation line prediction. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we use three parallel curvilinear lines to represent the top boundary, center line and bottom boundary of each row separator, respectively. Each curvilinear line is represented by = 15 points, whose x-coordinates are set to 1 , 2 , 3 , ..., , respectively. For each row separator, the y-coordinates of its 3 points are predicted  by our SepRETR model directly. Here, we set = 16 ? for the ? x-coordinate. For y-coordinates in the column branch, we only need replace with . As depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>, our SepRETR contains two modules: a reference point detection module and a DETR decoder for separation line regression. The reference point detection module tries to predict a reference point for each row separator from the enhanced feature map first. The features of detected reference points are taken as object queries and fed into a DETR decoder to generate an enhanced embedding for each query. These enhanced query embeddings are then independently decoded into separation line coordinates and class labels by feedforward networks. Both of the two modules are attached to a shared high-resolution feature map ? ? ? ? ? , which is generated by adding a 1 ? 1 convolutional layer and an up-sampling layer sequentially to . 1) Reference point detection. This module tries to predict a reference point for each row separator at a fixed position along the width direction of the raw image. To this end, each pixel in the ? column of ? is fed into a sigmoid classifier to predict a score to estimate the probability that a reference point is located at its position ( , ). Here, we set the hyper-parameter as ? 4 ? for row line prediction and as ? 4 ? for column line prediction in all experiments. Given the probability of each pixel in the ? column of ? , we apply non-maximal suppression by using a 7 ? 1 maxpooling layer on this column to remove duplicate reference points. After that, top-100 row reference points are selected and further filtered by a score threshold of 0.05. The remaining row reference points are taken as the object queries of the DETR decoder in the row separation line regression module.</p><p>2) Separation line regression. For the sake of efficiency, we don't use transformer encoders to enhance the features output by the CNN backbone. Instead, we concatenate the ? 1 , ? 2 , ..., and ? columns of the high-resolution feature map ? to create a new down-sampled feature map ? ? ? ? . Then, the features of row reference points extracted from ? at their positions are treated as object queries and fed into a 3-layer transformer decoder to interact with for separation line regression. The positional embedding of position ( , ) is generated by concatenating the sinusoidal embeddings of normalized coordinates and , which is the same as in DETR. After enhanced by the transformer decoder, the feature of each query is fed into two feedforward networks for classification and regression, respectively. The ground truth of y-coordinates for row separator regression are normalized to . Prior-enhanced bipartite matching. Given a set of predictions and their corresponding ground truth objects from an input image, DETR used the Hungarian algorithm to assign ground-truth labels to the system predictions. However, it is found that the original bipartite matching algorithm in DETR is unstable in the training stage <ref type="bibr" target="#b16">[17]</ref>, i.e., a query could be matched with different objects in a same image in different training epochs, which slows down model convergence significantly. We find that most of the reference points detected in the first stage locate between the top and bottom boundaries of their corresponding row separators consistently in different training epochs, so we leverage this prior information to match each reference point with its closest ground-truth (GT) separator directly. In this way, the matching results will become stable during training. Specifically, we generate a cost matrix by measuring the distance between each reference point and each GT separator. If a reference point is located between the top and bottom boundaries of a GT separator, the cost is set to the distance from this reference point to the GT reference point of this separator. Otherwise, the cost is set to . Based on this cost matrix, we use the Hungarian algorithm to produce an optimal bipartite matching between reference points and ground truth separators. After getting the optimal matching result, we further remove the pair with cost to bypass unreasonable label assignments. The experiments in Sec. <ref type="bibr">4.4</ref> show that the convergence of our SepRETR becomes much faster with our prior-enhanced bipartite matching strategy.</p><p>Auxiliary separation line segmentation. This auxiliary branch aims to predict whether each pixel is located in the region of any separator. We add an up-sampling operation followed by a 1 ? 1 convolutional layer and a sigmoid classifier after to predict a binary mask ? ? ?1 for calculating this auxiliary loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation Network based Cell Merging</head><p>After separation line prediction, we intersect row lines with column lines to generate a grid of cells and use a relation network <ref type="bibr" target="#b52">[52]</ref> to recover spanning cells by merging some adjacent cells. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we first use RoI Align algorithm <ref type="bibr" target="#b8">[9]</ref> to extract a 7 ? 7 ? feature map from 2 based on the bounding box of each cell, which is then fed into a two-layer MLP with 512 nodes at each layer to generate a 512-d feature vector. These cell features can be arranged in a grid with rows and columns to form a feature map ? ? ?512 , which is then enhanced by three repeated feature enhancement blocks to obtain wider context information and fed into a relation network to predict the relationship between adjacent cells. Each feature enhancement block contains three parallel branches with a row-level max-pooling layer, a column-level max-pooling layer and a 3x3 convolutional layer, respectively. The output feature maps of these three branches are concatenated together and convoluted by a 1 ? 1 convolutional layer for dimension reduction. In the relation network, for each pair of adjacent cells, we concatenate their features and an 18-d spatial compatibility feature introduced in <ref type="bibr" target="#b52">[52]</ref>. A binary classifier is then applied on this feature to predict whether these two cells should be merged or not. The classifier is implemented with a 2-hidden-layer MLP with 512 nodes at each hidden layer and a sigmoid activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function</head><p>The loss functions for training the split module and the cell merging module in TSRFormer are defined in this section. For the split module, we take row separator prediction as an example, and denote the corresponding loss items as * . Likewise, we can also calculate the losses for column separator prediction, denoted as * .</p><p>Reference point detection. We adopt a variant of focal loss <ref type="bibr" target="#b21">[22]</ref> to train the row reference point detection module:</p><formula xml:id="formula_0">= ? 1 ?? =1 (1 ? ) ( ), * = 1 (1 ? * ) (1 ? ), ?<label>(1)</label></formula><p>where is the number of row separation lines, and are two hyper-parameters set to 2 and 4 respectively as in <ref type="bibr" target="#b15">[16]</ref>, and * are the predicted and ground-truth labels for the ? pixel in the ? column of ? . Here, * has been augmented with unnormalized Gaussians, which are truncated at the boundary of separators, to reduce the penalty around the ground-truth reference point locations. Specifically, let ( , ) denote the ground-truth reference point for the ? row separator, which is the intersection point of the center line of this row separator and the vertical line = .</p><p>The vertical distance between the top and bottom boundaries of the ? row separator is taken as its thickness, denoted as . Then, * can be defined as follows:</p><formula xml:id="formula_1">* = ? ? ? ? ? ? ? (? ( ? ) 2 2 2 ), ? ( ? 2 , + 2 ) 0, ?<label>(2)</label></formula><p>where = ?? 2 2 (10) is adaptive to the thickness of the separator to make sure that * within this row separator is no less than 0.1.</p><p>Separation line regression. Let = {( , )| = 1, ..., } denote the set of ground-truth row separators, where and indicate the target class and row separator position respectively, * = {( * , * )| = 1, ..., } denote the set of predictions. After getting the optimal bipartite matching result?, the loss of row separation line regression can be calculated as:</p><formula xml:id="formula_2">= ?? =1 [ ( , * ?( ) ) + 1 1 1 { ??} ( , * ( ) ) ]<label>(3)</label></formula><p>where is focal loss and is L1 loss. Auxiliary segmentation loss. The auxiliary segmentation loss of row separators is a binary cross-entropy loss:</p><formula xml:id="formula_3">= 1 | | ?? ( , ) ? ( ( , ), * ( , ))<label>(4)</label></formula><p>where denotes the set of sampled pixels from , ( , ) and * ( , ) denote the predicted and ground-truth labels for the pixel ( , ) in respectively. * ( , ) is 1 only if this pixel is located within a row separator, otherwise it is 0.</p><p>Cell merging. The loss of the cell merging module is a binary cross-entropy loss:</p><formula xml:id="formula_4">= 1 | | ?? ? ( , * )<label>(5)</label></formula><p>where denotes the set of sampled cell pairs, and * denote the predicted and ground-truth labels for the ? cell pair, respectively.</p><p>Overall loss. All the modules in TSRFormer can be trained jointly. The overall loss function is as follows:</p><formula xml:id="formula_5">= ( + ) + + + + + (6)</formula><p>where is a control parameter set to 0.2 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets and Evaluation Protocols</head><p>We conduct experiments on three popular public benchmarks, including SciTSR <ref type="bibr" target="#b1">[2]</ref>, PubTabNet <ref type="bibr" target="#b55">[55]</ref> and WTW <ref type="bibr" target="#b25">[26]</ref>, to verify the effectiveness of the proposed method. Moreover, we also collected a more challenging in-house dataset, which includes many challenging tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes, to demonstrate the superiority of our TSRFormer. There are also 716 complicated tables selected by authors from the testing set to create a more challenging test subset, called SciTSR-COMP. In this dataset, the cell adjacency relationship metric <ref type="bibr" target="#b6">[7]</ref> is used as the evaluation metric.</p><p>PubTabNet <ref type="bibr" target="#b55">[55]</ref> contains 500,777 training, 9,115 validating, and 9,138 testing images generated by matching the XML and PDF representations of scientific articles. All the tables are axis-aligned. Since the annotations of the testing set are not released, we only report results on the validation set. This work proposed a new Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition task, which can identify both table structure recognition and OCR errors. However, taking OCR errors into account may cause unfair comparison because of different OCR models used by different TSR methods. Some recent works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b54">54]</ref> have proposed a modified TEDS metric named TEDS-Struct to evaluate table structure recognition accuracy only by ignoring OCR errors. We also use this modified metric to evaluate our approach on this dataset.</p><p>WTW <ref type="bibr" target="#b25">[26]</ref> contains 10,970 training images and 3,611 testing images collected from wild complex scenes. This dataset focuses on bordered tabular objects only and contains the annotated information of table id, tabular cell coordinates and row/column information. We crop table regions from original images for both training and testing, and follow <ref type="bibr" target="#b25">[26]</ref> to use the cell adjacency relationship (IoU=0.6) <ref type="bibr" target="#b5">[6]</ref> as the evaluation metric of this dataset.</p><p>In-House dataset contains 40,590 training images and 1,053 testing images, cropped from heterogeneous document images including scientific publications, financial statements, invoices, etc. Most images in this dataset are captured by cameras so tables in these images may be skewed or even curved. Some examples can be found in <ref type="figure" target="#fig_3">Fig. 4 and Fig. 5</ref>. The cTDaR TrackB metric <ref type="bibr" target="#b3">[4]</ref> is used for evaluation. We use GT text boxes as table contents and report results based on IoU=0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>All experiments are implemented in Pytorch v1.6.0 and conducted on a workstation with 8 Nvidia Tesla V100 GPUs. We use ResNet18-FPN as the backbone and set the channel number of 2 to 64 in all experiments. The weights of RestNet-18 are initialized with a pre-trained model for the ImageNet classification task. The models are optimized by AdamW <ref type="bibr" target="#b26">[27]</ref> algorithm with batch size 16. We use a polynomial decay schedule with the power of 0.9 to decay  learning rate, and the initial learning rate, betas, epsilon and weight decay are set as 1e-4, (0.9, 0.999), 1e-8 and 5e-4, respectively. Synchronized BatchNorm is applied during training. In SepRETR based split modules, we set the channel number of ? / ? to 256, and the query dimension, head number and dimension of feedforward networks in transformer decoders to 256, 16 and 1024, respectively. In the training phase, we randomly rescale the shorter side of table images to a number in {416, 512, 608, 704, 800} while keeping the aspect ratio for all datasets except WTW. For WTW, we generate the ground-truth (GT) separation lines by extending the borders of annotated cells and follow <ref type="bibr" target="#b25">[26]</ref> to resize both sides of each training image to 1024 pixels. Given the GT of separation lines, we follow <ref type="bibr" target="#b27">[28]</ref> to generate the GT masks of auxiliary segmentation branches in split module and the GT of cell merging module. Then, the center line and two boundaries of each mask will be considered as the GT of regression targets. In each image, a mini-batch of 1024 positive pixels and 1024 negative pixels are randomly sampled for each auxiliary segmentation branch. Furthermore, we sample a minibatch of 64 hard positive and 64 hard negative cell pairs for the cell merging module. The hard samples are selected with the OHEM <ref type="bibr" target="#b40">[41]</ref> alogrithm. During training, we first train the reference point detection and auxiliary segmentation modules jointly for epochs and then jointly train these two modules and the separation line regression module for epochs. Finally, the cell merging module is further added and jointly trained for another epochs. Here, is set as 12 for PubTabNet and 20 for the other datasets.</p><p>In the testing phase, we rescale the longer side of each image to 1024 while keeping the aspect ratio for SciTSR, PubTabNet and in-house dataset. For WTW, the strategy is the same as in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with Prior Arts</head><p>We compare our proposed TSRFormer with several state-of-the-art methods on public SciTSR, PubTabNet and WTW datasets. For Sc-iTSR, since the evaluation tool provided by the authors contains two different settings (consider or ignore empty cells) and some previous works did not explain which one they used, we report the results of both settings. As reported in <ref type="table" target="#tab_0">Table 1</ref>, our approach has achieved state-of-the-art performance on the testing set and the complicated   subset, respectively. The excellent result on SciTSR-COMP demonstrates that our method is more robust to complicated tables. On PubTabNet, as shown in <ref type="table" target="#tab_1">Table 2</ref>, our method has achieved 97.5% in TEDS-Struct score, which is 0.8% better than that of LGPMA (the winner of ICDAR 2021 Competition on Scientific Literature Parsing Task B). To verify the effectiveness of our approach on bordered distorted/curved tabular objects in wild scenes, we conduct experiments on WTW dataset and the results in <ref type="table" target="#tab_2">Table 3</ref> show that our method is 1.0% better than Cycle-CenterNet (specially designed for this scenario) in F1-score. In order to verify the effectiveness of TSRFormer for more challenging borderless tables, we re-implement another split-and-merge based method SPLERGE <ref type="bibr" target="#b44">[45]</ref> and compare our approach with it on serveral datasets. For fair comparison, we leverage the same model architecture of TSRFormer and just implement another separation line prediction module which first enhances feature maps by row/column level poolings and then predict axis-aligned separators through classifying pixels in horizantal/vertical slices. As shown in <ref type="table" target="#tab_3">Table 4</ref>, the re-implemented SPLERGE can achieve competitve results on SciTSR and PubTabNet datasets while it is still 11.4% worse than TSRFormer in F1-score on our challenging in-house dataset. The qualitative results in <ref type="figure" target="#fig_3">Fig. 5 and Fig. 4</ref> illustrate that our approach is robust to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>We conduct a series of experiments to evaluate the effectiveness of different modules in our approach on our in-house dataset. Effectiveness of SepRETR based split module. To verify the effectiveness of our regression based separator prediction module, we follow RobusTabNet <ref type="bibr" target="#b27">[28]</ref> to implement another segmentation based split module by removing the SepRETR based separation line regression module and directly using auxiliary segmentation branches for separation line prediction. The heuristic mask-to-line module is also the same as in <ref type="bibr" target="#b27">[28]</ref>. The results in <ref type="table" target="#tab_4">Table 5</ref> show that our separator regression module is significantly better than the segmentation based split module. <ref type="figure" target="#fig_5">Fig. 6</ref> shows some qualitative results. It's very hard for the post-processing module to handle such low-quality masks well. In contrast, our regression based approach is heuristics free and robust to such challenging tables.</p><p>Ablation studies of the design of SepRETR. We also conduct the following ablation studies to further examine the contributions of three key components in SepRETR, i.e., the transformer decoder, features used in cross-attention and set prediction. For the experiments without set prediction, we design a heuristic rule for label assignment. If a reference point is located between the two boundaries of a separator, its corresponding query is treated as a positive sample and the regression target is the separator it locates in. Otherwise, the query of this reference point is a negative sample. Since this strategy may assign more than one queries to a separation line, to remove duplicate results, we apply NMS on polygons generated from the two boundaries of each predicted line. As shown in <ref type="table" target="#tab_5">Table  6</ref>, using transformer decoders to help each query leverage both global context and local information can significantly improve the performance of SepRETR based split module. Moreover, the last two rows in <ref type="table" target="#tab_5">Table 6</ref> show that using sampled high-resolution feature map and can further improve the F1-score by 0.5%. Although the result without set prediction is good, we find that this approach is very sensitive to some heuristic designs like the rules of label assignment and NMS. On the contrary, training SepRETR with set prediction loss can not only achieve better results, but also get rid of the limitations of such heuristic designs.</p><p>Effectiveness of prior-enhanced bipartite matching strategy. We conduct several experiments by training the SepRETR based split module with different matching strategies and epochs. As shown in <ref type="table" target="#tab_6">Table 7</ref>, training the model with the original strategy in DETR by 40 epochs achieves much higher accuracy than training by 20 epochs, which means the split module has not fully converged. In contrast, using the proposed prior-enhanced matching strategy can achieve better results. The small performance gap between models trained with 20 and 40 epochs shows that these two models have converged well, which demonstrates that our prior-enhanced matching strategy can make convergence much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we presented TSRFormer, a new approach for table structure recognition, which contains two effective components: a SepRETR based split module for separation line prediction and a relation network based cell merging module for spanning cell recovery. Compared with previous image segmentation based separation line detection methods, our SepRETR-based separation line regression approach can achieve higher TSR accuracy without relying on heuristic mask-to-line modules. Furthermore, experimental results show that the proposed prior-enhanced bipartite matching strategy can accelerate the convergence speed of two-stage DETR effectively. Consequently, our approach has achieved state-of-theart performance on three public benchmarks, including SciTSR, PubTabNet and WTW. We have further validated the robustness of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or curved shapes on a more challenging real-world in-house dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of the proposed TSRFormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of our SepRETR for row separation line prediction.X = 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>An example of ground truth row separation lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of our approach. (a-b) are from SciTSR, (c-d) are from PubTabNet, (e-h) are from WTW, (i-l) are from the in-house dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of SPLERGE (blue) and TSR-Former (red) on distorted tables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results of segmentation based approach with SCNN (middle) and our approach (right) for row separation line prediction on a challenging curved table with borderless cells and large blank spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on SciTSR dataset. * denotes the evaluation results without taking empty cells into account.</figDesc><table><row><cell>Methods</cell><cell>SciTSR (%)</cell><cell>SciTSR-COMP (%)</cell></row><row><cell></cell><cell cols="2">Prec. Rec. F1 Prec. Rec. F1</cell></row><row><cell cols="3">TabStruct-Net [36] 92.7 91.3 92.0 90.9 88.2 89.5</cell></row><row><cell>GraphTSR [2]</cell><cell cols="2">95.9 94.8 95.3 96.4 94.5 95.5</cell></row><row><cell>LGPMA [35]</cell><cell cols="2">98.2 99.3 98.8 97.3 98.7 98.0</cell></row><row><cell>FLAG-Net [23]</cell><cell cols="2">99.7 99.3 99.5 98.4 98.6 98.5</cell></row><row><cell>TSRFormer</cell><cell cols="2">99.5 99.4 99.4 99.1 98.7 98.9</cell></row><row><cell>TSRFormer*</cell><cell cols="2">99.7 99.6 99.6 99.4 99.1 99.2</cell></row><row><cell cols="3">SciTSR [2] contains 12,000 training samples and 3,000 testing</cell></row><row><cell cols="3">samples of axis-aligned tables cropped from scientific literatures.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on PubTabNet dataset.</figDesc><table><row><cell>Methods</cell><cell cols="3">Training Dataset TEDS (%) TEDS-Struct (%)</cell></row><row><cell>EDD [55]</cell><cell>PubTabNet</cell><cell>88.3</cell><cell>-</cell></row><row><cell>TableStruct-Net [36]</cell><cell>SciTSR</cell><cell>-</cell><cell>90.1</cell></row><row><cell>GTE [54]</cell><cell>PubTabNet</cell><cell>-</cell><cell>93.0</cell></row><row><cell>LGPMA [35]</cell><cell>PubTabNet</cell><cell>94.6</cell><cell>96.7</cell></row><row><cell>FLAG-Net [23]</cell><cell>SciTSR</cell><cell>95.1</cell><cell>-</cell></row><row><cell>TSRFormer</cell><cell>PubTabNet</cell><cell>-</cell><cell>97.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on WTW dataset.</figDesc><table><row><cell>Methods</cell><cell cols="3">Prec. (%) Rrec. (%) F1-score (%)</cell></row><row><cell>Cycle-CenterNet [26]</cell><cell>93.3</cell><cell>91.5</cell><cell>92.4</cell></row><row><cell>TSRFormer</cell><cell>93.7</cell><cell>93.2</cell><cell>93.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of the enhanced SPLERGE and TSR-Former on different datasets.</figDesc><table><row><cell>Methods</cell><cell>Dataset</cell><cell cols="4">Prec. (%) Rec. (%) F1. (%) TEDS-Struct (%)</cell></row><row><cell>SPLERGE</cell><cell>SciTSR</cell><cell>99.3</cell><cell>98.9</cell><cell>99.1</cell><cell>-</cell></row><row><cell>TSRFormer</cell><cell>SciTSR</cell><cell>99.5</cell><cell>99.4</cell><cell>99.4</cell><cell>-</cell></row><row><cell cols="2">SPLERGE SciTSR-COMP</cell><cell>98.8</cell><cell>98.0</cell><cell>98.4</cell><cell>-</cell></row><row><cell cols="2">TSRFormer SciTSR-COMP</cell><cell>99.1</cell><cell>98.7</cell><cell>98.9</cell><cell>-</cell></row><row><cell>SPLERGE</cell><cell>PubTabNet</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.1</cell></row><row><cell cols="2">TSRFormer PubTabNet</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.5</cell></row><row><cell>SPLERGE</cell><cell>In-house</cell><cell>85.4</cell><cell>82.3</cell><cell>83.8</cell><cell>-</cell></row><row><cell>TSRFormer</cell><cell>In-house</cell><cell>95.1</cell><cell>95.3</cell><cell>95.2</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies of several modules in TSRFormer.</figDesc><table><row><cell></cell><cell cols="5">SCNN Aux-seg. SepRETR Cell Merging F1. (%)</cell></row><row><cell>Segmentation based</cell><cell>? ?</cell><cell>? ? ?</cell><cell></cell><cell>?</cell><cell>83.5 90.0 92.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>88.6</cell></row><row><cell>Regression</cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>91.0</cell></row><row><cell>based</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>92.6</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>95.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies of the design of SepRETR.</figDesc><table><row><cell>Cross-attention</cell><cell>Transformer</cell><cell>Set</cell><cell></cell></row><row><cell>Feature</cell><cell>Decoder</cell><cell cols="2">Prediction F1. (%)</cell></row><row><cell>-</cell><cell></cell><cell></cell><cell>90.5</cell></row><row><cell>-</cell><cell></cell><cell>?</cell><cell>90.7</cell></row><row><cell>,</cell><cell>?</cell><cell></cell><cell>92.2</cell></row><row><cell>,</cell><cell>?</cell><cell>?</cell><cell>92.1</cell></row><row><cell>,</cell><cell>?</cell><cell>?</cell><cell>92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Effectiveness of prior-enhanced matching strategy.</figDesc><table><row><cell cols="3">Matching Strategy #Epochs F1. (%)</cell></row><row><cell>Original in DETR</cell><cell>20</cell><cell>90.1</cell></row><row><cell>Prior-enhanced</cell><cell>20</cell><cell>92.6</cell></row><row><cell>Original in DETR</cell><cell>40</cell><cell>91.6</cell></row><row><cell>Prior-enhanced</cell><cell>40</cell><cell>92.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* This work was done when Zheng Sun, Chixiang Ma, Mingze Li and Jiawei Wang were interns in Multi-Modal Interaction Group, Microsoft Research Asia, Beijing, China.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Da</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houjin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxuan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04729</idno>
		<title level="m">Complicated table structure recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Challenges in endto-end neural scientific table recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="894" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ICDAR 2019 competition on table detection and recognition (cTDaR)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangcai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>D?jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinqin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kleber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1510" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast convergence of detr with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3621" to="3630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A methodology for evaluating algorithms for table understanding in PDF documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>G?bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermelinda</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Orsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM symposium on Document engineering</title>
		<meeting>the 2012 ACM symposium on Document engineering</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ICDAR 2013 table competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>G?bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermelinda</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Orsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1449" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Guided table structure recognition through anchor optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Khurram Azeem Hashmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad Zeshan</forename><surname>Muhammad Noman Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Afzal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="113521" to="113534" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbiao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaquan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingcong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01846</idno>
		<title level="m">Xin Tang, and Rong Xiao. 2021. PingAn-VCGroup&apos;s Solution for ICDAR 2021 Competition on Scientific Table Image Recognition to Latex</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Table structure recognition based on textblock arrangement and ruled line position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Itonori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="765" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Table structure extraction with bi-directional gated recurrent unit networks</title>
		<editor>Saqib Ali Khan, Syed Muhammad Daniyal Khalid, Muhammad Ali Shahzad, and Faisal Shafait</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The t-recs table recognition and analysis system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kieninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Document Analysis Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identifying and understanding tabular material in compound documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laurentini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="405" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lionel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01305</idno>
		<title level="m">DN-DETR: Accelerate DETR Training by Introducing Query DeNoising</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tablebank: Table benchmark for image-based table detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th language resources and evaluation conference</title>
		<meeting>The 12th language resources and evaluation conference</meeting>
		<imprint>
			<date type="published" when="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive Scaling for Archival Table Structure Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu-Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="80" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GFTE: graph-based financial table extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="644" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
	<note>Feature pyramid networks for object detection</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">2021. Show, Read and Reason: Table Structure Recognition with Flexible Context Aggregator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<biblScope unit="page" from="1084" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbiao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12329</idno>
		<title level="m">Zhu, and Lei Zhang. 2022. DAB-DETR: Dynamic anchor boxes are better queries for DETR</title>
		<imprint>
			<date type="published" when="2022-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parsing Table Structures in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujiao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongpan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="944" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chixiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Huo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09056</idno>
		<title level="m">Robust Table Detection and Structure Recognition from Heterogeneous Document Images</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conditional detr for fast training convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Depu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3651" to="3660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to recognize tables in free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><forename type="middle">Yong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica Li Teng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tablenet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shubham Singh Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monika</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lovekesh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="128" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devashish</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Gadpal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Kapadni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Visave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Sultanpure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="572" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking table recognition using graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Shah Rukh Qasim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaisheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Table structure recognition using top-down and bottom-up cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajoy</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual Understanding of Complex Table Structures from Document Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajoy</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2299" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Texus: A unified framework for extracting and understanding tables in pdf documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roya</forename><surname>Rastan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hye-Young</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shepherd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="895" to="918" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepdesrt: Deep learning for detection and structure recognition of tables in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICDAR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1162" to="1167" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Andreas Dengel, and Sheraz Ahmed</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Configurable table structure recognition in untagged PDF documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Shigarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Mikhailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Altaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM symposium on document engineering</title>
		<meeting>the 2016 ACM symposium on document engineering</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="119" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">DeepTabStR: deep learning based table structure recognition</title>
		<editor>Shoaib Ahmed Siddiqui, Imran Ali Fateh, Syed Tahseen Raza Rizvi, Andreas Dengel, and Sheraz Ahmed</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1403" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation for table structure recognition in documents</title>
		<editor>Shoaib Ahmed Siddiqui, Pervaiz Iqbal Khan, Andreas Dengel, and Sheraz Ahmed</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1397" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking transformer-based set prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3611" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep Splitting and Merging for Table Structure Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Martinez</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2019.00027</idno>
		<ptr target="https://doi.org/10.1109/ICDAR.2019.00027" />
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="114" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Table structure understanding and its performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ihsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1479" to="1497" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Anchor detr: Query design for transformer-based detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07107</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ReS2TIM: Reconstruct syntactic structures from table images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="749" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Li</surname></persName>
		</author>
		<title level="m">TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="1295" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Efficient detr: improving end-to-end object detector with dense prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01318</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Relationship proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5678" to="5686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Split, embed and merge: An accurate table structure recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenrong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">108565</biblScope>
			<date type="published" when="2022-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy Xin Ru</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image-based table recognition: data, model, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elaheh</forename><surname>Shafieibavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio Jimeno</forename><surname>Yepes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="564" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A deep semantic segmentation model for imagebased table structure recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajun</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="274" to="280" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
