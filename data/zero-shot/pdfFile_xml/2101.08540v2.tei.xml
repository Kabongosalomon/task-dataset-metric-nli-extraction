<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Activity Graph Transformer for Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megha</forename><surname>Nawhal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<settlement>Burnaby</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<settlement>Burnaby</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Borealis AI</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Activity Graph Transformer for Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Activity Graph Transformer, an end-to-end learnable model for temporal action localization, that receives a video as input and directly predicts a set of action instances that appear in the video. Detecting and localizing action instances in untrimmed videos requires reasoning over multiple action instances in a video. The dominant paradigms in the literature process videos temporally to either propose action regions or directly produce frame-level detections. However, sequential processing of videos is problematic when the action instances have nonsequential dependencies and/or non-linear temporal ordering, such as overlapping action instances or re-occurrence of action instances over the course of the video. In this work, we capture this non-linear temporal structure by reasoning over the videos as non-sequential entities in the form of graphs. We evaluate our model on challenging datasets: THUMOS14, Charades, and EPIC-Kitchens-100. Our results show that our proposed model outperforms the stateof-the-art by a considerable margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual understanding of human activities in untrimmed videos involves reasoning over multiple action instances with varying temporal extents. This problem has been formally studied in the setup of temporal action localization, i.e., given a human activity video, the goal is to predict a set of action labels and their corresponding start and end timestamps indicating their occurrence in the video. Reasoning over untrimmed human activity videos for action localization is particularly challenging due to the idiosyncrasies of the videos such as: <ref type="bibr" target="#b0">(1)</ref> overlap -the action instances may have overlaps in their temporal extents indicating non-sequential temporal ordering of the instances; <ref type="bibr" target="#b1">(2)</ref> non-sequential dependencies -some action instances may have temporal dependencies but are separated by other unrelated action instances and/or durations of no action; and (3) re-occurrence -instances belonging to same category may appear more than once over the course of the video. In  <ref type="figure">Figure 1</ref>. Main Idea. Given an untrimmed human activity video, we directly predict the set of action instances (label, start time, end time) that appear in the video. We observe that human activity videos contain non-sequential dependencies (illustrated by the ground truth instances as colored bars). In this work, we propose Activity Graph Transformer that captures this non-sequential structure by reasoning over such videos as graphs. Overall, the network receives a video and directly infers a set of action instances. The network achieves this by transforming a set of graphstructured abstract queries into contextual embeddings which are then used to provide predictions of action instances. It is trained end-to-end using classification and regression losses. this work, we propose a novel end-to-end learnable model for temporal action localization that receives a video as an input and directly predicts the set of action instances that appear in the video. Existing approaches for the task of temporal action localization predominantly fall into two paradigms. First is the local-then-global paradigm where the video-level predictions are obtained by postprocessing of local (i.e. framelevel or snippet-level) predictions using sequence modeling techniques such as recurrent neural networks, temporal convolutions and temporal pooling <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr">63,</ref><ref type="bibr">65]</ref>. Second is the proposal-then-classification paradigm which involves generation of a sparse set of class agnostic segment proposals from the overall video followed by classification of the action categories for each proposal using either two-stage learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr">68,</ref><ref type="bibr">69]</ref> or endto-end learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">61,</ref><ref type="bibr">66]</ref>.</p><p>The local-then-global paradigm does not utilize the overall temporal context provided by the activity in the video as the local predictions are solely based on visual information confined to the frame or the snippet. For instance, consider the example in <ref type="figure">Figure 1</ref>, these approaches would miss out on important relevant information provided by 'mix pasta' in predicting 'put spoon' or may produce imprecise predictions when the temporal extents of instances 'take colander' or 'open cupboard' overlap.</p><p>Alternatively, the proposal-then-classification paradigm generates a subset of proposals by processing the video as a sequence. As a result, these approaches suffer from limited receptive field for incorporating temporal information, and do not capture non-sequential temporal dependencies effectively. This problem is further aggravated in the case of overlapping action instances. For instance, in the example in <ref type="figure">Figure 1</ref>, 'open cupboard' and 'close cupboard' share information but are separated by other, potentially overlapping, action instances such as 'take colander'. Due to such ordering, when generating proposals corresponding to 'close cupboard', these approaches are unlikely to capture the dependency with the visual information pertaining to 'open cupboard'. Furthermore, these approaches use heuristics to perform non-maximal suppression of proposals that might result in imprecise localization outcomes when the action instances vary widely in their temporal extents.</p><p>As such, both these types of approaches process videos sequentially to either generate direct local predictions or action proposals and are problematic when action instances reoccur, overlap, or have non-sequential dependencies. These observations suggest that although a video has a linear ordering of frames, the reasoning over the video need not be sequential. We argue that modeling the non-linear temporal structure is a key requirement for effective reasoning over untrimmed human activity videos. In this work, we seek a temporal action localization model that: <ref type="bibr" target="#b0">(1)</ref> captures the temporal structure in complex human activity videos, <ref type="bibr" target="#b1">(2)</ref> does not rely on heuristics or postprocessing of the predictions, and (3) is trained end-to-end.</p><p>Towards this goal, we formulate temporal action localization as a direct set prediction task. We propose a novel temporal action localization model, Activity Graph Transformer (AGT), an end-to-end learnable model that receives a video as input and predicts the set of action instances that appear in the video. In order to capture the non-linear temporal structure in videos, we reason over videos as nonsequential entities, specifically, learnable graph structures. Particularly, we map the input video to graph-structured embeddings using an encoder-decoder transformer architecture that operates using graph attention. A final feed forward network then uses these embeddings to directly predict the action instances. Thus, we propose a streamlined end-toend training process that does not require any heuristics.</p><p>To summarize, our contributions are as follows: (1) we propose an encoder-decoder transformer based model Activity Graph Transformer that reasons over videos as graphs and can be trained end-to-end, and (2) we achieve state-ofthe-art performance on the task of temporal action localization on challenging human activity datasets, namely, THU-MOS14 <ref type="bibr" target="#b22">[23]</ref>, Charades <ref type="bibr" target="#b44">[45]</ref>, and EPIC-Kitchens100 <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we discuss the prior work relevant to temporal action localization and graph based modeling in videos.</p><p>Temporal Action Localization. Early methods for temporal action localization use temporal sliding windows and design hand-crafted features to classify action within each window <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr">64]</ref>. However, these approaches are computationally inefficient as they apply classifiers on windows of all possible sizes and locations in the entire video.</p><p>With the advances in convolutional neural networks, recent approaches fall into two dominant paradigms: (1) local-then-global, (2) proposal-then-classification. The methods following the local-then-global paradigm rely on obtaining temporal boundaries of actions based on local (i.e. frame-level or snippet-level) predictions and perform videolevel reasoning using temporal modeling techniques such as explicit modeling of action durations or transitions <ref type="bibr" target="#b40">[41,</ref><ref type="bibr">65]</ref>, recurrent neural networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr">63]</ref>, temporal pooling <ref type="bibr" target="#b24">[25]</ref>, temporal convolutions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">39]</ref>, and temporal attention <ref type="bibr" target="#b38">[39]</ref>. However, these approaches does not utilize the overall temporal context of the videos as local predictions are computed using only the frame/snippet information.</p><p>The methods based on proposal-then-classification paradigm formulate temporal action localization as the mirror problem of object detection in the temporal domain. Inspired by the progress in object detection <ref type="bibr" target="#b15">[16]</ref> techniques, some methods employ a two-stage training framework <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr">68</ref>, 69] -they generate a set of class-agnostic segment proposals in the first stage and predict an action label for each proposal in the second stage. Most recent methods in this direction focus on improving the proposal generation stage <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">68]</ref>, while a few propose a more accurate classification stage <ref type="bibr" target="#b41">[42,</ref><ref type="bibr">69]</ref>.</p><p>Recently, some end-to-end trainable architectures have also been proposed <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">61,</ref><ref type="bibr">62,</ref><ref type="bibr">66]</ref>. However, these methods also process the video as a sequence and, thus, have limited receptive field for capturing temporal information. They do not capture non-sequential temporal dependencies in action instances. Moreover, these approaches use heuristics during training (e.g. intersection-over-union thresholds) to perform non-maximal suppression in the set of proposals. This might lead to poor localization performance when the action instances vary widely in their temporal extents as it might skip some highly overlapping pro-posals. To address these problems in object detection, <ref type="bibr" target="#b4">[5]</ref> propose a transformer based end-to-end learnable architecture that implicitly learns the non-max suppression and perform object detection using proposals as abstract encodings.</p><p>In contrast to the above approaches, we formulate temporal action localization as a direct set prediction task. We propose to reason over untrimmed videos as non-sequential entities (i.e. graphs) as opposed to existing methods that perform sequential reasoning. Our approach is inspired by <ref type="bibr" target="#b4">[5]</ref> in that we propose an end-to-end learnable transformer based model for direct set prediction. But unlike <ref type="bibr" target="#b4">[5]</ref>, the transformer model in our approach operates graphs.</p><p>Additionally, there are other realms of work on temporal action localization in weakly supervised setting <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr">58]</ref> and spatio-temporal action localization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">48]</ref>. These are beyond the scope of this paper.</p><p>Action Recognition. Action recognition methods operate on short video clips that are trimmed such that a single action instance spans the video duration and, hence, are not suitable for untrimmed videos containing multiple actions. Nonetheless, models pretrained for the task of action recognition provide effective feature representations for tasks related to untrimmed videos. A wide variety of action recognition approaches have been proposed ranging from earlier methods based on hand-crafted features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27</ref>, 55] to convolutional models such as I3D <ref type="bibr" target="#b45">[46]</ref>, 3D-CNN <ref type="bibr" target="#b50">[51]</ref> through to advanced temporal modeling [57, 59, 67] and graph modeling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">60]</ref> techniques. In this paper, we use I3D <ref type="bibr" target="#b45">[46]</ref> pretrained on the Kinetics dataset <ref type="bibr" target="#b5">[6]</ref> for feature extraction.</p><p>Graph-based Modeling for Videos. The advances in graph convolutional networks (GCNs) <ref type="bibr" target="#b25">[26]</ref> have inspired several recent approaches for video based tasks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">60</ref>]. Most of the graph based approaches for videos represent either the input space (i.e. videos or derived visual information) as graphs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">60]</ref> or the output space (i.e. labels) as graphs <ref type="bibr" target="#b51">[52]</ref>. In contrast, we design our model based on the insight that both the input space (i.e. features derived from videos) and the output space (i.e. labels and timestamps for the action) are graph-structured for the task of temporal action localization. Specifically, we propose an encoderdecoder transformer architecture to learn the mapping between the input and output space. Furthermore, GCNs require the information pertaining to the nodes and edges a priori. In contrast, we learn the graph structure (i.e. both nodes and edges) from the data itself using self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this section, we present the problem formulation and provide a detailed description of our proposed approach. Problem Formulation. The task of temporal action localization involves prediction of the category labels as well as start and end timestamps of the actions that occur in a given video. In this work, we formulate this task as a direct set prediction problem wherein each element in the predicted set denotes an action instance in a video. Specifically, given a video V , the goal is to predict a set A where the i-th element a (i) = (c (i) , t (i)</p><formula xml:id="formula_0">s , t (i) e</formula><p>) denotes an action instance in the video depicting action category c (i) that starts at time 0 ? t</p><formula xml:id="formula_1">(i) s ? T , ends at time 0 ? t (i) e ? T , for i ? {1, 2, . . . , |A|}.</formula><p>Here, |A| is the number of action instances present in the video and T is the duration of the video. Thus, |A| and T vary based on the input video.</p><p>Towards this goal, we propose Activity Graph Transformer (AGT), an end-to-end learnable model that receives a video as input and directly infers the set of action instances (label, start time, end time) in the video. Our approach consists of: (1) a network that predicts a set of action instances in a single forward pass; and (2) a loss function to train the network by obtaining a unique alignment between the predicted and ground truth action instances. We contend that effective reasoning over untrimmed human activity videos requires modeling the non-linear temporal structure in the videos. In our approach, we seek to capture this structure by employing graphs. Specifically, we propose a novel encoder-decoder transformer network that leverages graph based self-attention to reason over the videos. We describe the details of our approach below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Activity Graph Transformer</head><p>As shown in <ref type="figure">Figure 2</ref>, Activity Graph Transformer (AGT) consists of three components: (1) backbone network to obtain a compact representation of the input video; (2) transformer network consisting of an encoder network and a decoder network that operates over graphs; and (3) prediction heads for the final prediction of action instances of the form (label, start time, end time). The encoder network receives the compact video-level representation from the backbone network and encodes it to a latent graph representation, referred to as context graph. The decoder network receives graph-structured abstract query encodings (referred to as action query graph) as input along with the context graph. The decoder uses the context graph to transform the action query graph to a graph-structured set of embeddings. Each node embedding of this decoder output is fed into a feed forward network to obtain predictions of action instances. The whole AGT network is trained endto-end using a combination of classification and regression losses for the action labels and timestamps respectively. Refer to Algorithm 1 for an overview of one training iteration of AGT. We provide detailed description of the components below. Self-Attention FFN Graph-to-Graph Attention Graph</p><p>Self-Attention FFN + <ref type="figure">Figure 2</ref>. Model Overview. Activity Graph Transformer (AGT) receives a video as input and directly predicts a set of action instances that appear in the video. The input video is fed into a backbone network to obtain a compact representation (Section 3.1.1). Then, the encoder network (Section 3.1.2) receives the compact video-level representation from the backbone network and encodes it to a latent graph representation context graph. The decoder network (Section 3.1.3) receives the context graph along with graph-structured abstract query encodings action query graph. The decoder transforms the action query graph to a graph-structured set of embeddings. Each node embedding of the decoder output is fed into a prediction head (Section 3.1.4). The network is trained end-to-end (Section 3.1.5) using classification and regression losses for the action labels and timestamps of the action instances respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Backbone</head><p>To obtain a compact representation for the input video V containing T frames, any 3D convolutional network can be used to extract the features. In our implementation, we chunk the videos into short overlapping segments of 8 frames and use an I3D model pretrained on the Kinetics <ref type="bibr" target="#b5">[6]</ref> dataset to extract features of dimension C (= 2048) from the segments, resulting in video-level feature v = [v <ref type="bibr" target="#b0">(1)</ref> , v <ref type="bibr" target="#b1">(2)</ref> . . . v <ref type="bibr">(Nv)</ref> ] where N v is the number of chunks used in the feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Transformer Encoder</head><p>The backbone simply provides a sequence of local features and does not incorporate the overall context of the video or the temporal structure in the video. Therefore, we use an encoder network that receives the video-level feature as input and encodes this video representation to a graph (referred to as the context graph). Intuitively, the encoder is designed to model the interactions among the local features using self-attention modules. The context graph is initialized with video-level feature v (i) (of dimension C = 2048) as the i-th node for i ? {1, 2, . . . , N v }. Usually, transformer networks use fixed positional encoding <ref type="bibr" target="#b36">[37]</ref> to provide position information of each element in the input sequence. In contrast, in our setting, we contend that the video features have a nonlinear temporal structure. Thus, we provide the positional information using learnable positional encodings p v as additional information to the video feature v. The positional encoding p (i) v corresponds to the i-th node in the graph and is of the same dimension as the node. Next, the graph nodes are from the same video and hence, they are related to each other. However, their connection information (edges) is not known a priori. Thus, we model the interactions among these nodes as learnable edge weights. This is enabled by the graph self-attention module (described below).</p><p>We design the transformer encoder network E as a sequence of L e blocks, wherein, an encoder block E for ? {1, 2, . . . , L e } consists of a graph self-attention module followed by a feed forward network. The output of the encoder network is the context graph</p><formula xml:id="formula_2">h Le = [h (1) Le , h (2) Le . . . h (Nv) Le ] where h (i)</formula><p>Le is the i-th node and is of dimension d (same for each block). The output of the -th encoder block h and the final output of the encoder h Le are defined as:</p><formula xml:id="formula_3">h 0 = v h = E (h ?1 , p v ) h Le = E Le ? ? ? ? ? E 1 (v, p v ).</formula><p>(1)</p><p>Graph Self-Attention. This module aims to model interactions among graph structured variables along with learnable edge weights. Here, we describe the graph self-attention module in the context of the ( + 1)-th encoder block E +1 . For simplicity of notation, let x be the output of theth block of the encoder, i.e.,</p><formula xml:id="formula_4">x = E (h ?1 , p v ). x is a graph contains N v nodes x (1) , x (2) , . .</formula><p>. , x (Nv) which are connected using learnable edge weights. The graph selfattention module first performs graph message passing (as described in <ref type="bibr" target="#b53">[54]</ref>) to produce the output x , with the i-th node of the output defined as</p><formula xml:id="formula_5">x (i) = x (i) + K k=1 ? j?Ni ? k ij W k g x (j) ,<label>(2)</label></formula><p>where represents concatenation operator, K is the number of parallel heads in the self-attention module, ? is a non-linear function (leaky ReLU in our case), N i represents the set of neighbours of the i-th node, W k g is the learnable transformation weight matrix. ? k ij are the self attention co-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 A training iteration of AGT model</head><p>Inputs: video V containing T frames; number of action query encodings N o ; ground truth action instances A =</p><formula xml:id="formula_6">{a (i) } |A| i=1</formula><p>Initializations: Backbone initialized to I3D model <ref type="bibr" target="#b5">[6]</ref> pretrained on Kinetics dataset; initialize action query graph q with N o random encodings 1: compute features using backbone 2: compute context graph using encoder (see Eq. 1) 3: compute output embeddings using decoder (see Eq. 4) 4: for i ? 1, 2, . . . , N o do <ref type="bibr">5:</ref> predict action instance? (i) using prediction head 6: end for 7: compute optimal matching? between</p><formula xml:id="formula_7">{a (i) } |A| i=1 and {? (i) } No i=1 using matcher 8: compute final loss L H between {a (i) } |A| i=1 and {? (?(i)) } |A| i=1</formula><p>using Eq. 9 <ref type="bibr">9:</ref> backpropagate L H efficients computed by the k-th attention head described as:</p><formula xml:id="formula_8">? k ij = exp(f (w T a,k [W k g x (i) ||W k g x (j) ])) m?Ni exp(f (w T a,k [W k g x (i) ||W k g x (m) ])) ,<label>(3)</label></formula><p>where ? T represents a transpose operator, f is a non-linear activation (leaky ReLU in our case) and w a,k is the attention coefficients. ? k ij is the attention weight and denotes the strength of the interaction between i-th and j-th node of the input graph of the module. Subsequent to the message passing step, we apply batch normalization and a linear layer. This is then followed by a standard multi-head self-attention layer (same as in <ref type="bibr" target="#b52">[53]</ref>). Overall, the graph self-attention module models interactions between the nodes, i.e., local features derived from the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Transformer Decoder</head><p>Based on the observation that the action instances in the video have a non-linear temporal structure, we design the decoder to learn a graph-structured set of embeddings which would subsequently be used for predicting the action instances. Intuitively, the output graph provided by the decoder serves as the latent representation for the set of action instances depicted in the video.</p><p>The inputs of the transformer decoder are: (1) a graphstructured abstract query encodings, referred to as action query graph q, containing N o nodes wherein each node is a learnable positional encoding of dimension d (same as the dimension used in the encoder); and (2) the context graph h Le containing N v nodes (obtained from the encoder). We assume that the number of nodes in the action query graph N o is fixed and is sufficiently larger than the maximum number of action instances per video in the dataset. This idea of using representations of prediction entities as positional query encodings is inspired from <ref type="bibr" target="#b4">[5]</ref>. However, unlike the independent queries in <ref type="bibr" target="#b4">[5]</ref>, we use graph-structured encodings for the decoder. To learn the interactions among the graph-structured query embeddings, we use graph selfattention modules (same module as used in transformer encoder). Additionally, we use graph-to-graph attention module (described below) to learn interactions between the context graph, i.e., the latent representation of the input video, and the graph-structured query embeddings, i.e., the latent representations of the action queries.</p><p>The overall decoder network D consists of L d blocks, wherein, a decoder block D for ? {1, 2, . . . , L d } consists of a graph self-attention module followed by a graphto-graph attention module, and then a feed forward network. The decoder block D has an output y and the final output of the decoder</p><formula xml:id="formula_9">y L d = [y (1) L d , y (2) L d , . . . , y (No) L d ].</formula><p>They are defined as:</p><formula xml:id="formula_10">y 0 = q y = D (y ?1 , h Le ) y L d = D L d ? ? ? ? ? D 1 (q, h Le )<label>(4)</label></formula><p>Graph-to-Graph Attention. The graph-to-graph attention module aims to learn the interactions between two different graphs referred to as a source graph and a target graph. Here, we describe this module in the context of the decoder block D +1 . The input to this block is the output y of the previous decoder block D . This is fed to the graph selfattention module in the block D +1 , and the output is used as the target graph for the graph-to-graph attention module. The source graph for this module (in any decoder block) is the context graph h Le . For simplicity of notation, let x s denote the source graph (i.e. h Le ) and x t denote the target graph. Here, the source and target graphs may contain different number of nodes. In our case, x s contains N v nodes and x t contains N o nodes. The graph-to-graph attention module first performs message passing from source graph to target graph to provide an output x t , with the i-th node defined as</p><formula xml:id="formula_11">x (i) t = x (i) t + K k=1 ? j?Ni ? k ij W k s x (j) s ,<label>(5)</label></formula><p>where W k s is the learnable transformation weight matrix for the source graph. Other symbols denote the same entities as in graph self-attention (Section 3.1.2). ? k ij is the attention coefficient for k-th attention head between i-th node of the source graph and j-th node of the target graph computed as:</p><formula xml:id="formula_12">? k ij = exp(f (w stT a,k [W k s x (i) s ||W k t x (j) t ])) m?Ni exp(f (w stT a,k [W k s x (i) s ||W k t x (m) t ]))<label>(6)</label></formula><p>where w st a,k is the graph-to-graph attention coefficients, and W k s and W k t are the learnable transformation weight matrices for source and target graphs respectively. Other symbols denote the same entities as in graph self-attention. Similar to the transformer encoder, subsequent to the message passing step, we apply batch normalization and a linear layer. This is then followed by a standard multi-head self-attention layer (same as in <ref type="bibr" target="#b52">[53]</ref>). Overall, the graph-to-graph attention module models the interactions between the latent representations of the input video and the action queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Prediction Heads</head><p>The decoder network provides a set of embeddings where the embeddings serve as the latent representations for the action instances in the video. This output graph y L d contains N o nodes. We use these N o node embeddings to obtain predictions for N o action instances using prediction heads. The prediction heads consist of a feed forward network (FFN) with ReLU activation which provides the start time and end time of the action instance normalized with respect to the overall video duration. Additionally, we use a linear layer with a softmax function to predict the categorical label corresponding to the action instance.</p><p>Therefore, when provided with the i-th node embedding y</p><formula xml:id="formula_13">(i) L d , the prediction head provides prediction? (i) = (c (i) ,t (i) s ,t (i) e ) wherec (i) ,t (i) s andt (i)</formula><p>e are the category label, start time and end time for the i-th action instance for i ? {1, 2, . . . , N o }. Note that the ground truth set would contain a variable number of action instances, whereas N o is larger than the maximum number of action instances per video in the dataset. This calls for a need to suppress irrelevant predictions. We do this by introducing an additional class label ? indicating no action (similar to <ref type="bibr" target="#b4">[5]</ref>). As such, this non-maximal suppression (typically performed using heuristics in existing methods <ref type="bibr" target="#b6">[7]</ref>) is learnable in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Loss functions</head><p>To train the overall network, we align the predictions with the ground truth action instances using a matcher module which optimizes a pair-wise cost function. This provides a unique matching between the predicted and ground truth action instances. Subsequently, our model computes losses corresponding to these matched pairs of predicted and ground truth action instances to train the overall network end-to-end.</p><p>Matcher. The matcher module finds an optimal matching between the predicted set of action instances (that contains fixed number of elements for every video) and the ground truth set of action instances (that contains a variable number of elements depending on the video). To obtain this matching, we design a matching cost function and employ the Hungarian algorithm to obtain an optimal matching between the two sets as described in prior work <ref type="bibr" target="#b48">[49]</ref>.</p><p>Formally, let A be the ground truth set of action in-</p><formula xml:id="formula_14">stances A = {a (i) } |A| i=1 , where a (i) = (c (i) , t (i) s , t (i) e ) and A be the predicted set of action instances? = {? (i) } No i=1 where? (i) = (c (i) ,t (i) s ,t (i) e</formula><p>). In our model, we assume that N o is larger than the number of actions in any video in the dataset. Therefore, we assume that ground truth set A also is a set of size N o by padding the remaining (N o ? |A|) elements with ? element indicating no action. The optimal bipartite matching between the two sets reduces to choosing the permutation of N o elements? from the set of all possible permutations ? No that results in lowest value of the matching cost function L m . Thus,? = argmin ??? No L m (a (i) ,? (?(i)) ), where L m (a (i) ,? (?(i)) ) is the matching cost function between ground truth a (i) and prediction with index ?(i). The matching cost function incorporates the class probabilities of the action instances and the proximity between predicted and ground truth timestamps. Specifically, we define the cost function as:</p><formula xml:id="formula_15">L m (a (i) ,? (?(i)) ) = ?1 {c (i) =?}p?(i) (c (i) ) + 1 {c (i) =?} L s (s (i) ,s (?(i)) ),<label>(7)</label></formula><p>where</p><formula xml:id="formula_16">s (i) = [t (i) s , t (i) e ] ands (?(i)) = [t (?(i)) s ,t (?(i)) e</formula><p>], and p ?(i) (c (i) ) is the probability of class c (i) for prediction ?(i) and L s represents segment loss that measures proximity in the timestamps of the instances. The segment loss is defined as a weighted combination of an L 1 loss (sensitive to the durations of the instances) and an IoU loss (invariant to the durations of the instances) between the predicted and ground-truth start and end timestamps. It is expressed as:</p><formula xml:id="formula_17">L s = ? iou L iou (s (i) ,s (?(i)) ) + ? L1 ||s (i) ?s (?(i)) || 1 ,<label>(8)</label></formula><p>where ? iou , ? L1 ? R are hyperparameters. Subsequent to obtaining the optimal permutation?, we compute the Hungarian loss L H over all the matched pairs as follows:</p><formula xml:id="formula_18">L H = No i=1 ? logp?(c (i) ) + 1 {c (i) =?} L s (s (i) ,s (?(i)) ) .<label>(9)</label></formula><p>This loss is used to train our AGT model end-to-end. We provide further implementation details of the model in the supplementary material.</p><p>In summary, our proposed Activity Graph Transformer performs temporal action localization using an encoderdecoder based architecture leveraging graph based attention modules. We jointly optimize all parameters of our model to minimize the regression loss for the start and end timestamps of the action instances and the cross entropy losses for the corresponding action labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted several experiments to demonstrate the effectiveness of our proposed approach. In this section, we report the results of our evaluation.</p><p>Datasets. We use three benchmark datasets for evaluation. They vary in their extent of overlap in action instances, the number of action instances per video, and the number of action categories in the dataset. Thus, these datasets together serve as a challenging testbed for our model.</p><p>THUMOS14 <ref type="bibr" target="#b22">[23]</ref> contains 200 videos in training set and 213 videos in testing set for the the task of action localization. This dataset has 20 action categories. The videos contain an average of 15 action instances per video with an average of 8% overlapping with other instances.</p><p>Charades <ref type="bibr" target="#b44">[45]</ref> is large scale dataset containing 9848 videos of daily indoor activities. This dataset has 157 action categories. Videos in the dataset contain an average of 6 action instances per video with an average of 79% of overlapping instances in a video. This dataset is challenging because of the high degree of overlap in the action instances.</p><p>EPIC-Kitchens100 <ref type="bibr" target="#b9">[10]</ref> contains 700 egocentric videos of daily kitchen activities. This dataset contains 289 noun and 97 verb classes. Videos in the dataset contain an average of 128 action instances per video with an average of 28% overlapping instances in a video.</p><p>Comparison with state-of-the-art. We compare the performance of our proposed AGT with the state-of-the-art methods. We use mean average precision as the metric to evaluate the model. To ensure fair comparison, we use the same evaluation protocol as used by state-of-the-art methods for each of the datasets. <ref type="table">Table 1</ref> shows that the our AGT achieves upto 3.5% improvement over state-of-the-art for THUMOS14 dataset and consistently shows performance improvement across all IoU thresholds. <ref type="table">Table 2</ref> shows the comparisons with state-of-the-art methods on Charades dataset. Our model achieves 13% improvement in the Charades dataset. We also perform comparison on recenty released EPIC-Kitchens100 dataset for classification of verb, noun, and action (i.e. both verb and noun) classes. <ref type="table" target="#tab_1">Table 3</ref> indicates that our model performs consistently for all three tasks for EPIC-Kitchens100 datasets across all IoU thresholds. Overall, these results clearly show that our proposed method AGT outperforms the state-of-the-art methods by a considerable margin.</p><p>Impact of graph based reasoning. To demonstrate the importance of reasoning over videos as graphs, we conducted ablation studies by removing the graph based reasoning components from either the encoder or the decoder or both (i.e. overall transformer network) in our model. Specifically, this is implemented by removing the graph message passing layers from the attention modules (i.e., graph selfattention module and graph-to-graph attention module) in the encoder and/or decoder blocks in the network. Intuitively, when the graph message passing module is removed from the whole transformer network, the transformer encoder treats the input as a sequence and the transformer decoder treats the action queries as independent. <ref type="table">Table 4</ref> shows the performance of these ablated versions of our model. The results clearly show that eliminating the graphbased reasoning module hurts the localization performance.</p><p>The results also suggest that graph-based modeling is more useful in the encoder than in the decoder. We believe this  <ref type="table">Table 4</ref>. Ablation Study (Impact of graph based reasoning).</p><p>We report performance of ablated versions of our AGT model. We report mAP for evaluation performance (higher is better). We remove graph reasoning in the encoder (E) and/or decoder (D) of the transformer. and indicates whether a component (encoder or decoder) contains graph message passing module or not respectively. EPIC(A), EPIC (V), EPIC (N) indicates task 'Action', 'Verb', 'Noun' classification on EPIC-Kitchens100. is because the graph reasoning performed by the encoder is more useful in capturing the non-sequential dependencies as it operates directly on the video features. For better readability, here, we provide the mAP values averaged over the various intersection-over-union thresholds (tIoU) for THU-MOS14 and EPIC-Kitchens100. For mAP values at specific thresholds, refer to the supplementary.</p><p>Impact of temporal resolution. To evaluate the impact of temporal resolution, we experimented with different frame rates for the input video. <ref type="table">Table 5</ref> shows the results suggesting higher resolution leads to better performance as the higher temporal resolution provides more information in the input. However, our results also show that lower resolution  does not lead to any major drop in performance. For better readability, here, we provide the mAP values averaged over the various intersection-over-union thresholds (tIoU) for THUMOS14 and EPIC-Kitchens100. mAP values at specific thresholds are available in the supplementary.</p><p>Qualitative Results. We visualize the predictions of the model on two different samples in <ref type="figure" target="#fig_3">Figure 3</ref>. The visualizations indicate that our model is able to predict the correct number of action instances as well as correct action categories with minimal errors in start and end timestamps. We believe this is because video content around the start and end timestamps in some instances do not contain enough information pertaining to the action. We provide additional visualizations of predictions in the supplementary. Additionally, refer to the supplementary for experiments on performance of our model with varied number of layers and heads in the transformer and ablations of loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel end-to-end learnable encoder-decoder transformer model for the task of temporal action localization in untrimmed human activity videos.</p><p>Our approach aims to model the non-linear temporal structure in such videos by reasoning over the videos as graphs using graph self-attention mechanisms. The experimental evaluation showed that our model achieves state-of-the-art performance on the task of temporal action localization on challenging human activity datasets. Overall, this work highlights the importance of reasoning over videos as nonsequential entities and shows that graph-based transformers are an effective means to model complex activity videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>We report additional quantitative results and qualitative analysis and provide implementation details of our model. Specifically, this document contains the following.</p><p>? Code provided in the folder agt code.zip</p><p>? Additional quantitative evaluation -Section A.1.1: <ref type="table">Supplemental tables for Table 4</ref> and <ref type="table">Table 5</ref> from the main paper to report mAP values at specific IoU thresholds -Section A.1.2: Ablation study of loss function (Eq. 9 in the main paper). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Additional Quantitative Evaluation</head><p>In this section, we report the quantitative evaluation of our proposed AGT model to supplement the quantitative evaluation in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Supplemental Tables</head><p>In the main paper, we only reported the mAP averaged over different IoU thresholds for THUMOS14 and EPIC-Kitchens100 dataset ( <ref type="table">Table 4</ref> and <ref type="table">Table 5</ref> in main paper). For completeness, we report mAP at specific IoU thresholds in <ref type="table">Table T1 and Table T2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Ablation Study: Loss function</head><p>Note that for any version of the loss function, the model requires cross entropy loss to be able to classify the action label pertaining to an instance. The model also requires some form of regression loss to produce predictions pertaining to the start and end timestamps of an action instance. Recall, <ref type="table">Table T1</ref>. <ref type="table">Supplemental Tables: Impact of graph based rea-</ref>soning. We report performance of ablated versions of our AGT model. We remove graph reasoning in the encoder (E) and/or decoder (D) of the transformer. and indicates whether a component (encoder or decoder) contains graph message passing module or not respectively. EPIC (A), EPIC (V), EPIC (N) indicates task 'Action', 'Verb', 'Noun' classification on EPIC-Kitchens100. We report the mean average precision at different intersection over union thresholds (mAP@tIoU) for <ref type="figure" target="#fig_3">tIoU? {0.1, 0.2, 0.3, 0</ref>  <ref type="table">Table T2</ref>. Supplemental Tables: Impact of temporal resolution. Performance of our AGT for different temporal resolutions of input video. Here, SR indicates sampling rate of frames for feature extraction, i.e., SR=1/k means frames sampled at 1/k -th factor of the original frame rate. EPIC (A), EPIC (V), EPIC (N) indicate tasks 'Action', 'Verb', 'Noun' classification on dataset EPIC-Kitchens100. We report the mean average precision at different intersection over union thresholds (mAP@tIoU) for tIoU? {0.1, 0.2, 0.3, 0.4, 0.5}. ? indicates higher is better. our overall loss (see Eq. (9) in the main paper) is a combination of cross-entropy loss and regression loss, i.e., segment loss L s . The segment loss contains two components: L 1 loss and IoU loss L iou . <ref type="table" target="#tab_1">Table T3</ref> shows the results of the performance of our model when trained with ablated <ref type="table" target="#tab_1">Table T3</ref>. Ablation Study: Loss function. We report performance of our AGT model when trained with ablated versions of the loss function. We report mAP for evaluation performance (higher is better). We train the model with a combination of cross-entropy loss and segment loss containing L1 loss and/or IoU loss Liou. and indicate whether the specific component of the segment loss is used or not respectively. EPIC(A), EPIC (V), EPIC (N) indicate tasks 'Action', 'Verb', 'Noun' classification on EPIC-Kitchens100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>L 1 : versions of the segment loss. The results indicate that the models trained with only L 1 loss perform better than the ones trained with only IoU loss L iou . Additionally, models trained with both losses are better than the ones trained with only one of the losses. Nonetheless, all versions of our AGT model perform better than state-of-the-art methods. We only provide the mAP values averaged over the various intersection-over-union thresholds (tIoU) for THUMOS14 and EPIC-Kitchens100. <ref type="table">Table T4</ref> shows the results of the performance of our model with different number of layers in encoder and decoder component of the transformer. While increase in number of layers increases the training time, we did not observe much difference in the performance of the model with increased depth of the transformer components. We only provide the mAP values averaged over the various intersectionover-union thresholds (tIoU) for THUMOS14 and EPIC-Kitchens100.</p><formula xml:id="formula_19">L 1 : L 1 : L iou : L iou : L iou :<label>THUMOS14</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Impact of number of layers</head><p>A.1.4 Impact of number of heads <ref type="table">Table T5</ref> shows the results of the performance of our AGT model with different number of heads in the attention modules of the transformer. The results suggest a slight improvement with more number of heads in the transformer network. We only provide the mAP values averaged over the various intersection-over-union thresholds (tIoU) for THUMOS14 and EPIC-Kitchens100.</p><p>A.1.5 Impact of action query graph size <ref type="table">Table T6</ref> shows the results of the performance of our AGT model with different number of node encodings in the action query graph. Intuitively, a very large size of action <ref type="table">Table T4</ref>. Impact of number of layers. We report performance of our AGT model with different number of layers in encoder and decoder. We report mAP for evaluation performance (higher is better). EPIC (A), EPIC (V), EPIC (N) indicates task 'Action', 'Verb', 'Noun' classification on EPIC-Kitchens100. #E indicates number of layers in encoder and #D indicates number of layers in decoder.  <ref type="table">Table T5</ref>. Impact of number of heads. We report performance of our AGT model with different number of heads in the attention modules of the transformer network. We report mAP for evaluation performance (higher is better). EPIC (A), EPIC (V), EPIC (N) indicates task 'Action', 'Verb', 'Noun' classification on EPIC-Kitchens100. #heads indicates number of heads in attention modules of the transformer. query graph implies the model will require more time to learn the non-maximal suppression of the irrelevant predictions. On the other hand, a very small size of action query graph might limit the ability of model to learn complex structure in the action instances. Note that, any value used for our experiments is higher than the maximum number of action instances per video in the dataset. The results suggest <ref type="table">Table T6</ref>. Impact of action query graph size. We report performance of our AGT model with different number of nodes in the action query graph. We report mAP for evaluation performance (higher is better). EPIC (A), EPIC (V), EPIC (N) indicates task 'Action', 'Verb', 'Noun' classification on EPIC-Kitchens100. #queries indicates number of nodes in the action query graph. minor improvement with more number of nodes in the action query graph, however, the models with more number of nodes require longer training times. Our experiments also suggest that when the size of the action query graph is reduced, the localization performance of our model degrades. We only provide the mAP values averaged over the various intersection-over-union thresholds (tIoU) for THUMOS14 and EPIC-Kitchens100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Additional Qualitative Analysis</head><p>In this section, we visualize the results of our proposed model AGT to supplement the qualitative analysis in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Visualization: Predictions</head><p>We provide additional visualizations of the predictions of our AGT on several diverse samples in <ref type="figure">Figure F1</ref>. The visualizations indicate that our model is able to predict the correct number of action instances as well as most of the correct action categories with minimal errors in start and end timestamps for videos containing overlapping instances with varying temporal extents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Visualization: Learned Graphs</head><p>We visualize the learned action query graph in <ref type="figure">Figure F2</ref>. by observing the graph embeddings obtained from the last layer of decoder. For better visibility, we do not plot the nodes (or their edges) that are classified as no action (i.e. class label ?) by the prediction head. Note that the edge matrix is also learnable in our model. For the purpose of this visualization, we obtained the edge weights from the attention coefficients in the self-attention based graph message passing module . We show samples with reoccurring and/or overlapping action instances. The visualizations demonstrate that the model indeed learns non-linear dependencies among the action instances that appear in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Analysis: Effect of Action Instance Durations</head><p>We conduct further analysis to study the performance of our model in terms of the durations of the action instances. <ref type="figure" target="#fig_3">Figure F3</ref> shows the trend of segmentation error, i.e., L 1 norm computed between the ground truth and predicted timestamps of actions instances plotted against the duration of the ground truth instances (normalized with respect to the video duration). The error is computed over normalized values of the timestamps. This analysis indicates that action instances with larger durations (with respect to the whole video duration) have lower segmentation errors in their predictions as compared to the instances with smaller durations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Technical Details</head><p>In this section, we provide additional implementation details to supplement the model section in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Additional details</head><p>Detailed Architecture. <ref type="figure">Figure F4</ref> presents the architecture of our AGT in detail. Activity Graph Transformer (AGT) consists of three components: (1) backbone network to obtain features corresponding to the input video; (2) transformer network consisting of an encoder network and a decoder network that operates over graphs; and (3) prediction heads for the final prediction of action instances of the form (label, start time, end time). The encoder network receives the compact video-level representation from the backbone network and encodes it to a latent graph representation, referred to as context graph. The decoder network receives graph-structured abstract query encodings (referred to as action query graph) as input along with the context graph. The decoder uses the context graph to transform the action query graph to a graph-structured set of embeddings. Each node embedding of this decoder output is fed into a prediction head to obtain predictions of action instances. The whole AGT network is trained end-to-end using a combination of classification and regression losses for the action labels and timestamps respectively.</p><p>Positional Encoding. Positional encoding layer consists of a layer that retrieves encodings based on an integer in- <ref type="figure">Figure F1</ref>. Visualization: Predictions. Visualization of predictions and groundtruth action instances <ref type="figure">Figure F2</ref>. Visualization: Learned Graphs. Visualizations of embeddings corresponding to the last layer of the decoder and ground truth instances. The thickness of edges show the strength of interaction between the nodes. For ease of visibility, the nodes have been numbered based on the order of their predictions sorted with respect to the start time (i.e., node 0 represents the instance that starts first). These visualizations demonstrate that the model indeed learns non-linear dependencies between the action instances in a video. The legend below each figure shows the action labels corresponding to the color coded elements. For details on the visualization process, please refer to Section A.2.2 . <ref type="figure" target="#fig_3">Figure F3</ref>. Analysis (THUMOS14). Analysis of segmentation error (L1 loss) with respect to the duration of corresponding ground truth instances. All the values are normalized with respect to the overall video duration. We observe that the action instances of longer durations have lower segmentation errors in their predictions. dex provided to it. In our case, given a video feature v = [v <ref type="bibr" target="#b0">(1)</ref> , v <ref type="bibr" target="#b1">(2)</ref> . . . v (Nv) ], the positional encoding layer receives input i and provides an embedding p Action Query Graph. Similar to positional encoding layer, the N o encodings in the action query graph q is obtained using an embedding layer. Specifically, the layer receives i as input to provide i-th node q (i) of the query graph where i ? 1, 2, . . . , N o . In our implementation, we use torch.nn.Embedding in Pytorch to implement this. The weights of this layer are learnable during training.</p><p>Losses. For completeness, we describe the IoU loss (L iou ) which is used as a component of segment loss L s to train our model. The segment loss is described as:</p><formula xml:id="formula_20">L s = ? iou L iou (s (i) ,s (?(i)) ) + ? L1 ||s (i) ?s (?(i)) || 1 ,<label>(10)</label></formula><p>where ? iou , ? L1 ? R are hyperparameters.</p><formula xml:id="formula_21">L iou (s (i) ,s (?(i)) ) = 1 ? |s (i) ?s (?(i)) | |s (i) ?s (?(i)) |<label>(11)</label></formula><p>where |.| is the duration of the instance, i.e., difference between end and start timestamp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Training Details</head><p>Feature Extraction &amp; Data augmentation. To obtain I3D features corresponding to an input video V containing T frames sampled at a specific sample rate, we first divide the video into short overlapping segments of 8 frames with an overlap of 4 frames resulting in T chunks. We use an I3D Positional encodings <ref type="figure">Figure F4</ref>. Detailed Architecture Architecture of Activity Graph Transformer. Please see Section A.3.1 for details. 'Q','K','V' are query, key and value to the self-attention layer as described in <ref type="bibr" target="#b52">[53]</ref>.</p><p>model pretrained on the Kinetics <ref type="bibr" target="#b5">[6]</ref> dataset to extract features of dimension C (= 2048). In our implementation, we obtain two-stream features (both RGB and flow streams). We obtain features for these T chunks to obtain a tensor of size T ? 2048. Here, the length of the video T depends on the duration of the video, and, hence the size of the temporal channel (i.e. T ) of the feature tensor varies based on the input.</p><p>To prevent severe overfitting, we perform data augmentation to train our model on the features directly obtained from I3D model (described above). We use a hyperparameter N max v as the maximum size of temporal channel used for training. This helps in stabilizing the training as the video datasets contain high variance in their duration. If the size of temporal channel of the video tensor T is less than N max v , we repeat each element in the temporal channel ? times (? = 4) in our implementation to obtain a modified tensor of size ?T ? 2048 and then randomly sample T elements from the modified tensor. If the size of temporal channel of the video tensor T is more than N max v , we just randomly sample T elements from the modified tensor. Note that, positional encoding is applied on this feature of size N v = min(T , N max v ). We find such data augmentation during training to be crucial to prevent overfitting and obtain good performance of our model, especially for smaller datasets such as THU-MOS14. During testing, if the size of temporal channel of the video tensor T is less than N max v , we don't perform any augmentation. If the size of temporal channel of the video tensor T is more than N max v , we uniformly sample T elements from the feature in order to match the maximum index of the positional encoding layer.</p><p>Furthermore, to perform training in minibatches, we apply 0-padding to ensure all elements have the same size as the largest element of the batch. For training efficiency and minimizing the amount of 0-padding, we sort all the dataset based on the duration of the video. We observe that this type of batch formation leads to improvement in training speed without affecting the model performance.</p><p>Hyperparameters. We provide the hyperparameters used to train our model below.</p><p>We train all our models using AdamW optimizer <ref type="bibr" target="#b30">[31]</ref> with a learning rate of 1e-5 and a weight decay of 1e-5 for 3000k steps. We reduce the learning rate by factor of 10 after 2000k steps. The hyperparameters in the loss functions ? L1 and ? iou are set to 5 and 3 respectively for all our experiments. All the learnable weights are initialized using Xavier initialization.</p><p>For our experiments, we sample frames at 1/4 of the original frame rate and obtain the I3D features as decribed earlier. We do not finetune the I3D model.</p><p>We mention the dataset specific hyperparameters below. THUMOS14. We do not use dropout for this dataset. We use maximum number of nodes in the context graph N max v equal to 256. The size of the action query graph is 300 for our experiments (except when conducting ablation on the size of action query graph). We use base model dimension in the transformer as 512 and set the number of encoder and decoder layers as 4 (except when conducting ablation on the number of layers).</p><p>Charades. We use dropout with default probability 0.1. We use maximum number of nodes in the context graph N max v equal to 64. The size of the action query graph is 100 for our experiments (except when conducting ablation on the size of action query graph). We use base model dimension in the transformer as 512 and set the number of encoder and decoder layers as 4 (except when conducting ablation on the number of layers).</p><p>Epic-Kitchens100. We do not use dropout for this dataset. We use maximum number of nodes in the context graph N max v equal to 1024. The size of the action query graph is 1200 for our experiments (except when conducting ablation on the size of action query graph). We use base model dimension in the transformer as 512 and set the number of encoder and decoder layers as 4 (except when conducting ablation on the number of layers).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>/glass/bottle drinking from a cup/glass/bottle putting a cup/glass/bottle somewhere taking a cup/glass/bottle from somewhere putting something on a shelf</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative Results. Visualization of ground truth and predicted action instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>-</head><label></label><figDesc>Section A.1.3: Impact of different number of layers in transformer encoder and decoder. -Section A.1.4: Impact of different number of heads in attention modules of the transformer. -Section A.1.5: Impact of different number of nodes in the action query graph. ? Additional qualitative analysis -Section A.2.1: Visualization of predictions -Section A.2.2: Visualization of graphs learned by the model -Section A.2.3: Analysis of AGT predictions based on the duration of action instances ? Technical details -Section A.3.1: Details of the architecture of AGT -Section A.3.2: Details of initialization, data augmentation, and hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ing to the i-th element of the video feature v (i) where i ? 1, 2, . . . , N v . In our implementation, the embedding size is same as that of the video feature so as to allow addition of the positional encodings and input video features. Since the weights of the layer are learnable during training, the positional encoding layer is learnable. We use torch.nn.Embedding in Pytorch to implement it. This layer initialization requires maximum possible value of N v in the features corresponding to the video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison with state-of-the-art (THUMOS14). We report the mean average precision at different intersection over union thresholds (mAP@tIoU) for tIoU? {0.1, 0.2, 0.3, 0.4, 0.5}. ? indicates higher is better. Comparison with state-of-the-art (Charades).</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">mAP@tIoU ?</cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell cols="6">Oneata et al. [35] 36.6 33.6 27.0 20.8 14.4</cell></row><row><cell>Wang et al. [56]</cell><cell cols="4">18.2 17.0 14.0 11.7</cell><cell>8.3</cell></row><row><cell>Caba et al. [4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.5</cell></row><row><cell cols="6">Richard et al. [41] 39.7 35.7 30.0 23.2 15.2</cell></row><row><cell>Shou et al. [44]</cell><cell cols="5">47.7 43.5 36.3 28.7 19.0</cell></row><row><cell cols="6">Yeung et al. [63] 48.9 44.0 36.0 26.4 17.1</cell></row><row><cell>Yuan et al. [64]</cell><cell cols="5">51.4 42.6 33.6 26.1 18.8</cell></row><row><cell>Buch et al. [3]</cell><cell>-</cell><cell>-</cell><cell>37.8</cell><cell>-</cell><cell>23.0</cell></row><row><cell>Shou et al. [42]</cell><cell>-</cell><cell>-</cell><cell cols="3">40.1 29.4 23.3</cell></row><row><cell>Yuan et al. [65]</cell><cell cols="5">51.0 45.2 36.5 27.8 17.8</cell></row><row><cell>Buch et al. [2]</cell><cell>-</cell><cell>-</cell><cell>45.7</cell><cell>-</cell><cell>29.2</cell></row><row><cell>Gao et al. [14]</cell><cell cols="5">60.1 56.7 50.1 41.3 31.0</cell></row><row><cell>Dai et al. [8]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">33.3 25.6</cell></row><row><cell>Xu et al. [61]</cell><cell cols="5">54.5 51.5 44.8 35.6 28.9</cell></row><row><cell>Zhao et al. [69]</cell><cell cols="5">66.0 59.4 51.9 41.0 29.8</cell></row><row><cell>Lin et al. [30]</cell><cell>-</cell><cell>-</cell><cell cols="3">53.5 45.0 36.9</cell></row><row><cell>Chao et al. [7]</cell><cell cols="5">59.8 57.1 53.2 48.5 42.8</cell></row><row><cell>Zeng et al. [66]</cell><cell cols="5">69.5 67.8 63.6 57.8 49.1</cell></row><row><cell>Xu et al. [62]</cell><cell cols="5">66.1 64.2 54.5 47.6 40.2</cell></row><row><cell>AGT (Ours)</cell><cell cols="5">72.1 69.8 65.0 58.1 50.2</cell></row><row><cell cols="6">We report mean average precision (mAP) computed using</cell></row><row><cell cols="6">Charades v1 localize setting in [45]. ?: higher is better.</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mAP ?</cell></row><row><cell cols="3">Predictive-corrective (Dave et al. [11])</cell><cell></cell><cell></cell><cell>8.9</cell></row><row><cell cols="3">Two-stream (Siggurdson et al. [45])</cell><cell></cell><cell></cell><cell>8.9</cell></row><row><cell cols="4">Two-stream + LSTM (Siggurdson et al. [45])</cell><cell></cell><cell>9.6</cell></row><row><cell>R-C3D (Xu et al. [61])</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12.7</cell></row><row><cell>SSN (Zhao et al. [69])</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16.4</cell></row><row><cell>I3D baseline [38]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>17.2</cell></row><row><cell cols="3">Super-events (Piergiovanni et al. [39])</cell><cell></cell><cell></cell><cell>19.4</cell></row><row><cell cols="2">TGM (Piergiovanni et al. [39])</cell><cell></cell><cell></cell><cell></cell><cell>22.3</cell></row><row><cell>Mavroudi et al. [33]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>23.7</cell></row><row><cell cols="6">3D ResNet-50 + super-events (Piergiovanni et al. [40]) 25.2</cell></row><row><cell>AGT (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state-of-the-art (EPIC-Kitchens100).We report mean average precision at different intersection over union thresholds (mAP@tIoU) for tIoU? {0.1, 0.2, 0.3, 0.4, 0.5}. We use the validation split in the original dataset for testing. ? indicates higher is better.</figDesc><table><row><cell>Method</cell><cell>Task</cell><cell></cell><cell cols="3">mAP@tIoU ?</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell></cell><cell>Verb</cell><cell>10.51</cell><cell>9.24</cell><cell>7.67</cell><cell>6.40</cell><cell>5.12</cell></row><row><cell>Damen</cell><cell cols="2">Noun 10.71</cell><cell>8.73</cell><cell>6.75</cell><cell>5.05</cell><cell>3.35</cell></row><row><cell cols="2">et al. [10] Action</cell><cell>6.78</cell><cell>6.03</cell><cell>4.94</cell><cell>4.04</cell><cell>3.35</cell></row><row><cell></cell><cell>Verb</cell><cell cols="2">12.01 10.25</cell><cell>8.15</cell><cell>7.12</cell><cell>6.14</cell></row><row><cell>AGT</cell><cell cols="2">Noun 11.63</cell><cell>9.33</cell><cell>7.05</cell><cell>6.57</cell><cell>3.89</cell></row><row><cell>(Ours)</cell><cell>Action</cell><cell>7.78</cell><cell>6.92</cell><cell>5.53</cell><cell>4.22</cell><cell>3.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>[ 55 ]</head><label>55</label><figDesc>Heng Wang and Cordelia Schmid. Action recognition with improved trajectories. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013. 3 [56] Limin Wang, Yu Qiao, and Xiaoou Tang. Action recognition and detection by combining motion and appearance features. Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua Lin. Temporal action detection with structured segment networks. In Proceedings of the IEEE International Conference on Computer Vision (CVPR), 2017. 1, 2, 7</figDesc><table><row><cell>[69]</cell></row><row><cell>THUMOS14 Action Recognition Challenge, 2014. 7</cell></row><row><cell>[57] Limin Wang, Yu Qiao, and Xiaoou Tang. Action recogni-</cell></row><row><cell>tion with trajectory-pooled deep-convolutional descriptors.</cell></row><row><cell>In Proceedings of the IEEE Conference on Computer Vision</cell></row><row><cell>and Pattern Recognition (CVPR), 2015. 3</cell></row><row><cell>[58] Limin Wang, Yuanjun Xiong, Dahua Lin, and Luc Van Gool.</cell></row><row><cell>Untrimmednets for weakly supervised action recognition</cell></row><row><cell>and detection. In Proceedings of the IEEE conference on</cell></row><row><cell>Computer Vision and Pattern Recognition (CVPR), 2017. 3</cell></row><row><cell>[59] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua</cell></row><row><cell>Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment</cell></row><row><cell>networks: Towards good practices for deep action recogni-</cell></row><row><cell>tion. In Proceedings of the European Conference on Com-</cell></row><row><cell>puter Vision (ECCV), 2016. 3</cell></row><row><cell>[60] Xiaolong Wang and Abhinav Gupta. Videos as space-time</cell></row><row><cell>region graphs. In Proceedings of the European Conference</cell></row><row><cell>on Computer Vision (ECCV), 2018. 3</cell></row><row><cell>[61] Huijuan Xu, Abir Das, and Kate Saenko. R-c3d: Region con-</cell></row><row><cell>volutional 3d network for temporal activity detection. In Pro-</cell></row><row><cell>ceedings of the IEEE Conference on Computer Vision and</cell></row><row><cell>Pattern Recognition (CVPR), 2017. 1, 2, 7</cell></row><row><cell>[62] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and</cell></row><row><cell>Bernard Ghanem. G-tad: Sub-graph localization for tempo-</cell></row><row><cell>ral action detection. In Proceedings of the IEEE Conference</cell></row><row><cell>on Computer Vision and Pattern Recognition, 2020. 2, 7</cell></row><row><cell>[63] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-</cell></row><row><cell>Fei. End-to-end learning of action detection from frame</cell></row><row><cell>glimpses in videos. In Proceedings of the IEEE Conference</cell></row><row><cell>on Computer Vision and Pattern Recognition, pages 2678-</cell></row><row><cell>2687, 2016. 1, 2, 7</cell></row><row><cell>[64] Jun Yuan, Bingbing Ni, Xiaokang Yang, and Ashraf A Kas-</cell></row><row><cell>sim. Temporal action localization with pyramid of score dis-</cell></row><row><cell>tribution features. In Proceedings of the IEEE Conference on</cell></row><row><cell>Computer Vision and Pattern Recognition (CVPR), 2016. 2,</cell></row><row><cell>7</cell></row><row><cell>[65] Zehuan Yuan, Jonathan C Stroud, Tong Lu, and Jia Deng.</cell></row><row><cell>Temporal action localization by structured maximal sums.</cell></row><row><cell>In Proceedings of the IEEE Conference on Computer Vision</cell></row><row><cell>and Pattern Recognition (CVPR), 2017. 1, 2, 7</cell></row><row><cell>[66] Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong,</cell></row><row><cell>Peilin Zhao, Junzhou Huang, and Chuang Gan. Graph con-</cell></row><row><cell>volutional networks for temporal action localization. In Pro-</cell></row><row><cell>ceedings of the IEEE International Conference on Computer</cell></row><row><cell>Vision (ICCV), 2019. 1, 2, 7</cell></row><row><cell>[67] Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, and Hanli</cell></row><row><cell>Wang. Real-time action recognition with enhanced motion</cell></row><row><cell>vector cnns. In Proceedings of the IEEE Conference on Com-</cell></row><row><cell>puter Vision and Pattern Recognition (CVPR), 2016. 3</cell></row><row><cell>[68] Peisen Zhao, Lingxi Xie, Chen Ju, Ya Zhang, Yanfeng Wang,</cell></row><row><cell>and Qi Tian. Bottom-up temporal action localization with</cell></row><row><cell>mutual regularization. In Proceedings of the European Con-</cell></row><row><cell>ference on Computer Vision (ECCV), 2020. 1, 2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueran</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rescaling egocentric vision. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predictive-corrective networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scc: Semantic context cascade for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayner</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Videograph: Recognizing minutes-long human activities in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
		<idno>2019. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV Workshop on Scene Graph Representation and Learning</title>
		<meeting>the ICCV Workshop on Scene Graph Representation and Learning</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Actionbytes: Learning from trimmed videos to localize actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action tubelet detector for spatiotemporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision(IJCV)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representation learning on visual-symbolic graphs for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Effrosyni</forename><surname>Mavroudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B?jar</forename><surname>Benjam?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ego-topo: Environment affordances from egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action and event recognition with fisher vectors on a compact feature set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph for video captioning with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Boxiao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal gaussian mixture layer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning latent superevents to detect multiple activities in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Avid dataset: Anonymized videos from diverse countries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Online real-time multiple spatiotemporal action localisation and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cuzzolin</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Combining the right features for complex event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video relationship reasoning using gated spatio-temporal energy graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
