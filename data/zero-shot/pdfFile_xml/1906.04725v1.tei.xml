<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Clouds of Oriented Gradients for 3D Detection of Objects, Surfaces, and Indoor Scene Layouts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-11">11 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
							<email>jrenzhile@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Technology</orgName>
								<address>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
							<email>sudderth@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Technology</orgName>
								<address>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">E B</forename></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Computer Sciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92697-3435</postCode>
									<settlement>Irvine</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Clouds of Oriented Gradients for 3D Detection of Objects, Surfaces, and Indoor Scene Layouts</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Ren is with School of Interactive Computing</title>
						<meeting> <address><addrLine>Georgia</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2019-06-11">11 Jun 2019</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop new representations and algorithms for three-dimensional (3D) object detection and spatial layout prediction in cluttered indoor scenes. We first propose a clouds of oriented gradient (COG) descriptor that links the 2D appearance and 3D pose of object categories, and thus accurately models how perspective projection affects perceived image gradients. To better represent the 3D visual styles of large objects and provide contextual cues to improve the detection of small objects, we introduce latent support surfaces. We then propose a "Manhattan voxel" representation which better captures the 3D room layout geometry of common indoor environments. Effective classification rules are learned via a latent structured prediction framework. Contextual relationships among categories and layout are captured via a cascade of classifiers, leading to holistic scene hypotheses that exceed the state-of-the-art on the SUN RGB-D database. R W Two-dimensional object detection is a widely studied problem. Dalal and Triggs [24] introduced the histogram of oriented gradient (HOG) descriptor to model 2D object appearance using image gradients. Building on HOG, Felzenszwalb et al. [25] used a discriminately-trained part-based model to represent objects. This method is effective because it explicitly models object parts as latent variables and thus captures some object style and pose variations. More recently, many papers have used convolutional neural networks (CNNs) to extract rich features from images [26], [27], [28], [29], [30]. For domains where large sets of labeled images are available, CNNs lead to state-of-the-art performance with efficient detection speed [31], [32]. Increasingly, real-world computer vision systems often incorporate depth data as an additional input to increase accuracy and robustness. With depth maps we can reconstruct point cloud representations of scenes, leading to significant recent advances in 3D object classification [33], [34], point cloud segmentation [35], [36], cuboid-based geometric modeling [37], [38], [39], room layout prediction [40], [41], 3D contextual modeling [42], [43], and 3D shape reconstruction [44], <ref type="bibr" target="#b45">[45]</ref>. Here, we focus on the related problem of 3D object detection.</p><p>In outdoor scenes, localizing objects with 3D cuboids has become a standard in the popular KITTI autonomous driving benchmark <ref type="bibr" target="#b15">[16]</ref>. 3D detection systems model car shape and occlusion patterns using LiDAR or stereo inputs <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b49">[48]</ref>, <ref type="bibr" target="#b50">[49]</ref>, <ref type="bibr" target="#b51">[50]</ref>, and may also incorporate additional overhead imagery <ref type="bibr" target="#b52">[51]</ref>. 3D cuboid representations are more powerful than 2D bounding boxes because they contain more information about 3D object locations, physical occupancy, and orientation. However, many outdoor 3D detection systems are specialized to vehicles and pedestrians, and may not generalize to cluttered indoor environments.</p><p>Other work has localized indoor objects with <ref type="formula">3D</ref> cuboids [52], [53], but achieving high accuracy is challenging due to the significant shape variations found in cluttered, real-world environments. Several recent methods have incorporated CAD models to learn object shape [10], [33], [54] or hallucinate alternative viewpoints for appearance-based matching [55], [56], [57]. While CAD models are a potentially powerful information source, there does not exist an abundant supply of models for all categories, and many methods are limited to a small number of object categories [55]. Moreover, example-based methods [10] may be computationally inefficient due to the need to match each exemplar to each image. For robotics applications, a 3D convolutional neural network was designed to detect simple objects in real time [58]. In 2015, Song et al. introduced a SUN RGB-D dataset [15] containing 10,335 RGB-D images with accurate 3D cuboid annotations for indoor objects, room layouts, and scene categories. The size of</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Index Terms-3D scene understanding, object detection, room layout estimation, structured prediction, cascaded classification. ! I S understanding of three-dimensional (3D) scenes plays an increasingly important role in modern robotic systems and autonomous vehicles. The last decade has seen major advances in semantic understanding of 2D images <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. However, images of indoor (home or office) environments remain challenging for existing methods due to the prevalence of clutter and occlusions. Advances in depth sensor technology can reduce ambiguities in standard RGB images, enabling breakthroughs in scene layout prediction <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, support surface prediction <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, semantic parsing <ref type="bibr" target="#b8">[9]</ref>, and object detection <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. A growing number of annotated RGB-D datasets have been constructed to train and evaluate indoor scene understanding methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>Holistic indoor scene understanding <ref type="bibr" target="#b14">[15]</ref> requires integrated detection of objects and the room layouts (walls, floors, and ceilings) that surround them. While object detection is often formalized as the prediction of a 2D bounding box <ref type="bibr" target="#b0">[1]</ref>, 2D representations are insufficient for many real-world applications because they do not explicitly represent object orientations or contextual relationships. We instead propose to detect the 3D size, position, and orientation of object instances via bounding cuboids (convex polyhedra). 3D cuboid detection is a standard task in indoor and outdoor scene understanding benchmarks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Descriptors constructed from point cloud representations of RGB-D images are frequently used for 3D object detection. For example, Song et al. <ref type="bibr" target="#b16">[17]</ref> use the truncated signed distance function (TSDF) to define descriptors for candidate 3D bounding cuboids. But given the diverse variation in the appearance of indoor object categories, accurately modeling how appearance varies with object style and 3D viewpoint is very challenging <ref type="bibr" target="#b17">[18]</ref>. We thus design a novel, orientation-adaptive gradient descriptor that uses perspective geometry to better detect objects observed from diverse 3D viewpoints.</p><p>Basic discriminative scene parsing algorithms detect each category independently, but often have many false positives. Previous work has used manually engineered heuristics to prune false detections <ref type="bibr" target="#b9">[10]</ref> or combined CAD models with layout cues to model scenes <ref type="bibr" target="#b18">[19]</ref>. In this paper, we significantly boost detection accuracy via a cascaded classification framework <ref type="bibr" target="#b19">[20]</ref> that learns contextual relationships among object categories, as well as relationships between objects and the overall room layout. This efficient approach allows initial detections of visually distinctive objects to lead to holistic scene interpretations of higher quality.</p><p>To estimate the spatial layout used in our cascaded classifiers, we assume an orthogonal "Manhattan" room structure <ref type="bibr" target="#b20">[21]</ref>. Many previous methods predict 2D projections of the underlying 3D room structure <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, but small 2D alignment errors may lead to poor 3D layout estimates. We avoid this by using a structured prediction framework to directly estimate 3D layouts from RGB-D images, and propose a Manhattan voxel representation that (like our object descriptors) is adapted to the geometry of indoor scenes. Our learning-based approach is more robust to the noisy depth estimates produced by practical RGB-D cameras, and thus avoids errors made by simpler layout prediction heuristics <ref type="bibr" target="#b14">[15]</ref>.</p><p>Holistic indoor scene understanding is particularly challenging because smaller objects, like lamps and monitors, only occupy a tiny fraction of the room volume. Bottom-up detectors thus have high computational demands (many candidate bounding cuboids must be considered) and typically produce many false positives. To address this challenge, we note that many small objects are supported by the surfaces of large objects <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and augment our cuboid representations with latent support surfaces. While surface heights are estimated without explicit training annotations, modeling them nevertheless boosts the accuracy of our furniture detectors. When integrated into our cascaded classification framework, support surfaces constrain the search space for small objects, and thereby improve detection speed as well as accuracy.</p><p>In summary, we propose a general framework for learning detectors for multiple object categories using only RGB-D annotations. We first introduce a cloud of oriented gradient (COG) descriptor that robustly links 3D object pose to 2D image boundaries, and discuss extensions that further boost performance (Sec. 3). Because a major cause of feature inconsistency across object arXiv:1906.04725v1 [cs.CV] 11 Jun 2019 instances is variation in the location of the supporting surface, we model this height as a latent variable, and use it to distinguish different visual styles and detect smaller objects (Sec. 4). We also introduce a Manhattan voxel representation to predict room layout directly from RGB-D data (Sec. 5). We use a structured prediction framework to learn an algorithm that aligns 3D cuboid hypotheses to RGB-D data, and a cascaded classifier to incorporate contextual cues from other object instances and categories, as well as the overall 3D layout (Sec. 6). We evaluate our algorithm on the challenging SUN RGB-D dataset <ref type="bibr" target="#b14">[15]</ref> and achieve state-of-the-art accuracy in the 3D detection of 19 object categories (Sec. 7).</p><p>the dataset matches that of the PASCAL-VOC dataset <ref type="bibr" target="#b0">[1]</ref> and motivates several recent research projects. Some methods utilize pre-trained 2D detectors and region proposals as priors <ref type="bibr" target="#b1">[2]</ref>, and localize 3D bounding boxes via a separate CNN <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b50">[49]</ref>, <ref type="bibr" target="#b60">[59]</ref>, <ref type="bibr" target="#b61">[60]</ref>. These methods are efficient and can achieve decent accuracy, but are sensitive to failures of the 2D object detector, which may not generalize to objects seen from novel 3D viewpoints.</p><p>Detecting support surfaces is an essential first step in understanding the geometry of 3D scenes for such tasks as surface normal estimation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b62">[61]</ref> and shape retrieval <ref type="bibr" target="#b63">[62]</ref>. Silberman et al. <ref type="bibr" target="#b5">[6]</ref> use semantic segmentation to model object support relationships; this work was later extended by Guo et al. <ref type="bibr" target="#b7">[8]</ref> for support surface prediction. We instead use 3D support surface representations to improve the accuracy of our models of object style, and the speed of our detectors for small object categories.</p><p>To enable more holistic understanding of 3D scenes, we also predict the locations of walls, ceilings, and floors; this structure is sometimes called the room layout <ref type="bibr" target="#b14">[15]</ref>. Some related work has predicted 2D projections of the 3D layout <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b64">[63]</ref>, <ref type="bibr" target="#b65">[64]</ref>, or used CNNs to directly predict the 3D layout <ref type="bibr" target="#b66">[65]</ref>. In this paper, we use the geometric structure of typical indoor environments to design a Manhattan voxel representation that leads to accurate 3D layout predictions.</p><p>More broadly, holistic scene understanding systems integrate forms of semantic object reasoning, spatial context modeling, and scene type identification <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b67">[66]</ref>. Often, models for each subtask are learned independently, and then integrated via conditional random fields (CRFs) like that proposed by Lin et al. <ref type="bibr" target="#b53">[52]</ref>. However, rich scene models lead to complex graph structures and challenging inference problems. Hoiem et al. <ref type="bibr" target="#b68">[67]</ref> jointly estimate the camera viewpoint and detect objects, Zhang et al. <ref type="bibr" target="#b43">[43]</ref> use predefined room configurations to adjust object localizations, while Ren et al. <ref type="bibr" target="#b69">[68]</ref> utilize scene type to refine detector outputs. We instead adapt the cascaded prediction framework <ref type="bibr" target="#b19">[20]</ref> to learn multi-stage models capturing detector accuracies and contextual relationships among objects and the room layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M D G &amp; A</head><p>Feature extraction is one of the most important steps for object detection algorithms. 2D object detectors typically use either handcrafted features based on image gradients <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref> or learned features from deep neural networks <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>. For 3D object detection systems with additional depth inputs, Gupta et al. <ref type="bibr" target="#b8">[9]</ref> use horizontal disparity, height above the ground, and the angle of the local surface normal to encode images as a three channel (HHA) map for learning with CNNs. While convolutional processing of 2D images may be used to extract features from 2D bounding boxes, it does not directly model 3D cuboids. Song et al. propose a deep sliding shape <ref type="bibr" target="#b16">[17]</ref> method that combines TSDF features <ref type="bibr" target="#b9">[10]</ref> with standard 2D CNN features to describe 3D cuboids, but do not explicitly model 3D cuboid orientation. Our object detectors are learned from 3D oriented cuboid annotations in the SUN-RGBD dataset <ref type="bibr" target="#b14">[15]</ref>. We discretize each cuboid into a 5 ? 5 ? 5 grid of (large) voxels, and extract features for these 5 3 = 125 cells. Voxel dimensions are scaled to match the size of each instance. We use standard descriptors for the 3D geometry of the observed depth image, and propose a novel cloud of oriented gradient (COG) descriptor of RGB appearance. We also introduce simple extensions that improve its performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Object Geometry: 3D Density and Orientation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Point Cloud Density</head><p>Conditioned on a 3D cuboid annotation or detection hypothesis i, suppose voxel contains N i points. We use perspective projection to find the silhouette of each voxel in the image, and compute the area A i of that convex region. The point cloud density feature for voxel then equals ? a i = N i /A i . Normalization gives robustness to depth variation of the object in the scene. We normalize by the local voxel area, rather than by the total number of points in the cuboid as in some related work <ref type="bibr" target="#b9">[10]</ref>, to give greater robustness to partial object occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">3D Normal Orientations</head><p>Various representations, such as spin images <ref type="bibr" target="#b70">[69]</ref>, have been proposed for the vectors normal to a 3D surface. As in <ref type="bibr" target="#b9">[10]</ref>, we build a 25-bin histogram of normal orientations within each voxel, and estimate the normal orientation for each 3D point via a plane fit to its 15 nearest neighbors. This feature ? b i captures the surface shape of cuboid i via patterns of local 3D orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clouds of Oriented Gradients (COG)</head><p>The histogram of oriented gradient (HOG) descriptor <ref type="bibr" target="#b24">[24]</ref> forms the basis for many effective object detection methods <ref type="bibr" target="#b0">[1]</ref>. Edges are a very natural foundation for indoor scene understanding, due to the strong occluding contours generated by common objects. However, as gradient orientations are determined by 3D object orientation and perspective projection, HOG descriptors that are naively extracted in 2D image coordinates generalize poorly.</p><p>To address this issue, some previous work has restrictively assumed that parts of objects are near-planar so that image warping may be used for alignment <ref type="bibr" target="#b17">[18]</ref>, or that all objects have a 3D pose aligned with the global "Manhattan world coordinates" of the room <ref type="bibr" target="#b3">[4]</ref>. The bag of boundaries (BOB) <ref type="bibr" target="#b71">[70]</ref> descriptor builds separate gradient-based models for each of several distinct 3D Voxels in similar positions of chairs COG Binning HOG Binning COG Binning HOG Binning <ref type="figure">Fig. 2</ref>. For two corresponding voxels (red and green) on two chairs, we illustrate the orientation histograms that would be computed by a standard HOG descriptor <ref type="bibr" target="#b24">[24]</ref> in 2D image coordinates, and our COG descriptor in which perspective geometry is used to align descriptor bins. Even though these object instances are very similar, their 3D pose leads to wildly different HOG descriptors.</p><p>viewpoints, rather than using geometry to generalize across 3D viewpoints. Some previous 3D extensions of the HOG descriptor <ref type="bibr" target="#b72">[71]</ref>, <ref type="bibr" target="#b73">[72]</ref> assume that a full 3D model is given. In recent work <ref type="bibr" target="#b74">[73]</ref>, 3D cuboid hypotheses were used to aggregate standard 2D features from a deep convolutional neural network, but the deep features are not conditioned on object orientations. Our cloud of oriented gradient (COG) feature accurately describes the 3D appearance of objects with complex 3D geometry, as captured by RGB-D cameras from any viewpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">2D Gradient Computation</head><p>We compute gradients by applying filters [?1, 0, 1], [?1, 0, 1] T to the RGB channels of the unsmoothed 2D image. The maximum responses across color channels are the gradients (dx, dy) in the x and y directions, with corresponding magnitude dx 2 + dy 2 . We follow similar implementation details to the gradient computations used in HOG descriptors <ref type="bibr" target="#b24">[24]</ref>. The 2D unsigned gradients are then aggregated in each voxel to define our 3D COG descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">3D Orientation Bins</head><p>The standard HOG descriptor <ref type="bibr" target="#b24">[24]</ref> for cell of object i uses nine evenly spaced gradient histogram bins,</p><formula xml:id="formula_0">(o (1) i , . . . , o (9) i ). For all object instances, o (1) i = [1, 0]</formula><p>T is aligned with the horizontal image direction. As shown in <ref type="figure">Fig. 2</ref>, HOG descriptors may thus be inconsistent for (even nearly identical) objects in distinct poses.</p><p>Because objects from the same category typically have similar local 3D structure, for each oriented 3D cuboid proposal, we instead model local gradient statistics in a canonical 3D coordinate frame. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, we define nine evenly spaced 3D orientation bins (O <ref type="bibr" target="#b0">(1)</ref> i , . . . , O (9) i ) on the front surface (xy-plane) of each voxel within the cuboid. For all instances, O <ref type="bibr" target="#b0">(1)</ref> i is aligned with the horizontal 3D x-axis (dark blue lines in <ref type="figure" target="#fig_0">Fig. 1</ref>). Given the camera's intrinsic matrix K, and the extrinsic matrix [R i |t i ] encoding the relative 3D pose of cuboid i, we use perspective projection to map 3D orientation bins to 2D image coordinates:</p><formula xml:id="formula_1">o (j) i 1 ? K [R i |t i ] O (j) i 1 .<label>(1)</label></formula><p>This transform aligns the 2D orientation bins for distinct 3D cuboids. For each pixel that back-projects to 3D voxel , we accumulate its unsigned 2D gradient in the corresponding projected orientation bin to define a nine-dimensional COG feature ? c i . Some previous work has warped images to align with fixed 2D orientation bins <ref type="bibr" target="#b3">[4]</ref>, but such affine transformations may be unstable for objects with non-planar geometry. Our COG descriptor can be seen as accumulating standard gradients with warped histogram bins, rather than warping images to match fixed orientation bins. This innovation enables our later learning algorithms to better generalize to novel 3D views of complex objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Normalization and Aliasing</head><p>We bilinearly interpolate gradient magnitudes between neighboring orientation bins <ref type="bibr" target="#b24">[24]</ref>. To normalize the histogram ? c i for voxel in cuboid i, we then set ? c i ? ? c i / ||? c i || 2 + for a small &gt; 0. Accounting for all orientations and voxels, the dimension of the COG feature ? c i is 5 3 ? 9 = 1125.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extensions of the COG Descriptor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">View-to-Camera Features</head><p>For single view RGB-D inputs, objects like nightstands and other furniture may only expose one planer surface to the camera. At test time, the features of a 3D cuboid proposal oriented away from the camera may resemble those of a correct detection (see <ref type="figure">Fig. 3</ref>) because voxel features are computed by first rotating the cuboid to a canonical coordinate frame. However, due to the selfocclusions that occur in real objects, the features modeled by the COG descriptor would in fact not be visible when objects are facing away from the camera. Therefore, we add features to represent objects' orientation with respect to the camera, and learn to distinguish implausible object hypotheses. Specifically, we compute the cosine x of the angle between the cuboid orientation and its viewing angle from camera in horizontal direction. Then we define a set of radial basis functions of the form</p><formula xml:id="formula_2">f j (x) = exp ? (x ? ? j ) 2 2? 2 ,<label>(2)</label></formula><p>and space the basis function centers ? j evenly between [?1, 1] with step size 0.2. The bandwidth ? = 0.5 was chosen using validation data. Radial basis expansions are a standard non-linear regression method, and can be seen as a layer of a neural network. We expand the camera angle using this basis representation plus a bias feature, producing an 11-dimensional view-to-camera feature ? d i . <ref type="figure">Fig. 3</ref>. A false positive 3D detection for the nightstand category that occurs without a view-to-camera feature (top). The COG feature is similar to that of a correct detection (bottom) due to bilateral symmetry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Expanded Cuboid Features</head><p>Many object detection systems have a pre-processing stage that generates bounding box proposals that contain objects with welldefined boundaries, instead of amorphous background areas <ref type="bibr" target="#b75">[74]</ref>. Using a region proposal network to maximize the "objectness" score of predicted bounding boxes <ref type="bibr" target="#b76">[75]</ref> is thus an essential first step for many state-of-the-art object detection systems <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b26">[26]</ref>. Objectness scores are usually determined from the difference between local and surrounding appearances of each object. Instead of designing a separate pre-processing step, we build such contextual cues into our cuboid features. For each cuboid proposal, we expand its size to capture an additional layer of voxels in each direction, so that each cuboid is now described by 7 ? 7 ? 7 voxels.</p><p>Before discussing the training algorithm, we preview the learned weights of COG descriptors for the chair and toilet categories in <ref type="figure" target="#fig_1">Fig. 4</ref>. Toilets are typically placed against the wall in cluttered bathrooms, while there is typically free space around chairs, and thus our expanded cuboid features capture differences between these categories that improve detection accuracy.</p><p>The structure of our expanded cuboid feature has some similarities to the "zoom-out" features originally proposed for 2D image segmentation <ref type="bibr" target="#b77">[76]</ref>, and used by Song et al. <ref type="bibr" target="#b16">[17]</ref> for 3D detection. We provide ablation studies in <ref type="table" target="#tab_0">Table 1</ref>, and demonstrate that this extension is very effective in modeling the geometric structure surrounding each cuboid, improving object detection accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chair Toilet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Structured Prediction of Object Cuboids</head><p>For each voxel in some cuboid B i annotated in training image I i , we have one point cloud density feature ? a i , 25 surface normal histogram features ? b i , and 9 COG appearance features ? c i . For each cuboid i, we have 12 camera view features ? d i . Using expanded features with 7 3 = 343 voxels, our overall representation of cuboid i is then ?(</p><formula xml:id="formula_3">I i , B i ) = [{? a i , ? b i , ? c i } 343 =1 , ? d i ].</formula><p>Cuboids are aligned via annotated orientations as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, using the gravity direction provided in the SUN-RGBD dataset <ref type="bibr" target="#b14">[15]</ref>.</p><p>For each object category c independently, using those images which contain visible instances of that category, our goal is to learn a prediction function h c : I ? B that maps an RGB-D image I to a 3D bounding box B = (L, ?, S, y). Here L is the center of the cuboid in 3D, ? is the cuboid orientation, S is the physical size of the cuboid along the three axes determined by its orientation, and y is a binary variable indicating whether the object is present in that area of the 3D scene. We assume objects have a base upon which they are typically supported, and thus ? is a scalar rotation with respect to the ground plane.</p><p>Given n training examples of category c, we use an nslack formulation of the structural support vector machine (SVM) objective <ref type="bibr" target="#b78">[77]</ref> with margin rescaling constraints:</p><formula xml:id="formula_4">min w c ,? ?0 1 2 w T c w c + C n n i=1 ? i subject to w T c [?(I i , B i ) ? ?(I i ,B i )] ? ?(B i ,B i ) ? ? i , for allB i ? B i , i = 1, . . . , n.<label>(3)</label></formula><p>Here ?(I i , B i ) are the features for oriented cuboid hypothesis B i given RGB-D image I i , B i is the ground-truth cuboid annotation, and B i is the set of possible alternative cuboids. For training images with multiple instances, as in previous work on 2D detection <ref type="bibr" target="#b79">[78]</ref> we add multiple copies to the training set, each time removing the subset of 3D points contained in other instances.</p><p>Given some ground truth cuboid B and estimated cuboidB, we define the loss function as follows. If a scene contains ground truth cuboid B and indicator variable? = 1, we compute</p><formula xml:id="formula_5">?(B,B) = 1 ? IOU(B,B) ? 1 + cos(? ? ?) 2 .<label>(4)</label></formula><p>Here, IOU(B,B) is the volume of the 3D intersection of the cuboids, divided by the volume of their 3D union. The loss is bounded between 0 and 1, and is smallest when the IOU(B,B) is near 1 and the orientation error ? ?? ? 0. The loss approaches 1 if either position or orientation is completely wrong. If a scene does not contain any ground truth instances of the object and the indicator variable? = 0 for the cuboid proposal, the loss equals 0. We penalize all other cases with a loss of 1. We solve the losssensitive objective of Eq. (3) using a cutting-plane method <ref type="bibr" target="#b78">[77]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Cuboid Hypotheses</head><p>We create cuboid proposals in a sliding-window fashion using discretized 3D world coordinates, with 16 candidate orientations. We discretize cuboid sizes using empirical statistics of the cuboid annotations in the training database: {0.1, 0.3, 0.5, 0.7, 0.9} width quantiles, {0.25, 0.5, 0.75} depth quantiles, and {0.3, 0.5, 0.8} height quantiles. Every combination of cuboid size, 3D position on the ground plane (whose height is estimated as described in Sec. 5), and 3D orientation is then evaluated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Relative Importance of 3D Cuboid Features</head><p>We explore the relative importance of different features for the detection of 5 large objects in <ref type="table" target="#tab_0">Table 1</ref>. We first trained our detector with geometric features only (Geom), with COG only (COG), with both geometric and COG features (Geom+COG), adding the camera-view feature (Geom+COG+view), and finally utilizing the expanded cuboid feature (Geom+COG+view+expanded).</p><p>The COG feature and geometric features have complementary advantages in 3D object detection, and combining them leads to improved performance. The average accuracies of object detectors improve when additional features are added, demonstrating that each step of our feature design is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M L S S</head><p>Geometric descriptors and COG descriptors are able to capture local shapes and appearances, but objects have widely varying visual styles. Moreover, 3D cuboids are labeled by different annotators from Mechanical Turk to construct the SUN RGB-D dataset <ref type="bibr" target="#b14">[15]</ref>, and thus objects in the same category may have inconsistent 3D annotations. As a result, voxel features are sometimes noisy and inconsistent across different object instances (see <ref type="figure" target="#fig_2">Fig. 5</ref>).</p><p>To explicitly model different visual styles within each object category, a classical approach is to use part-based models <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[25]</ref> where objects are explained by spatially arranged parts. For many object categories, the height of the support surface is the primary cause of style variations <ref type="figure" target="#fig_2">(Fig. 5</ref>). Therefore, we explicitly model the support surface as a latent part for each object.</p><p>By modeling support surfaces we can also constrain the search space for small object detectors. Such detectors are otherwise computationally challenging to learn, and perform poorly due to the large set of 3D pose hypotheses. COG descriptor for bed COG descriptor for bed surface <ref type="figure">Fig. 6</ref>. Visualization of 3D detection of beds and pillows using latent support surfaces. Given input RGB-D images, we use our learned COG descriptor to localize 3D objects and infer latent support surfaces (shaded) for 3D proposals of beds (red). Then we search for pillows (green) that lie on top of the inferred support surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Latent Structural SVM Learning</head><p>Some previous work was specifically designed to predict support surface regions <ref type="bibr" target="#b7">[8]</ref> from labeled training data, but the predicted support surfaces are not semantically meaningful. We instead treat the height of the support surface of each object as a latent variable and use latent structural SVMs <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b80">[79]</ref> to learn the detector.</p><p>We follow the notation in Sec. 3.4 with an updated objective. For each category c, our goal is to learn a prediction function I ? (B, h) that maps an RGB-D image I to a 3D bounding box B = (L, ?, S, y) along with its relative surface height h. The latent variable h is defined as the relative surface height to the bottom of the cuboid. We discretize cuboid height to 7 slices, and thus h localizes the support surface to one of those slices (see <ref type="figure" target="#fig_4">Fig. 7</ref>).</p><p>Given n training examples of category c, we want to solve the following optimization problem:</p><formula xml:id="formula_6">min w c ,? ?0 1 2 w T c w c + C n n i=1 ? i subject to max h i ?H w T c ?(I i , B i , h i ) ? max h i ?H w T c ?(I i ,B i ,h i ) ? ?(B i ,B i ,h i ) ? ? i , for allB i ? B i , i = 1, . . . , n.</formula><p>Here B i is the target cuboid, B i is the set of possible cuboids, and H is the set of possible surface heights. ?(I, B, h) are the features associated to cuboid B whose relative surface height is indicated by h. We first discretize B into 5 ? 5 ? 5 voxels and compute geometric, COG, view-to-camera, and expanded cuboid features, as denoted by ? cuboid <ref type="figure">(I, B)</ref>. Then we discretize B with finer resolutions at the vertical dimension into 5 ? 5 ? 7 voxels and take the h-th slice from the bottom to represent cuboid feature, as denoted by ? surface <ref type="figure">(I, B, h)</ref>. We use the same loss function defined in Sec. 3.4. To train the model with latent support surfaces, we first pre-train cuboid descriptors (geometric features, COG, view-tocamera, and scene layout features) without modeling support surfaces. We then extract the center slice of pre-trained cuboid descriptors and concatenate it to the pre-trained models. Finally, we initialize the support surface height indicator vector randomly in [0, 1]. With this informative initialization, we find that the (0, 0, 0, 1, 0, 0, 0) CCCP algorithm <ref type="bibr" target="#b81">[80]</ref> is effective at solving the (non-convex) latent structural SVM learning problem <ref type="bibr" target="#b80">[79]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Small Object Detection via Supporting Surfaces</head><p>While indoor scenes typically contain some large furniture like beds and chairs, many other objects with comparatively small physical size are very challenging to detect <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Some algorithms are specifically designed to detect small objects in 2D images using multi-scale methods <ref type="bibr" target="#b82">[81]</ref>, <ref type="bibr" target="#b83">[82]</ref>, but they cannot be directly applied to 3D object detection.</p><p>A severe issue for detecting small objects is that the search space can be enormous, and thus training and testing with slidingwindow cuboid proposals can be computationally intractable. But note that small objects, such as pillows and monitors and lamps, are usually placed on top of other objects with support surfaces. If we only search for small objects on predicted support surfaces, the search space will be greatly reduced. As a result, the inference speed will be improved and object proposals will have fewer false positives. This is another benefit of modeling support surfaces.</p><p>In our implementation, we first detect large objects and furniture that rest on the ground. Then using the cascaded detection framework described in Sec. 6, we only search for smaller objects on top of the support surfaces of those large objects with positive confidence scores. We reduce the voxel grid to 3 ? 3 ? 3 for lamps and pillows due to their small size, and to 3 ? 1 ? 3 for monitors and TVs due to their flat shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R L G : M V</head><p>Given an RGB-D image, indoor scene parsing requires not only object detection, but also room layout (floor, ceiling, wall) prediction <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b64">[63]</ref>. Such "free space" understanding is crucial for applications like robot navigation. Simple RGB-D layout prediction methods <ref type="bibr" target="#b14">[15]</ref> work by fitting planes to the observed point cloud data, but are sensitive to outliers. We propose a more accurate learning-based approach to predicting Manhattan geometries that utilizes our COG descriptor. The orthogonal walls of a standard room can be represented via a cuboid <ref type="bibr" target="#b84">[83]</ref>, and we could define geometric features via a standard voxel discretization <ref type="figure" target="#fig_5">(Fig. 8, bottom left)</ref>. However, because corner voxels usually contain the intersection of two walls, they then mix 3D normal vectors with very different orientations. This discretization also ignores points outside of the hypothesized cuboid, and may match subsets of rooms with wall-like structure.</p><p>We propose a novel Manhattan voxel <ref type="figure" target="#fig_5">(Fig. 8, bottom right)</ref> discretization for 3D layout prediction. We first discretize the vertical space between floor and ceiling into 6 equal bins. We then use a threshold of 0.15m to separate points near the walls from those in the interior or exterior of the hypothesized layout. Further using diagonal lines to split bins at the room corners, the overall space is discretized in 12 ? 6 = 72 bins. For each vertical layer, regions R 1:4 model the scene interior whose point cloud distribution varies widely across images. Regions R 5:8 model points near the assumed Manhattan wall structure: R 5 and R 6 should contain orthogonal planes, while R 5 and R 7 should contain parallel planes. Regions R 9:12 capture points outside of the predicted layout, as might be produced by depth sensor errors on transparent surfaces.</p><p>We again use the S-SVM formulation of Eq. (3) to predict Manhattan layout cuboids M = (L, ?, S). The loss function ?(M,M) is as in Eq. (4), except we use the "free-space" IOU defined by <ref type="bibr" target="#b14">[15]</ref>, and account for the fact that orientation is only identifiable modulo 90 ? rotations. Because layout annotations do not necessarily have Manhattan structure, the ground truth layout is defined as the cuboid hypothesis with the largest free-space IOU.</p><p>We predict floors and ceilings as the 0.001 and 0.999 quantiles of the 3D points along the gravity direction, and discretize orientation into 18 evenly spaced angles between 0 and 180 ? . We then propose layout candidates that capture at least 80% of all 3D points, and are bounded by the farthest and closest 3D points. For typical scenes, there are 5,000 to 20,000 layout hypotheses.</p><p>C L S C If the learned object detectors are independently applied for each category, there may be many false positives where a "piece" of a large object is detected as a smaller object (see <ref type="figure" target="#fig_7">Fig. 9</ref>). Song et al. <ref type="bibr" target="#b9">[10]</ref> reduce such errors via a heuristic reduction in confidence scores for small detections on large image segments. To avoid such manual engineering, which must be tuned to each category for peak performance, we propose to directly learn the relationships among detections of different categories. As room geometry is also an important cue for object detection, we integrate Manhattan layout hypotheses for holistic scene understanding <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b53">[52]</ref>.</p><p>Classically, structured prediction of spatial relationships is often accomplished via undirected Markov random fields (MRFs) <ref type="bibr" target="#b85">[84]</ref>. As shown in <ref type="figure" target="#fig_7">Fig. 9</ref>, this generally leads to a fully connected graph <ref type="bibr" target="#b86">[85]</ref> because there are relationships among every pair of object categories. An extremely challenging MAP estimation (or energy minimization) problem must then be solved at every training iteration, as well as for each test image, so learning and prediction are costly.</p><p>We propose to instead adapt cascaded classification <ref type="bibr" target="#b19">[20]</ref> to the modeling of contextual relationships in 3D scenes. In this approach, "first-stage" detections as in Sec. 3.4 become input features to "second-stage" classifiers that estimate confidence in the correctness of cuboid hypotheses. This can be interpreted as a directed graphical model with hidden variables. Marginalizing the first-stage variables recovers a standard, fully-connected undirected graph. Crucially however, the cascaded representation is far more efficient: training decomposes into independent learning problems for each node (object category), and optimal test classification is possible via a rapid sequence of local decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Contextual Features</head><p>For an overlapping pair of detected bounding boxes B i and B j , we denote their volumes as V(B i ) and V(B j ), the volume of their overlap as O(B i , B j ), and the volume of their union as U(B i , B j ). We characterize their geometric relationship via three features: First-stage detectors provide a most-probable layout hypothesis, as well as a set of detections (following non-maximum suppression) for each category. For a bounding box B i with confidence score z i , there may be several overlapping bounding boxes of categories c ? {1, . . . , C}. Letting i c be the instance of category c with maximum confidence z i c , features ? i for bounding box B i are created via a quadratic function of z i , S 1:3 (i, i c ), A(B i , M), and a radial basis expansion of D(B i , M). Relationships between secondstage layout candidates and object cuboids are modeled similarly.</p><formula xml:id="formula_7">S 1 (i, j) = O(B i ,B j ) V (B i ) , S 2 (i, j) = O(B i ,B j ) V (B j ) , and the intersection-over- union S 3 (i, j) = O(B i , B j ) U(B i ,</formula><p>For small objects that are placed on the support surfaces of large objects, 3D overlap features are noisy. We replace 3D overlap with 2D overlap scores from the top-down view of the scene <ref type="figure" target="#fig_0">(Fig. 10)</ref>. See the Appendix for further details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Contextual Learning</head><p>Due to the directed graphical structure of the cascade, each second-stage detector may be learned independently. The objective is a simple binary classification: is the candidate detection a true positive, or a false positive? During training, each detected bounding box for each class is marked as "true" if its intersectionover-union score to a ground truth instance is greater than 0.25, and is the largest among such detections. We train a standard binary SVM with a radial basis function (RBF) kernel</p><formula xml:id="formula_8">K(B i , B j ) = exp ??||? i ? ? j || 2 .<label>(5)</label></formula><p>The bandwidth parameter ? is chosen using validation data. While we use a RBF kernel for all reported experiments, the performance of a linear SVM is only slightly worse, and cascaded classification still provides useful performance gains for that more scalable training objective.</p><p>To train the second-stage layout predictor (the bottom node in <ref type="figure" target="#fig_7">Fig. 9</ref>), we combine the object-layout features with the Manhattan voxel features from Sec. 5, and again use S-SVM training to optimize the free-space IOU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Contextual Prediction</head><p>During testing, given the set of cuboids found in the firststage sliding-window search, we apply the second-stage cascaded classifier to each cuboid B i to get a new contextual confidence score z i . The overall confidence score used for precision-recall evaluation is then z i + z i , to account for both the original belief <ref type="figure" target="#fig_0">Fig. 10</ref>. To model contextual relationships between small objects and the large objects supporting them, we compute the 2D areas and overlaps between 3D bounding boxes (left) seen from a top-down view (right).</p><p>from the geometric and COG features and the correcting power of contextual cues. The second-stage layout prediction is directly provided by the second-stage S-SVM classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E</head><p>We train our 3D object detection algorithm solely on the SUN RGB-D dataset <ref type="bibr" target="#b14">[15]</ref> with 5285 training images, and report performance on 5050 test images for all 19 object categories ( <ref type="table" target="#tab_2">Table 2)</ref>. The NYU Depth dataset <ref type="bibr" target="#b5">[6]</ref> has 3D cuboid labels for 1449 images, but annotations are noisy and inconsistent. Some previous work has only evaluated detection performance on this small dataset <ref type="bibr" target="#b55">[54]</ref>, or defined their own annotations for 3D cuboids <ref type="bibr" target="#b60">[59]</ref>. We do not evaluate on the NYU Depth dataset because it is a subset of SUN RGB-D.</p><p>We evaluate detection performance via the intersection-overunion (IOU) with ground-truth cuboid annotations, and consider the predicted cuboid to be correct when the IOU is above 0.25. To evaluate the layout prediction performance, we calculate the free space IOU with human annotations. We provide results demonstrating the effectiveness of our 3D scene understanding system, and the importance of both appearance and context features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Modeling Latent Support Surfaces</head><p>For objects such as beds, tables, and desks, modeling support surface as a latent variable helps capture the intra-class style variations within each cuboid. We visualize examples of inferred support surfaces in <ref type="figure" target="#fig_0">Figure 15</ref>. For objects that do not have explicit "support surfaces", such as bathtub, bookshelf, and sink, our model can be viewed as a single part-based model and is also effective for 3D object detection. Note that the goal of this work is to model latent support surfaces to boost 3D detection accuracy, not to predict accurate supporting regions in scenes. We do not use any annotations of support surfaces when training, and also do not evaluate our performance on surface prediction benchmarks <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Small Object Detection</head><p>Detecting small objects is a challenging task, and achieving high accuracy remains an open research problem. Without modeling support surfaces, our baseline detectors completely fail to detect small objects because the search space is large and 3D object proposals contain many false positives. Using simple heuristics to check support relationships in the SUN-RGBD annotations, we find that more than 95% of lamps/pillows/monitors/TVs are <ref type="figure" target="#fig_0">Fig. 11</ref>. Comparison of our Manhattan voxel 3D layout predictions (blue) to the SUN RGB-D baseline <ref type="bibr" target="#b14">[15]</ref> (green) and the ground truth annotations (red). Our learning-based approach is less sensitive to outliers and degrades gracefully in cases where the true scene structure violates the Manhattan world assumption.</p><p>placed on the surface of night-stands/tables/beds/desks/dressers. As shown in <ref type="table" target="#tab_2">Table 2</ref>, searching on predicted surfaces thus enables our algorithm to discover small objects with higher precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">The Importance of Context</head><p>To show that the cascaded classifier helps to prune false positives, we evaluate detections using the confidence scores from the firststage classifier (surface), as well as the updated confidence scores from the second-stage classifier using all object-to-object features (+context). As shown in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure" target="#fig_0">Fig. 12</ref>, adding a contextual cascade clearly boosts performance. Furthermore, when objectto-scene-layout features are included (+layout), performance increases further. This result demonstrates that even if a small number of object categories are of primary interest, building models of the broader scene can be very beneficial.</p><p>We show some representative detection results in <ref type="figure" target="#fig_0">Fig. 14.</ref> In the first image our chair detector is confused and fires on part of the sofa, but with the help of contextual cues of other detected bounding boxes, these false positives are pruned away. For a fixed threshold across all object categories, we have as many true detections while producing fewer false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Cubical Voxels versus Manhattan Voxels</head><p>We use the free-space IOU <ref type="bibr" target="#b14">[15]</ref> to evaluate layout prediction performance. Using standard cubical voxels, our performance (72.33) is similar to the heuristic SUN RGB-D baseline <ref type="bibr">(73.4, [15]</ref>). Combining Manhattan voxels with structured learning, performance increases to 78.96, demonstrating the effectiveness of this improved discretization. Furthermore, if we also incorporate contextual cues from detected objects, the score improves to 80.03. We provide layout prediction examples in <ref type="figure" target="#fig_0">Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Computational Speed</head><p>We implemented our algorithm using MATLAB in a 2.5GHz single core CPU. The computational speed of our detector is 10-30min per image. The most time-consuming part is the feature computation step, which could be improved by using parallel computing with multi-core CPUs or GPUs. With pre-computed cuboid features for each RGB-D image, the inference time is 2sec for each object category. With pre-computed contextual features among all objects, the cascaded prediction framework takes less than 0.5sec on average. The training time ranges from 2 to 12 hours per category, depending on the number of training instances.</p><p>Other deep learning-based 3D detection systems <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b50">[49]</ref> typically have a region proposal step that highly constrains the search space for all object categories. Our cuboid proposals are dense and extensive, and thus the computational speed is usually slower. This limitation of our system could be potentially alleviated by pre-processing the data using a region proposal network <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Comparison to Other Methods</head><p>This paper has several differences from our preliminary work <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b87">[86]</ref>. Our use of expanded cuboid features is new, and contributes to our overall 3D detection performance. Some implementation details also differ, for example <ref type="bibr" target="#b87">[86]</ref> uses scene category features while this paper does not. Also <ref type="bibr" target="#b11">[12]</ref> uses a 6 ? 6 ? 6 discretization of cuboids into voxels, and uses only images containing at least one object instance for structural SVM training of detectors.</p><p>Compared to other methods that use CNN features <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b61">[60]</ref> pretrained on external datasets, our COG-based 3D object detector has comparable or better performance even without the contextual cues provided by our cascaded classifier. Conventional CNNs for 3D detection <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b61">[60]</ref> are trained to produce weighted confidence scores for each of multiple object categories, while our first-stage detector is instead tuned to discriminatively localize individual categories in 3D. Our subsequent cascaded prediction <ref type="bibr" target="#b19">[20]</ref> of contextual relationships between object detections has structural similarities to a multi-stage neural network, but it is trained using (convex) structural SVM loss functions and designed to have a more interpretable, graphical structure. Interestingly, our overall cascaded approach is more accurate than standard 3D CNNs <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b50">[49]</ref>, <ref type="bibr" target="#b61">[60]</ref> in the detection of both 10 and 19 object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C</head><p>We propose a geometric framework for 3D cuboid detection and Manhattan layout prediction from RGB-D images. Using our novel COG descriptor of 3D cuboid appearance, we train accurate 3D object detectors for nineteen categories, as well as a cascaded classifier that learns contextual cues to boost performance. Modeling the height of support surfaces as latent variables further increases detection accuracy for large objects, and constrains the search space to make the detection of small objects feasible.</p><p>Our scene representations are learned directly from RGB-D data without external CAD models, and thus may be easily generalized to many other object categories. Gradient-based detectors incorporating cloud of oriented gradient (COG) features achieve state-of-the-art performance on the challenging SUN RGB-D dataset. We hypothesize that our improvement over baseline methods incorporating deep learning is due to the superior ability of COG descriptors to generalize to novel 3D viewpoints. Incorporating similar geometric invariances into convolutional networks is a promising area for future research.   <ref type="bibr" target="#b14">[15]</ref>. Modeling support surfaces (+surface) simultaneously helps detect large objects and reduces false positives for small objects (last 4 categories). The final stage of the cascaded classifier (+cascade) models object context and possibly also layout context (+layout). These cues reduce false positives and boost average performance to the state-of-the-art for the first 10, as well as all 19, object categories.  <ref type="bibr" target="#b14">[15]</ref>. We compare our COG detector with latent support surfaces, and possibly also context and layout cues, to the deep sliding shape (DSS) method <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bathtub</head><p>Bookshelf Sofa Monitor Lamp Detections with confidence scores larger than the same threshold for each stage of our cascaded classification framework. Notice that using contextual information helps prune away false positives while preserving true positives.</p><p>Groundtruth Annotations for RGB-D Images Our Final Stage 3D Detection Output <ref type="figure" target="#fig_0">Fig. 15</ref>. Visualizing our final stage 3D detections for objects with high confidence scores. Support surfaces are depicted with faded colors inside each large object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C F</head><p>We give a more detailed specification of the contextual features we use to model object-object and object-layout relationships. The first-stage detectors provide a most-probable layout hypothesis, as well as a set of detections (following non-maximum suppression) for each category. For each bounding box B i with confidence score z i , there may be several bounding boxes of various categories c ? {1, 2, ..., C} that overlap with it. We let i c be the instance of category c with the maximum confidence score z i c . The features ? i for bounding box B i are then as follows: 1) Constant bias feature, and confidence score z i from the first-stage detector. 2) For m ? {1, 2, 3}, c ? {1, 2, ..., C}, we calculate S m (i, i c ), S m (i, i c ) ? z i c , S m (i, i c ) ? z i and concatenate those numbers. 3) For c ? {1, 2, ..., C}, we calculate the difference in confidence score from each first-stage detector, z i ? z i c , and concatenate those numbers. 4) For D(B i , M), we consider radial basis functions of the form in Eq. 2. For a typical indoor scene, the largest object-to-wall distance is usually less than 5m, therefore we space the basis function centers ? j evenly between 0 and 5 with step size 0.5, and choose ? = 0.5. We expand D(B i , M) using this radial basis expansion. 5) The absolute value of cosine D(B i , M): | cos(D(B i , M))|.</p><p>To model the second-stage layout candidates, we select the bounding box i c with the highest confidence score z i c from the first-stage classifier in each category c ? {1, 2, ..., C}, and use the following features for layout M i with confidence score z i : </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Given input RGB and Depth images (left), we align oriented cuboids and transform observed data into a canonical coordinate frame. For each voxel, we then extract (from left to right) point cloud density features, 3D normal orientation histograms, and COG descriptors of back-projected image gradient orientations. COG bins (left) are colored to show the alignment between instances. The value of the point cloud density feature is proportional to the voxel intensity, each 3D orientation histogram bin is assigned a distinct color, and COG features are proportional to the normalized energy in each orientation bin, similarly to HOG descriptors<ref type="bibr" target="#b24">[24]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Visualizing the learned weights for the COG (left) and expanded COG (right) features. Although chairs and toilets have similar geometric structures, the appearance of the 3D environment immediately surrounding them is different, producing local contextual cues captured by our expanded COG features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Different surface heights for instances of the "desk" category in SUN the RGB-D dataset<ref type="bibr" target="#b14">[15]</ref> lead to inconsistent 3D COG representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Finally we add an indicator vector for support surface height, so that ?(I, B, h) = [? cuboid (I, B), ? surface (I, B, h), 0, ..., 1, ..., 0].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>COG features for 3D cuboids and support surfaces. The surface feature is computed within a single slice of the cuboid, and concatenated with an indicator vector encoding the relative height. Expanded cuboid features are not visualized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Models for the 3D layout geometry of indoor scenes. Top: Ground truth annotation. Bottom: Top-down view of the scene and two voxel-based quantizations. We compare a regular voxel grid (left) to our Manhattan voxels (right; dashed red line is the layout hypothesis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>B j ) . To model contextual relations between objects and the scene layout M [52], we compute the distance D(B i , M) and angle A(B i , M) of cuboid B i to the closest wall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Cascaded classifiers capture contextual relationships among objects. From left to right: (i) A traditional undirected MRF representation of contextual relationships. Colored nodes represent object categories, and black nodes represent the room layout. (ii) A directed graphical representation of cascaded classification, where the first-stage detectors are hidden variables (dashed) that model contextual relationships among object and layout hypotheses (solid). Marginalizing the hidden nodes recovers the undirected MRF. (iii) First-stage detections independently computed for each category as in Sec. 3.4. (iv) Second-stage detections (Sec. 6) efficiently computed using our directed representation of context, and capturing contextual relationships between objects and the overall scene layout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Precision-recall curves for several object categories, including monitors which are supported by the surfaces of other objects, on the SUN RGB-D dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Visualization of learned 3D COG descriptors with expanded cuboid features for several categories. Reference orientation bins with larger weights are darker, providing a 3D visualization of the typical appearance of each object category. Cuboid sizes are matched to the median of the training data.Ground TruthFirst-stage Second Stage Second StageFig. 14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 )A</head><label>1</label><figDesc>All the features used in the first-stage to model M i using Manhattan Voxels. 2) For c ? {1, 2, ..., C}, we calculate the radial basis expansion for D(B i c , M i ), and its product with z i and z i c . 3) For c ? {1, 2, ..., C}, we calculate the absolute value of the cosine of D(B i c , M i ): | cos(D(B i c , M i ))|, | cos(D(B i c , M i ))| ? z i and | cos(D(B i c , M i ))| ? z i c . 4) For c ? {1, 2, ..., C}, we calculate the difference in confidence score from each first-stage detector, z i ? z i c , and concatenate those numbers. This research is supported in part by the Office of Naval Research (ONR) under Award Numbers N00014-13-1-0644 and N00014-17-1-2094, and by a pilot grant from the Brown University Center for Vision Research.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Average precision scores for five object categories (bed, bathtub, nightstand, chair, toilet) given various sets of 3D cuboid features.</figDesc><table><row><cell>Geom</cell><cell>45.0</cell><cell>37.9</cell><cell>2.3</cell><cell>36.2</cell><cell>55.2</cell></row><row><cell>COG</cell><cell>52.5</cell><cell>42.8</cell><cell>6.7</cell><cell>22.6</cell><cell>49.6</cell></row><row><cell>Geom+COG</cell><cell>53.0</cell><cell>49.8</cell><cell>12.8</cell><cell>39.0</cell><cell>63.6</cell></row><row><cell>Geom+COG+view</cell><cell>52.8</cell><cell>53.2</cell><cell>16.8</cell><cell>40.4</cell><cell>57.8</cell></row><row><cell>Geom+COG+view+expanded</cell><cell>63.8</cell><cell>63.8</cell><cell>29.2</cell><cell>64.1</cell><cell>80.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Bathtub Bed Bookshelf Chair Desk Dresser Nightstand SofaTable Toilet Box Door Counter Garbage-bin Sink Pillow Monitor TV</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lamp</cell><cell>mAP (10)</cell><cell>mAP (19)</cell></row><row><cell>surface</cell><cell cols="3">69.9 73.2 19.0</cell><cell cols="2">63.2 35.9 19.9</cell><cell cols="3">26.0 58.1 46.7 81.9 12.6 3.4</cell><cell>5.7</cell><cell>30.4</cell><cell cols="2">30.8 8.3</cell><cell cols="5">10.0 1.6 23.7 49.4 32.6</cell></row><row><cell>+context</cell><cell cols="3">71.3 76.8 24.8</cell><cell cols="2">67.4 40.4 22.8</cell><cell cols="4">39.0 60.5 51.4 85.6 14.4 4.1 12.6</cell><cell>34.7</cell><cell cols="2">38.7 7.7</cell><cell cols="5">11.4 2.1 24.0 54.0 36.3</cell></row><row><cell>+layout</cell><cell cols="3">72.0 76.8 25.5</cell><cell cols="2">67.2 41.0 23.7</cell><cell cols="4">39.7 60.4 51.7 85.6 14.4 4.1 12.7</cell><cell>34.9</cell><cell cols="2">39.4 7.6</cell><cell cols="5">11.4 2.1 25.4 54.3 36.6</cell></row><row><cell>DSS [17]</cell><cell cols="3">44.2 78.8 11.9</cell><cell cols="2">61.2 20.5 6.4</cell><cell cols="3">15.4 53.5 50.3 78.9 1.5 0.0</cell><cell>4.1</cell><cell>20.4</cell><cell cols="2">32.3 13.3</cell><cell cols="5">0.2 0.5 18.4 42.1 26.9</cell></row><row><cell>C3D [68]</cell><cell cols="3">60.3 82.9 33.9</cell><cell cols="2">63.7 22.1 18.5</cell><cell cols="4">30.6 56.5 57.3 85.7 18.5 5.0 10.1</cell><cell>25.7</cell><cell cols="2">35.0 16.1</cell><cell cols="5">4.7 4.8 25.3 51.2 34.6</cell></row><row><cell cols="4">Ren2018 [86] 76.2 73.2 32.9</cell><cell cols="2">60.5 34.5 13.5</cell><cell cols="4">30.4 60.4 55.4 73.7 19.5 5.4 10.7</cell><cell>34.6</cell><cell cols="2">75.3 12.5</cell><cell cols="5">1.6 2.1 16.9 51.0 36.3</cell></row><row><cell cols="4">Ren2016 [12] 58.3 63.7 31.8</cell><cell cols="2">62.2 45.2 15.5</cell><cell cols="2">27.4 51.0 51.3 70.1 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-47.6 -</cell></row><row><cell cols="4">Lahoud [60] 43.5 64.5 31.4</cell><cell cols="2">48.3 27.9 25.9</cell><cell cols="2">41.9 40.4 37.0 80.4 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-45.1 -</cell></row><row><cell cols="4">Frustum [49] 43.3 81.1 33.3</cell><cell cols="2">64.2 24.7 32.0</cell><cell cols="2">58.1 61.1 51.1 90.9 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-54.0 -</cell></row><row><cell>SS [10]</cell><cell>-</cell><cell>43.0</cell><cell>-</cell><cell>28.2 -</cell><cell>-</cell><cell>-</cell><cell>20.6 19.7 60.9 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Experimental results on the SUN RGB-D dataset</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric reasoning for single image structure recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2136" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Thinking inside the box: Using appearance models and context based on room geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="224" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimating the 3D layout of indoor scenes and its clutter from depth sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1273" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unfolding an indoor origami world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="687" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support surface prediction in indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3D object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="634" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1525" to="1533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a database of 3D scenes from user annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2711" to="2718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multi-view RGB-D object dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1817" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3D object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D object detection and viewpoint estimation with a deformable 3D cuboid model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint 3D object and layout inference from a single RGB-D image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cascaded classification models: Combining models for holistic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Manhattan world: Compass direction from a single image by Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="941" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient structured prediction for 3D indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2815" to="2822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast dynamic programming for labeling problems with ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1728" to="1735" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3D shapenets for 2.5D object recognition and next-best-view prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A linear approach to matching cuboids in RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3D-based reasoning with blocks, support, and stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Localizing 3d cuboids in singleview images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Roomnet: End-to-end room layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Box in the box: Joint 3D layout and object reasoning from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imagining the unseen: Stability-based cuboid arrangements for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH ASIA)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deepcontext: Context-encoding neural pathways for 3D holistic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning category-specific deformable 3D models for object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">3D object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3D bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko?eck?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="5632" to="5640" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Data-driven 3D voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1903" to="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-view 3D object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3D object detection with RGBD cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Blocks world revisited: Image understanding using qualitative geometry and mechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="482" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Aligning 3D models to RGB-D images of cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Seeing 3D chairs: Exemplar part-based 2D-3D alignment using a large dataset of CAD models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Parsing IKEA objects: Fine pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">FPM: Fine pose parts-based model with 3D CAD models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="478" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Voxnet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Amodal detection of 3D objects: Inferring 3D bounding boxes from 2d ones in rgb-depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">2D-driven 3D object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Marr revisited: 2D-3D alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5965" to="5974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1849" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning informative edge maps for indoor scene layout prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Layoutnet: Reconstructing the 3d room layout from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Putting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Context-assisted 3d (c3d) object detection from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3D scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">From contours to 3d object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Payet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="983" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">3D extended histogram of oriented gradients (3dhog) for classification of road users in urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Velastin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for 3D object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3D object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deepbox: Learning objectness with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2479" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3376" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Structured output regression for detection with partial occulsion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The concave-convex procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="915" to="936" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">R-CNN for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Sampling bedrooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Pero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2009" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Structured learning and prediction in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="185" to="365" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">3D object detection with latent support surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
