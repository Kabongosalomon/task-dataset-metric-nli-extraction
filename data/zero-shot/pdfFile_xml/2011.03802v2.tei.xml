<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Symmetric Parallax Attention for Stereo Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
							<email>wangyingqian16@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Ying</surname></persName>
							<email>yingxinyi18@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
							<email>yangjungang@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Symmetric Parallax Attention for Stereo Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although recent years have witnessed the great advances in stereo image super-resolution (SR), the beneficial information provided by binocular systems has not been fully used. Since stereo images are highly symmetric under epipolar constraint, in this paper, we improve the performance of stereo image SR by exploiting symmetry cues in stereo image pairs. Specifically, we propose a symmetric bi-directional parallax attention module (biPAM) and an inline occlusion handling scheme to effectively interact cross-view information. Then, we design a Siamese network equipped with a biPAM to super-resolve both sides of views in a highly symmetric manner. Finally, we design several illuminance-robust losses to enhance stereo consistency. Experiments on four public datasets demonstrate the superior performance of our method. Source code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With recent advances in stereo vision, dual cameras are commonly adopted in mobile phones and autonomous vehicles. Using the complementary information (i.e., crossview information) provided by binocular systems, the resolution of image pairs can be enhanced. However, it is challenging to achieve good performance in stereo image superresolution (SR) due to the following issues: 1) Varying parallax. Objects at different depths have different disparity values and thus locate at different positions along the horizontal epipolar line. It is challenging to capture reliable stereo correspondence and effectively integrate cross-view information for stereo image SR. 2) Information incorporation. Since context information within a single view (i.e., intra-view information) is crucial and contributes to stereo image SR in a different manner, it is important but challenging to fully incorporate both intra-view and cross-view information. 3) Occlusions &amp; boundaries. In occlusion and boundary areas, pixels in one view cannot find their corre-* Yingqian Wang and Xinyi Ying contribute equally to this work and are co-first authors. ? Corresponding author: Jungang Yang. spondence in the other view. In this case, only intra-view information is available for stereo image SR. It is challenging to fully use cross-view information in non-occluded regions while maintaining promising performance in occluded regions.</p><p>Recently, several methods have been proposed to address the above issues. Wang et al. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> addressed the varying parallax issue by proposing a parallax attention module (PAM), and developed a PASSRnet for stereo image SR. Ying et al. <ref type="bibr" target="#b31">[32]</ref> addressed the information incorporation issue by equipping several stereo attention modules (SAMs) to the pre-trained single image SR (SISR) networks. Song et al. <ref type="bibr" target="#b18">[19]</ref> addressed the occlusion issue by checking stereo consistency using disparity maps regressed by parallax attention maps. Although continuous improvements have been achieved, the inherent correlation within stereo image pairs are still under exploited, which hinders the performance of stereo image SR.</p><p>Since super-resolving left and right images are highly symmetric, the inherent correlation within an image pair can be fully used by exploiting its symmetry cues. In this paper, we improve the performance of stereo image SR by exploiting symmetries on three levels. 1) On the module level, we design a symmetric bi-directional parallax attention module (biPAM) to interact cross-view information. With our biPAM, occlusion maps can be generated and used as a guidance for cross-view feature fusion. 2) On the network level, we propose a Siamese network equipped with our biPAM to super-resolve both left and right images. Experimental results demonstrate that jointly super-resolving both sides of views can better exploit the correlation between stereo images and is contributive to SR performance. 3) On the optimization level, we exploit symmetry cues by designing several bilateral losses. Our proposed losses can enforce stereo consistency and is robust to illuminance changes between stereo images. We perform extensive ablation studies to validate the effectiveness of our method. Comparative results on the KITTI 2012 <ref type="bibr" target="#b3">[4]</ref>, KITTI 2015 <ref type="bibr" target="#b13">[14]</ref>, Middlebury <ref type="bibr" target="#b16">[17]</ref> and Flickr1024 <ref type="bibr" target="#b26">[27]</ref> datasets have demonstrated the competitive performance of our method as compared to many state-of-the-art SR methods.</p><formula xml:id="formula_0">Conv-0 RDB-1 RDB-4 RDB-1 RDB-4 biPAM RDB-F CALayer RDB-R1 RDB-R4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-pixel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction Cross-view Interaction Reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-1f</head><p>Conv-1f RDB-F CALayer RDB-R1 RDB-R4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-3f</head><p>Conv-0  Our proposed method is named iPASSR since it is an improved version of our previous PASSRnet <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>. The contributions of this paper are as follows: 1) We propose to exploit symmetry cues for stereo image SR. Different from PASSRnet, our iPASSR can super-resolve both sides of views within a single inference. 2) We develop a symmetric and bi-directional parallax attention module. Compared to PAMs in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>, our biPAM is more compact and can effectively handle occlusions. 3) As demonstrated in the experiments, our iPASSR can achieve significant performance improvements over PASSRnet with a comparable model size.</p><p>The rest of this paper is organized as follows. In Section 2, we briefly review the related work. In Section 3, we introduce our proposed method including network architecture, occlusion handling scheme, and loss functions. Experimental results are presented in Section 4. Finally, we conclude this paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single Image SR. SISR is a long-standing problem and has been investigated for decades. Recently, deep learningbased SISR methods have achieved promising performance in terms of both reconstruction accuracy <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> and visual quality <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b33">34]</ref>. Dong et al. <ref type="bibr" target="#b2">[3]</ref> proposed the first CNN-based SR network named SRCNN to reconstruct high-resolution (HR) images from low-resolution (LR) inputs. Kim et al. <ref type="bibr" target="#b7">[8]</ref> proposed a very deep network (VDSR) with 20 layers to improve SR performance. Afterwards, SR networks became increasingly deep and complex, and thus more powerful in intra-view information exploitation. Lim et al. <ref type="bibr" target="#b11">[12]</ref> proposed an enhanced deep SR network (EDSR) using both local and global residual connections. Zhang et al. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> combined residual connection with dense connection, and proposed residual dense network (RDN) to fully exploit hierarchical feature representations. More recently, the performance of SISR has been further improved by RCAN <ref type="bibr" target="#b35">[36]</ref>, RNAN <ref type="bibr" target="#b36">[37]</ref> and SAN <ref type="bibr" target="#b1">[2]</ref>.</p><p>Stereo Image SR. Compared to SISR which exploits context information within only one view, stereo image SR aims at using the cross-view information provided by stereo images. Jeon et al. <ref type="bibr" target="#b5">[6]</ref> proposed a network named StereoSR to learn a parallax prior by jointly training two cascaded sub-networks. The cross-view information is integrated by concatenating the left image and a stack of right images with different pre-defined shifts. Wang et al. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> proposed a parallax attention module to learn stereo correspondence with a global receptive field along the epipolar line. Ying et al. <ref type="bibr" target="#b31">[32]</ref> proposed a stereo attention module and embedded it into pre-trained SISR networks for stereo image SR. Song et al. <ref type="bibr" target="#b18">[19]</ref> combined self-attention with parallax attention for stereo image SR. Furthermore, stereo consistency was addressed by using disparity maps regressed from parallax attention maps. Yan et al. <ref type="bibr" target="#b28">[29]</ref> proposed a domain adaptive stereo SR network (i.e., DASSR). Specifically, they first explicitly estimated disparities using a pretrained stereo matching network <ref type="bibr" target="#b6">[7]</ref> and then warped views to the other side to incorporate cross-view information. More recently, Xu et al. <ref type="bibr" target="#b27">[28]</ref> incorporated bilateral grid processing into CNNs and proposed a BSSRnet for stereo image SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce our method in details. We first introduce the architecture of our network in Section 3.1, then describe the inline occlusion handling scheme in Section 3.2. Finally, we present the losses in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>Our network takes a pair of LR RGB stereo images I input L and I input R as its inputs to generate HR RGB stereo images I SR L and I SR R . As shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, our network is highly symmetric and the weights of its left and right branches are shared. Given LR input stereo images, our network sequentially performs feature extraction, cross-view interaction, and reconstruction.</p><p>Feature Extraction. In our feature extraction module, input stereo images I input L , I input R ? R H?W ?3 are first fed to a convolution layer (i.e., Conv-0) to generate initial features F 0 L , F 0 R ? R H?W ?64 , which are then fed to 4 cascaded residual dense blocks (RDBs) 1 for deep feature extraction. As shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, 4 convolutions with a growth rate of 24 are used within each RDB to achieve dense feature representation. Note that, features from all the layers in an RDB are concatenated and fed to a 1?1 convolution to generate fused features for local residual connection.</p><p>Cross-view Interaction. We propose a bi-directional parallax attention module (biPAM) to interact cross-view information of stereo features. Since hierarchical feature representation is beneficial to stereo correspondence learning <ref type="bibr" target="#b22">[23]</ref>, we form the inputs of our biPAM by concatenating the output features of each RDB in our feature extraction module. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>(c), the input stereo features are first fed to a batch-normalization (BN) layer and a transition residual block (i.e., ResB), and then separately fed to 1?1 convolutions to generate F U , F V ? R H?W ?64 . To achieve disentangled pairwise parallax attention, we follow <ref type="bibr" target="#b30">[31]</ref> and feed F U and F V to a whiten layer to obtain normalized features F U and F V according to</p><formula xml:id="formula_1">F U (h, w, c) = F U (h, w, c) ? 1 W W i=1 F U (h, i, c), (1) F V (h, w, c) = F V (h, w, c) ? 1 W W i=1 F V (h, i, c). (2)</formula><p>To generate left and right attention maps, F V is first transposed to F V T ? R H?64?W , and then batch-wisely multiplied (see Section 3.2) with F U to produce an initial score map S ? R H?W ?W . Then, softmax normalization is applied to S and S T along their last dimension to generate attention maps M R?L and M L?R , respectively. To achieve cross-view interaction, both left and right features (generated by Conv-1f in <ref type="figure" target="#fig_0">Fig. 1</ref>(a)) need to be converted to the other side by taking a batch-wise matrix multiplication with the corresponding attention maps, i.e.,</p><formula xml:id="formula_2">F R?L = M R?L ? F R ,<label>(3)</label></formula><formula xml:id="formula_3">F L?R = M L?R ? F L ,<label>(4)</label></formula><p>where ? denotes the batch-wise matrix multiplication.</p><p>To avoid unreliable correspondence in occlusion and boundary regions, we propose an inline occlusion handling scheme to calculate valid masks V L and V R . The final converted features F R?L and F L?R can be obtained by</p><formula xml:id="formula_4">F R?L = V L F R?L + (1 ? V L ) F L ,<label>(5)</label></formula><p>1 The insights of using RDBs for feature extraction are two-folds: First, RDB can generate features with large receptive fields and dense sampling rates, which are beneficial to stereo correspondence estimation. Second, RDB can fully use features from all the layers via local dense connection. The generated hierarchical features are beneficial to SR performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Right Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MR?L (h,:,:)</head><formula xml:id="formula_5">(h,w1,w2) (h,w2) (a) (b) Softmax ? = (h,w1) I L I R h h MR?L (h,:,:) disparity=0</formula><p>disparity=5 disparity=10 <ref type="figure">Figure 2</ref>: A toy example to depict the stereo correspondence. The gray, red, and green regions in I L and I R denote objects with a disparity of 0, 5, and 10 pixels, respectively. For simplicity, only a profile of M R?L at height h is visualized, which corresponds to the regions marked by yellow strokes in (a). Occlusions (colored in black on the strokes) are implicitly encoded in the attention maps as empty intervals. (b) The right stroke can be converted into the left side by multiplying it with M R?L .</p><formula xml:id="formula_6">F L?R = V R F L?R + (1 ? V R ) F R ,<label>(6)</label></formula><p>where represents element-wise multiplication. Note that, values in V L and V R range from 0 (occluded) to 1 (nonoccluded). According to Eqs. <ref type="formula" target="#formula_4">(5)</ref> and <ref type="formula" target="#formula_6">(6)</ref>, occluded regions of converted features (i.e., F R?L , F L?R ) can be filled with the corresponding features from the target view (i.e., F L , F R ), resulting in continuous spatial distributions.</p><p>Reconstruction. Similar to the feature extraction module, we use RDB as the basic block in our reconstruction module. Taking the left branch as an example, F R?L is first concatenated with F L and then fed to an RDB (i.e., RDB-F) for initial feature fusion. The output feature F init f L ? R H?W ?128 is then fed to a channel attention layer (i.e., CALayer <ref type="bibr" target="#b35">[36]</ref>) and a convolution layer (i.e., Conv-2f ) to produce the final fused feature F f L ? R H?W ?64 . Afterwards, F f L is fed to 4 cascaded RDBs, a convolution layer (i.e., Conv-3f ), and a sub-pixel layer <ref type="bibr" target="#b17">[18]</ref> to generate the super-resolved left image I SR L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inline Occlusion Handling Scheme</head><p>By using biPAM, the stereo correspondence can be generated in a symmetric manner. More importantly, the occlusions can be derived by checking the stereo consistency using the attention maps M R?L and M L?R .</p><p>Here, we use a toy example in <ref type="figure">Fig. 2</ref> to illustrate how occlusions are implicitly encoded in the parallax attention maps. Given a pair of stereo images I L and I R ? R H?W , parallax attention maps M R?L , M L?R ? R H?W ?W can be generated. As illustrated in <ref type="figure">Fig. 2(a)</ref>, we visualize a profile of M R?L at height h (i.e., M R?L (h, :, :)), which corresponds to the yellow strokes in the left and right images. Note that, black strokes represent occluded regions.</p><p>It can be observed from <ref type="figure">Fig. 2</ref>(a) that: 1) Occlusions occur near object edges where the depth values change suddenly, or occur near image boundaries (more specifically, left boundary of the left view and right boundary of the right view).</p><p>2) The occluded regions correspond to the empty intervals in the attention maps since their counterparts in the other view are unavailable. These two observations demonstrate that occlusions are implicitly encoded in the parallax attention maps and can be calculated by checking the cycle consistency using M R?L and M L?R . Specifically, the right image can be converted into the left side according to</p><formula xml:id="formula_7">I R?L = M R?L ? I R ,</formula><p>where ? represents the batch-wise matrix multiplication. As shown in <ref type="figure">Fig. 2(b)</ref>, the product of a slice of the right image (i.e., I R (h, :)) and the corresponding profile of the attention map (i.e., M R?L (h, :, :)) determines the slice of the converted left image at the same height (i.e., I R?L (h, :)). All these resulting slices are concatenated to produce I R?L .</p><p>Note that, softmax normalization has been performed along the third dimension of M R?L and M L?R . Therefore, M R?L (h, w 1 , w 2 ) can be considered as the matching possibility between I R (h, w 2 ) and I L (h, w 1 ). Furthermore, the possibility that I L (h, w 1 ) is first converted to I R and then re-converted to I L (h, w 1 ) can be calculated according to</p><formula xml:id="formula_8">P L (h, w 1 ) = W w2=1 M R?L (h, w 1 , w 2 ) ? M L?R (h, w 2 , w 1 ). (7)</formula><p>Note that, P L (h, w 1 ) is close to 0 if point (h, w 1 ) is occluded in the right view. Consequently, P L can be used to represent occlusions in the left image. Due to noise and rectification errors in stereo images, we relax the constraint in Eq. 7 by ?2 pixels in this work:</p><formula xml:id="formula_9">P L (h, w 1 ) = 2 ?=?2 W w2=1 M R?L (h, w 1 + ?, w 2 )? M L?R (h, w 2 , w 1 ).<label>(8)</label></formula><p>To maintain training stability, the left valid mask V L is calculated according to V L = tanh(? P L ), where ? was empirically set to 5 in our implementation. The right valid mask V R can be generated following a similar way. <ref type="figure" target="#fig_1">Figure 3</ref> shows some examples of the generated valid masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Losses</head><p>The overall loss function of our network is defined as:</p><formula xml:id="formula_10">L = L SR + ?(L res photo + L res cycle + L smooth + L res cons ),<label>(9)</label></formula><p>where L SR , L res photo , L res cycle , L smooth , and L res cons represent SR loss, residual photometric loss, residual cycle loss, smoothness loss, and residual stereo consistency loss, respectively. ? represents the weight of the regularization term and was empirically set to 0.1 in this work. The SR loss is defined as the L 1 distance between the super-resolved and groundtruth stereo images:</p><formula xml:id="formula_11">L SR = I SR L ? I HR L 1 + I SR R ? I HR R 1 ,<label>(10)</label></formula><p>where I SR L and I SR R represent the super-resolved left and right images, I HR L and I HR R represent their groundtruth HR images. Due to exposure difference and non-Lambertain surfaces, the illuminance intensity between stereo images can vary significantly (see <ref type="figure" target="#fig_2">Fig. 4</ref>). In these cases, the photometric loss and cycle loss used in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19]</ref> can lead to a mismatch problem. To handle this problem, we calculate these losses using residual images to improve their robustness to illuminance changes. Specifically, we introduce X L = I HR L ? I LR L ? ? and X R = I HR R ? I LR R ? ?, where ? and ? represent bicubic upsampling and downsampling, and X L and X R represent the absolute values of the left and right residual images, respectively. Consequently, the residual photometric loss and residual cycle consistency loss can be formulated as:</p><formula xml:id="formula_12">L res photo = V L (X L ? M R? L ? X R ) 1 + V R (X R ? M L?R ? X L ) 1 ,<label>(11)</label></formula><formula xml:id="formula_13">L res cycle = V L (X L ? M R?L ? M L?R ? X L ) 1 + V R (X R ? M L?R ? M R?L ? X R ) 1 .<label>(12)</label></formula><p>Residual photometric and cycle losses introduce two benefits. First, since illuminance components can be eliminated, more consistent and illuminance-robust stereo correspondence can be learned by our biPAM. Second, since residual images mainly contain high-frequency components, our biPAM can pay more attention to texture-rich regions, which is contributive to SR performance.</p><p>Apart from the aforementioned losses, we also employ smoothness loss to encourage smoothness in correspondence space. That is, </p><formula xml:id="formula_14">L smooth = M i,j,k ( M(i, j, k) ? M(i + 1, j, k) 1 + M(i, j, k) ? M(i, j + 1, k + 1) 1 ),<label>(13)</label></formula><formula xml:id="formula_15">L res cons = V L (Y L ? M R?L ? Y R ) 1 + V R (Y R ? M L?R ? Y L ) 1 .<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the datasets and implementation details, then perform ablation studies to validate our design choices. Finally, we compare our iPASSR to several state-of-the-art SISR and stereo image SR methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Implementation Details</head><p>We used 800 images from the training set of Flickr1024 <ref type="bibr" target="#b26">[27]</ref> and 60 images from Middlebury <ref type="bibr" target="#b16">[17]</ref> as the training data. For images from the Middlebury dataset, we followed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref> to perform bicubic downsampling with a factor of 2 to generate HR images. For test, we followed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref> to generate our test set by using 5 images from Middlebury <ref type="bibr" target="#b16">[17]</ref>, 20 images from KITTI 2012 <ref type="bibr" target="#b3">[4]</ref> and 20 images from KITTI 2015 <ref type="bibr" target="#b13">[14]</ref>. Moreover, we used the test set of Flickr1024 <ref type="bibr" target="#b26">[27]</ref> for additional evaluation. We used the bicubic downsampling approach to generate LR images. During the training phase, the generated LR images were cropped into patches of size 30 ? 90 with a stride of 20, and their HR counterparts were cropped accordingly. These patches were randomly flipped horizontally and vertically for data augmentation.</p><p>Peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) were used as quantitative metrics. To achieve fair comparison with <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>, we followed these methods to calculate PSNR and SSIM on the left views with their left boundaries (64 pixels) being cropped. Moreover, to comprehensively evaluate the performance of stereo image SR, we also report the average PSNR and SSIM scores on stereo image pairs (i.e., (Left + Right) /2) without any boundary cropping.</p><p>Our network was implemented in PyTorch on a PC with two Nvidia RTX 2080Ti GPUs. All models were optimized using the Adam method with ? 1 = 0.9, ? 2 = 0.999 and a batch size of 36. The initial learning rate was set to 2?10 ?4 and reduced to half after every 30 epochs. The training was stopped after 80 epochs since more epochs do not provide further consistent improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Cross-view information. We removed biPAM and retrained a single branch of our iPASSR on the same training set as our original network. In addition, we also used pairs of replicated left images as inputs to directly perform inference using our original network. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the network trained with single images (i.e., iPASSR with single input) suffers a decrease of 0.299 dB in PSNR as compared to the original network. If replicated left images were used as inputs, the performance of the variant (i.e., iPASSR with replicated inputs) is also notably inferior to our original network. These results demonstrate the importance of cross-view information for stereo image SR.</p><p>Siamese network architecture. We investigate the benefits introduced by our Siamese network architecture by retraining the network with stereo images as inputs but only super-resolving the left view (i.e., Asymmetric iPASSR). It can be observed in <ref type="table" target="#tab_1">Table 1</ref> that the PSNR score achieved by Asymmetric iPASSR is marginally lower than our iPASSR (25.548 v.s. 25.615). That is because, the symmetric Siamese network structure can help to better exploit the cross-view information to improve the SR performance.</p><p>Losses. We retrained our network using different losses to validate their effectiveness. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the PSNR value of our network is decreased from 25.615 to 25.527 if only the SR loss is considered. That is, our network cannot well incorporate cross-view information without using the additional losses for regularization. In contrast, the SR performance is gradually improved if the photometric loss, cycle loss, smoothness loss, and stereo consistency loss are added. Note that, a 0.159 dB PSNR improvement is introduced when the network is trained with  these losses calculated on residual images. As demonstrated in Section 3.3, by applying these residual losses, the illuminance changes between stereo images can be eliminated and the high-frequency texture regions can be focused on. Moreover, we visualize the attention maps of scene Motorcycle and Sword2 <ref type="bibr" target="#b16">[17]</ref> in <ref type="figure" target="#fig_3">Fig. 5</ref>. It can be observed that the attention maps trained only with the SR loss suffer from heavy noise ( <ref type="figure" target="#fig_0">Fig. 5 (a1)</ref>) and missing correspondence ( <ref type="figure" target="#fig_3">Fig. 5 (a2)</ref>). When the residual photometric loss is introduced, the noise can be reduced but the problem of missing correspondence cannot be handled. That is because, the initial score map S has similar values at different locations in textureless regions (e.g., regions marked by the blue stroke in scene Sword2). Consequently, a single point in the left view can be correlated to a number of points along the epipolar line in the right view, resulting in ambiguities in attention maps. When the smoothness loss is added, noise can be eliminated but the problem of missing correspondence becomes more severe <ref type="figure" target="#fig_0">(Figs. 5(c1) and (c2)</ref>). In contrast, if the residual cycle loss is added, the missing correspondence problem can be handled but the noise cannot be reduced ( <ref type="figure" target="#fig_0">Fig. 5(d1)</ref>). This problem can be handled by introducing both smoothness loss and residual cycle loss (Figs. 5 (e1) and (e2)). Finally, the proposed residual stereo consistency loss can further enhance the stereo consistency to produce accurate and reasonable attention maps.</p><p>Whiten layer. We validate the effectiveness of whiten layers by removing them from our biPAM (i.e., iPASSR w/o whiten layer). As shown in <ref type="table" target="#tab_3">Table 3</ref>, the average PSNR value  That is because, the whiten layers can help to generate robust pairwise correspondence which is beneficial to stereo image SR.</p><p>Valid mask. We demonstrate the effectiveness of valid mask by removing it from both our network and losses (i.e., iPASSR w/o valid mask). That is, the converted features in biPAM are directly concatenated with the original features on the target side. Meanwhile, all the losses are applied equally to all spatial locations without considering occlusions. It can be observed in <ref type="table" target="#tab_3">Table 3</ref> that the average PSNR value suffers a decrease of 0.137 dB (26.179 v.s. 26.316) if the valid mask is not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to state-of-the-arts methods</head><p>In this section, we compare our iPASSR to several stateof-the-art methods, including four SISR methods i.e., VDSR <ref type="bibr" target="#b7">[8]</ref>, EDSR <ref type="bibr" target="#b11">[12]</ref>, RDN <ref type="bibr" target="#b37">[38]</ref>, RCAN <ref type="bibr" target="#b35">[36]</ref>) and three stereo image SR methods 2 (i.e., StereoSR <ref type="bibr" target="#b5">[6]</ref>, PASSRnet <ref type="bibr" target="#b24">[25]</ref>, SR-Res+SAM <ref type="bibr" target="#b31">[32]</ref>). Note that, we retrained all SISR methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref> on our training set for fair comparison.</p><p>Quantitative results. As shown in <ref type="table">Table 4</ref>, our iPASSR achieves the highest PSNR and SSIM values on the KITTI 2012 and KITTI 2015 datasets for 2? and 4? SR. For the Middlebury and Flickr1024 datasets, our iPASSR outper- <ref type="table">Table 4</ref>: Quantitative results achieved by different methods for 2? and 4?SR. #Params. represents the number of parameters of the networks. Here, PSNR/SSIM values achieved on both the cropped left images (i.e., Left) and a pair of stereo images (i.e., (Left + Right) /2) are reported. The best results are in red and the second best results are in blue.  <ref type="figure" target="#fig_4">Figs. 6 and 7</ref>, respectively. Readers can view this demo video for better comparison. Since input LR images are degraded by the downsampling operation, the SR process is highly ill-posed especially for 4?SR. In such cases, SISR methods only use spatial information and can- <ref type="bibr" target="#b2">3</ref> It is worth noting that DRCN <ref type="bibr" target="#b8">[9]</ref>, DRRN <ref type="bibr" target="#b19">[20]</ref> and LapSRN <ref type="bibr" target="#b9">[10]</ref> which have comparable number of parameters as our iPASSR were not included for comparison since they have already been outperformed by PASSRnet as demonstrated in <ref type="bibr" target="#b24">[25]</ref>. In this paper, we investigate the performance gap between our method and the top-performing SISR methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref>, which is the first attempt in this area. We hope these comparative results can inspire the future research of stereo image SR.  <ref type="figure">Figure 8</ref>: Visual results achieved by different methods on real-world images <ref type="bibr" target="#b26">[27]</ref> for 2?SR. not well recover the missing details. In contrast, our iPASSR use cross-view information to produce more faithful details with fewer artifacts. Moreover, the images generated by our iPASSR are more stereo-consistent than those generated by PASSRnet and SRRes+SAM.</p><p>Performance on real-world images. We test the performance of different methods on real-world stereo images by directly applying them to an HR image pair from the Flickr1024 dataset <ref type="bibr" target="#b26">[27]</ref>. As shown in <ref type="figure">Fig. 8</ref>, our iPASSR achieves better perceptual quality than the compared methods. It is worth noting that, left and right views of an image pair may suffer different degrees of degradation in realworld cases (e.g., in the region marked by the red box, the left image suffers more severe blurs than the right one). SISR methods cannot well recover the missing details by using the intra-view information only. In contrast, our iPASSR benefits from the cross-view information and produce images with less blurring artifacts.</p><p>Benefits to disparity estimation. As stereo-consistent and HR image pairs are beneficial to disparity estimation, we investigate this benefit by using the super-resolved stereo images for disparity estimation. We performed 4? downsampling on the images from the test sets of the Scene-  <ref type="figure">Figure 9</ref>: Qualitative results achieved by GwcNet <ref type="bibr" target="#b4">[5]</ref> using 4?SR stereo images generated by different SR methods.</p><p>Flow dataset 4 <ref type="bibr" target="#b12">[13]</ref>, and used different methods to superresolve these LR images to their original resolution. Then, we applied GwcNet [5] to these super-resolved stereo images for disparity estimation. The original HR images and bicubicly upsampled images were used to produce the upper bound and the baseline results, respectively. End-pointerror (EPE) and t-pixel error rate (&gt; tpx) were used as quantitative metrics to evaluate the estimated disparity. As shown in <ref type="table" target="#tab_6">Table 5</ref>, a 0.529 (i.e., 79.3%) increase in EPE is introduced when HR input images are replaced with the bicubicly interpolated ones. It demonstrates that the details (e.g., edges and textures) in the stereo images are important to disparity estimation. Note that, our iPASSR can better reduce the error by providing high-quality and stereoconsistent stereo images. The visual examples in <ref type="figure">Fig 9</ref> demonstrate that the disparity map corresponding to our method is more accurate and close to the one estimated from HR stereo images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>During the retraining of SISR methods, we noticed that the training dataset has an influence on the SR performance. To investigate the influence of training datasets, we used EDSR and RCAN developed on different datasets to perform  stereo image SR. As shown in <ref type="table" target="#tab_7">Table 6</ref>, EDSR and RCAN achieve better performance when trained on the DIV2K dataset <ref type="bibr" target="#b20">[21]</ref>. That is because, the DIV2K dataset was specifically developed for SISR and has higher-quality images than existing stereo image datasets. To demonstrate this claim, we use three no-reference image quality assessment metrics <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30]</ref> to evaluate the image quality of these datasets. As shown in <ref type="table" target="#tab_8">Table 7</ref>, the DIV2K dataset achieves the best results in terms of all the metrics. It demonstrates that high-quality training images can introduce a notable performance gain to deep SR networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a method to exploit symmetry cues for stereo image SR. We first proposed a bi-directional parallax attention module (biPAM) and an inline occlusion handling scheme to effectively interact cross-view information, and then equipped biPAM to a Siamese network to develop our iPASSR. Moreover, we proposed several residual losses to achieve robustness to illuminance changes. Extensive ablation studies were performed to validate the effectiveness of our design choices, and comparative results on four public datasets demonstrated the state-of-the-art performance of our method. Furthermore, we made an in-depth analysis on the benefits of stereo image SR to disparity estimation, and the influence of training datasets to image SR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of our iPASSR network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>An illustration of valid masks generated by our occlusion handling scheme. Red regions have small values and represent heavy occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>An illustration of illuminance changes in stereo image pairs. View our demo video for better visualization.where M ? {M R?L , M L?R }. Here, M R?L (i, j, k) ? M R?L (i + 1, j, k) 1 enforces the correspondence between I R (i+1, k) and I L (i+1, j) to be close to the correspondence between I R (i, k) and I L (i, j).Finally, we introduce residual stereo consistency loss to achieve stereo consistency between super-resolved left and right images. Specifically, the LR residuals between superresolved images and groundtruth images are calculated according to Y L = I HR L ? I SR L ? and Y R = I HR R ? I SR R ?, respectively, and the residual stereo consistency loss is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of attention maps generated by our iPASSR trained with different losses. (a) L SR , (b) L SR + ?L res photo , (c) L SR + ?(L res photo + L smooth ), (d) L SR + ?(L res photo + L res cycle ), (e) L SR + ?(L res photo + L smooth + L res cycle ), (f) L SR + ?(L res photo + L smooth + L res cycle + L res cons ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visual results (2?) achieved by different methods on the KITTI 2015 (top) and Middlebury datasets (bottom). suffers a decrease of 0.191 dB if whiten layers are removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results achieved on the KITTI 2015 dataset by our method with different cross-view information incorporation schemes for 4?SR. Here, PSNR/SSIM of the cropped left views are reported.</figDesc><table><row><cell>Models</cell><cell>Inputs</cell><cell>PSNR/SSIM</cell></row><row><cell>iPASSR with single input</cell><cell>Left</cell><cell>25.316/0.7753</cell></row><row><cell>iPASSR with replicated inputs</cell><cell>Left-Left</cell><cell>25.400/0.7775</cell></row><row><cell>Asymmetric iPASSR</cell><cell>Left-Right</cell><cell>25.548/0.7829</cell></row><row><cell>iPASSR</cell><cell>Left-Right</cell><cell>25.615/0.7850</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results achieved on the KITTI 2015 dataset<ref type="bibr" target="#b13">[14]</ref> by iPASSR with different losses for 4?SR. "Res" represents L photo , L cycle , and L cons calculated on residual images. Here, PSNR/SSIM values of the cropped left views are reported.</figDesc><table><row><cell>LSR</cell><cell>Lphoto</cell><cell>Lsmooth</cell><cell>Lcycle</cell><cell>Lcons</cell><cell>Res</cell><cell cols="2">PSNR/SSIM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">25.527/0.7827</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">25.535/0.7815</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">25.481/0.7795</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">25.552/0.7839</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">25.570/0.7839</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">25.456/0.7775</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">25.615/0.7850</cell></row><row><cell>Motorcycle</cell><cell>(a1)</cell><cell>(b1)</cell><cell>(c1)</cell><cell></cell><cell>(d1)</cell><cell>(e1)</cell><cell>(f1)</cell></row><row><cell>Sword2</cell><cell>(a2)</cell><cell>(b2)</cell><cell>(c2)</cell><cell></cell><cell>(d2)</cell><cell>(e2)</cell><cell>(f2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results achieved on the KITTI 2015 dataset by iPASSR with different settings in biPAM for 4?SR. Here, PSNR/SSIM values of the cropped left images (i.e., Left) and a pair of stereo images (i.e., (Left + Right) /2) are reported.</figDesc><table><row><cell>Models</cell><cell></cell><cell></cell><cell></cell><cell>Left</cell><cell cols="2">(Left + Right) /2</cell><cell></cell></row><row><cell cols="3">iPASSR w/o whiten layer</cell><cell cols="2">25.535/0.7830</cell><cell cols="2">26.125/0.8037</cell><cell></cell></row><row><cell cols="3">iPASSR w/o using valid mask</cell><cell cols="2">25.574/0.7843</cell><cell cols="2">26.179/0.8051</cell><cell></cell></row><row><cell>iPASSR</cell><cell></cell><cell></cell><cell cols="2">25.615/0.7850</cell><cell cols="2">26.316/0.8084</cell><cell></cell></row><row><cell>Left</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Right</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bicubic</cell><cell>EDSR</cell><cell>RDN</cell><cell>RCAN</cell><cell>StereoSR</cell><cell>PASSRnet</cell><cell>iPASSR</cell><cell>HR</cell></row><row><cell>Left</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Right</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bicubic</cell><cell>EDSR</cell><cell>RDN</cell><cell>RCAN</cell><cell>StereoSR</cell><cell>PASSRnet</cell><cell>iPASSR</cell><cell>HR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>We do not present 2?SR results of SRRes+SAM<ref type="bibr" target="#b31">[32]</ref> and 4?SR results of StereoSR<ref type="bibr" target="#b5">[6]</ref> since their models are unavailable. RDN, and RCAN. Note that, the model sizes of our iPASSR are comparable to PASSRnet but significantly smaller than EDSR, RDN and RCAN 3 . Although a large model enables rich and hierarchical feature representation which can boost the SR performance, we decided to keep our iPASSR lightweight and improve SR performance by exploiting cross-view information in stereo images.Qualitative results. Qualitative results for 2? and 4? SR are shown in</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell>Scale</cell><cell cols="2">#Params.</cell><cell cols="2">KITTI 2012</cell><cell cols="2">Left KITTI 2015</cell><cell>Middlebury</cell><cell>KITTI 2012</cell><cell>(Left + Right) /2 KITTI 2015 Middlebury</cell><cell>Flickr1024</cell></row><row><cell></cell><cell>Bicubic</cell><cell></cell><cell>2?</cell><cell>-</cell><cell></cell><cell cols="2">28.44/0.8808</cell><cell cols="2">27.81/0.8814</cell><cell>30.46/0.8979</cell><cell>28.51/0.8842</cell><cell>28.61/0.8973</cell><cell>30.60/0.8990</cell><cell>24.94/0.8186</cell></row><row><cell></cell><cell>VDSR [8]</cell><cell></cell><cell>2?</cell><cell>0.66M</cell><cell></cell><cell cols="2">30.17/0.9062</cell><cell cols="2">28.99/0.9038</cell><cell>32.66/0.9101</cell><cell>30.30/0.9089</cell><cell>29.78/0.9150</cell><cell>32.77/0.9102</cell><cell>25.60/0.8534</cell></row><row><cell></cell><cell>EDSR [12]</cell><cell></cell><cell>2?</cell><cell>38.6M</cell><cell></cell><cell cols="2">30.83/0.9199</cell><cell cols="2">29.94/0.9231</cell><cell>34.84/0.9489</cell><cell>30.96/0.9228</cell><cell>30.73/0.9335</cell><cell>34.95/0.9492</cell><cell>28.66/0.9087</cell></row><row><cell></cell><cell>RDN [38]</cell><cell></cell><cell>2?</cell><cell>22.0M</cell><cell></cell><cell cols="2">30.81/0.9197</cell><cell cols="2">29.91/0.9224</cell><cell>34.85/0.9488</cell><cell>30.94/0.9227</cell><cell>30.70/0.9330</cell><cell>34.94/0.9491</cell><cell>28.64/0.9084</cell></row><row><cell></cell><cell>RCAN [36]</cell><cell></cell><cell>2?</cell><cell>15.3M</cell><cell></cell><cell cols="2">30.88/0.9202</cell><cell cols="2">29.97/0.9231</cell><cell>34.80/0.9482</cell><cell>31.02/0.9232</cell><cell>30.77/0.9336</cell><cell>34.90/0.9486</cell><cell>28.63/0.9082</cell></row><row><cell></cell><cell cols="2">StereoSR [6]</cell><cell>2?</cell><cell>1.08M</cell><cell></cell><cell cols="2">29.42/0.9040</cell><cell cols="2">28.53/0.9038</cell><cell>33.15/0.9343</cell><cell>29.51/0.9073</cell><cell>29.33/0.9168</cell><cell>33.23/0.9348</cell><cell>25.96/0.8599</cell></row><row><cell></cell><cell cols="2">PASSRnet [25]</cell><cell>2?</cell><cell>1.37M</cell><cell></cell><cell cols="2">30.68/0.9159</cell><cell cols="2">29.81/0.9191</cell><cell>34.13/0.9421</cell><cell>30.81/0.9190</cell><cell>30.60/0.9300</cell><cell>34.23/0.9422</cell><cell>28.38/0.9038</cell></row><row><cell></cell><cell cols="2">iPASSR (ours)</cell><cell>2?</cell><cell>1.37M</cell><cell></cell><cell cols="2">30.97/0.9210</cell><cell cols="2">30.01/0.9234</cell><cell>34.41/0.9454</cell><cell>31.11/0.9240</cell><cell>30.81/0.9340</cell><cell>34.51/0.9454</cell><cell>28.60/0.9097</cell></row><row><cell></cell><cell>Bicubic</cell><cell></cell><cell>4?</cell><cell>-</cell><cell></cell><cell cols="2">24.52/0.7310</cell><cell cols="2">23.79/0.7072</cell><cell>26.27/0.7553</cell><cell>24.58/0.7372</cell><cell>24.38/0.7340</cell><cell>26.40/0.7572</cell><cell>21.82/0.6293</cell></row><row><cell></cell><cell>VDSR [8]</cell><cell></cell><cell>4?</cell><cell>0.66M</cell><cell></cell><cell cols="2">25.54/0.7662</cell><cell cols="2">24.68/0.7456</cell><cell>27.60/0.7933</cell><cell>25.60/0.7722</cell><cell>25.32/0.7703</cell><cell>27.69/0.7941</cell><cell>22.46/0.6718</cell></row><row><cell></cell><cell>EDSR [12]</cell><cell></cell><cell>4?</cell><cell>38.9M</cell><cell></cell><cell cols="2">26.26/0.7954</cell><cell cols="2">25.38/0.7811</cell><cell>29.15/0.8383</cell><cell>26.35/0.8015</cell><cell>26.04/0.8039</cell><cell>29.23/0.8397</cell><cell>23.46/0.7285</cell></row><row><cell></cell><cell>RDN [38]</cell><cell></cell><cell>4?</cell><cell>22.0M</cell><cell></cell><cell cols="2">26.23/0.7952</cell><cell cols="2">25.37/0.7813</cell><cell>29.15/0.8387</cell><cell>26.32/0.8014</cell><cell>26.04/0.8043</cell><cell>29.27/0.8404</cell><cell>23.47/0.7295</cell></row><row><cell></cell><cell>RCAN [36]</cell><cell></cell><cell>4?</cell><cell>15.4M</cell><cell></cell><cell cols="2">26.36/0.7968</cell><cell cols="2">25.53/0.7836</cell><cell>29.20/0.8381</cell><cell>26.44/0.8029</cell><cell>26.22/0.8068</cell><cell>29.30/0.8397</cell><cell>23.48/0.7286</cell></row><row><cell></cell><cell>PASSRnet</cell><cell></cell><cell>4?</cell><cell>1.42M</cell><cell></cell><cell cols="2">26.26/0.7919</cell><cell cols="2">25.41/0.7772</cell><cell>28.61/0.8232</cell><cell>26.34/0.7981</cell><cell>26.08/0.8002</cell><cell>28.72/0.8236</cell><cell>23.31/0.7195</cell></row><row><cell></cell><cell cols="2">SRRes+SAM [32]</cell><cell>4?</cell><cell>1.73M</cell><cell></cell><cell cols="2">26.35/0.7957</cell><cell cols="2">25.55/0.7825</cell><cell>28.76/0.8287</cell><cell>26.44/0.8018</cell><cell>26.22/0.8054</cell><cell>28.83/0.8290</cell><cell>23.27/0.7233</cell></row><row><cell></cell><cell cols="2">iPASSR (ours)</cell><cell>4?</cell><cell>1.42M</cell><cell></cell><cell cols="2">26.47/0.7993</cell><cell cols="2">25.61/0.7850</cell><cell>29.07/0.8363</cell><cell>26.56/0.8053</cell><cell>26.32/0.8084</cell><cell>29.16/0.8367</cell><cell>23.44/0.7287</cell></row><row><cell>Left Right</cell><cell>Note: Bicubic</cell><cell>EDSR</cell><cell>RDN</cell><cell>RCAN</cell><cell cols="2">PASSRnet</cell><cell>SRRes+SAM</cell><cell>iPASSR</cell><cell>HR</cell></row><row><cell>Left</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Right</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Bicubic</cell><cell>EDSR</cell><cell>RDN</cell><cell>RCAN</cell><cell cols="2">PASSRnet</cell><cell>SRRes+SAM</cell><cell>iPASSR</cell><cell>HR</cell></row><row><cell cols="10">Figure 7: Visual results (4?) achieved by different methods</cell></row><row><cell cols="10">on the KITTI 2015 (top) and Flickr1024 datasets (bottom).</cell></row><row><cell cols="10">forms all stereo image SR methods, but is slightly inferior</cell></row><row><cell cols="2">to EDSR,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Left Right Left Right Left Right</head><label></label><figDesc></figDesc><table><row><cell>KITTI2012_test_000013</cell><cell>Bicubic</cell><cell>EDSR</cell><cell>RDN</cell><cell>RCAN</cell><cell>StereoSR</cell><cell>PASSRnet</cell><cell>iPASSR</cell></row><row><cell></cell><cell>Bicubic</cell><cell>EDSR</cell><cell>RDN</cell><cell>RCAN</cell><cell>StereoSR</cell><cell>PASSRnet</cell><cell>iPASSR</cell></row><row><cell>Flickr1024_test_0023</cell><cell>Bicubic</cell><cell>EDSR</cell><cell>RDN</cell><cell>RCAN</cell><cell>StereoSR</cell><cell>PASSRnet</cell><cell>iPASSR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Quantitative results achieved by GwcNet [5] on 4? SR stereo images. All these metrics were averaged on the test set of the SceneFlow dataset<ref type="bibr" target="#b12">[13]</ref>, where lower values indicate better performance. Best results are in red and the second best results are blue.</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell>EPE</cell><cell>&gt;1px (%)</cell><cell cols="2">&gt;2px (%)</cell><cell cols="2">&gt;3px (%)</cell></row><row><cell></cell><cell>Bicubic</cell><cell></cell><cell>1.196</cell><cell>11.5</cell><cell>5.96</cell><cell></cell><cell>4.28</cell></row><row><cell></cell><cell>VDSR [8]</cell><cell></cell><cell>1.068</cell><cell>10.8</cell><cell>5.37</cell><cell></cell><cell>3.80</cell></row><row><cell></cell><cell cols="2">PASSRnet [25, 23]</cell><cell>1.019</cell><cell>11.5</cell><cell>5.44</cell><cell></cell><cell>3.72</cell></row><row><cell></cell><cell cols="2">SRRes+SAM [32]</cell><cell>0.991</cell><cell>11.1</cell><cell>5.18</cell><cell></cell><cell>3.57</cell></row><row><cell></cell><cell cols="2">iPASSR (ours)</cell><cell>0.949</cell><cell>10.0</cell><cell>4.79</cell><cell></cell><cell>3.35</cell></row><row><cell></cell><cell>HR</cell><cell></cell><cell>0.667</cell><cell>6.77</cell><cell>3.34</cell><cell></cell><cell>2.38</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EPE=1.002</cell><cell>EPE=0.991</cell><cell>EPE=0.983</cell><cell cols="2">EPE=0.890</cell><cell>EPE=0.763</cell></row><row><cell></cell><cell>TEST-C-0003</cell><cell>GT</cell><cell>VDSR</cell><cell>PASSRnet</cell><cell>SRRes+SAM</cell><cell>iPASSR</cell><cell></cell><cell>HR</cell></row><row><cell>EPE=1.224</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>EPE=0.885</cell><cell>EPE=0.883</cell><cell>EPE=0.858</cell><cell cols="2">EPE=0.697</cell><cell>EPE=0.509</cell></row><row><cell></cell><cell>TEST-C-0054</cell><cell>GT</cell><cell>VDSR</cell><cell>PASSRnet</cell><cell>SRRes+SAM</cell><cell>iPASSR</cell><cell></cell><cell>HR</cell></row><row><cell>EPE=0.912</cell><cell>TEST-C-0112</cell><cell>GT</cell><cell>VDSR EPE=1.488</cell><cell>PASSRnet EPE=1.738</cell><cell>SRRes+SAM EPE=1.499</cell><cell cols="2">iPASSR EPE=1.295</cell><cell>HR EPE=0.991</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EPE=1.025</cell><cell>EPE=1.050</cell><cell>EPE=1.027</cell><cell cols="2">EPE=0.863</cell><cell>EPE=0.631</cell></row><row><cell></cell><cell>TEST-C-0115</cell><cell>GT</cell><cell>VDSR</cell><cell>PASSRnet</cell><cell>SRRes+SAM</cell><cell>iPASSR</cell><cell></cell><cell>HR</cell></row><row><cell>EPE=1.888</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>EPE=1.265</cell><cell>EPE=1.304</cell><cell>EPE=1.320</cell><cell cols="2">EPE=1.048</cell><cell>EPE=0.863</cell></row><row><cell></cell><cell>TEST-C-0139</cell><cell>GT</cell><cell>VDSR</cell><cell>PASSRnet</cell><cell>SRRes+SAM</cell><cell>iPASSR</cell><cell></cell><cell>HR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EPE=1.600</cell><cell>EPE=1.548</cell><cell>EPE=1.856</cell><cell cols="2">EPE=1.414</cell><cell>EPE=1.007</cell></row><row><cell>EPE=1.023</cell><cell>TEST-C-0148</cell><cell>GT</cell><cell>VDSR</cell><cell>PASSRnet</cell><cell>SRRes+SAM</cell><cell>iPASSR</cell><cell></cell><cell>HR</cell></row><row><cell>EPE=1.579</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EPE=2.008</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparative results achieved by EDSR and RCAN with different training sets for both 2? and 4?SR. 2? 31.06/0.925 30.77/0.935 35.34/0.951 28.58/0.909 EDSR stereo 2? 30.95/0.923 30.73/0.934 34.95/0.949 28.66/0.908 RCAN div2k 2? 31.16/0.926 30.88/0.945 35.42/0.952 28.64/0.910 RCAN stereo 2? 31.02/0.923 30.77/0.934 34.90/0.949 28.63/0.908 EDSR div2k 4? 26.62/0.809 26.39/0.814 29.48/0.842 23.58/0.735 EDSR stereo 4? 26.35/0.802 26.04/0.804 29.23/0.840 23.46/0.729 RCAN div2k 4? 26.65/0.809 26.45/0.814 29.56/0.845 23.60/0.737 RCAN stereo 4? 26.44/0.803 26.22/0.807 29.30/0.840 23.48/0.729</figDesc><table><row><cell>Method</cell><cell>KITTI2012</cell><cell>KITTI2015</cell><cell>Middlebury Flickr1024</cell></row><row><cell>EDSR div2k</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>No-reference perceptual quality scores of different SR datasets. Both the average value and the standard deviation are reported. Lower scores of BRISQUE [15], NIQE [16] and higher scores of CEIQ [30] indicate better quality.</figDesc><table><row><cell>Dataset</cell><cell>BRISQUE (?)</cell><cell>NIQE (?)</cell><cell>CEIQ (?)</cell></row><row><cell>KITTI 2012</cell><cell>17.30 (? 6.60)</cell><cell>3.22 (?0.42)</cell><cell>3.31 (?0.14)</cell></row><row><cell>KITTI 2015</cell><cell>26.41 (? 5.26)</cell><cell>3.23 (?0.48)</cell><cell>3.34 (?0.19)</cell></row><row><cell>Middlebury</cell><cell>14.88 (? 9.19)</cell><cell>3.77 (?0.99)</cell><cell>3.31 (?0.21)</cell></row><row><cell>Flickr1024</cell><cell>19.10 (?13.57)</cell><cell>3.40 (?0.99)</cell><cell>3.25 (?0.36)</cell></row><row><cell>DIV2K</cell><cell>11.40 (?11.98)</cell><cell>2.99 (?1.05)</cell><cell>3.36 (?0.30)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We do not compare our method to SPAMnet<ref type="bibr" target="#b18">[19]</ref> and DASSR<ref type="bibr" target="#b28">[29]</ref> because: (1) their codes and models are unavailable, (2) The evaluation schemes in<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref> are different from those in<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>, so that we cannot directly copy the PSNR and SSIM scores in their papers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partially supported in part by the National Natural Science Foundation of China (Nos. 61972435, 61401474, 61921001).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Glean: Generative latent bank for largefactor image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3273" to="3282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhancing the spatial resolution of stereo images using a parallax prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung-Hwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inchang</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stereonet: Guided hierarchical refinement for real-time edge-aware depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="573" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nera</forename><surname>Ne?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Porter</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stereoscopic image super-resolution with stereo consistent feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonil</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungil</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somi</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="12031" to="12038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image superresolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="114" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring sparsity in image super-resolution for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parallax attention for unsupervised stereo correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised degradation representation learning for blind superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning parallax attention for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Flickr1024: A large-scale dataset for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3852" to="3857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep bilateral learning for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpu</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disparity-aware domain adaptation in stereo image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahetiyaer</forename><surname>Bare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">No-reference quality assessment of contrast-distorted images using contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A stereo attention module for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="496" to="500" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep unfolding network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3217" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Designing a practical degradation model for deep blind image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep plug-andplay super-resolution for arbitrary blur kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1671" to="1681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Residual non-local attention networks for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Residual dense network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
