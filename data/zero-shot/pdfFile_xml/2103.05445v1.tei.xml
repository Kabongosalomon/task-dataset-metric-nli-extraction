<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pixel-wise Anomaly Detection in Complex Driving Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><forename type="middle">Di</forename><surname>Biase</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Blum</surname></persName>
							<email>hermann.blum@ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
							<email>rsiegwart@ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
							<email>cesarc@ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<title level="a" type="main">Pixel-wise Anomaly Detection in Complex Driving Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Anomaly scenarios overview. There are three possible outcomes when a segmentation network encounters an anomalous instance. First, anomaly instances are properly segmented and classified as one of the training classes (i.e bird is confused as a person) (top). Second, anomaly instances are over-segmented with multiple classes (i.e dog is detected as a combination of person, vegetation, and terrain classes) (middle). And third, anomaly instances are blended with the background, not detected (i.e boxes blend with the street segmentation) (bottom). Our proposed method produces robust predictions for all scenarios, while previous approaches fail to handle at least one of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The inability of state-of-the-art semantic segmentation methods to detect anomaly instances hinders them from being deployed in safety-critical and complex applications, such as autonomous driving. Recent approaches have focused on either leveraging segmentation uncertainty to identify anomalous areas or re-synthesizing the image from the semantic label map to find dissimilarities with the input image. In this work, we demonstrate that these two methodologies contain complementary information and can be combined to produce robust predictions for anomaly segmentation. We present a pixel-wise anomaly detection framework that uses uncertainty maps to improve over existing re-synthesis methods in finding dissimilarities between the input and generated images. Our approach works as a general framework around already trained segmentation networks, which ensures anomaly detection without compromising segmentation accuracy, while significantly out-performing all similar methods. Top-2 performance across a range of different anomaly datasets shows the robustness of our approach to handling different anomaly instances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in deep learning have shown significant improvements in the field of computer vision. Neural networks have become the de-facto methodology for classification, object detection, and semantic segmentation due to their high accuracy in comparison to previous methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>. However, while the predictions of these networks are highly accurate, they usually fail when encountering anomalous inputs (i.e. instances outside the training distribution of the network).</p><p>With this work, we focus on the inability of existing semantic segmentation models to localize anomaly instances and how this limitation hinders them from being deployed in safety-critical, in-the-wild scenarios. Consider the case of a self-driving vehicle that uses a semantic segmentation model. If the agent encounters an anomalous object (i.e. a wooden box in the middle of the street), the model could wrongly classify this object as part of the road and lead the vehicle to crash.</p><p>To detect such anomalies in the input, we build our approach upon two established groups of methods. The first group uses uncertainty estimation to detect anomalies. Their intuition follows that a low-confidence prediction is likely an anomaly. However, uncertainty estimation methods themselves are still noisy and inaccurate. Previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b3">4]</ref> have shown that these models fail to detect many unexpected objects. Example failure cases are shown in <ref type="figure">Figure 1</ref> (top and bottom) where the anomaly object is either detected but miss-classified or non-detected and blended with the background. In both cases, the segmentation network is overconfident about its prediction and, thus, the estimated uncertainty (softmax entropy) is low.</p><p>The second group focuses on re-synthesizing the input image from the predicted semantic map and then comparing the two images (input and generated) to find the anomaly. These models have shown promising results when dealing with segmentation overconfidence but fail when the segmentation outputs a noisy prediction for the unknown object, as shown in <ref type="figure">Figure 1</ref> (middle). This failure is explained by the inability of the synthesis model to reconstruct noisy patches of the semantic map, which complicates finding the differences between input and synthesized images.</p><p>In this paper, we propose a novel pixel-level anomaly framework that combines uncertainty and re-synthesis approaches in order to produce robust predictions for the different anomaly scenarios. Our experiments show that uncertainty and re-synthesis approaches are complementary to each other, and together they cover the different outcomes when a segmentation network encounters an anomaly.</p><p>Our framework builds upon previous re-synthesis methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38]</ref> of reformulating the problem of segmenting unknown classes as one of identifying differences between the input image and the re-synthesised image from a predicted semantic map. We improve over those frameworks by integrating different uncertainty measures, such as softmax entropy <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>, softmax difference <ref type="bibr" target="#b30">[31]</ref>, and perceptual differences <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8]</ref> to assist the dissimilarity network in differentiating the input and generated images. The proposed framework successfully generalizes to all anomalies scenarios, as shown in <ref type="figure">Figure 1</ref>, with minimal additional computation effort and without the need to jeopardize the segmentation network accuracy (no re-training necessary), which is one common flaw of other anomaly detec-tors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Besides maintaining state-of-the-art performance in segmentation, eliminating the need for re-training also reduces the complexity of adding an anomaly detector to future segmentation networks, as training these networks is non-trivial.</p><p>We evaluate our framework in public benchmarks for anomaly detection, where we compare to methods similar to ours that not compromise segmentation accuracy, as well as those requiring full retraining. We also demonstrate that our framework is able to generalize to different segmentation and synthesis networks, even when these models have lower performance. We replace the segmentation and synthesis models with lighter architectures to prioritize speed in time-critical scenarios like autonomous driving.</p><p>In summary, our contributions are the following:</p><p>-We present a novel pixel-wise anomaly detection framework that leverages the best features of existing uncertainty and re-synthesis methodologies.</p><p>-Our approach is robust to the different anomaly scenarios, achieving state-of-the-art performance on the Fishyscapes benchmark while maintaining state-ofthe-art segmentation accuracy.</p><p>-Our proposed framework is able to generalize to different segmentation and synthesis networks, serving as a wrapper methodology to existing segmentation pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The task of localizing anomalous instances in semantic segmentation has been studied under out-of-distribution (OoD) detection and anomaly segmentation. In this section, we review the methods which could be used for pixel-wise anomaly detection, and exclude approaches that could only be applied for image-level OoD classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Anomaly Segmentation via Uncertainty Estimation</head><p>Methods that estimate the uncertainty of a model for a given input may estimate high uncertainty for inputs that are not anomalies, e.g. due to high input noises. Regardless of this and other differences, anomaly detection is a common benchmark method for uncertainty estimation, based on the assumption that anomalous inputs should come with higher uncertainty than any training data.</p><p>Early methods measure uncertainty from the predicted softmax distributions and classify the samples as OoD by using simple statistics <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. While these approaches are good baselines for image-level OoD classification, they Available at https://github.com/giandbt/SynBoost. usually fail in anomaly segmentation. Specifically, the estimated (aleatoric) uncertainty is often high at object boundaries, where no single label can be assigned with certainty, and not at anomalous instances as desired. <ref type="bibr" target="#b30">[31]</ref> mitigated this shortcoming by aggregating different dispersion measurements (e.g., entropy, and difference in softmax probability) and then predicting areas of potential high error in the segmentation. Then, <ref type="bibr" target="#b27">[28]</ref> demonstrated that these high error areas can be used to localize anomalies by visual feature differences.</p><p>Alternative approaches use Bayesian NNs with MC dropout to estimate pixel uncertainty <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21]</ref>. These methods differentiate between aleatoric (noise inherent in the observations) and epistemic uncertainties (uncertainty in the model), therefore mitigating the problem of object boundaries, but still fail to detect anomalies on a pixel level accurately. As shown in <ref type="bibr" target="#b23">[24]</ref>, they yield many false positive predictions, as well as miss-matches between anomaly instances and uncertain areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Anomaly Segmentation via Outlier Exposure</head><p>Anomaly segmentation can also be accomplished by training a network to differentiate inliers against unseen samples by using an auxiliary dataset of outliers <ref type="bibr" target="#b14">[15]</ref>. <ref type="bibr" target="#b1">[2]</ref> was one of the first approaches to use outlier exposure for dense predictions using ImageNet <ref type="bibr" target="#b31">[32]</ref> as the OoD dataset. Then, <ref type="bibr" target="#b2">[3]</ref> build upon this methodology by modifying the segmentation network to predict the semantic map as well as the outlier map. Note that this requires re-training the segmentation network as a multi-task model, which has lead to drop in performance <ref type="bibr" target="#b35">[36]</ref>. The biggest shortcoming of these approaches is that they train from OoD samples, which could compromise their ability to generalize to all possible anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Anomaly Segmentation via Image Re-synthesis</head><p>Promising recent methods follow the approach of reconstructing the input image using generative models. The intuition behind this methodology is that the generated image will yield appearance differences with respect to the input image where anomalies are present, as the model cannot handle these instances. Early work on this subfield used autoencoders to re-synthesize the original image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. However, these methods mostly generated a lowerquality version of the input image <ref type="bibr" target="#b23">[24]</ref>. More recent methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref> re-synthesize the input image from the predicted semantic map using a generative adversarial network. The photo-realistic image is then compared to the original image by a discrepancy or comparison module to localize the anomaly instances.</p><p>These approaches benefit from not needing to re-train the segmentation network as they work as a wrapper method. Additionally, they do not require OoD samples which helps them to generalize to never-seen anomalies instances. However, the performance of these methods are limited by the ability of the discrepancy module to differentiate between features in the input and generated images, which could be challenging for complex driving scenes. With our work, we demonstrate that feeding uncertainty information of the scene to the discrepancy network significantly improves the ability of the module to detect anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We propose a detection framework for segmenting anomalous instances. Our framework is inspired by recent re-synthesis approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref>, while extending them to include the benefits of uncertainty estimation methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref>. We first introduce our framework with its respective modules (Sec. 3.1). Next, we describe how to train the modules to better handle the different anomaly scenarios (Sec. 3.2). Finally, we combine our framework's output with the calculated uncertainty maps to have a final ensemble method that reduces the false positives and overconfidence in the predictions (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pixel-wise Anomaly Detection Framework</head><p>Our proposed framework follows the same base structure as <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b11">[12]</ref>, where the input image gets segmented, a reconstruction is synthesized from the segmentation map, and a dissimilarity module detects anomalies by comparing input and synthesized image. However, we extended each component to predict and/or use uncertainty measurements to improve the final anomaly prediction. <ref type="figure">Figure 2</ref> shows a high-level summary of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Segmentation Module</head><p>The segmentation module takes the input image and feeds it into a segmentation network, such as <ref type="bibr" target="#b41">[42]</ref> or <ref type="bibr" target="#b39">[40]</ref>, in order to obtain a semantic map. In addition to the semantic map, we also compute two dispersion measures to quantify the uncertainty in the semantic map prediction. These two dispersion measurements are the softmax entropy H <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref> and the softmax distance D (i.e. the difference between the two largest softmax values), which have shown to be beneficial in understanding errors within the segmentation <ref type="bibr" target="#b30">[31]</ref>. For each pixel x, these two measurements are calculated as follows:</p><formula xml:id="formula_0">H x = ? c?classes p(c) log 2 p(c)<label>(1)</label></formula><formula xml:id="formula_1">D x = 1 ? max c?classes p(c) + max c ?classes\(arg max c p(c)) p(c ) (2)</formula><p>where p(c) is the softmax probability for class c. We normalize both quantities to [0, 1]. <ref type="figure">Figure 2</ref>. Anomaly Segmentation Framework. We first pass the input image through a segmentation network, which will output a semantic map and two uncertainty maps. The predicted semantic map is then processed by the synthesis network to generate a photo-realistic image. Perceptual difference is then calculated by comparing features between the input and generated images. Lastly, all the predicted images and the input are sent to the spatial-aware dissimilarity module to produce the anomaly prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Synthesis Module</head><p>The synthesis module generates a realistic image out of the given semantic map with pixel-to-pixel correspondence. It is trained as a conditional generative adversarial network (cGAN) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b24">25]</ref> to fit the generative distribution to the distribution of input images from the semantic model.</p><p>While the synthesis module is trained to produce photorealistic images and is well able to produce realistic cars, buildings, or pedestrians, the semantic map misses essential information like color or appearance to allow for a direct per-pixel value comparison. We therefore calculate the perceptual difference V between the original and synthesized image. This novel feature map is inspired from the perceptual loss presented in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b7">[8]</ref>, which is commonly used in cGANs methods. The idea is to find the pixels that have the most different feature representations using Ima-geNet <ref type="bibr" target="#b31">[32]</ref> pre-trained VGG as feature extractor <ref type="bibr" target="#b32">[33]</ref>. The difference in these representations allow us to compare objects based on their image content and spatial structure, as opposed to low level features such as color and texture. If the anomaly object is not detected or wrongly classified, the synthesized image would be generated with the wrong feature representation, and thus the perceptual difference should detect these discrepancies with the input image.</p><p>For every pixel x of the input image and corresponding pixel r from the synthesized image, the perceptual difference is calculated as follows:</p><formula xml:id="formula_2">V (x, r) = N i=1 1 M i F (i) (x) ? F (i) (r) 1<label>(3)</label></formula><p>where F (i) denotes the i-th layer with M i elements of the VGG network and N layers. For consistency, these dispersion measure is also normalized between [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Dissimilarity Module</head><p>The dissimilarity module takes as input the original image, generated image, and semantic map, as well as the uncertainty maps (softmax entropy, softmax distance, perceptual difference) calculated in previous steps. Then, the network combines these features to predict the anomaly segmentation map. The dissimilarity module is divided into three components (encoder, fusion module, and decoder) as seen in <ref type="figure">Figure 3</ref>. Implementation details can be found in Appendix A.1.</p><p>Encoder. Each image is passed through an encoder to extract features. Similarly to <ref type="bibr" target="#b23">[24]</ref>, we use a pre-trained VGG <ref type="bibr" target="#b32">[33]</ref> network for both the original and re-synthesized images as well as a simple CNN to process the semantic map. We also added another simple CNN to encode the uncertainty maps, which are all concatenated. Fusion Module. At each level of the feature pyramid, we concatenate the input, synthesis, and semantic feature maps and pass them through a 1 ? 1 convolution. With this first step, we are training the network to differentiate between the original and generated images, as it is common for re-synthesis methods. Additionally, we use the resulting feature map and perform a point-wise correlation with the uncertainty feature map. This step guides the network to pay attention to high-uncertain areas in the feature map.</p><p>Decoder. We then decode each feature map and concatenate it with the corresponding higher level in the pyramid until we get our anomaly segmentation prediction. Note <ref type="figure">Figure 3</ref>. Dissimilarity Module Architecture. Given the input, synthesized, semantic, and uncertainty images, we extract high to low-level features for each image with a CNN. For each level, we then concatenate the input (blue), synthesized (yellow), and semantic (red) features and fuse them with a 1x1 convolution. The resulting map is used to calculate a correlation with the features from the uncertainty maps (green). The output of the fusion module (purple) is then fed to a decoder to produce the predicted anomaly segmentation. Note the semantic map is used in the decoder block to ensure a spatial aware prediction by using a SPADE normalization <ref type="bibr" target="#b28">[29]</ref>.</p><p>that the decoder block uses the semantic map as an input since it uses a spatial-aware normalization as presented in <ref type="bibr" target="#b28">[29]</ref>. This normalization was used to ensure semantic information is not wash-away during the decoding process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Procedure</head><p>Anomaly instances by definition include any object which does not belong to the training classes. As such, it is crucial to ensure the proposed methodology is robust to detect any anomaly and does not overfit to specific objects from an OoD dataset. Additionally, the training needs to be general to cover all three anomaly scenarios of <ref type="figure">Figure 1</ref>.</p><p>The segmentation and synthesis module are simply trained on the inlier segmentation dataset. This is therefore free of any assumptions on anomalies, and also ensures that the segmentation module is solely trained on the segmentation task without balancing in any other factors.</p><p>To train the dissimilarity module, <ref type="bibr" target="#b23">[24]</ref> solved the problem of not needing OoD dataset by generating synthetic data to simulate segmentation maps in the presence of anomalies. This method replaced the class of randomly-chosen object instances in the ground truth semantic map for an alternative random class. Then, these altered semantic maps are synthesized (with the already trained module), creating visual differences between real and generated images. These differences are used by the dissimilarity network to train and detect discrepancies. Even though this approach does not require seen OoD objects during training, it falls short to train a fully robust pixel-wise anomaly detection. First, this training data generator only covers one anomaly scenario ( <ref type="figure">Figure 1 -top)</ref> but lacks examples for the other two. Second, this approach trains the dissimilarity network using ground truth semantic maps, but during inference, it uses predicted maps. This change in distribution can negatively affect the performance of the network to detect anomalies. Lastly, specific to the methodology presented in Sec. 3.1, the altered instances would not correlate to uncertain areas. As such, these training examples cannot leverage information from uncertainty maps.</p><p>We expanded the training data generator by adding a second source of training examples. Specifically, we label objects within the void class in ground truth semantic maps as anomalies. The void class is commonly used in segmentation datasets <ref type="bibr" target="#b5">[6]</ref> to cover objects that do not belong to any of the training classes, which falls within the definition of an anomaly. This additional source of labels helped us to overcome the challenges of the previous data generator. First, the predicted semantic map in these void regions will closely match the three anomaly scenarios seen during inference <ref type="figure">(Figure 1</ref>), as opposed to just one. Second, the dissimilarity network will train using predicted semantic maps as opposed to ground truth which prevents any domain adaption during inference. Lastly, void regions usually match with high uncertainty pixels which guide the dissimilarity network to use uncertainty information.</p><p>Note that by adding these void class objects as anoma-lies, we lose the benefit of not requiring OoD data during training. However, by using both approaches, our proposed methodology is still able to generalize to unseen objects as shown in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Predictions Ensemble</head><p>Until now, we have used the calculated uncertainty maps (i.e. softmax entropy, softmax distance and perceptual difference) as an attention mechanism in the dissimilarity module. However, as shown in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b27">[28]</ref>, these uncertainty maps are already anomaly predictions by themselves. Thus, we can exploit the uncertainty estimates to complement the output of the dissimilarity network in order to detect all the anomaly scenarios. With this ensemble, we mitigate any possible overconfidence of the dissimilarity network, a common problem with deep learning models as explained by <ref type="bibr" target="#b10">[11]</ref>.</p><p>We ensemble these predictions using a weighted average, where the weights are selected by grid search. Additionally, we tested learning the ensemble weights in an endto-end training of the dissimilarity network. These results were not satisfactory as the network still produces overconfident predictions. Further details about end-to-end training can be found under Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments Set-up</head><p>Module Implementations. The segmentation and synthesis modules use publicly available state-of-the-art networks already trained on Cityscapes <ref type="bibr" target="#b5">[6]</ref>. Specifically, the segmentation module uses the work presented by <ref type="bibr" target="#b41">[42]</ref>, while the synthesis module uses the generator trained by <ref type="bibr" target="#b24">[25]</ref>. A full description of the dissimilarity network implementation can be found in Appendix A.1. It is important to note that the segmentation module uses the original Cityscapes resolution of 2048 x 1024 as its input, the synthesis module downsamples the resolution by two and finally, the dissimilarity module downsamples the original resolution by four. This downscaling was done due to GPU memory constrains and for faster inference time.</p><p>Datasets. We evaluated the performance of our framework with the Fishyscapes benchmark <ref type="bibr" target="#b3">[4]</ref>. Fishyscapes is a public benchmark for uncertainty/anomaly estimation in semantic segmentation for urban driving. The benchmark is divided into three sets: FS Lost &amp; Found (L&amp;F), FS Static and FS Web. For all datasets, we provide qualitative evaluations on the public validation images, but submitted our method to the benchmark for quantitative results on the private test sets. FS Lost &amp; Found is a set of real images captured in <ref type="bibr" target="#b29">[30]</ref> with anomalous objects in front of the vehicle. The test set has 275 images. FS Static blends anomalous objects from Pascal VOC <ref type="bibr" target="#b8">[9]</ref> into validation images from Cityscapes. The test set has 1,000 images. FS Web is a dynamically changing dataset, which overlay objects crawled from the internet using a list of keywords. Methods are only evaluated on data that is crawled after submission to the benchmark, which is why we can only report our method's performance on FS Web Oct. 2020.</p><p>Note that there is another anomaly segmentation benchmark named Street Hazards <ref type="bibr" target="#b12">[13]</ref>, which some related works have used to evaluate their performance in pixel-wise anomaly detection <ref type="bibr" target="#b37">[38]</ref>. Unfortunately, this benchmark is not compatible with our data generation and training procedure. As explained in Sec. 3.2, our method requires instance labels, as well as a void class to successfully learn to differentiate the synthesis and original image. The training dataset of Street Hazards does not provide either. We therefore could not evaluate on the Street Hazards Benchmark.</p><p>Evaluation Metrics. To assess the performance of the framework against existing methods, we use the same metrics presented in the Fishyscapes benchmark for anomaly detection: average precision (AP) and the false positive rate at 95% true positive rate (FPR95). It is important to note that previous works have used the receiver operating curve (ROC) as their metric for anomaly segmentation ( <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b37">[38]</ref>). Nonetheless, ROC is not well-suited for highly imbalance problems, such as anomaly detection, as explained in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Baselines. We compare our approach against all existing methods shown in the Fishyscapes benchmark. As of November 2020, the benchmark includes Dirichlet DeepLab <ref type="bibr" target="#b25">[26]</ref>, Outlier Head <ref type="bibr" target="#b2">[3]</ref>, Bayesian DeepLab <ref type="bibr" target="#b26">[27]</ref>, Embedding Density <ref type="bibr" target="#b3">[4]</ref>, and Softmax Entropy <ref type="bibr" target="#b13">[14]</ref>. If a method has multiple variants, we show only the best.</p><p>We also compare our method against Image Re-synthesis <ref type="bibr" target="#b23">[24]</ref>, as our framework builds upon it. Note that an official submission for this approach has not been done for Fishyscapes. As such, we implemented our own version of Image Re-synthesis and submitted it to the benchmark for comparison, which we named Image Resynthesis++. Appendix A.2 contains details about our implementation and a quantitative comparison against the original work.</p><p>In order to ensure a fair comparison between the aforementioned methodologies, we divide the baselines into two groups depending on whether they require retraining the segmentation network or not. This split is intended to differentiate between methods that compromise segmentation accuracy to detect anomalies and methods that work as a wrapper to state-of-the-art (SOTA) segmentation models. To measure the difference between these two groups, we also report each method's mean intersection over union (mIOU) for all Cityscapes classes. This ensures that each approach produces competitive semantic segmentation pre-Results for the Fishyscapes benchmark can be found here: https: //fishyscapes.com/results.  <ref type="table">Table 1</ref>. Comparison between anomaly segmentation methods. Our method achieves higher AP and lower FPR95 than previous methods that do not compromise segmentation performance (class mIOU on Cityscapes). It also achieves secondbest performance when compared to all existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>dictions, while still detecting the anomalous instances. <ref type="table">Table 1</ref> shows quantitative comparisons between our proposed framework and the baselines discussed in Sec. 4.1 for the FS benchmark test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We first compare our approach against existing methodologies that do not jeopardize the performance of the segmentation model. Within this sub-group, our technique outperforms all previous best methods for the three datasets. Specifically, our method significantly improved the AP on all datasets. Additionally, the approach reduce the FPR95 for FS L&amp;F and FS Web by 50%, while having comparable performance on this metric against the previous best method in FS Static. These results show the value of our contributions when grouped together. A detailed ablation study that quantifies the improvements of each added component can be found under Sec 5.1.</p><p>We then compare our proposed approach against methods that impact the segmentation network performance. In this comparison, our model had the best performance on FS L&amp;F in both AP and FPR95. Additionally, our approach achieves the second-best AP, behind Outlier Head <ref type="bibr" target="#b2">[3]</ref> by 16% and 4% in FS Static and FS Web respectively. Our framework achieves comparable state-of-the-art performance on anomaly segmentation, while still maintaining state-of-the-art performance in semantic segmentation. Our pipeline accomplishes these results by working as a general framework which could potentially be implemented on top of different segmentation models (Sec. 5.2 and shows the framework's ability to generalize to different segmentation and synthesis networks). Note that one of the limitations of wrapper methods is their extended running time in comparison to methods that predict anomaly and segmentation in a single model. Details about inference time for our framework can be found in Appendix A.3.</p><p>It is important to note that our technique is the only method that achieves top-2 performance in the Fishyscapes benchmark, showing the generalization ability of our method in different test sets. In comparison, previous methods, such as Dirichlet DeepLab <ref type="bibr" target="#b25">[26]</ref>, has high AP in FS L&amp;F, but low AP in FS Static and FS Web. Similarly, Outlier Head <ref type="bibr" target="#b2">[3]</ref> has a high AP in FS Static and FS Web, but the method does not generalize well for FS L&amp;F.</p><p>Qualitative comparison between our proposed framework and baselines for uncertainty methods <ref type="bibr" target="#b13">[14]</ref> and imageresynthesis methods <ref type="bibr" target="#b23">[24]</ref> can be found under Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>When comparing all methods in <ref type="table">Table 1</ref>, excluding ours, one could come to the conclusion that there is a trade-off between segmentation performance and anomaly detection. Our framework invalidates this hypothesis and shows that state-of-the-art anomaly detection does not have to come at a cost of segmentation quality. In the following, we discuss key insights that contribute to this result. <ref type="table" target="#tab_1">Table 2</ref> provides results for an ablation study analyzing the contribution of individual components in the proposed method. We first find that both our training data generator and adding the uncertainty maps have significant improvements to the framework. The improvement due to the training data confirms the importance of covering all three anomaly scenarios of <ref type="figure">Figure 1</ref>, as opposed to just the one in <ref type="bibr" target="#b23">[24]</ref>. The further improvement due to uncertainty maps confirms our hypothesis that resynthesis and uncertainty carry complementary information, and combining these approaches leads to overall better performance. Note that removing the training data generator entails that the dissimilarity network does not train to use the uncertainty maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>We also find that combining the uncertainty maps (softmax entropy, softmax distance, and perceptual difference) with the output of the dissimilarity model greatly reduces the FPR95 for both datasets. This indicates that our dissimilarity module, like many deep CNNs, tends to be overconfident in its predictions <ref type="bibr" target="#b10">[11]</ref>. The ensemble helps to reduce this issue. The FPR95 improvement does not correlate to AP as we see a drop in FS Lost &amp; Found. This drop is expected as we are combining our framework's prediction with a weaker detector in order to reduce FPR95 for safetycritical applications. Nonetheless, we see a boost in AP for FS Static. We explain this increment as FS Static artificially blends anomaly objects into urban landscapes images. As such, uncertainty methods outperform in these images as it is easier to detect the overlay of the objects. In general, we expect a small drop in performance by AP using ensemble predictions, but a much larger improvements for FPR95. Finally, we observe that the ensemble has more consistent performance across trainings on all metrics. This is expected as the impact of the dissimilarity module training on the overall performance of the ensemble is more limited than in the standalone case. Nonetheless, it is also a desirable property when deploying in safety critical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Framework Generalization</head><p>The dissimilarity module in the proposed framework serves as a wrapper method for the segmentation and synthesis networks. In other words, the architecture shown in <ref type="figure">Figure 3</ref> is independent of the specific segmentation and synthesis approaches, as long as the segmentation network has a softmax layer as its output.</p><p>We validate this generalization ability by re-training our dissimilarity network with different segmentation and synthesis techniques than the ones presented in Sec. 4.1. Specifically, we chose ICNet as our segmentation module <ref type="bibr" target="#b38">[39]</ref> and SPADE as our synthesis module <ref type="bibr" target="#b28">[29]</ref>. We selected these networks to create a lighter version of our pipeline, which we called Ours Light. These lighter networks are significantly faster than the ones introduced in Sec. 4.1, but with lower performance for their respective tasks. <ref type="table">Table 3</ref> shows the performance of our best and lighter frameworks, as well as Image resynthesis++ as our baseline. Ours Light significantly outperforms the Image resynthe-sis++ baseline, even though this lighter version is using segmentation and synthesis modules with lower performance  <ref type="table">Table 3</ref>. Performance comparison between best and lighter frameworks. Our framework generalizes well to different segmentation and synthesis, even those with lower performance.</p><p>(i.e 83.5% vs 70.6% class mIOU on Cityscapes). This study demonstrates that not only the dissimilarity network generalizes to different segmentation and synthesis networks, but it also performs well even when the segmentation and synthesis networks produce lower quality outputs. <ref type="table">Table 3</ref> also indicates a direct correlation between the performance of the segmentation and synthesis modules and the anomaly detection accuracy. Ours outperforms Ours Light in all metrics, as Ours uses state-of-the-art networks. These results agree with our intuition that resynthesis methods are highly related to the quality of segmentation and synthesis networks. The better the predictions of these two modules, the easier the dissimilarity module differentiates between the input and synthesized image. As these networks improve in the upcoming years, we expect an improvement in performance for our anomaly detector framework. An inference time analysis between Ours and Ours Light can be found in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We investigate pixel-level anomaly detection for complex driving scenes (i.e urban landscapes). We design an anomaly segmentation framework that combines two complementary approaches to anomaly detection: uncertainty and re-synthesis methods. Specifically, our framework leverages uncertainty measurement maps (i.e. softmax entropy, softmax distance and perceptual differences) to guide a dissimilarity network to find the differences between the input image and a generated image from the predicted semantic map. The presented approach significantly outperforms both re-synthesis and uncertainty based methods on the Fishyscapes benchmark, where it is the best overall method across datasets. It does not put any constraint on the segmentation network, and therefore can be used with any already trained state-of-the-art segmentation model. In fact, we demonstrate that our method also works well with lighter segmentation and synthesis networks, making it ready for deployment in autonomous machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Implementation of Dissimilarity Network</head><p>The dissimilarity module is derived from six unique components: two encoding architectures, one fusion module, and three decoder blocks. These components are reused across the network to build the final architecture. <ref type="figure">Figure 3</ref> shows a high-level view of all the components interconnected.</p><p>We follow the naming convention used in <ref type="bibr" target="#b15">[16]</ref> and Pix2PixHD <ref type="bibr" target="#b36">[37]</ref> to explain each architecture. Let ck ? sn denote a 3x3 Convolution-RELU layer with k filters and stride n. dk denote a 7x7 Convolution-RELU layer with k filters and stride 1. m2 denotes a 2x2 max pooling layer. sp ? 19 denotes a SPADE normalization-SELU layer <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20]</ref>, which uses the 19 channels from the predicted semantic map as one of its inputs. tk denotes a 2x2 transposed convolution with k filters. Lastly, r2 denote a 1x1 Convolution layer with 2 filters</p><p>Input and Generated Image Encoder. The first encoder uses the same base architecture as VGG16 <ref type="bibr" target="#b32">[33]</ref>: c64?s1, c64?s1, m2, c128?s1, c128?s1, m2, c256?s1, c256 ? s1, c256 ? s1, m2, c512 ? s1, c512 ? s1, c512 ? s1. This architecture outputs four feature maps, one after each resolution. In other words, we output the features after the max pooling layer, as well as the final feature map from the encoder. This encoder shares weights for encoding both the input and synthesized image.</p><p>Semantic and Dispersion Maps Encoder. This other encoder architecture is divided as: d32, c64?s2, c128?s2, c256 ? s2. The encoder outputs the feature maps after each convolutional layer block. Note that we use different weights to encode semantic information and uncertainty information.</p><p>Fusion Module. The fusion module concatenates the input, synthesized, and semantic feature maps at each resolution level. We then run a 1x1 convolution to extract important areas in the map, as well as reducing complexity. Finally, we perform a point-wise correlation between the dispersion feature map and the resulting map from the 1x1 convolution. This module will output a total of four feature maps -one for each resolution.</p><p>Decoder Blocks. There are four decoder blocks used in the dissimilarity network. The first and second one follow the same structure: c256 ? s1, sp ? 19, c256 ? s1, sp ? 19, t256. The third one is divided as: c384?s1, sp?19, c128? s1, sp ? 19, t1258, while the last one follows: c192 ? s1, sp ? 19, c64 ? s1, sp ? 19, r2. The first decoder block takes the feature map from the lowest resolution. All subsequent decoder blocks take as input the concatenation of the feature map from the fusion module and the output of the previous decoder block.</p><p>The dissimilarity network was trained for fifty (50) epochs, using the Adam <ref type="bibr" target="#b18">[19]</ref> solver and a learning rate of 0.0001. We reduce the learning rate on plateau with a patience of 10 epochs. Additionally, we augment the training images by flipping around the vertical axis and normalizing them using mean and standard deviation values from Ima-geNet <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Re-Implementation of Image Re-Synthesis</head><p>Image Re-synthesis <ref type="bibr" target="#b23">[24]</ref> is a synthesis-based framework for anomaly segmentation. It consist of a segmentation model S, a synthesis model G, and a discrepancy network D. Given a natural image, the framework will first predict a semantic label map with S. Then, the synthesis model G will re-synthesis the semantic map and finally the discrepancy network D detects meaningful distances caused by mislabeled objects by comparing the natural and synthesized images. The method adopts Bayesian SegNet <ref type="bibr" target="#b16">[17]</ref> and PSP Net <ref type="bibr" target="#b39">[40]</ref> as its segmentation models and Pix2PixHD <ref type="bibr" target="#b36">[37]</ref> as its synthesis model.</p><p>Performance of re-synthesis methods in anomaly segmentation, such as Image Re-synthesis, are highly related to the quality of the segmentation and synthesis predictions. If we improve these predictions, the discrepancy network will have an easier task detecting anomalies.</p><p>To ensure that performance differences between our method and <ref type="bibr" target="#b23">[24]</ref> do not come from the differences in the segmentation or synthesis modules, we re-implemented Image Re-synthesis with state-of-the-art segmentation and synthesis networks. Specifically, we replace their segmentation and synthesis networks with the same networks used in our framework ( <ref type="bibr" target="#b41">[42]</ref> for segmentation and <ref type="bibr" target="#b24">[25]</ref> for synthesis). By doing so, the only differences between both methods are our contributions explained in Section 3. <ref type="table">Table 4</ref> shows the differences between our implementation (Image Resynthesis++) against the results presented in <ref type="bibr" target="#b23">[24]</ref>. In our experiments, we use the same datasets and metrics used in the original publication. Specifically, we use Lost &amp; Found (L&amp;F) <ref type="bibr" target="#b29">[30]</ref> (i.e. images in a driving environment with small road hazards) and Road Anomaly <ref type="bibr" target="#b23">[24]</ref> (i.e online images with anomalous objects located on or near the road) as our datasets, as well as the area under the curve for the receiver operating curve (AUC ROC) as our performance metric. We also added a variation of Lost &amp; Found, where we restrict evaluation to the road, as defined by the ground-truth annotations.</p><p>Our implementation achieves better performance across the different datasets, thus concluding our Image Resyn-thesis++ to be a stronger baseline when compared to our framework. The final implementation was submitted to the Fishyscapes Benchmark (private test set) to ensure equal comparison in more challenging anomaly dataset, such as FS Lost &amp; Found and FS Static.</p><p>We use the AUC ROC as our performance metric in these  <ref type="bibr" target="#b4">[5]</ref>. Thus, for our main experiments, we use more reliable metrics for imbalance problems, such as average precision (AP) and false positive rate at 95% true positive rate (FPR95). Note that the Road Anomaly Dataset was not used in our main experiments, as it only contains sixty (60) images, which are not enough to ensure proper generalization abilities within anomaly segmentation. Additionally, the annotations are not consistent for the anomaly objects. For example, a rock in the middle of the road is labeled as an anomaly. However, the same style of rock next to the road is classfied as an inlier. <ref type="table" target="#tab_4">Table 5</ref> shows the inference time for each module in the proposed approach. Note that the perceptual difference dispersion map also requires running a CNN. As such, we also added its inference time to the running complexity. The estimated times are the average of running an image one hundred (100) times in our framework. We use an NVIDIA 1080Ti GPU with 11GB GPU memory with an input resolution for each module as described in Sec. 4.1. Additionally, we also evaluate the inference speed of our lighter framework (explained in Sec. 5.2) as a comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Computational Complexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Ensemble with End-to-End Training</head><p>As stated in Sec. 3.3, there are benefits to be gained by combining the calculated uncertainty maps (i.e softmax entropy, softmax distance and perceptual difference) with the output of the dissimilarity network. In our main work, we combine these predictions using a weighted average, where the weights are selected empirically through a grid search. An alternative approach would be to learn these weights during the training of the dissimilarity network. This type of training would entail adding a learnable parameter (scalar) for each prediction map at the end of the dissimilarity network. Then, the model can optimize the weights to produce <ref type="table" target="#tab_1">Segmentation  1256  47  2048x1024  Synthesis  192  62  1024x512  Perceptual Difference 13  13  512x256  Dissimilarity  53  53  512x256</ref> Total (ms) 1514 175 -  <ref type="table">Table 6</ref>. Ensemble Prediction Comparisons. Grouping the predictions through end-to-end training shows comparable results in AP against empirically selected weights (grid search). However, end-to-end training increases significantly the FPR95 as the network gets overconfident with its predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Module Ours Ours Light Resolution</head><p>an end-to-end ensemble prediction. <ref type="table">Table 6</ref> compares the results between empirical and lernable weights using the validation set for FS Lost &amp; Found and FS Static. We first find a discrepancy in AP performance. The end-to-end ensemble performs better in FS L&amp;F while empirical weights perform slightly better in FS Static. As stated in Sec. 5.1, we explain that the ensemble prediction with empirical weights outperforms in FS Static since it is easier for uncertainty methods to detect artificially blended objects. This behavior is less evident in the end-toend training since the network optimizes the weights before generating a final prediction. In general, we expected endto-end ensemble to outperform emperical weights in AP as the network optimizes its weights more efficiently.</p><p>The biggest insight from this comparison is shown when comparing the FPR95. In this metric, The end-to-end training significantly decreases performance when compared to empirical weights. These results are consistent with our ablation study in Sec. 5.1, where we show that deep CNNs (e.g. dissimilarity module) tend to be overconfident with its prediction. Thus, by training in an end-to-end fashion, we are still prone to generating overconfidence outputs. As we intent to deploy our framework in safety critical environments (e.g. autonomous driving), we selected the empirical weights as our best model. <ref type="figure">Figure 4</ref> and <ref type="figure" target="#fig_0">Figure 5</ref> display example predictions of our approach in validation images from FS Lost &amp; Found and FS Static. Additionally, we show a qualitative comparison between our technique, an uncertainty estimation method (Softmax Entropy <ref type="bibr" target="#b13">[14]</ref>), and an image re-synthesis method (Image Re-synthesis <ref type="bibr" target="#b23">[24]</ref>). These images emphasizes the robustness of our framework for all anomalies scenarios <ref type="figure">(Figure 1</ref>), in comparison with previous methods. Note that the anomaly detection framework did not train with any of the anomalous instances, and they are seen for the first time during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Example Predictions</head><p>Additionally, some failure cases are presented in <ref type="figure">Figure 6</ref>. Common errors are derived scenes that differ urban landscape, anomaly instances that blend well with the background or small anomalous objects that only cover few pixels in the image. <ref type="figure">Figure 4</ref>. Framework example predictions. Qualitative comparison between proposed framework and baseline for uncertainty methods <ref type="bibr" target="#b13">[14]</ref> and image-resynthesis methods <ref type="bibr" target="#b23">[24]</ref>. The proposed framework outperforms both previous methods detecting anomalies instances. The first five images are from the FS Lost &amp; Found, while the next five are from FS Static. Pixels labeled as void are excluded from the prediction visualizations for the three methods shown, as they are also excluded in the anomaly benchmarks. Our framework reliable detects all three outcomes when a segmentation network encounters an anomalous instances, as explained in <ref type="figure">Figure 1</ref>. First four images are from FS Lost &amp; Found, while the next four are from FS Static. Softmax Entropy <ref type="bibr" target="#b13">[14]</ref> and Image Resynthesis <ref type="bibr" target="#b23">[24]</ref> are also shown as reference. Pixels labeled as void are excluded from the prediction visualizations for the three methods shown, as they are also excluded in the anomaly benchmarks. <ref type="figure">Figure 6</ref>. Failure cases. Our framework still fails at detecting some challenging anomaly instances. Common errors are derived scenes that differ urban landscape (top two images), anomaly instances that blend well with the background (middle two images) or small anomalous objects that only cover few pixels in the image. Softmax Entropy <ref type="bibr" target="#b13">[14]</ref> and Image Resynthesis <ref type="bibr" target="#b23">[24]</ref> are also shown as reference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 .</head><label>5</label><figDesc>Additional predictions examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>40 ? 5 62 ? 5 26 ? 1 w/o ensemble 58 ? 9 66 ? 8 57 ? 6 41 ? 12 w/o unc. maps 39 ? 9 64 ? 10 38 ? 8 51 ? 4 w/o data generator 15 ? 5 63 ? 12 14 ? 4 57 ? 11 &amp; w/o unc. maps Image Resyn.++ 6 ? 1 48 ? 12 8 ? 1 63 ? 18 Ablation Study. Ensemble, uncertainty maps, and data generators contribute to better overall performance. Image Resynthesis++ is used for comparison as our method builds upon it. Results are given as average and standard deviation over five random weight initializations.</figDesc><table><row><cell>Method</cell><cell>FS L&amp;F ?AP ?FPR95 ?AP ?FPR95 FS Static</cell></row><row><cell>Full Framework</cell><cell>55 ? 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Computational Complexity Study. Inference time and input resolution for each module in the proposed framework. Average results for one hundred (100) experiments using NVIDIA 1080Ti GPU.</figDesc><table><row><cell>Method</cell><cell>FS L&amp;F ?AP ?FPR95 ?AP ?FPR95 FS Static</cell></row><row><cell cols="2">Ours w grid search 55.1 39.6 61.5 25.6</cell></row><row><cell cols="2">Ours w end-to-end 59.6 58.6 61.1 37.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Discriminative out-of-distribution detection for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sini?a?egvi?</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07703</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simultaneous semantic segmentation and outlier detection in presence of domain shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sini?a?egvi?</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The fishyscapes benchmark: Measuring blind spots in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03215</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Special issue on learning from imbalanced data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kotcz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time small obstacle detection on highways using compressive rbm road reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Creusot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Munawar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="162" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04599</idno>
		<title level="m">On calibration of modern neural networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">This is not what i imagined: Error detection for semantic segmentation through visual dissimilarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haldimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. A benchmark for anomaly segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11132</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04606</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?nter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Training confidence-calibrated classifiers for detecting outof-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09325</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayadurgam</forename><surname>Srikant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02690</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting the unexpected via image resynthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Nakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to predict layout-to-image conditional convolutions for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Evaluating bayesian deep learning methods for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jishnu</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12709</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detection and retrieval of out-of-distribution objects in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Oberdiek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot A</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lost and found: detecting small road hazards for self-driving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Prediction error meta classification in semantic segmentation: Detection via aggregated dispersion measures of softmax probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Colling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas-Paul</forename><surname>Hack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>H?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schlicht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Gottschalk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00648</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08237</idno>
		<title level="m">Matthijs Douze, and Herv? J?gou. Fixing the train-test resolution discrepancy: Fixefficientnet</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13379,2020.3</idno>
		<title level="m">Revisiting multitask learning in the deep learning era</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Synthesize then compare: Detecting failures and anomalies for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingda</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08440</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8856" to="8865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
