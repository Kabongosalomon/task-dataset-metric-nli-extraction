<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
							<email>yxge@link</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowan</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
							<email>gjyin@mail.ustc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
							<email>yishuai@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hkzhaohaiyu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN. ? ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification (reID) is a challenging task, with the purpose of matching pedestrian images with the same identity across multiple cameras. With the wide usage of deep learning methods, reID performances by different algorithms increase rapidly. There are various attempts on learning representations with deep neural networks, however, posture variations, blur and occlusion still pose great challenges for learning discriminative features. Two types of methods were used for addressing the issues, aligning pedestrian images <ref type="bibr" target="#b0">[1]</ref> or integrating human pose information by learning bodyregion features <ref type="bibr" target="#b1">[2]</ref>. However, these works also require auxiliary pose information in the inference stage, which limits the generalization of the algorithms to new images without pose information. Meanwhile, the computational cost increases due to more complicated inference of pose estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: The image encoder in the FD-GAN is trained to learn robust identity-related and poseunrelated representations with assistance of pose-guided image generator and discriminators. During inference, it does not need pose information and additional computational cost.</p><p>Generative adversarial network (GAN) is gaining increasing attention for image generation. Recently, some works exploited GANs' potential on aiding current person reID algorithms. Zheng et al. <ref type="bibr" target="#b2">[3]</ref> proposed a semi-supervised structure which learns generative images with label smoothing regularization for outliers (LSRO) regularization. PTGAN <ref type="bibr" target="#b3">[4]</ref> was proposed to bridge the domain gap between separate datasets. In addition to image synthesis, GAN can be used for representation learning as well. In this work, we propose a novel identity-related representation learning framework for robust person re-identification.</p><p>The proposed Feature Distilling Generative Adversarial Network (FD-GAN) maintains identity feature consistency under pose variation without increasing the complexity of inference (illustrated in <ref type="figure">Figure 1</ref>). It adopts a Siamese structure for feature learning. Each of the branch consists of an image encoder and an image generator. The image encoder embeds person visual features given the input images. The image generator generates new person images conditioned on the pose information and the input person features by the encoder. Multiple discriminators are integrated in the framework to distinguish inter-branch and intra-branch relations between generated images by the two branches.</p><p>The proposed identity discriminator, the pose discriminator, and the verification classifier together with a reconstruction loss and a novel same-pose loss jointly regularizes the feature learning process for achieving robust person reID. With the adversarial losses, identity-irrelevant information, such as pose and background appearance, in the input image is mitigated from the visual features by the image encoder. More importantly, during inference, additional pose information is no longer needed and saves additional computational cost. Our method outperforms previous works in three widely-used reID datasets, i.e. Market-1501 <ref type="bibr" target="#b4">[5]</ref>, CUHK03 <ref type="bibr" target="#b5">[6]</ref> and DukeMTMC-reID <ref type="bibr" target="#b6">[7]</ref> datasets.</p><p>Overall, this paper has the following contributions. 1) We propose a novel framework, FD-GAN, to learn identity-related and pose-unrelated representations for person re-identification with posevariation. Unlike existing alignment or region-based learning methods, our framework does not require extra auxiliary pose information or increase the computational complexity during inference. 2) Although person image generation is an auxiliary task for our framework, the generated person images by our proposed method show better quality than existing specific person-generation methods.</p><p>3) The proposed FD-GAN achieves state-of-the-art re-identification performance on Market-1501 <ref type="bibr" target="#b4">[5]</ref>, CUHK03 <ref type="bibr" target="#b5">[6]</ref>, and DukeMTMC-reID <ref type="bibr" target="#b6">[7]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generative Adversarial Network (GAN). Goodfellow et al. <ref type="bibr" target="#b7">[8]</ref> first introduced the adversarial process to learn generative models. The GAN is generally composed of a generator and a discriminator, where the discriminator attempts to distinguish the generated images from real distribution and the generator learns to fool the discriminator. A set of constraints are proposed in previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> to improve the training process of GANs, e.g., interpretable representations are learned by using additional latent code in <ref type="bibr" target="#b11">[12]</ref>. GAN-based algorithms shows excellent performance in image generation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. In terms of person image generation, PG 2 was proposed to synthesize person images in arbitrary poses in <ref type="bibr" target="#b17">[18]</ref>. Siarohin et al. <ref type="bibr" target="#b18">[19]</ref> designed a single-stage approach with deformable skip connections in the generator for better deformable human generation. Zanfir et al. <ref type="bibr" target="#b19">[20]</ref> transferred the appearance from the source image onto the target image while preserving the target shape and clothing segmentation layout. In contrast, our method aims at learning person features for person reID with assistance of GANs. Apart from person image synthesis, pose-disentangled representations were learned for face recognition by DR-GAN <ref type="bibr" target="#b20">[21]</ref>, which has key differences with our method. Our experimental results show that our proposed FD-GAN performs  <ref type="figure">Figure 2</ref>: The Siamese structure of the proposed FD-GAN. Robust identity-related and pose-unrelated features are learned by the image encoder E with a verification loss and the auxiliary task of generating fake images to fool identity and pose discriminators. A novel same-pose loss term is introduced to further encourage learning identity-related and pose-unrelated visual features.</p><p>better than DR-GAN on person reID. Our main goal is to decompose the pose information from the image features via adversarial training for learning identity-related and pose-unrelated representation.</p><p>Person Re-identification (ReID). Person reID <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> is a challenging task due to various human poses, domain differences, occlusions, etc. Two main types of methods were adopted in previous works, i.e. learning discriminative person representations <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> and metric learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. PAN <ref type="bibr" target="#b0">[1]</ref> aligns pedestrians and learn person features simultaneously without any extra annotation. Zhao et al. <ref type="bibr" target="#b1">[2]</ref> proposed SpindleNet for learning person features of different body regions with additional human pose information. Most recent methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> designed more complicated frameworks to learn more robust representations with increasing computational cost or requiring extra information during inference.</p><p>Inspired by the excellent performances of GAN-based structures for image generation, there were previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4]</ref> begining to design GAN-based algorithms to improve verification performance of person reID. Zheng et al. <ref type="bibr" target="#b2">[3]</ref> introduced a semi-supervised pipeline for jointly training generated images and real images from training dataset by the proposed LSRO method for regularizing unlabelled data. PTGAN <ref type="bibr" target="#b3">[4]</ref> was proposed to bridge the domain gap between separate person reID datasets. Due to the challenges from pose diversity for person reID datasets, we propose an novel GAN-based framework for distilling identity-related features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Distilling Generative Adversarial Network</head><p>Our proposed Feature Distilling Generative Adversarial Network (FD-GAN) aims at learning identityrelated and pose-unrelated person representations, in order to handle large pose variations across images in person reID.</p><p>The overall framework of our proposed method is shown in <ref type="figure">Fig. 2</ref>. The proposed FD-GAN adopts a Siamese structure, including an image encoder E, an image generator G, an identity verification classifier V and two adversarial discriminators, i.e., the identity discriminator D id and the pose discriminator D pd . For each branch of the network, it takes a person image and a target pose landmark map as inputs. The image encoder E at each branch first transforms the input person image into feature representations. An identity verification classifier is utilized to supervise the feature learning for person reID. However, using only the verification classifier makes the encoder generally encode not only person identity information but also person pose information, which makes the learned features sensitive to person pose variation. To make the learned features robust and   eliminate pose-related information, we added an image generator G conditioned on the features from the encoder and a target pose map. The assumption is intuitive, if the learned person features are pose-unrelated and identity-related, then it can be used to accurately generate the same person's image but with different target poses. An identity discriminator D id and a pose discriminator D pd are integrated to regularize the image generation process. Both D id and D pd are conditional discriminators that classify whether the input image is real or fake conditioned on the input identity or pose. They are not used to classify different identities and poses. The image generator together with the image encoder are encouraged to fool the discriminators with fake generated images. Taking advantages of the Siamese structure, a novel same-pose loss minimizing the difference between the fake generated images of the two branches is also utilized, which is shown to further distill pose-unrelated information from input images. The entire framework is joint trained in an end-to-end manner. For inference, only the image encoder E is used without auxiliary pose information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image encoder and image generator</head><p>The structures of the image encoder E and image generator G are illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>(a). Given an input image x, the image encoder E utilizes ResNet-50 as backbone network to encode the input image into a 2048-dimensional feature vector. The image generator G takes the encoded person features and target pose map as inputs, and aims at generating another image of the same person specified by the target pose. The target pose map is represented by an 18-channel map, where each channel represents the location of one pose landmark's location and the one-dot landmark location is converted to a Gaussian-like heat map. It is encoded by a 5-block Convolution-BN-ReLU sub-network to obtain a 128-dimensional pose feature vector. The visual features, target pose features, and an additional 256-dimensional noise vector sampled from standard Gaussian distribution are then concatenated and input into a series of 5 convolution-BN-dropout-ReLU upsampling blocks to output the generated person images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identity verification classifier</head><p>Given the visual features of the two input images from the image encoder, the identity verification classifier V determines whether the two images belong to the same person. Person identity verification is the main task for person re-identification and ensures learned features to capture identity information of person images. The structure of the classifier is shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b), which takes visual features of two person images as inputs and feeds them through element-wise subtraction, element-wise square, a batch normalization layer, a fully-connected layer, and finally a sigmoid non-linearity function to output the probability that the input image pair belongs to the same person. This classifier is trained with binary cross-entropy loss. Let x 1 , x 2 represent the two input person images, and d(x 1 , x 2 ) represents the output same-person confidence score by our sub-network. The identity verification classifier V is trained with the following binary cross-entropy loss,</p><formula xml:id="formula_0">L v = ?C log d(x 1 , x 2 ) ? (1 ? C)(1 ? log d(x 1 , x 2 )),<label>(1)</label></formula><p>where C is the ground-truth label. C = 1 if x 1 , x 2 belong to the same person and C = 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image generation with identity and pose discriminators</head><p>To regularize the image encoder E to learn only identity-related information, the following person image generator G is trained with the identity discriminator D id and the pose discriminator D pd to generate person images with target poses. Given the input image x k (k = 1 or 2 for two branches) and the target pose p, the generated image y k is required to have the same person identity with x k but with the target pose p. The identity discriminator is utilized to maintain identity-related information in the encoded visual features, while the pose discriminator aims to eliminate pose-related information from the features.</p><p>Identity discriminator D id is trained to distinguish whether the generated person image and the input person image of the same branch belong to the same person. The image generator would try to fool the identity discriminator to ensure the encoded visual feature contains sufficient identity-related information. The identity discriminator sub-network has a similar network structure (see <ref type="figure" target="#fig_1">Figure 3</ref>(c)) to the identity verification classifier V . However, its ResNet-50 sub-network for visual feature encoding does not share weights with that of our image encoder E, because the identity discriminator D id aims at distinguishing the identity between the real/fake images, while our image encoder targets at learning pose-unrelated person features. There is domain gap between the two tasks and sharing weights hinders feature learning process of the image encoder. Such an argument is supported by our experiments. Let y k represent the real person image having the same identity with input image x k and the target pose p. The adversarial loss of the identity discriminator D i can then be defined as</p><formula xml:id="formula_1">L id = max D id 2 k=1 E y k ?Y [log D id (x k , y k )] + E y k ?Z [log(1 ? D id (x k , y k ))]<label>(2)</label></formula><p>where Y and Z represent the true data distribution and generated data distribution by the image generator G.</p><p>Pose discriminator D pd is proposed to distinguish whether the generated person image y k (for k = 1 or 2) matches the given target pose p. The sub-network structure of pose discriminator is shown in <ref type="figure" target="#fig_1">Figure 3(d)</ref>. It adopts the PatchGAN <ref type="bibr" target="#b39">[40]</ref> structure. The input image and pose map (after Gaussianlike heat-map transformation) is first concatenated along the channel dimension and then processed by 4 convolution-ReLU blocks and a sigmoid non-linearity to obtain an image-pose matching confidence map with values between 0 and 1. Each location of the confidence map represents the matching degree between the input person image and the pose landmark map. The image generator G would try to fool the pose discriminator D pd to obtain high matching confidences with fake generated images. The adversarial loss of D dp is then formulated as</p><formula xml:id="formula_2">L pd = max D pd 2 k=1 E y k ?Y [log D pd ([p, y k ])] + E y k ?Z [log(1 ? D pd ([p, y k ]))] ,<label>(3)</label></formula><p>where D pd utilizes the concatenated person image and pose landmark map as inputs.</p><p>However, we observe that the pose discriminator D pd might overfit the poses, i.e., D pd might remember the correspondences between specific poses and person appearances, because each image's pose is generally unique. For instance, if we use a blue-top person's pose as the target pose, the generated image of a red-top person might end up having blue top. To solve this problem, we propose an online pose map augmentation scheme. During training, for each pose landmark, its 1-channel Gaussian-like heat-map is obtained with a random Gaussian bandwidth in some specific range. In this way, we can create many pose maps for the same pose and mitigate the pose overfitting problem.</p><p>Reconstruction loss. The responsibility of G is not only confusing the discriminators, but also generating images that are similar to the ground-truth images. However, the discriminators alone cannot guarantee generating human-perceivable images. Therefore, a reconstruction loss is introduced to minimize the L1 differences between the generated image y k and its corresponding real image y k , which is shown to be helpful for more stable convergence of training the generator.</p><formula xml:id="formula_3">L r = 2 k=1 1 mn y k ? y k 1 ,<label>(4)</label></formula><p>where mn is the number of pixels in the real/fake images. When there is no corresponding groundtruth image y k for an input image x k and a target pose p, this loss is not utilized.</p><p>Same-pose loss. The purpose of the image generator G is to help the image encoder distill only pose-unrelated information. We input the same person's two different images and the same target pose to both branches of our Siamese network, if the conditioning visual features in the two branches are truly only identity-related, then the two generated images should be similar in appearance. Therefore, we propose a same-pose loss to minimize the differences between the two generated images of the same person and with the target pose,</p><formula xml:id="formula_4">L sp = 1 mn y 1 ? y 2 1 ,<label>(5)</label></formula><p>which encourages the learned visual features from E of the two input images to only be identityrelated while ignoring other factors.</p><p>The overall training objective. The above mentioned classifier loss, discriminator losses and reconstruction losses work collaboratively for learning identity-related and pose-unrelated representations.</p><p>The overall loss function could be defined by</p><formula xml:id="formula_5">L = L v + ? id L id + ? pd L pd + ? r L r + ? sp L sp ,<label>(6)</label></formula><p>where ? id , ? pd , ? r , ? sp are the weighting factors for the auxiliary image generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training scheme</head><p>There are three stages for training our proposed framework. In the first stage, our Siamese baseline model, which includes only the image encoder E and identity verification classifier V , is pretrained on a person reID dataset with only identity cross-entropy loss L v in Eq. (1). The pre-trained network weights are then used to initialize E, V , and identity discriminator D id in stage-II. In the second stage, parameters of E and V are fixed. We then train G, identity discriminator D id , and pose discriminator D pd with the overall objective L in Eq. <ref type="bibr" target="#b5">(6)</ref>. Finally, the whole network is finetuned jointly in an end-to-end manner. For each training mini-batch, it contains 128 person image pairs, with 32 of them belonging to same persons (positive pairs) and 96 of them belonging to different persons (negative pairs). All images are resized to 256 ? 128. The Gaussian bandwidth for obtaining pose landmark heat-map is uniformly sampled in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>In training stages II and III, the discriminators and other parts of the network are alternatively optimized. When jointly optimizing the generator G, the image encoder E and the verification classifier V , the overall objective Eq. (6) is used. When optimizing the discriminators D id and D pd , only adversarial losses L id and L pd are adopted.</p><p>Stage I: ReID baseline pretraining. Our Siamese baseline only includes the image encoder E and identity verification classifier V . The ResNet-50 sub-network is first initialized with ImageNetpretrained weights <ref type="bibr" target="#b40">[41]</ref>. The network is optimized by Stochastic Gradient Descent (SGD) with momentum 0.9. The initial learning rates are set to 0.01 for E and 0.1 for V , and they are decreased to 0.1 of their previous values every 40 epochs. The stage-I training process iterates for 80 epochs.</p><p>Stage II: FD-GAN pretraining. With E and V fixed, We integrate G, D id , and D pd into the framework in stage-II. Adam optimizer is adopted for optimizing G and SGD for D id and D pd . The initial learning rates for G, D id , D pd are set as 10 ?3 , 10 ?4 , 10 ?2 , respectively. Learning rates maintain the same for the first 50 epochs, and then gradually decrease to 0 in the following 50 epochs. The loss weights are set as ? id = 0.1, ? pd = 0.1, ? r = 10, ? sp = 1. We took the label smoothness scheme <ref type="bibr" target="#b41">[42]</ref> for better balancing between the generator and the discriminator.</p><p>Stage III: Global finetuning. For finetuning the whole framework end-to-end, we use Adam for optimizing E, G and V , and SGD for D id , D pd after loading the pre-trained weights from stage-II. Specifically, the initial learning rates are set to 10 ?6 , 10 ?6 , 10 ?5 , 10 ?4 , 10 ?4 for E, G, V , D id , D pd , respectively. Learning rates remain the same for the first 25 epochs, and then gradually decay to 0 in the following 25 epochs. Batch normalization layers in E is fixed to achieve better performance. For loss weights, ? id = 0.1, ? pd = 0.1, ? r = 10, ? sp = 1 are set as the weights for different loss terms. ? ? We tune hyperparameters on the validation set of Market-1501 <ref type="bibr" target="#b4">[5]</ref>, and directly use the same hyperparameters for DukeMTMC-reID <ref type="bibr" target="#b6">[7]</ref> and CUHK03 <ref type="bibr" target="#b5">[6]</ref> datasets. n/a n/a n/a n/a n/a n/a 59.8 81.4 40.7 62.5 baseline (Siamese) n/a n/a ? n/a n/a n/a 72.  </p><formula xml:id="formula_6">FD-GAN (no D pd ) ? ? ? ? ? ? 73.0 88.0 - - FD-GAN (no D id ) ? ? ? ? ? ? 72.8 89.2 - - FD-GAN (no D id &amp; D pd ) ? ? ? ? ? ? 71.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparison to DR-GAN [21]</head><p>There is an existing work, DR-GAN <ref type="bibr" target="#b20">[21]</ref> based on conditional GAN <ref type="bibr" target="#b42">[43]</ref>, which tries to learn poseinvariant identity representations for face recognition. It also adopts an encoder-decoder structure with a discriminator for classifying both identity. Comparison results in Section 4.2 demonstrate the advantages of our proposed method over DR-GAN on the person reID task.</p><p>This is because there are three key differences between the proposed FD-GAN and DR-GAN, which make our algorithm superior. 1) We adopt a Siamese network structure, which enables us to use the same-pose loss to encourage encoding only learning identity-related information, while DR-GAN does not have such a loss term. 2) We do not share the weights between the ResNet-50 networks in the image encoder and in the identity discriminator. We observe that identity verification and real/fake image identity discrimination are two tasks in different domains and therefore their weights should not be shared. 3) Our Siamese structure utilizes a verification classifier instead of a cross-entropy classifier, which shows better person reID performance than a single-branch network does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and evaluation metrics</head><p>In this paper, three datasets are used for performance evaluation, including Market-1501 <ref type="bibr" target="#b4">[5]</ref>, CUHK03 <ref type="bibr" target="#b5">[6]</ref>, and DukeMTMC-reID <ref type="bibr" target="#b6">[7]</ref>. The Market-1501 dataset <ref type="bibr" target="#b4">[5]</ref> consists of 12,936 images of 751 identities for training and 19,281 images of 750 identities in the gallery set for testing. The CUHK03 dataset <ref type="bibr" target="#b5">[6]</ref> contains 14,097 training images of 1,467 identities captured from two cameras. The original training and testing protocol is used. The DukeMTMC-reID dataset <ref type="bibr" target="#b6">[7]</ref> is a subset of the pedestrian tracking dataset DukeMTMC for image-based reID. It contains 16,522 images of 702 identities for training. Mean average precision (mAP) and CMC top-1 accuracy are adopted for performance evaluation on all the three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Component analysis of the proposed FD-GAN</head><p>In this section, component analysis is conducted to demonstrate the effectiveness of components in the FD-GAN framework, including the Siamese structure, and the use of verification classifier and same-pose loss. We also compare with DR-GAN <ref type="bibr" target="#b20">[21]</ref>, which also proposes to learning pose disentangled features. Our Siamese baseline model is only the ResNet-50 image encoder E with our identity verification classifier V . The analysis is conducted on Market-1501 <ref type="bibr" target="#b4">[5]</ref> and DukeMTMC-reID <ref type="bibr" target="#b6">[7]</ref> datasets and the results are shown in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Siamese structure. We first compare the our Siamese reID baseline (denoted as baseline (Siamese)) with the single branch ResNet-50 <ref type="bibr" target="#b43">[44]</ref> baseline trained with cross-entropy loss on person IDs (denoted as baseline (single)). The Siamese baseline outperforms single-branch baseline by 12.7% and 20.6% in terms of mAP on the two datasets.</p><p>Proposed FD-GAN, with online pose map augmentation and adversarial discriminators. Based on the Siamese structure, we build our proposed FD-GAN framework. We can observe that the proposed FD-GAN gains significant improvements from our Siamese baseline in terms of both mean AP and top-1 accuracy on both two reID datasets. There are 5.2% and 3.2% mAP improvements in terms of mAP on the two datasets. To show the effectiveness of our proposed online pose map augmentation, we test removing it when training our FD-GAN (denoted as FD-GAN w/o pose aug. in <ref type="table" target="#tab_2">Table 1</ref>). It results in a .5% performance drop for both datasets. In order to validate the effects of the two discriminators D id and D pd , we test removing them separately and together (denoted as FD-GAN w/o D id or D pd . in <ref type="table" target="#tab_2">Table 1</ref>). It results in not only obviously performance drop, but also poorer generated images.</p><p>DR-GAN <ref type="bibr" target="#b20">[21]</ref>, verification loss, same-pose loss, and not sharing image encoder. We also study the effectiveness of using verification loss and same-pose loss, and not sharing image encoder weights to identity discriminator. Original DR-GAN's pose discriminator classifies each face image into one of 13 poses. For fair comparison, we first test integrating DR-GAN into our Siamese baseline (denoted as Siamese DR-GAN), which could be viewed our FD-GAN without the same-pose loss and also sharing weights between E and D id . Since our network uses pose map as input condition, we use our conditional pose discriminator D pd to replace DR-GAN's pose discriminator. The Siamese DR-GAN even performs worse than our Siamese baseline on the DukeMTMC-reID dataset. Our proposed FD-GAN outperforms it by over 4% mAP on both datasets. We also try removing both verification classifier and same-pose loss (denoted as FD-GAN w/o sp. &amp; veri.), removing only identity verification classifier (denoted as FD-GAN w/o veri.), removing only same-pose loss (denoted as FD-GAN w/o sp.) from our proposed FD-GAN and only sharing weights between E and D id (denoted as FD-GAN share E) . Results in <ref type="table" target="#tab_2">Table 1</ref> show that both the verification loss and same-pose loss are indispensable to achieve superior performance on person reID. Also, not sharing weights between E and D id results in better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-arts</head><p>We compare our proposed FD-GAN with the state-of-the-art person reID methods including VI+LSRO <ref type="bibr" target="#b2">[3]</ref>, JLML <ref type="bibr" target="#b49">[50]</ref>, PA <ref type="bibr" target="#b47">[48]</ref>, etc. on the three datasets, Market-1501 <ref type="bibr" target="#b4">[5]</ref> , CUHK03 <ref type="bibr" target="#b5">[6]</ref> , and DukeMTMC-reID <ref type="bibr" target="#b6">[7]</ref>. The results are listed in <ref type="table" target="#tab_5">Table 2</ref>. Note that only single query results from published papers are compared in order to make a fair comparison.</p><p>By finetuning the FD-GAN based on ResNet-50 <ref type="bibr" target="#b43">[44]</ref> baseline network structure, our proposed FD-GAN outperforms previous approaches and achieves state-of-the-art performance. We can achieve 90.5% top-1 accuracy and 77.7% mAP on the Market-1501 dataset <ref type="bibr" target="#b4">[5]</ref>, 92.6% top-1 accuracy and 91.3% mAP on CUHK03 dataset <ref type="bibr" target="#b5">[6]</ref>, and 80.0% top-1 accuracy and 64.5% mAP on the DukeMTMC-reID dataset <ref type="bibr" target="#b6">[7]</ref>, which demonstrates the effectiveness of the proposed feature distilling FD-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Person image generation and visual analysis</head><p>Comparison of person image generation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Although generating person images is only an auxiliary task in our FD-GAN to learn more robust person features. We are interested in comparing  <ref type="figure">Figure 4</ref>: (a) Generated person images by our proposed method and <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> on test images of Market-1501 dataset <ref type="bibr" target="#b4">[5]</ref>. (b) Two examples of the generated images from the Market-1501 dataset <ref type="bibr" target="#b4">[5]</ref>. (First row) the ground-truth images of target poses. (Second-third rows) input images and the generated images with different target poses on training images of Market-1501 dataset <ref type="bibr" target="#b4">[5]</ref>.</p><p>the generated images with images by other specifically designed person generation methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. <ref type="figure">Figure 4(a)</ref> shows the generated person images by state-of-the-art person generation methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and our FD-GAN. One can clearly see that our proposed method better understand the concept of "backpack" and could generate correct upper and lower body clothes. We argue that the key is using person identity supervisions to make the encoder learn better identity-related features. Our Siamese structure and the same-pose loss also contribute to achieving consistent generation results.</p><p>Visualization for learned features. The proposed FD-GAN framework not only improves the discriminative capability of visual features but could also be used as a visualization tool for manually examining learned feature representations. The quality of learned person features have direct impact on the generated person images. We can therefore tell what aspects of person appearances are captured by the features. For instance, for "input 1_b" in <ref type="figure">Figure 4</ref>(b), its generated frontal images do not show colored pattern on the upper body but only the general colors and shapes of the upper and lower bodies, which might demonstrate that the learned image encoder focus on embedding the overall appearances of persons but fail to capture the distinguishable details in appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed the novel FD-GAN for learning identity-related and pose-unrelated person representations with human pose guidance. Novel Siamese network structure as well as novel losses ensure the framework learns more pose-invariant features for robust person reID. Our proposed framework achieves state-of-the-art performance on person reID without using additional computational cost or extra pose information during inference. The generated person images also show higher quality than existing specific person-generation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Network structures of (a) the generator G and the image encoder E, (b) the verification classifier V , (c) the identity discriminator D id , (d) the pose discriminator D pd .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Component analysis of the proposed FD-GAN on Market-1501<ref type="bibr" target="#b4">[5]</ref> and DukeMTMC-reID<ref type="bibr" target="#b6">[7]</ref> datasets in terms of top-1 accuracy (%) and mAP (%)</figDesc><table><row><cell>Networks</cell><cell>not share E</cell><cell>Components Lsp Lv L pd L id pose map aug.</cell><cell>Market-1501[5] mAP top-1</cell><cell>DukeMTMC-reID[7] mAP top-1</cell></row><row><cell>baseline (single)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Experimental comparison of the proposed approach with state-of-the-art methods on Market-1501<ref type="bibr" target="#b4">[5]</ref>, CUHK03<ref type="bibr" target="#b5">[6]</ref>, and DukeMTMC-reID<ref type="bibr" target="#b6">[7]</ref> datasets. Top-1 accuracy(%) and mAP(%) are reported.</figDesc><table><row><cell>Methods</cell><cell cols="2">Market-1501 [5] mAP top-1</cell><cell cols="2">CUHK03 [6] mAP top-1</cell><cell cols="2">DukeMTMC-reID [7] mAP top-1</cell></row><row><cell>BoW+KISSME [5]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>12.1</cell><cell>25.1</cell></row><row><cell>LOMO+XQDA [37]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>17.0</cell><cell>30.8</cell></row><row><cell>OIM Loss [45]</cell><cell>60.9</cell><cell>82.1</cell><cell>72.5</cell><cell>77.5</cell><cell>47.4</cell><cell>68.1</cell></row><row><cell>MSCAN [39]</cell><cell>53.1</cell><cell>76.3</cell><cell>-</cell><cell>74.2</cell><cell>-</cell><cell>-</cell></row><row><cell>DCA [39]</cell><cell>57.5</cell><cell>80.3</cell><cell>-</cell><cell>74.2</cell><cell>-</cell><cell>-</cell></row><row><cell>SpindleNet [2]</cell><cell>-</cell><cell>76.9</cell><cell>-</cell><cell>88.5</cell><cell>-</cell><cell>-</cell></row><row><cell>k-reciprocal [46]</cell><cell>63.6</cell><cell>77.1</cell><cell>67.6</cell><cell>61.6</cell><cell>-</cell><cell>-</cell></row><row><cell>VI+LSRO [3]</cell><cell>66.1</cell><cell>84.0</cell><cell>87.4</cell><cell>84.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Basel+LSRO [3]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.1</cell><cell>67.7</cell></row><row><cell>OL-MANS [47]</cell><cell>-</cell><cell>60.7</cell><cell>-</cell><cell>61.7</cell><cell>-</cell><cell>-</cell></row><row><cell>PA [48]</cell><cell>63.4</cell><cell>81.0</cell><cell>-</cell><cell>85.4</cell><cell>-</cell><cell>-</cell></row><row><cell>SVDNet [49]</cell><cell>62.1</cell><cell>82.3</cell><cell>84.8</cell><cell>81.8</cell><cell>56.8</cell><cell>76.7</cell></row><row><cell>JLML [50]</cell><cell>65.5</cell><cell>85.1</cell><cell>-</cell><cell>83.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Proposed FD-GAN</cell><cell>77.7</cell><cell>90.5</cell><cell>91.3</cell><cell>92.6</cell><cell>64.5</cell><cell>80.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Input Pose GT [18] [19] Ours Input Pose GT [18] [19] Ours Input Pose GT [18] [19] Ours</figDesc><table><row><cell>(a)</cell></row><row><cell>(b)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by SenseTime Group Limited, the General Research Fund sponsored by the Research Grants Council of Hong Kong (Nos. CUHK14213616, CUHK14206114, CUHK14205615, CUHK14203015, CUHK14239816, CUHK419412, CUHK14207814, CUHK14208417, CUHK14202217), the Hong Kong Innovation and Technology Support Program (No. ITS/121/15FX).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pedestrian alignment network for large-scale person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets. In: NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="2172" to="2180" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Perceptual generative adversarial networks for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generative attribute controller with conditional filtered generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hiramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Shadow detection with conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Yago Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="405" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deformable gans for pose-based human image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuiliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human appearance transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rgb-infrared cross-modality person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A two stream siamese convolutional neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tahboub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep group-shuffling random walk for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end deep kronecker-product matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Transferring a semantic representation for person re-identification and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stepwise metric promotion for unsupervised video person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">One-shot metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page" from="1268" to="1277" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Improved techniques for training gans. In: NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Efficient online local metric adaptation via negative samples for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07256</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Person re-identification by deep joint learning of multi-loss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
