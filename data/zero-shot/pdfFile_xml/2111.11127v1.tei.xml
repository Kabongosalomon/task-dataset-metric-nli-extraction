<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Myope Models -Are face presentation attack detection models short-sighted?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">C</forename><surname>Neto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INESC TEC</orgName>
								<address>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculdade de Engenharia da</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">F</forename><surname>Sequeira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INESC TEC</orgName>
								<address>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INESC TEC</orgName>
								<address>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculdade de Engenharia da</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Myope Models -Are face presentation attack detection models short-sighted?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Presentation attacks are recurrent threats to biometric systems, where impostors attempt to bypass these systems. Humans often use background information as contextual cues for their visual system. Yet, regarding face-based systems, the background is often discarded, since face presentation attack detection (PAD) models are mostly trained with face crops. This work presents a comparative study of face PAD models (including multi-task learning, adversarial training and dynamic frame selection) in two settings: with and without crops. The results show that the performance is consistently better when the background is present in the images. The proposed multi-task methodology beats the state-of-the-art results on the ROSE-Youtu dataset by a large margin with an equal error rate of 0.2%. Furthermore, we analyze the models' predictions with Grad-CAM++ with the aim to investigate to what extent the models focus on background elements that are known to be useful for human inspection. From this analysis we can conclude that the background cues are not relevant across all the attacks. Thus, showing the capability of the model to leverage the background information only when necessary.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Presentation attacks are one of the weaknesses and dangers posed to face recognition systems (FRS). Apart from a few exceptions <ref type="bibr" target="#b15">[16]</ref>, most of the methods used in presentation attack detection (PAD) rely on tight face crops <ref type="bibr" target="#b16">[17]</ref>. In other words, cropping the faces removes everything in the image that is not part of a face. Thus, the information provided as input for these systems is somehow limited, leading to myopic (i.e. short-sighted) face PAD models. This pre-processing step has several advantages, including the capability to process several faces per frame and it is a fact that first generation face PAD methods were designed to rely only on the face region, in order to be backwards compatible with FRS. However, on the other hand, it removes spatial and contextual information present in the original image. The human visual cortex can process this spatial and contextual information to identify some attacks meant to fool the human inspection. Moreover, in some cases, a human can still be fooled by face crops of replay attacks, if the resolution of the replay device is high enough. Similarly, machine vision systems can likely learn to leverage that information when it is available. Furthermore, they can decide if it is more important to focus on the contextual information or on the face itself.</p><p>In this work, we aim at further studying an alternative approach to discarding upfront the background, already adopted in some previous works in the literature. Thus, in this work we investigate how different approaches perform in the presence of contextual background. In the experiments, we perform a comparative study of a state-of-the-art supervised binary classification model and its combination with an adversarial approach (in this, two embeddings are produced, one containing information that is useful for the prediction and the other containing nuisances present in the input, which are optimized to minimize the mutual information between them), on three datasets, using face images with and without crops. For a more thorough evaluation, we elected the ROSE-Youtu dataset due to the fact that it offers a high variety of attacks not available in the other datasets. Thus, we evaluated a multi-task model which was optimized to also distinguish between the different attacks and the previous combined with an adversarial approach. Furthermore, we designed a novel experiment based on multiple instance learning methods. With this, we attempted at creating a dynamic frame selection system, passing the responsibility of selecting the frame most likely to include an attack to the model. Differently from the previous approaches, this method requires more processing power since it goes through all frames in a video.</p><p>It has been shown in the literature that black-box models, such as deep neural networks, can learn unpredictable patterns and focus their decision on "unexpected" regions of interest in the input. Therefore, we also evaluated the experiments from the perspective of explainable artificial intelligence (xAI). These evaluations are necessary to better understand the models' decisions and the errors due to their opaqueness <ref type="bibr" target="#b10">[11]</ref>. We use methods for the visualization of the elements that were important for the decision, such as Grad-CAM++ <ref type="bibr" target="#b3">[4]</ref>. The output of these methods allowed us to analyze the visual cues found by the model to detect attacks. We also note that in some cases, the models follow the same cues used by an human inspector. Thus, we reflect upon the influence of the background in the choice of future face PAD algorithms.</p><p>This study contributes to the awareness around the need to incorporate interpretability in face PAD methodologies. The models' performance improvements are attributed to the use of background and this can be corroborated by observing that, in fact, the models' decisions are made using the background cues.</p><p>The experiments use three datasets: ROSE-Youtu <ref type="bibr" target="#b19">[20]</ref>; NUAA <ref type="bibr" target="#b25">[26]</ref>; and Replay-attack <ref type="bibr" target="#b4">[5]</ref>. The state-of-the-art supervised binary classification model (BC), and BC combined with an adversarial approach are evaluated on the three datasets in two settings: with and without background. Furthermore, we evaluate the multi-task (MT) and dynamic frame selection (DFS) approaches using only the ROSE-Youtu dataset which includes a high diversity of attacks comprising both two-dimensional and three-dimensional information. Hence, it was used to study the impact of the background in the model performance and whether the background affects the capability of generalizing between attacks. However, the experiments were defined with two goals in mind: generalization between attacks, and generalization between subjects. The first goal is addressed, through the study of the performance of the model on attacks that were not seen previously during training. The second is addressed by using 50% of the subjects for testing. The BC, MT and adversarial approaches are evaluated with and without background in a cross-dataset scenario.</p><p>The major contributions of this work are: i) the evaluation (in three widely used datasets) of a state-of-the-art supervised binary classification model and its combination with an adversarial strategy in two alternative scenarios: face images with and without background; ii) a multi-task face PAD approach that leverages background and achieves state-of-the-art results on the ROSE-Youtu dataset; iii) a proposed methodology for frame selection strategy on the ROSE-Youtu dataset. It was not the focus of this study to see if specific models in the literature perform better with and without background, instead, it focuses on proposing simple and distinct approaches and analyzing whether the results are consistent across them with regard to the presence of background.</p><p>Besides this introduction and the conclusion, this document contains four major sections. First, in Section 2 there is a discussion of the related work and how it led to the current study. Afterwards, details on the experiments conducted are given in Section 3. The description of the datasets is given in Section 4. And finally, in Section 5 we present and discuss the obtained results and how it impacts the future of PAD methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Typically, previous works on face presentation attack detection do not leverage background information. And thus, removing it is a common practice and a frequent step on the preprocessing stage. The most common approach is to use a face crop, usually obtained through the use of deep learning-based face detection algorithms such as MTCNN <ref type="bibr" target="#b31">[32]</ref> and RetinaFace <ref type="bibr" target="#b5">[6]</ref>. Earlier methods used more traditional techniques, for instance, the Viola-Jones cascade detector <ref type="bibr" target="#b27">[28]</ref>.</p><p>Within the published works, it is possible to find reinforcement learning approaches <ref type="bibr" target="#b2">[3]</ref>, 3D-CNNs <ref type="bibr" target="#b18">[19]</ref>, a two stages approach relying on blinking <ref type="bibr" target="#b7">[8]</ref> and several other colour-based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>. The background usage is addressed in some works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16]</ref>, however, they did not perform comparative studies regarding performance, with and without the background, of several approaches. It is possible to find this comparison in other works, however, the proposed methods are based on conventional machine learning instead of end-to-end deep learning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b17">18]</ref>. Due to the nature of the ROSE-Youtu dataset, which contains three-dimensional and two-dimensional attacks, there are fewer methods tested on this dataset than on others. The variability of attacks included in the dataset significantly increase the difficulty of finding a model capable of performing well on all of them. For this reason, even methods that achieve almost zero error on other datasets, have worse performance on the ROSE-Youtu <ref type="bibr" target="#b6">[7]</ref>.</p><p>To the best of our knowledge, there has not been any method inspired by multiple instance learning applied to face PAD. However, there is an article on a similar technique used for the detection of deep fakes <ref type="bibr" target="#b20">[21]</ref>. Despite being a slightly different problem, the detection methodology has a significant overlap. The adversarial approach followed in our experiments was described first at <ref type="bibr" target="#b12">[13]</ref> and its capabilities to work with face PAD systems was evaluated one year later by Jaiswal et al. <ref type="bibr" target="#b13">[14]</ref>.</p><p>Producing and visualizing explanations of the predictions for face presentation attack detection is a relatively new topic. Sequeira et al. have explored the challenge of interpreting face PAD methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>. Their work described how the current evaluation metrics for PAD lack information regarding the elements that are being used for the prediction. In a sense, they argue that models can make accurate predictions but still base their decision on parts of the image that do not correspond to real face features or presentation artifacts as a human inspector would. In this work, we follow a similar approach to produce and analyze explanations. However, we use them to infer if the presence of contextual background leads to the use of certain visual cues in the image. At the same time, we look forward to seeing if other contextual elements are used to make correct predictions, for instance, reflections. Humans often use these elements to make their analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Myriads of attacks are constantly threatening biometric systems. However, in practice, we do not aim to identify the type of attack, thus the main goal is to infer if the image given to the sensor is an attack or if it is from a genuine person. The problem is, in its essence, formulated as a binary classification task. On top of the binary task, we also applied some different training processes. However, these do not affect the network at test time. The purpose of these distinct approaches is to understand if the background effect generalizes between approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attack or Genuine?</head><p>Which attack? <ref type="figure">Figure 1</ref>. Architecture of the multi-task learning model. It receives an image (yellow box) and includes a CNN (blue figure), two output heads (green figures), where the first is a binary head and the second has 8 output nodes. The output node for the genuine samples on both heads ensures that the main goal remains on the detection of attacks.</p><p>? Binary classification training (BC) -In the first approach, the only task that the backbone network is optimized for at training time is to classify between attacks and genuine. For this, we use a MobileNet v2 <ref type="bibr" target="#b22">[23]</ref> that outputs two values, which are activated with softmax. The optimization of the weights is done using the binary cross-entropy loss (Eq. 1).</p><formula xml:id="formula_0">BCE(y, p) = ?(y log(p) + (1 ? y) log(1 ? p)) (1)</formula><p>? Multi-task classification training (MT) -Whenever a model is optimized to distinguish between attacks and genuine images, it treats all the attacks equally. However, in practice, the attacks are not the same, and each possesses distinctive characteristics. And thus, we also formulated the training stage of a MobileNet v2, so it learns to distinguish between the seven different attacks. It is also possible that learning to discriminate between attacks also boosts the performance whenever the attack is unknown. Instead of having an output layer with two classes, the network has two output layers. The first has two output classes, whereas the other has eight (seven attacks and one genuine). In both cases, they are activated with softmax. Both layers are, simultaneously, updated with the binary cross-entropy 1 and cross-entropy 2 losses, respectively. These losses are combined as seen on Equation 3. Due to the risk of the second term of the equation being larger than the first, it was necessary to add an output node for genuine samples in both heads. <ref type="figure">Figure 1</ref> is a simplified visualization of the architecture of the model.</p><formula xml:id="formula_1">CE(y, p) = ? M c=1 y o,c log(p o,c )<label>(2)</label></formula><formula xml:id="formula_2">Loss M ulti (y 1 , p 1 , y 2 , p 2 ) = BCE(y 1 , p 1 ) + CE(y 2 , p 2 )<label>(3)</label></formula><p>? Adversarial training (Adv.) -In the images shown to the system, there is background information that is useful for the prediction, for instance, reflections. Nevertheless, not all the background information is useful. Part of it can be considered to be a nuisance. Hence, we explored also an approach that attempted to remove those parts of the image from the feature vector used for the classification task. This approach, known as Unsupervised Adversarial Invariance <ref type="bibr" target="#b12">[13]</ref>, produces two distinct embeddings (i.e. feature vectors). The first vector, e 1 , represents the features that are relevant to the prediction of the model, whereas the second, e 2 , comprises the information that should not be used for the prediction. Constructing the loss of such architecture requires four terms. The first two are maximization terms (Eq. 6 and 7). They attempt to reconstruct e 1 from e 2 and vice-versa. This attempts at removing any potential mutual information between both embeddings. The other two are minimization terms (Eq. 5). The first embedding uses e 1 to perform the classification task. Whereas the second apply some noise to e 1 , in the form of a dropout layer. From the noisy e 1 and from e 2 , it tries to reconstruct the input image. For the construction/reconstruction terms the loss used is the mean squared error (Eq. 4), while for the classification term we use either the binary cross-entropy (Eq. 1) or the multi-task loss (Eq. 3). The term ? controls the impact of the reconstruction loss on the overall loss. We start with ? = 0.025, and we increase by 0.025 at the end of each epoch. The architecture is represented in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><formula xml:id="formula_3">M SE(x, y) = D i=1 (x i ? y i ) 2<label>(4)</label></formula><p>Loss Adv (e 1 , e 2 , e 1 , e 2 ) = ?M SE(e 1 , e 2 )?M SE(e 2 , e 1 ) (5)</p><formula xml:id="formula_4">Loss Class (y, p, x, x ) = BCE(y, p) + ?M SE(x, x )<label>(6)</label></formula><p>Loss Class (y 1 , p 1 , y 2 , p 2 , x, x ) = ? Dynamic frame selection training (DFS) -Finally, we also propose an architecture to select the best frame for the detection of attacks. In state-of-the-art approaches, the training frames were fixed and previously selected from the list of possible frames. This, however, raises a couple of questions: 1) Do all the frames contain the same information for the prediction?; 2) If not, are we selecting the best frames to optimize the network?. We structured the optimization of this method in two stages: frame selection and learning. The frame selection stage processes all the frames in a video and computes their output with the model. From the outputs, if the video is from an attack it selects the three frames that have the lowest probability of being an attack. And the opposite if the video is from a genuine individual. Afterwards, the selected frames are used in the learning stage to optimize the network towards the video labels. At testing time the process is similar to the frame selection, the frame with the highest probability of being an attack is used for the classification. Perhaps, one of the most interesting aspects of this approach is that it can be integrated with the other previously mentioned. The frame selection step remains unchanged, while the training stage integrates the changes related to the other approaches. <ref type="figure" target="#fig_1">Figure 3</ref> shows the behavior of the described method for both frame selection and testing. The frame selection probability is the result of the attack probability produced by the binary classification layer of the model.  To evaluate and compare the performance of these PAD models, we collected the following metrics: the Bona fide Presentation Classification Error Rate (BPCER) (the proportion of bona fide presentations erroneously classified as attacks), and the Attack Presentation Classification Error Rate (APCER) (the proportion of presentation attack wrongly classified as bona fide) <ref type="bibr" target="#b11">[12]</ref>. Finally, we also collected the Equal Error Rate (EER), which is the error at the operation point where the APCER and BPCER have the same value. For the APCER and the BPCER we used a threshold of 0.5.</p><formula xml:id="formula_5">Loss M ulti task (y 1 , p 1 , y 2 , p 2 ) + ?M SE(x, x )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>The datasets used for the experimental evaluation are: ROSE-Youtu <ref type="bibr" target="#b19">[20]</ref>; NUAA <ref type="bibr" target="#b25">[26]</ref>; and Replay-Attack <ref type="bibr" target="#b4">[5]</ref>.</p><p>ROSE-Youtu <ref type="bibr" target="#b19">[20]</ref>: Contains, in its public version, 3350 videos with 20 different subjects. On average, video clips have a duration of 10 seconds. For each of the subjects, it contains around 150 to 200 videos captured from five mobile devices (all with different resolutions on their camera) and five lighting conditions. The front-facing camera was used with a distance between face and camera of about 30 to 50 centimeters.   Video which records a Mac LCD display #5</p><p>Paper mask with two eyes and mouth cropped out #6</p><p>Paper mask without cropping #7</p><p>Paper mask with the upper part cut in the middle There are eight different types of videos, which translates into eight classes. The first class represents the genuine samples, whereas each of the following seven represent an attack. The first two attacks are print attacks, while the third and fourth are replay attacks on a Lenovo and an Apple laptop, respectively. The remaining three are based on paper masks and are responsible for including three-dimensional information in the dataset. These attacks are described in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>We preprocessed the dataset into two different copies. For both, the frames of the videos were extracted and stored. For the second, a face was cropped by the MTCNN algorithm <ref type="bibr" target="#b31">[32]</ref> for all frames. Examples of these images are seen in <ref type="figure" target="#fig_3">Figure 4</ref>. We used the videos from the first 10 indexed subjects <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12)</ref> for training and the remaining 10 for testing.</p><p>NUAA <ref type="bibr" target="#b25">[26]</ref>: was one of the first public databases for training and evaluating the performance of face PAD methods. This database simulates a simple and general method that re-captures a printed photograph of users for attacking a face recognition system. The NUAA database contains real and presentation attack face images of 15 persons. For each person, both real and presentation attack images were captured in three different sessions using generic cheap webcams and real face and printed photograph of users. The NUAA database contains 5105 real and 7509 presentation attack face images in color space with 640 ? 480 pixels of image resolution. In this database, using the collected images, the training and testing sub-databases are predefined for training and testing of the PAD method, through which the performances of various PAD methods can be compared. In detail, the training database contains 1743 real and 1748 presentation attack face images, while the testing database contains 3362 real and 5761 presentation attack face images. Replay-Attack <ref type="bibr" target="#b4">[5]</ref>: this database for face PAD consists of 1300 video clips of photo and video attack attempts to 50 clients, under different lighting conditions. All videos are generated by either having a (real) client trying to access a laptop through a built-in webcam or by displaying a photo or a video recording of the same client for at least 9 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we present and discuss the evaluation results of the methods described. Regarding implementation details, all the methods described were optimized with Adam and a fixed learning rate of 0.001. The model used is a MobileNet v2 pre-trained on the ImageNet dataset. The images are resized to have a resolution of 224x224 and an RGB color scheme. The Grad-CAM++ <ref type="bibr" target="#b3">[4]</ref> was the visualization tool used to analyze the parts of the image relevant to the models' decisions. Train and test set splits were the same described in the publication of each dataset, so that the results can be compared with other works.</p><p>In <ref type="table" target="#tab_2">Table 2</ref> are shown the results obtained with the binary classification (BC) and its combination with the adversarial training (Adv.+BC) for the NUAA and Replay-Attack datasets. From both evaluations it is evident the performance improvement when the background is present in the images with a decrease in the EER to 0.00%.</p><p>The variety of attacks present in the ROSE-Youtu dataset motivates a multi-task learning approach. The experiments produced intended to evaluate the BC and the MT approaches with the inclusion and exclusion of contextual background. We further attempted to integrate them with the adversarial approach described in the previous section. The results for both BC and MT classification and their combination with the adversarial training strategy (Adv.+BC; Adv.+MT) can be seen in <ref type="table" target="#tab_3">Table 3</ref>. While the adversarial training did not lead to the expected im-provement, in all four scenarios the models' performance improved with the background. This seems to indicate that the background provides more information and favoured the performance error rates. On multi-task classification, the improvements on the EER are as high as 81%.  In a cross-dataset approach, we performed experiments in which the models trained with the ROSE-Youtu dataset were evaluated with the other two datasets. The results of these experiments are presented in <ref type="table" target="#tab_4">Table 4</ref>. These results are in line with the results depicted on <ref type="table" target="#tab_2">Tables 2 and 3</ref>. The comparison of the same-database and cross-database results show that the models' performance consistently improve when the background information is used.</p><p>Considering the performance gains from the use of background, we used this scenario to explore the proposed multitask (MT) and dynamic frame selection (DFS) strategies. The results of all approaches evaluated for the ROSE-Youtu dataset are presented in <ref type="table">Table 5</ref>. Unexpectedly, the performance of the DFS methods and the ones that used adversarial training produced worse results than the simple binary and multi-task classification. The BC and MT approaches performed well at detecting attacks, as can be seen by the low value of the APCER, 0.25% and 0.15%, respectively. Regarding the detection of bonafide images, the BC performed worse than several of the other methods and the MT had the lowest BPCER of them all, 0.40%. We extended the experiments of the multi-task classification approach for different evaluation configurations, one and unseen attack (the results can be seen in <ref type="table">Table 6</ref> and <ref type="table">Table 7</ref>, respectively). The multi-task classification was also integrated with the adversarial training and the dynamic frame selection for a better and more complete comparison.</p><p>The one attack configuration selects one attack to be used for both training and testing. This is intended to see how hard is to overfit the model to that attack and to distinguish it from genuine images. The results for this configuration are visible in <ref type="table">Table 6</ref> and it is possible to see that while the three approaches are capable of overfitting to the majority of the attacks, the attack #4 remains challenging to the adversarial and DFS approaches.</p><p>The unseen configuration selects one attack to be removed from training and to be the only one used for testing. This is to evaluate the capability of the model to generalize to novel attacks and to evaluate the challenges that each attack present to the network. The results for this configuration are seen in <ref type="table">Table 7</ref>, and it is possible to observe that both DFS and adversarial approaches have difficulties in generalizing to unseen attacks. Especially attack #4. The low performance of this attack can be explained by the high resolution of the replay attack device that increases the difficulty of the task. The multi-task approach excelled at detecting both attacks and bonafide samples, achieving an equal-error rate better than any other approach. And thus, it was the approach used to compare with the state-of-the-art for the ROSE-YOUTU dataset. he methods compared report their results on similar train/test split, as specified on the database publication document <ref type="bibr" target="#b19">[20]</ref>. Our results, as seen in <ref type="table" target="#tab_6">Table 8</ref> are better than the state-of-the-art when we include background and slightly better when there is no background. Despite the good results, it is important to note that the methods presented in this document were not designed to be the best performing methods at cross-dataset configuration. Instead, they are intended to allow a relevant study regarding the presence of background for this specific dataset.</p><p>In <ref type="figure">Figure 5</ref> is depicted the ROC curve of the MT model with the x-axis displayed at the log-scale for a better visualization. The model is indeed nearly perfect at detecting the attacks and the bona-fide images in the ROSE-Youtu dataset.</p><p>Finally, we produced explanations of our model for an example of each category of attacks. For the replay attack, we produced the explanations in <ref type="figure">Figures 6a and 6d</ref>. In these figures, it is possible to observe that the models leveraged the presence of reflections in the attack image, whenever there is background. When the background is not present there are no cues to justify the decision of the model, which  <ref type="bibr" target="#b19">[20]</ref> 16.4 Color <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> 13.9 De Spoofing <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref> 12.3 RCTR-all spaces <ref type="bibr" target="#b6">[7]</ref> 10.7 ResNet-18 <ref type="bibr" target="#b8">[9]</ref> 9.3 SE-ResNet18 <ref type="bibr" target="#b9">[10]</ref> 8.6 AlexNet <ref type="bibr" target="#b19">[20]</ref> 8.0 DR-UDA (SE-ResNet18) <ref type="bibr" target="#b28">[29]</ref> 8.0 DR-UDA (ResNet-18) <ref type="bibr" target="#b28">[29]</ref> 7.2 3D-CNN <ref type="bibr" target="#b18">[19]</ref> 7.0 Blink-CNN <ref type="bibr" target="#b7">[8]</ref> 4.6 DRL-FAS <ref type="bibr" target="#b2">[3]</ref> 1.8</p><p>Ours w/ Background 0.2 is in fact wrong. <ref type="figure">Figures 6b and 6e</ref> shows the explanations for a paper mask attack, and as expected, the explanations do not rely on the background. Instead, the model directs its focus to the mask area for the final prediction. The area is similar on both versions of the model with and without background. Finally, the print attack explanations are seen in <ref type="figure">Figures 6c and 6f</ref>. These figures shows that once more the model is capable of understanding the conditions of the image given and directs its focus to an important background artefact, the pin holding the image. And again, the version without background does not highlight any relevant cue that explains the prediction of the model. Hence, we further argue in favor of a better explanability factor in the models that include background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This work explored how consistently the background impacts the performance of distinct methods for face presentation attack detection. The experiments corroborated the view that a face PAD model is capable of leveraging both background and face elements to make a correct prediction.</p><p>Our approach surpassed the state-of-the-art results for the ROSE-YOUTU dataset by a large margin. The multitask model leverages background artefacts to improve the detection of specific attacks. Moreover, we also present some alternative approaches, dynamic frame selection and adversarial training, that we believe were limited by the lack of a large database of face presentation attacks. Their results were consistent with the one from the multi-task model.</p><p>We further contribute to improve the explainability of these models by analyzing the predictions. This analyze conducted with the Grad-CAM++ algorithm highlighted that models that include the background of the images can leverage the presence of certain artifacts. On the other hand, when the background is not present the generated explanations seem to be non-informative. Hence, due to their similarity with the human vision with regards to the areas used for the prediction, models that leverage the background provide more explanations for their predictions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of the adversarial learning model. It receives an input image (yellow box) and includes a CNN (blue figure), two feature vectors e1 and e2 (green boxes). These are used to reconstruct each other, decode (purple figure) the input and to classify the input. This architecture is deeply based on the Unsupervised Adversarial Invariance [13].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of the method for dynamic frame selection. It includes a CNN (blue figure) with shared weights that processes all the frames (yellow boxes) of a video and selects one (lower yellow box) based on a specific criteria.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Attack #4 (g) Attack #1 (h) Attack #6 (i) Genuine (j) Genuine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Samples collected from the ROSE-YOUTU dataset<ref type="bibr" target="#b19">[20]</ref> containing images from attacks and genuine captures. On the top row, cropped images are displayed. Whereas the bottom row contains the exact same images, but with all the background information included.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Receiver operating characteristic curve for the multi-task model on the ROSE-Youtu dataset with background. X-axis is at log-scale. Explanations generated with Grad-CAM++ for different attacks of subject #23. -B indicates that the model was trained and tested on images with background. The (d) image was wrongly classified as genuine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>List of attacks present in the ROSE-YOUTU dataset<ref type="bibr" target="#b19">[20]</ref>.</figDesc><table><row><cell>Attack</cell><cell>Description</cell></row><row><cell>-</cell><cell>Genuine (bona fide)</cell></row><row><cell>#1</cell><cell>Still printed paper</cell></row><row><cell>#2</cell><cell>Quivering printed paper</cell></row><row><cell>#3</cell><cell>Video which records a Lenovo LCD display</cell></row><row><cell>#4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of four different approaches with their versions with and without background. The columns represent the dataset used for both training and testing. The reported values represent the EER in %.</figDesc><table><row><cell>Method</cell><cell cols="3">Background NUAA Replay-Attack</cell></row><row><cell>BC</cell><cell>No Yes</cell><cell>2.91 0.00</cell><cell>0.00 0.00</cell></row><row><cell>Adv.+BC</cell><cell>No Yes</cell><cell>3.03 0.00</cell><cell>0.33 0.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of four different approaches with their versions with and without background on the ROSE-Youtu dataset. APCER, BPCER and EER are displayed as %. In bold is the best result per column.</figDesc><table><row><cell>Method</cell><cell cols="4">Background APCER BPCER EER</cell></row><row><cell>BC</cell><cell>No Yes</cell><cell>0.49 0.25</cell><cell>2.20 2.03</cell><cell>1.32 0.73</cell></row><row><cell>MT</cell><cell>No Yes</cell><cell>1.34 0.15</cell><cell>1.17 0.40</cell><cell>1.26 0.24</cell></row><row><cell>Adv.+BC</cell><cell>No Yes</cell><cell>1.42 0.52</cell><cell>2.71 1.29</cell><cell>1.76 0.76</cell></row><row><cell>Adv.+MT</cell><cell>No Yes</cell><cell>1.18 0.29</cell><cell>2.93 1.11</cell><cell>1.91 0.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison of four different approaches with their versions with and without background. Results for models trained on the ROSE-Youtu dataset and tested on the datasets of each column. The reported values represent the EER in %.</figDesc><table><row><cell>Method</cell><cell cols="3">Background NUAA Replay-Attack</cell></row><row><cell>BC</cell><cell>No Yes</cell><cell>22.04 13.45</cell><cell>29.43 12.29</cell></row><row><cell>MT</cell><cell>No Yes</cell><cell>23.61 3.89</cell><cell>26.13 13.91</cell></row><row><cell>Adv.+BC</cell><cell>No Yes</cell><cell>28.31 18.11</cell><cell>26.03 17.12</cell></row><row><cell>Adv.+MT</cell><cell>No Yes</cell><cell>35.66 23.85</cell><cell>26.15 19.46</cell></row><row><cell cols="4">Table 5. Comparison of all the seven different approaches explored</cell></row><row><cell cols="4">in the ROSE-Youtu dataset. All of the approaches leveraged back-</cell></row><row><cell cols="4">ground information. In bold is the best result per column.</cell></row><row><cell>Method</cell><cell cols="3">APCER (%) BPCER (%) EER (%)</cell></row><row><cell>BC</cell><cell>0.25</cell><cell>2.03</cell><cell>0.73</cell></row><row><cell>MT</cell><cell>0.15</cell><cell>0.40</cell><cell>0.24</cell></row><row><cell>Adv. + BC</cell><cell>0.52</cell><cell>1.29</cell><cell>0.76</cell></row><row><cell>Adv. + MT</cell><cell>0.29</cell><cell>1.11</cell><cell>0.60</cell></row><row><cell>DFS</cell><cell>0.54</cell><cell>4.68</cell><cell>1.62</cell></row><row><cell>MT + DFS</cell><cell>0.31</cell><cell>2.23</cell><cell>0.69</cell></row><row><cell>Adv. + DFS</cell><cell>2.15</cell><cell>1.78</cell><cell>1.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Evaluation of three approaches in the setting of one attack in the ROSE-Youtu dataset. In this setting, the attack in the first column is the only one used for training and testing. APCER, BPCER and EER are displayed as %. In bold is the best result per column. Evaluation of three approaches in the setting of unseen attack in the ROSE-Youtu dataset. In this setting, the attack in the first column is excluded from the training and is the only one used for testing. APCER, BPCER and EER are displayed as %. In bold is the best result per column.</figDesc><table><row><cell>Attack</cell><cell></cell><cell>MT</cell><cell></cell><cell cols="2">Adversarial MT</cell><cell></cell><cell></cell><cell>DFS MT</cell><cell></cell></row><row><cell></cell><cell cols="9">APCER BPCER EER APCER BPCER EER APCER BPCER EER</cell></row><row><cell>#1</cell><cell>0.00</cell><cell>0.20</cell><cell>0.05</cell><cell>0.20</cell><cell>0.29</cell><cell>0.27</cell><cell>0.50</cell><cell>0.22</cell><cell>0.50</cell></row><row><cell>#2</cell><cell>0.00</cell><cell>0.02</cell><cell>0.02</cell><cell>0.00</cell><cell>0.07</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>#3</cell><cell>0.00</cell><cell>0.16</cell><cell>0.11</cell><cell>0.00</cell><cell>0.29</cell><cell>0.25</cell><cell>0.00</cell><cell>0.45</cell><cell>0.00</cell></row><row><cell>#4</cell><cell>0.30</cell><cell>1.27</cell><cell>0.71</cell><cell>1.16</cell><cell>1.98</cell><cell>1.46</cell><cell>0.50</cell><cell>1.34</cell><cell>1.01</cell></row><row><cell>#5</cell><cell>0.00</cell><cell>0.02</cell><cell>0.00</cell><cell>0.00</cell><cell>0.09</cell><cell>0.05</cell><cell>0.00</cell><cell>0.45</cell><cell>0.00</cell></row><row><cell>#6</cell><cell>0.00</cell><cell>0.07</cell><cell>0.00</cell><cell>0.00</cell><cell>0.16</cell><cell>0.11</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>#7</cell><cell>0.00</cell><cell>0.05</cell><cell>0.00</cell><cell>0.00</cell><cell>0.11</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>Attack</cell><cell></cell><cell>MT</cell><cell></cell><cell cols="2">Adversarial MT</cell><cell></cell><cell></cell><cell>DFS MT</cell><cell></cell></row><row><cell></cell><cell cols="9">APCER BPCER EER APCER BPCER EER APCER BPCER EER</cell></row><row><cell>#1</cell><cell>1.00</cell><cell>0.85</cell><cell>0.95</cell><cell>1.00</cell><cell>6.90</cell><cell>2.87</cell><cell>1.00</cell><cell>6.90</cell><cell>2.30</cell></row><row><cell>#2</cell><cell>0.00</cell><cell>0.49</cell><cell>0.25</cell><cell>0.00</cell><cell>1.18</cell><cell>0.27</cell><cell>0.00</cell><cell>4.90</cell><cell>0.67</cell></row><row><cell>#3</cell><cell>3.09</cell><cell>3.41</cell><cell>3.27</cell><cell>1.44</cell><cell>3.07</cell><cell>2.61</cell><cell>2.14</cell><cell>10.24</cell><cell>5.79</cell></row><row><cell>#4</cell><cell>13.82</cell><cell>3.88</cell><cell>7.57</cell><cell>12.66</cell><cell>6.61</cell><cell>9.15</cell><cell>17.59</cell><cell>16.70</cell><cell>17.09</cell></row><row><cell>#5</cell><cell>0.00</cell><cell>1.98</cell><cell>0.65</cell><cell>0.25</cell><cell>0.58</cell><cell>0.45</cell><cell>0.50</cell><cell>4.90</cell><cell>1.49</cell></row><row><cell>#6</cell><cell>0.00</cell><cell>0.89</cell><cell>0.33</cell><cell>0.10</cell><cell>0.58</cell><cell>0.10</cell><cell>0.00</cell><cell>4.01</cell><cell>1.00</cell></row><row><cell>#7</cell><cell>0.35</cell><cell>3.63</cell><cell>1.77</cell><cell>2.32</cell><cell>7.68</cell><cell>5.20</cell><cell>0.51</cell><cell>9.58</cell><cell>1.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Comparison of the best proposed approaches, both with and without background, with the state-of-the-art. In bold is the best result per column.</figDesc><table><row><cell>Method</cell><cell>EER (%)</cell></row><row><cell>CoALBP (YCBCR) [20]</cell><cell>17.1</cell></row><row><cell>CoALBP (HSV)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to acknowledge the reviewers' comments that were crucial for the improvement of this work. This work was financed by National Funds through the Portuguese funding agency, FCT -Funda??o para a Ci?ncia e a Tecnologia within project UIDB/50014/2020, and within the PhD grant "2021.06872.BD".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest</head><p>The authors declare that there is no conflict of interest that could be perceived as prejudicing the impartiality of the research reported.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anomaly detection-based unknown face presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashasvi</forename><surname>Baweja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poojan</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Joint Conference on Biometrics (IJCB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face spoofing detection using colour texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinelabidine</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1818" to="1830" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Drl-fas: A novel framework based on deep reinforcement learning for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rizhao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="937" to="951" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Chattopadhay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prantik</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the effectiveness of local binary patterns in face antispoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Chingovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 BIOSIG-proceedings of the international conference of biometrics special interest group (BIOSIG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multilevel face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards Face Presentation Attack Detection Based on Residual Color Texture Representation. Security and Communication Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient two stage approach to detect face liveness : Motion based and deep learning based</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanbin</forename><surname>Salah Uddin Yusuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shidhartho</forename><surname>Islam Rohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 4th International Conference on Electrical Information and Communication Technology (EICT)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The opacity of artificial intelligence makes it hard to tell when decision-making is biased</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hutson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="40" to="45" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">ISO/IEC JTC1 SC37. Information Technology -Biometrics -Presentation attack detection Part 3: Testing and Reporting. ISO Int. Organization for Standardization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised Adversarial Invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><forename type="middle">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Abd-Almageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ropad: Robust presentation attack detection through unsupervised adversarial invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Ab-Dalmageed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Biometrics (ICB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Face despoofing: Anti-spoofing via noise modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="290" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shuffled patch-wise supervision for presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alperen</forename><surname>Kantarc?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasan</forename><surname>Dertli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haz?m Kemal</forename><surname>Ekenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference of the Biometrics Special Interest Group (BIOSIG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face spoofing and counter-spoofing: a survey of state-of-the-art algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rinku</forename><surname>Dakshina Ranjan Kisku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Datta Rakshit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context based face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning generalized deep feature representation for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2639" to="2652" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1794" to="1809" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sharp multiple instance learning for deepfake video detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1864" to="1872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ppgsecure: Biometric presentation attack detection using photopletysmograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewa</forename><forename type="middle">Magdalena</forename><surname>Nowara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Veeraraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="56" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An exploratory study of interpretability for face presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">F</forename><surname>Sequeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o Ribeiro</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpretable biometrics: Should we rethink how presentation attack detection is evaluated</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Sequeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 8th International Workshop on Biometrics and Forensics (IWBF)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face liveness detection from a single image with sparse low rank bilinear discriminative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="504" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face spoofing detection via ensemble of classifiers toward low-power devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Robson</forename><surname>Vareto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="521" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR</title>
		<meeting>the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised adversarial domain adaptation for crossdomain face presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="56" to="69" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5601</idno>
		<title level="m">Learn convolutional neural network for face anti-spoofing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Face liveness detection with component dependent descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 International Conference on Biometrics (ICB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
