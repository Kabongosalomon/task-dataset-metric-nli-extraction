<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comprehensive Study on Torchvision Pre-trained Models for Fine-grained Inter-species Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feras</forename><surname>Albardi</surname></persName>
							<email>feras.rb@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">King AbdulAziz University</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Dipu Kabir</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IISRI</orgName>
								<orgName type="institution" key="instit2">Deakin University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Mahbub</forename><forename type="middle">Islam</forename><surname>Bhuiyan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Smartify Pty Ltd</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parham</forename><forename type="middle">M</forename><surname>Kebria</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IISRI</orgName>
								<orgName type="institution" key="instit2">Deakin University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Khosravi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IISRI</orgName>
								<orgName type="institution" key="instit2">Deakin University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Nahavandi</surname></persName>
							<email>saeid.nahavandi@deakin.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IISRI</orgName>
								<orgName type="institution" key="instit2">Deakin University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution" key="instit1">Harvard Paulson</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<postCode>02134</postCode>
									<settlement>Allston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comprehensive Study on Torchvision Pre-trained Models for Fine-grained Inter-species Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Learning</term>
					<term>Transfer Learning</term>
					<term>PyTorch</term>
					<term>Torchvision</term>
					<term>Fine-grained</term>
					<term>Inter-species</term>
					<term>Image Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study aims to explore different pre-trained models offered in the Torchvision package which is available in the PyTorch library. And investigate their effectiveness on finegrained images classification. Transfer Learning is an effective method of achieving extremely good performance with insufficient training data. In many real-world situations, people cannot collect sufficient data required to train a deep neural network model efficiently. Transfer Learning models are pre-trained on a large data set, and can bring a good performance on smaller datasets with significantly lower training time. Torchvision package offers us many models to apply the Transfer Learning on smaller datasets. Therefore, researchers may need a guideline for the selection of a good model. We investigate Torchvision pretrained models on four different data sets: 10 Monkey Species, 225 Bird Species, Fruits 360, and Oxford 102 Flowers. These data sets have images of different resolutions, class numbers, and different achievable accuracies. We also apply their usual fullyconnected layer and the Spinal fully-connected layer to investigate the effectiveness of SpinalNet. The Spinal fully-connected layer brings better performance in most situations. We apply the same augmentation for different models for the same data set for a fair comparison. This paper may help future Computer Vision researchers in choosing a proper Transfer Learning model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>There exist various Machine Learning (ML) libraries that made it easier to develop and implement Neural Network (NN) models <ref type="bibr" target="#b1">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref>. One of these libraries is Pytorch <ref type="bibr" target="#b3">[3]</ref> which is an open-source framework used for the research, development, and deployment of ML systems. This library includes various packages to process data and to configure models according to requirements. One of these packages is Torchvision which consists of various data sets, pre-trained models, and image processing tools and techniques. Torchvision includes many pre-trained models for Transfer Learning (TL). TL is becoming more popular as it enables for the development of high-performance Neural Network (NN) models with low data volume.</p><p>Transfer Learning is an efficient technique of using previously acquired knowledge and skills in novel problems. It is also similar to educating humans with a much broader syllabus to achieve competencies for an unpredictable future. Deep Neural Networks (DNNs) requires adequate training samples for proper training. Insufficient training samples may result in poor performance <ref type="bibr" target="#b4">[4]</ref>- <ref type="bibr" target="#b6">[6]</ref>. TL is an efficient DNN training technique where initial layers of DNN are pre-trained with a large data set <ref type="bibr" target="#b7">[7]</ref>. The corresponding train data set trains only a few final layers. As a result, the user can get a well-trained NN for the specific data set of a small sample number, with lower computational overhead. TL is gaining huge popularity these days due to exceptional performance. Many researchers expect TL as the next driver of the commercial success of Machine Learning.</p><p>There are many pre-trained models in the Torchvision package. No model is good for all problems <ref type="bibr" target="#b8">[8]</ref>. A user may not choose a model by considering its performance on the ImageNet <ref type="bibr" target="#b9">[9]</ref> data set. Moreover, the user may have some limitation of resources and he/ she may choose a simpler lowperformance model instead of a computationally demanding one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PYTORCH PRE-TRAINED MODELS</head><p>In Torchvision, there are a total of twelve main pre-trained models that are to be used for the classification of images from the fine-grained inter-species data sets. And these are AlexNet <ref type="bibr" target="#b10">[10]</ref>, VGG <ref type="bibr" target="#b11">[11]</ref>, ResNet <ref type="bibr" target="#b12">[12]</ref>, SqueezeNet <ref type="bibr" target="#b13">[13]</ref>, DenseNet <ref type="bibr" target="#b14">[14]</ref>, Inception v3 <ref type="bibr" target="#b15">[15]</ref>, GoogLeNet <ref type="bibr" target="#b16">[16]</ref>, ShuffleNet v2 <ref type="bibr" target="#b17">[17]</ref>, MobileNet v2 <ref type="bibr" target="#b18">[18]</ref>, ResNeXt <ref type="bibr" target="#b19">[19]</ref>, Wide ResNet <ref type="bibr" target="#b20">[20]</ref> and MNASNet <ref type="bibr" target="#b21">[21]</ref>. Some models include multiple varieties such as ResNet and VGG <ref type="bibr" target="#b22">[22]</ref>- <ref type="bibr" target="#b24">[24]</ref>. In this section, we explore these different networks and define what differentiate each one from another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. AlexNet</head><p>Alex Krizhevsky introduced the novel convolutional neural network (CNN) named AlexNet in 2014. A year after competing in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" target="#b25">[25]</ref> in 2012. This challenge requires the classification of a subset of ImageNet with over one million images and a thousand classes. AlexNet performed very well as it was announced the winner of the best performance for that year with top-5 error of 15.3%, 10.8% lower than the previous top performer. AlexNet is considered to be the first CNN to be announced the winner for the competition.</p><p>AlexNet architecture consists of a total of 8 layers, 5 convolutional layers, and 3 max-pooling layers. This architecture had many characteristics that made it perform very well, one is the utilization of ReLU (Rectified Linear Unit) <ref type="bibr" target="#b26">[26]</ref> activation function within the convolutional layers in replacement of the popular Tanh at the time. ReLU offered better performance and lower computation time. AlexNet benefited as well from the training on multiple graphic processors which allowed to distribute work on different GPUs enabling for the training of a bigger model and lowering the computation time. AlexNet suffered from overfitting which is a general issue that many models suffer from. Overfitting occurs when the model is fitted too well to the data and is not able to generalize well to new data which means that the model do not classify new images very good. To solve this issue, AlexNet utilized some common solutions to the problem, data augmentation, and dropout layers <ref type="bibr" target="#b27">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. VGG</head><p>VGG (Visual Geometry Group) is a very popular CNN that was introduced in 2014 by Karen Simonyan and Andrew Zisserman from the University of Oxford. And similar to AlexNet, VGG participated in the ILSVRC challenge in 2014. VGG scored very well as it was announced the winner (2nd place) for that year with an accuracy of 97.7%. This architecture was able to surpass the performance of AlexNet as it had a deeper model design as well as replacing the large convolution kernels of AlexNet with multiple small convolution kernels of size 3x3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ResNet</head><p>Residual Neural Network abbreviated (ResNet) is a powerful network that was proposed in 2015 by Kaiming He et al. in their paper <ref type="bibr" target="#b12">[12]</ref>. ResNet made a very big impact on deep learning as its unique architectural design enabled it to go deeper with layers and provide great performance. It was not an easy matter to get good performance with very deep models as there has been the problem of vanishing gradient. Vanishing gradient is a major problem in machine learning, it occurs as more layers are added to the network. Each layer has a certain activation function, and when the gradient is back-propagated to the earlier layers multiple times this causes the gradient to approach to zero (vanish) which makes it very hard to update the weights with very small values, and thus, the model do not converge and train well. ResNet offered a solution to the problem, in their paper, the authors described the use of a unique neural network layer named the 'Residual Block'. These layers utilize 'skip connections' which are specific shortcuts to jump over some layers. These jumps usually skip over two or three layers, and these skip connections consists of a ReLU activation function and a batch normalization in between. The main intuition behind the utilization of this structure is to avoid vanishing gradients by the re-use of activation values from a previous layer to enable the neighboring layer to learn its weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. SqueezeNet</head><p>SqueezeNet has three main goals, first is to get more efficient in distributed training. Second is to export models smaller in size that are more convenient for professional and industrial clients. Lastly is to provide a model that is better deployed on embedded systems. SqueezeNet has two varieties, SqueezeNet v1.0 and SqueezeNet v1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. DenseNet</head><p>DenseNet was introduced in 2017 by Gao Huang et al. It is a powerful CNN that is similar to ResNet in using shortcut connections in between layers to solve the gradient descent problem. DenseNet utilizes 'Dense Connections' in between layers with the use of 'Dense Blocks' which holds an n number of 'Dense Layers'. Each Dense Layer consists of a 1x1 convolution filter for feature extraction and a 3x3 convolution filter to decrease the number of channels. And in Dense Blocks, each layer receives feature maps from all previous layers, and then it passes the output which is concatenated as input to all subsequent layers. This special structure allows for the use of fewer layers and the re-use of the features learned by the network, and having a narrower model (fewer layers) is easier to train with having fewer parameters to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. GoogLeNet And Inception v3</head><p>Inception <ref type="bibr" target="#b15">[15]</ref> [16] <ref type="bibr" target="#b28">[28]</ref> is a family of CNNs developed by Christian Szegedy et al. and Google with four different versions, Inception v1 which is the GoogLeNet, Inception v2, a refined version of Inception v1 with the introduction of batch normalization, Inception v3, a more refined version with additional convolution factorization, and Inception v4.</p><p>GoogLeNet (Inception v1) was the first member of the Inception family. This architecture was introduced in 2015 and prior to that the architecture participated in the ILSVRC challenge and did very well as it won 1st place and achieved a new State-of-the-Art for ImageNet in that year. GoogLeNet was proposed for the main problem of overfitting and the expensive computation of stacking too many layers. The main solution provided by the authors is to have convolution filters of various sizes (1x1, 3x3, and 5x5) at the same level. Having these different-size filters helps in extracting features of various details, big and small, and adding more filters make the network wider rather than deeper. GoogLeNet architecture consists of 'Inception Modules' that carries the essence of the Inception family, and it is what allows to have convolution filters of various sizes rather than one. The idea is to let the model decide on which filter to use rather than the manual selection of a filter that might not perform well. This works with the parallel use of different filters on the same input with the same padding, and then concatenating the feature map from each filter to create one-big feature map which to be passed as input to the next inception module.</p><p>Inception v3 was proposed in 2016. And similar to GoogLeNet it participated in the ILSVRC challenge in 2015 and scored 1st place for that year. Inception v3 holds a very similar structure to that of GoogLeNet, however, it does have some differences that enable it to generally perform better than GoogLeNet. Inception v3 architecture hold different changes, first is special convolution filter techniques (Factorized Convolutions, Smaller Convolutions, and Asymmetric Convolutions). second, a modified version of 'Auxiliary Classifiers'. Third is the use of 'Efficient Grid Size Reduction'. And lastly implementing 'Model Regularization via Label Smoothing. Factorized convolutions are utilized to reduce the number of parameters, whilst keeping the network efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. ShuffleNet v2</head><p>Ningning Ma et al. proposed ShuffleNet v2, a CNN based on the previous ShuffleNet v1 which aimed to build an efficient and effective network with lower computation needs than other networks. With ShuffleNet v2, the authors aimed to design a network that takes into consideration the direct metrics, such as speed and memory access cost, to measure the networks computational complexity rather than an indirect metric, such as FLOPs (Floating Point Operations). FLOPs are the default measure of the performance of a model, and specifically they are units to measure how many operations are needed to run a single instance of a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. MobileNet v2</head><p>MobileNet is a family of CNNs designed to work effectively and efficiently on mobile systems. MobileNet v2 was introduced in 2018 and it is based on the previous MobileNet v1 <ref type="bibr" target="#b29">[29]</ref> which was introduced in 2017. MobileNet v1 had two main layers forming the network, first is a 'Depth-wise Convolution' layer which applies a single convolution filter per input channel for lightweight filtering, and the other is a 'Point-wise Convolution' layer which consists of a 1x1 convolution filter aimed for computing linear combinations of input channels to extract new features, each of these two layers has non-linearity (ReLU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. ResNeXt</head><p>ResNeXt was proposed in a paper in 2017, a year after achieving 1st runner-up in the ILSVRC challenge. ResNeXt is based upon ResNet, and it differs by adding a 'NeXt' structure. This structure is a 'Cardinality' dimension which sticks on top of the width and depth of the ResNet architecture. This dimension refers to the size of the transformations set. ResNeXt is special as it replaces the use of regular linear functions in the neurons of the layers with using a non-linear function which is a part of a 'ResNeXt Block', the number of transformations in this block is referred to by the Cardinality, and after applying the required number of transformations, the results are aggregated together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Wide ResNet</head><p>Wide ResNet abbreviated (WRN) is a CNN proposed in 2016 by Sergey Zagoruyko and Nikos Komodakis. It is based on the concept of residual blocks introduced in the ResNet </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. MNASNet</head><p>MNASNet is a novel architecture proposed in 2018 by Mingxing Tan et al. Authors aimed to introduce a network that performs well with mobile devices as they require models that are smaller in size, yet perform effectively and efficiently. In their paper, the authors proposed three main ideas for their model, first is the introduction of a multi-objective architecture based on reinforcement learning which is used to automatically find the most suitable CNN for high performance. Second is the introduction of a novel 'Factorized Hierarchical Search Space' structure which is used as blocks in between layers and it is aimed for more efficient use of computational resources. lastly, the authors demonstrated the effectiveness of their model with regards to other architectures aimed for mobileuse, such as MobileNet, ShuffleNet, and SqueezeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>This study was performed in several systematic steps that generalizes over different data sets and models to ensure consistent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Phase 1. Data Collection and Preprocessing</head><p>There are four different sets of fine-grained Inter-species data. The first one, named 10 Monkey Species <ref type="bibr" target="#b30">[30]</ref>, is a set which includes a total of 1400 images of size 400x300 distributed among 10 different species of monkeys. These species include Alouattapalliata, Erythrocebuspatas, Cacajaocalvus, Macacafuscata, Cebuellapygmea, Cebuscapucinus, Micoargentatus, Saimirisciureus, Aotusnigriceps and Trachypithecusjohnii. The collection of these images was done with the help of search engine web scrapers. 10 Monkey Species is available on the popular data platform, Kaggle.</p><p>The second set named Fruits 360 <ref type="bibr" target="#b31">[31]</ref>. It includes data of different fruits and vegetables. This set is much bigger than 10 Money Species with more than 90,000 images of fruits and vegetables of size 100x100 and ranging from watermelons, apples and oranges to potatoes and onions and much more with 103 classes. 225 Bird Species <ref type="bibr" target="#b32">[32]</ref> is the third data set. It includes more than 30,000 training, testing, and validation images distributed among 225 classes of birds, all of these images are of size 224x224.</p><p>Oxford 102 <ref type="bibr" target="#b33">[33]</ref> is the fourth and last data set, and it consists of 102 classes of different flowers. This data set was created by the Visual Geometry Group of Oxford. The type of flowers available in the data set are commonly available in the United Kingdom. And each class holds between 40 and 258 images. Collection of images was done using web scrappers as well as manually capturing flower images.  One of the methods to increase the performance of models is to provide more training examples <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref>. this method can be accomplished with data augmentation where random examples are taken from the data set, and then apply different image processing methods on these examples, such as rotation, horizontal mirroring, and magnification. This allows for the addition of many examples to the training set. The main objective of utilizing image augmentation techniques is its proven effect on improving performance <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref>. Having a set with more images means that the model is faced with a bigger number of samples that can highly improve performance especially with low-volume data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Phase 2. Models Training</head><p>In this phase, we train each data set independently with 29 different pre-trained models available in the Torchvision    package. These models include AlexNet, VGG <ref type="bibr">(11, 13 16, 19,</ref>  <ref type="figure" target="#fig_1">BN-11, BN-13, BN-16, BN-19</ref>), ResNet <ref type="bibr" target="#b18">(18,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr">50,</ref><ref type="bibr">101,</ref><ref type="bibr">152)</ref>, SqueezeNet (1.0, 1.1), DenseNet (121, 169, 161, 201), Inception v3, GoogLeNet, ShuffleNet v2 (0.5, 1.0), MobileNet v2, ResNeXt (50, 101), Wide ResNet (50, 101) and MNASNet (1.0). For the training and development environment, we utilized Kaggle Kernels which are in essence Jupyter <ref type="bibr" target="#b38">[38]</ref> Notebooks as they offer ease of importing required datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Phase 3. Investigating Results</head><p>This phase includes the extraction of results and performance of each model with each data set and form it all together in tables in order to understand and compare how each model performs with fine-grained Inter-species data. We investigate the results to find the best possible configurations for these data with Torchvision pre-trained models, and to suggest new information that can possibly lead for the implementation of more performant models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Phase 4. Investigating SpinalNet</head><p>This last phase is actually a derivative of the third phase. Here we implement each data set with pre-trained models with a small configuration. This change is to replace the last fully connected layer in each model with the SpinalNet <ref type="bibr" target="#b39">[39]</ref> layer. SpinalNet is a novel network that is inspired from the human somatosensory system <ref type="bibr" target="#b40">[40]</ref>. This network consists of input row, intermediate row and output row where the intermediate row contains a small number of neurons, and each hidden layer receives a part of the input and outputs of the previous layer with input segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>Exploring the results of the first data set, 10 Monkey Species, it can be noticed that pre-trained models provided very good performance with regards to the low data volume. Training hyperparameters are a batch of 40 images (20 for DenseNet-169, DenseNet-161, DenseNet-201, and ResNeXt-101. This is due to memory limitations.), 10 epochs, SGD optimizer, and varying values of the learning rate which depends on the model in-hand. Most models provided above 90% accuracy, while others, such as SqueezeNet 1.0 ended with an accuracy of 65.80% (Normal FC) and 69.11% (Spinal FC). However, SqueezeNet 1.1 did provide a better accuracy with about 84.55% (Normal FC) and 82.35% (Spinal FC). It is critical to note that models which performed poorly, such as SqueezeNet and MNASNet, were able to perform much better when trained using the Adam <ref type="bibr" target="#b41">[41]</ref> optimizer instead of SGD, and that goes for both Normal and Spinal FC. However, it was decided to move forward with SGD results as to be consistent with all other models. And at the end of this section (in <ref type="figure">figure  9 and 10)</ref>, we illustrated the accuracy and loss over epochs for the Wide ResNet-101-2 model which was selected randomly to view performance with both Normal and Spinal FC layer.</p><p>In Fruits 360, training was performed on a batch size of 224 (112 for DenseNet-169, DenseNet-161, DenseNet-201, and ResNeXt-101. This is due to memory limitations.), and  SpinalNet is utilized to explore the performance of the novel network and compare it with the default network in each model. All models were trained the same. However, with SpinalNet we replace the default fully connected layer with the SpinalNet fully connected layer. SpinalNet provided very good performance as it has actually improved accuracies of most models over the default FC layer, an example would be with the 10 Monkey Species data set, MNASNet 1.0 provided an accuracy of 79.05% accuracy, and the same model with Spinal fully connected layer achieved an accuracy of 93.01%. And with the Oxford 102 Flower data set, VGG-13 provided an accuracy of 94.56%, and VGG-13 with Spinal fully connected layer achieved an accuracy of 95.01%. Although in a number of cases, the default FC layer did provide better performance. And overall, SpinalNet shows promising results in the classification of images, and it is possible to provide very good performance if integrated well with pre-trained models . At first, we aimed to train all models with a constant set of hyperparameters. However, it turned out that several models performed much better when adjusting a specific parameter, which is the learning rate, to different values than the default one. AlexNet, SqueezeNet, and others performed moderately when trained with the default learning rate value. And We found that using Adam instead of SGD helped drastically in improving performance. However, this contradicts our goal of utilizing a specific optimizer instead of utilizing different optimizers with different models which can potentially create inconsistency. And with several attempts, we found that adjusting the learning rate when using SGD helped greatly in providing similar, and in some cases better performance, than changing the optimizer to Adam. This improvement in adjusting the value of the learning rate with different models reflects to all data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>This study aimed to investigate the performance of various pre-trained models offered in the Torchvision package of Pytorch with fine-grained inter-species datasets. Each model has a unique architecture designed to perform best with visual data. We systematically fine-tuned each model with a configured set of hyperparameters to ensure consistency of results over different models. Models in the training of data sets with high volume of examples, such as the Fruits 360, provided great performance with most models reaching to 99% and above. This is also applicable with other data sets with a small number of examples, such as 10 Monkey Species, most models were still able to achieve very good results. We explored the novel SpinalNet architecture in efforts to confirm the improvement of pre-trained models performance. In many models, SpinalNet did perform very well reaching in some models to an accuracy of 100% in only 10 epochs for the Fruits 360 data set. And in other models, SpinalNet did perform well and was able to improve the accuracy. However, in many models, the normal fully connected layer performed similar or better. SpinalNet shows very promising results of improving the performance of pre-trained models. There are no specific models that showed best performance with the novel network. It may depend on the number of examples in the data set to provide better performance. SpinalNet provided better accuracies with data sets consisting of a large volume of examples.</p><p>ACKNOWLEDGMENT I (Feras) would like to thank my brother Faris for his continuous support and encouragement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>A batch of 32 samples from the Fruits 360 data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>A batch of 32 samples from the 225 Bird Species data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>A batch of 32 samples from the Oxford 102 flower data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>A batch of 32 augmented samples from the 10 Monkey Species data set. A batch of 32 augmented samples from the Fruits 360 data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>A batch of 32 augmented samples from the 225 Bird Species data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>A batch of 32 augmented samples from the Oxford 102 flower data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I NUMBER</head><label>I</label><figDesc>OF TRAINING , VALIDATION AND TEST EXAMPLES IN EACH DATASET, AND THE SIZE OF IMAGES. However, it aims to be shallower with fewer layers and less training time. Wide ResNet was proposed in efforts to solve certain limitations with the ResNet architecture. ResNet has an advantage in improving performance with a large number of layers, however, adding more layers causes the reuse of diminishing feature, this affects the network by making it slow to train. The authors of WRN proposed a wider architecture (has a wider ResNet block) and lower in-depth as well as having dropout layers in between layers of residual</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>No. of Images</cell><cell></cell><cell>Image</cell></row><row><cell></cell><cell cols="2">Training Validation</cell><cell>Test</cell><cell>Size</cell></row><row><cell>10 Monkey Species</cell><cell>1370</cell><cell>272</cell><cell>-</cell><cell>400 ? 300</cell></row><row><cell>Fruits 360</cell><cell>67692</cell><cell>22688</cell><cell>-</cell><cell>100 ? 100</cell></row><row><cell>225 Bird Species</cell><cell>31316</cell><cell>1125</cell><cell cols="2">1125 224 ? 224</cell></row><row><cell>Oxford 102 Flower</cell><cell>1020</cell><cell>1020</cell><cell>6149</cell><cell>Varying</cell></row><row><cell>architecture.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>blocks. Wider and lower in-depth means increasing the number of channel dimensions, and decreasing the number of layers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II NUMBER</head><label>II</label><figDesc>OF LAYERS AND PARAMETERS IN TORCHVISION PRE-TRAINED MODELS</figDesc><table><row><cell>Model</cell><cell>No. of</cell><cell>No. of</cell><cell>Year of</cell><cell>Top-1 error</cell></row><row><cell></cell><cell>Layers</cell><cell cols="2">Parameters Proposal</cell><cell>on Imagenet</cell></row><row><cell>AlexNet</cell><cell>8</cell><cell>61M</cell><cell>2014</cell><cell>43.45%</cell></row><row><cell>VGG-11</cell><cell>11</cell><cell>132M</cell><cell>2014</cell><cell>30.98%</cell></row><row><cell>VGG-13</cell><cell>13</cell><cell>133M</cell><cell>2014</cell><cell>30.07%</cell></row><row><cell>VGG-16</cell><cell>16</cell><cell>138M</cell><cell>2014</cell><cell>28.41%</cell></row><row><cell>VGG-19</cell><cell>19</cell><cell>143M</cell><cell>2014</cell><cell>27.62%</cell></row><row><cell>VGG-11 BN</cell><cell>11</cell><cell>132M</cell><cell>2014</cell><cell>29.62%</cell></row><row><cell>VGG-13 BN</cell><cell>13</cell><cell>133M</cell><cell>2014</cell><cell>28.45%</cell></row><row><cell>VGG-16 BN</cell><cell>16</cell><cell>138M</cell><cell>2014</cell><cell>26.63%</cell></row><row><cell>VGG-19 BN</cell><cell>19</cell><cell>143M</cell><cell>2014</cell><cell>25.76%</cell></row><row><cell>ResNet-18</cell><cell>18</cell><cell>11M</cell><cell>2015</cell><cell>30.24%</cell></row><row><cell>ResNet-34</cell><cell>34</cell><cell>21M</cell><cell>2015</cell><cell>26.70%</cell></row><row><cell>ResNet-50</cell><cell>50</cell><cell>25M</cell><cell>2015</cell><cell>23.85%</cell></row><row><cell>ResNet-101</cell><cell>101</cell><cell>44M</cell><cell>2015</cell><cell>22.63%</cell></row><row><cell>ResNet-152</cell><cell>152</cell><cell>50M</cell><cell>2015</cell><cell>21.69%</cell></row><row><cell>SqueezeNet 1.0</cell><cell>-</cell><cell>1M</cell><cell>2017</cell><cell>41.90%</cell></row><row><cell>SqueezeNet 1.1</cell><cell>-</cell><cell>1M</cell><cell>2017</cell><cell>41.81%</cell></row><row><cell>DenseNet-121</cell><cell>121</cell><cell>7M</cell><cell>2016</cell><cell>25.35&amp;</cell></row><row><cell>DenseNet-169</cell><cell>169</cell><cell>14M</cell><cell>2016</cell><cell>24.00%</cell></row><row><cell>DenseNet-161</cell><cell>161</cell><cell>28M</cell><cell>2016</cell><cell>22.35%</cell></row><row><cell>DenseNet-201</cell><cell>201</cell><cell>20M</cell><cell>2016</cell><cell>22.80%</cell></row><row><cell>Inception v3</cell><cell>48</cell><cell>27M</cell><cell>2015</cell><cell>22.55%</cell></row><row><cell>GoogLeNet</cell><cell>22</cell><cell>6M</cell><cell>2014</cell><cell>30.22%</cell></row><row><cell>ShuffleNet v2</cell><cell>-</cell><cell>2M</cell><cell>2018</cell><cell>30.64%</cell></row><row><cell>MobileNet v2</cell><cell>53</cell><cell>4M</cell><cell>2018</cell><cell>28.12%</cell></row><row><cell>ResNeXt-50</cell><cell>50</cell><cell>25M</cell><cell>2016</cell><cell>22.38%</cell></row><row><cell>ResNeXt-101</cell><cell>101</cell><cell>88M</cell><cell>2016</cell><cell>20.69%</cell></row><row><cell>Wide ResNet-50-2</cell><cell>50</cell><cell>68M</cell><cell>2016</cell><cell>21.49%</cell></row><row><cell>Wide ResNet-101-2</cell><cell>101</cell><cell>126M</cell><cell>2016</cell><cell>21.16%</cell></row><row><cell>MNASNet 1.0</cell><cell>-</cell><cell>4M</cell><cell>2018</cell><cell>26.49%</cell></row></table><note>Fig. 1. A batch of 32 samples from the 10 Monkey Species data set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>OF PRE-TRAINED MODELS WITH THE 10 MONKEY SPECIES DATA SET WITH BOTH DEFAULT AND SPINALNET FC</figDesc><table><row><cell>Model</cell><cell>Batch</cell><cell>Accuracy</cell><cell>Accuracy</cell></row><row><cell></cell><cell>Size</cell><cell cols="2">(Normal FC) (Spinal FC)</cell></row><row><cell>AlexNet</cell><cell>40</cell><cell>90.80%</cell><cell>90.44%</cell></row><row><cell>VGG-11</cell><cell>40</cell><cell>96.69%</cell><cell>96.69%</cell></row><row><cell>VGG-13</cell><cell>40</cell><cell>97.43%</cell><cell>97.43%</cell></row><row><cell>VGG-16</cell><cell>40</cell><cell>96.69%</cell><cell>97.43%</cell></row><row><cell>VGG-19</cell><cell>40</cell><cell>97.43%</cell><cell>97.79%</cell></row><row><cell>VGG-11 BN</cell><cell>40</cell><cell>95.59%</cell><cell>97.43%</cell></row><row><cell>VGG-13 BN</cell><cell>40</cell><cell>98.16%</cell><cell>98.53%</cell></row><row><cell>VGG-16 BN</cell><cell>40</cell><cell>98.53%</cell><cell>98.53%</cell></row><row><cell>VGG-19 BN</cell><cell>40</cell><cell>98.90%</cell><cell>98.90%</cell></row><row><cell>ResNet-18</cell><cell>40</cell><cell>94.85%</cell><cell>98.16%</cell></row><row><cell>ResNet-34</cell><cell>40</cell><cell>97.05%</cell><cell>98.89%</cell></row><row><cell>ResNet-50</cell><cell>40</cell><cell>97.05%</cell><cell>98.52%</cell></row><row><cell>ResNet-101</cell><cell>40</cell><cell>96.32%</cell><cell>99.26%</cell></row><row><cell>ResNet-152</cell><cell>40</cell><cell>96.32%</cell><cell>98.16%</cell></row><row><cell>SqueezeNet 1.0</cell><cell>40</cell><cell>65.80%</cell><cell>69.11%</cell></row><row><cell>SqueezeNet 1.1</cell><cell>40</cell><cell>84.55%</cell><cell>82.35%</cell></row><row><cell>DenseNet-121</cell><cell>40</cell><cell>95.95%</cell><cell>95.95%</cell></row><row><cell>DenseNet-169</cell><cell>20</cell><cell>95.58%</cell><cell>97.79%</cell></row><row><cell>DenseNet-161</cell><cell>20</cell><cell>93.75%</cell><cell>92.64%</cell></row><row><cell>DenseNet-201</cell><cell>20</cell><cell>96.69%</cell><cell>97.05%</cell></row><row><cell>Inception v3</cell><cell>40</cell><cell>97.79%</cell><cell>99.26%</cell></row><row><cell>GoogLeNet</cell><cell>40</cell><cell>97.05%</cell><cell>98.16%</cell></row><row><cell>ShuffleNet v2 0.5</cell><cell>40</cell><cell>91.91%</cell><cell>93.01%</cell></row><row><cell>ShuffleNet v2 1.0</cell><cell>40</cell><cell>96.32%</cell><cell>95.95%</cell></row><row><cell>MobileNet v2</cell><cell>40</cell><cell>94.85%</cell><cell>94.11%</cell></row><row><cell>ResNeXt-50</cell><cell>40</cell><cell>98.16%</cell><cell>98.89%</cell></row><row><cell>ResNeXt-101</cell><cell>20</cell><cell>97.79%</cell><cell>98.52%</cell></row><row><cell>Wide ResNet-50-2</cell><cell>40</cell><cell>98.16%</cell><cell>98.53%</cell></row><row><cell>Wide ResNet-101-2</cell><cell>40</cell><cell>98.53%</cell><cell>99.26%</cell></row><row><cell>MNASNet 1.0</cell><cell>40</cell><cell>79.05%</cell><cell>93.01%</cell></row><row><cell cols="4">a default selection of 10 epochs, SGD optimizer, and varying</cell></row><row><cell cols="4">values of the learning rate in accordance with different models.</cell></row><row><cell cols="4">Almost all Models provided great performance reaching to an</cell></row><row><cell cols="4">accuracy of above 99% with only 10 epochs. This is mostly</cell></row><row><cell cols="4">due to the fact that this data set has a high number of examples.</cell></row><row><cell cols="4">The only model which performed poorly is MNASNet with</cell></row><row><cell cols="4">an accuracy of 58.51% (Normal FC) and 61.17% (Spinal</cell></row><row><cell cols="4">FC). And similar to the previous data set, it was found that</cell></row><row><cell cols="4">MNASNet is able to provide great performance similar to that</cell></row><row><cell cols="4">of other models in the list when using the Adam optimizer.</cell></row><row><cell cols="4">However, and for consistency we moved forward with SGD</cell></row><row><cell cols="4">results. And it is worth noting that the best performance was</cell></row><row><cell cols="4">provided by the DenseNet169 model with the SpinalNet fully</cell></row><row><cell cols="3">connected layer with an accuracy of 100.0%.</cell><cell></cell></row><row><cell cols="4">225 Bird Species provided very good performance similar</cell></row><row><cell cols="4">to that of Fruits 360. With configured hyperparameters of</cell></row><row><cell cols="4">a batch size of 64 (32 for DenseNet-169, DenseNet-161,</cell></row><row><cell cols="4">DenseNet-201, and ResNeXt-101. This is due to memory</cell></row><row><cell cols="4">limitations.), 10 epochs, SGD optimizer, and varying values of</cell></row><row><cell cols="4">the learning rate which depends on the model in-hand. Most</cell></row><row><cell cols="4">models performed an accuracy of above 90%, such as VGG-</cell></row><row><cell cols="4">16 99.11% (Normal FC) and 99.20% (Spinal FC) as well</cell></row><row><cell cols="4">as the best performer the Wide ResNet-101-2 with 99.38%</cell></row><row><cell cols="4">(Normal FC) and 99.56% (Spinal FC). Lowest performance</cell></row><row><cell cols="4">was achieved with SqueezeNet 1.0 and SqueezeNet 1.1 with</cell></row><row><cell cols="4">an accuracy of 82.95% (Normal FC), 84.61% (Spinal FC) and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>OF PRE-TRAINED MODELS WITH THE FRUITS 360 DATA SET WITH BOTH DEFAULT AND SPINALNET FC 99.08% (Normal FC), 84.17% (Spinal FC) respectively.Last data set in this study is the Oxford 102 Flower data set. Most models were able to achieve an accuracy above 90% with different variations among them. Models, such as MobileNet v2 and AlexNet, achieved an accuracy of 91.02% and 92.78% respectively. And models, such as DenseNet-201, performed best with 98.29% for Normal FC and 98.36% for Spinal FC as well as Wide ResNet-101-2 which achieved 98.29% for both Normal and Spinal FC. It can be noticed that Spinal FC was able to averagely provide a better performance than that of Normal FC.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>OF PRE-TRAINED MODELS WITH THE 225 BIRD SPECIES DATA SET WITH BOTH DEFAULT AND SPINALNET FC</figDesc><table><row><cell>Model</cell><cell>Batch</cell><cell>Accuracy</cell><cell>Accuracy</cell></row><row><cell></cell><cell>Size</cell><cell cols="2">(Normal FC) (Spinal FC)</cell></row><row><cell>AlexNet</cell><cell>64</cell><cell>93.33%</cell><cell>93.24%</cell></row><row><cell>VGG-11</cell><cell>64</cell><cell>98.87%</cell><cell>99.02%</cell></row><row><cell>VGG-13</cell><cell>64</cell><cell>99.06%</cell><cell>99.11%</cell></row><row><cell>VGG-16</cell><cell>64</cell><cell>99.11%</cell><cell>99.20%</cell></row><row><cell>VGG-19</cell><cell>64</cell><cell>99.16%</cell><cell>99.20%</cell></row><row><cell>VGG-11 BN</cell><cell>64</cell><cell>99.02%</cell><cell>99.02%</cell></row><row><cell>VGG-13 BN</cell><cell>64</cell><cell>99.11%</cell><cell>99.19%</cell></row><row><cell>VGG-16 BN</cell><cell>64</cell><cell>99.08%</cell><cell>99.29%</cell></row><row><cell>VGG-19 BN</cell><cell>64</cell><cell>99.02%</cell><cell>99.11%</cell></row><row><cell>ResNet-18</cell><cell>64</cell><cell>98.48%</cell><cell>98.48%</cell></row><row><cell>ResNet-34</cell><cell>64</cell><cell>98.13%</cell><cell>98.66%</cell></row><row><cell>ResNet-50</cell><cell>64</cell><cell>98.66%</cell><cell>98.66%</cell></row><row><cell>ResNet-101</cell><cell>64</cell><cell>98.75%</cell><cell>98.93%</cell></row><row><cell>ResNet-152</cell><cell>64</cell><cell>98.75%</cell><cell>99.02%</cell></row><row><cell>SqueezeNet 1.0</cell><cell>64</cell><cell>82.95%</cell><cell>84.61%</cell></row><row><cell>SqueezeNet 1.1</cell><cell>64</cell><cell>88.08%</cell><cell>84.17%</cell></row><row><cell>DenseNet-121</cell><cell>64</cell><cell>98.75%</cell><cell>99.11%</cell></row><row><cell>DenseNet-169</cell><cell>32</cell><cell>98.57%</cell><cell>98.93%</cell></row><row><cell>DenseNet-161</cell><cell>32</cell><cell>99.02%</cell><cell>98.93%</cell></row><row><cell>DenseNet-201</cell><cell>32</cell><cell>98.75%</cell><cell>98.84%</cell></row><row><cell>Inception v3</cell><cell>64</cell><cell>97.95%</cell><cell>98.57%</cell></row><row><cell>GoogLeNet</cell><cell>64</cell><cell>98.31%</cell><cell>98.04%</cell></row><row><cell>ShuffleNet v2 0.5</cell><cell>64</cell><cell>83.82%</cell><cell>84.48%</cell></row><row><cell>ShuffleNet v2 1.0</cell><cell>64</cell><cell>94.43%</cell><cell>93.30%</cell></row><row><cell>MobileNet v2</cell><cell>64</cell><cell>98.48%</cell><cell>98.75%</cell></row><row><cell>ResNeXt-50</cell><cell>64</cell><cell>98.93%</cell><cell>98.40%</cell></row><row><cell>ResNeXt-101</cell><cell>32</cell><cell>98.75%</cell><cell>98.84%</cell></row><row><cell>Wide ResNet-50-2</cell><cell>64</cell><cell>99.29%</cell><cell>99.38%</cell></row><row><cell>Wide ResNet-101-2</cell><cell>64</cell><cell>99.38%</cell><cell>99.56%</cell></row><row><cell>MNASNet 1.0</cell><cell>64</cell><cell>96.95%</cell><cell>96.34%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>OF PRE-TRAINED MODELS WITH THE OXFORD 102 FLOWER DATA SET WITH BOTH DEFAULT AND SPINALNET FC</figDesc><table><row><cell>Model</cell><cell>Batch</cell><cell>Accuracy</cell><cell>Accuracy</cell></row><row><cell></cell><cell>Size</cell><cell cols="2">(Normal FC) (Spinal FC)</cell></row><row><cell>AlexNet</cell><cell>64</cell><cell>92.78%</cell><cell>92.66%</cell></row><row><cell>VGG-11</cell><cell>64</cell><cell>94.56%</cell><cell>94.67%</cell></row><row><cell>VGG-13</cell><cell>64</cell><cell>94.56%</cell><cell>95.01%</cell></row><row><cell>VGG-16</cell><cell>64</cell><cell>94.67%</cell><cell>95.01%</cell></row><row><cell>VGG-19</cell><cell>64</cell><cell>95.32%</cell><cell>95.17%</cell></row><row><cell>VGG-11 BN</cell><cell>64</cell><cell>94.62%</cell><cell>94.67%</cell></row><row><cell>VGG-13 BN</cell><cell>64</cell><cell>94.76%</cell><cell>95.01%</cell></row><row><cell>VGG-16 BN</cell><cell>64</cell><cell>94.97%</cell><cell>95.17%</cell></row><row><cell>VGG-19 BN</cell><cell>64</cell><cell>95.11%</cell><cell>95.38%</cell></row><row><cell>ResNet-18</cell><cell>64</cell><cell>95.83%</cell><cell>96.14%</cell></row><row><cell>ResNet-34</cell><cell>64</cell><cell>96.72%</cell><cell>96.72%</cell></row><row><cell>ResNet-50</cell><cell>64</cell><cell>97.43%</cell><cell>97.55%</cell></row><row><cell>ResNet-101</cell><cell>64</cell><cell>97.92%</cell><cell>97.92%</cell></row><row><cell>ResNet-152</cell><cell>64</cell><cell>97.92%</cell><cell>98.04%</cell></row><row><cell>SqueezeNet 1.0</cell><cell>64</cell><cell>95.01%</cell><cell>95.14%</cell></row><row><cell>SqueezeNet 1.1</cell><cell>64</cell><cell>95.26%</cell><cell>95.20%</cell></row><row><cell>DenseNet-121</cell><cell>64</cell><cell>97.58%</cell><cell>97.64%</cell></row><row><cell>DenseNet-169</cell><cell>32</cell><cell>98.00%</cell><cell>98.04%</cell></row><row><cell>DenseNet-161</cell><cell>32</cell><cell>98.04%</cell><cell>98.17%</cell></row><row><cell>DenseNet-201</cell><cell>32</cell><cell>98.29%</cell><cell>98.36%</cell></row><row><cell>Inception v3</cell><cell>64</cell><cell>98.17%</cell><cell>98.19%</cell></row><row><cell>GoogLeNet</cell><cell>64</cell><cell>95.93%</cell><cell>96.01%</cell></row><row><cell>ShuffleNet v2 0.5</cell><cell>64</cell><cell>92.78%</cell><cell>92.66%</cell></row><row><cell>ShuffleNet v2 1.0</cell><cell>64</cell><cell>93.22%</cell><cell>93.45%</cell></row><row><cell>MobileNet v2</cell><cell>64</cell><cell>91.02%</cell><cell>90.94%</cell></row><row><cell>ResNeXt-50</cell><cell>64</cell><cell>97.80%</cell><cell>97.80%</cell></row><row><cell>ResNeXt-101</cell><cell>32</cell><cell>98.00%</cell><cell>98.04%</cell></row><row><cell>Wide ResNet-50-2</cell><cell>64</cell><cell>98.04%</cell><cell>98.17%</cell></row><row><cell>Wide ResNet-101-2</cell><cell>64</cell><cell>98.29%</cell><cell>98.29%</cell></row><row><cell>MNASNet 1.0</cell><cell>64</cell><cell>95.67%</cell><cell>95.59%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Model Batch Accuracy Accuracy Size (Normal FC</title>
		<imprint/>
	</monogr>
	<note>Spinal FC</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Prepositioning of a land vehicle simulation-based motion platform using fuzzy logic and neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R C</forename><surname>Qazani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="10" to="446" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Application of extreme learning machine for short term output power forecasting of three grid-connected pv systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mekhilef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Olatomiwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shamshirband</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">journal of Cleaner Production</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="395" to="405" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural network-based uncertainty quantification: A survey of methodologies and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="36" to="218" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Performance evaluation and calibration of gantry-tau parallel mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R C</forename><surname>Qazani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pedrammehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iranian Journal of Science and Technology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1013" to="1027" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Transactions of Mechanical Engineering</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uncertainty-aware decisions in cloud computing: Foundations and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A model predictive control-based motion cueing algorithm with consideration of joints&apos; limitations for hexapod motion platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R C</forename><surname>Qazani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="708" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A model predictive controlbased motion cueing algorithm using an optimized nonlinear scaling for driving simulators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asadi</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohammadi</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R C</forename><surname>Qazani</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Lim</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1245" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and? 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimal uncertainty-guided neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kavousi-Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">106878</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A review of uncertainty quantification in deep learning: Techniques, applications and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pourpanah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezazadegan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Information Fusion</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Partial adversarial training for neural network-based uncertainty quantification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kavousi-Fard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">10 monkey species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename></persName>
		</author>
		<ptr target="https://www.kaggle.com/slothkong/10-monkey-species" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Kaggle</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fruit recognition from images using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oltean</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/moltean/fruits" />
	</analytic>
	<monogr>
		<title level="j">Acta Universitatis Sapientiae, Informatica</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="42" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">225 bird species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerry</forename></persName>
		</author>
		<ptr target="https://www.kaggle.com/gpiosenka/100-bird-species" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Kaggle</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A new gantry-tau-based mechanism using spherical wrist and model predictive control-based motion cueing algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R C</forename><surname>Qazani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotica</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1359" to="1380" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A novel axis symmetric parallel mechanism with coaxial actuated arms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pedrammehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R C</forename><surname>Qazani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 4th International Conference on Control, Automation and Robotics (ICCAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="476" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-fidelity hexarot simulation-based motion platform using fuzzy incremental controller and model predictive control-based motion cueing algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R C</forename><surname>Qazani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Systems Journal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5073" to="5083" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A linear time-varying model predictive control-based motion cueing algorithm for hexapod simulation-based motion platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R C</forename><surname>Qazani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Systems</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kluyver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bussonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frederic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Corlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Willing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Development Team</surname></persName>
		</author>
		<ptr target="https://eprints.soton.ac.uk/403913/" />
		<title level="m">Jupyter notebooks ? a publishing format for reproducible computational workflows</title>
		<editor>F. Loizides and B. Scmidt</editor>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="90" />
		</imprint>
	</monogr>
	<note>Positioning and Power in Academic Publishing: Players, Agents and Agendas</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M J</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Srinivasan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03347</idno>
		<title level="m">Spinalnet: Deep neural network with gradual input</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spinenet-6ma: A novel deep learning tool for predicting dna n6-methyladenine sites in genomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tayara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
