<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learn From All: Erasing Attention Consistency for Noisy Label Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learn From All: Erasing Attention Consistency for Noisy Label Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Noisy label learning</term>
					<term>Facial expression recognition</term>
					<term>Erasing attention consistency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0000?0003?4161?5020] , Chengrui Wang [0000?0003?0618?0797] , Xu Ling [0000?0002?3495?9434]  , and Weihong Deng [0000?0001?5952?6996]    Abstract. Noisy label Facial Expression Recognition (FER) is more challenging than traditional noisy label classification tasks due to the inter-class similarity and the annotation ambiguity. Recent works mainly tackle this problem by filtering out large-loss samples. In this paper, we explore dealing with noisy labels from a new feature-learning perspective. We find that FER models remember noisy samples by focusing on a part of the features that can be considered related to the noisy labels instead of learning from the whole features that lead to the latent truth. Inspired by that, we propose a novel Erasing Attention Consistency (EAC) method to suppress the noisy samples during the training process automatically. Specifically, we first utilize the flip semantic consistency of facial images to design an imbalanced framework. We then randomly erase input images and use flip attention consistency to prevent the model from focusing on a part of the features. EAC significantly outperforms state-of-the-art noisy label FER methods and generalizes well to other tasks with a large number of classes like CIFAR100 and Tiny-ImageNet. The code is available at https://github.com/zyh-uaiaaaa/Erasing-Attention-Consistency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Facial Expression Recognition (FER) has wide applications in the real world, such as driver fragile detection, service robots, and human-computer interaction <ref type="bibr" target="#b34">[35]</ref>. The most common paradigm for FER is the end-to-end supervised manner, whose performance largely relies on the massive high-quality annotated data. However, collecting large-scale datasets with fully precise annotations is usually expensive and time-consuming, sometimes even impossible. Furthermore, facial expression images have inherent inter-class similarity (all classes are human faces) and annotation ambiguity (some expression images are quite confusing), making noisy label FER more challenging than traditional noisy label classification tasks. On the other hand, it is well-known that deep neural networks have enough capacity to memorize large-scale data with even completely random labels, leading to poor performance in generalization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b47">48]</ref>. Therefore, robust FER with noisy labels has become an essential and challenging task in computer vision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Mainstream noisy label FER methods can be mainly classified into two categories, sample selection and label ensembling. SCN <ref type="bibr" target="#b37">[38]</ref> and RUL <ref type="bibr" target="#b49">[50]</ref> can be viewed as sample selection methods, which learn more from clean samples and then relabel the noisy samples. SCN <ref type="bibr" target="#b37">[38]</ref> uses a fully-connected layer to learn an importance weight for each sample and suppresses uncertain samples during the training phase. RUL <ref type="bibr" target="#b49">[50]</ref> learns uncertainty weights through comparison between different samples. IPA2LT <ref type="bibr" target="#b34">[35]</ref> and DMUE <ref type="bibr" target="#b34">[35]</ref> are label ensembling methods, which provide several labels for a single sample to better mine the latent truth. IPA2LT <ref type="bibr" target="#b34">[35]</ref> assigns each sample more than one labels with human annotations or model predictions while DMUE <ref type="bibr" target="#b34">[35]</ref> uses a multi-branch model to better mine the latent distribution in the label space. All the aforementioned methods get good performances under noisy label FER while they still have defects. Specifically, sample selection methods are based on the small-loss assumption <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48]</ref>, which might confuse hard samples and noisy samples as both of them have large loss values during the training process. Sample selection methods also need the noise rate, which is non-trivial in large-scale real-world datasets. Label ensembling methods provide different views of the same sample using several networks, similar to crowdsourcing in real FER applications. However, the extra information gain they bring might be noisy. Label ensembling methods might bring great computation overhead, making them less preferable in real applications. Thus, the noisy label FER problem demands better methods that do not need to know the noise rate or train several models to perform well.</p><p>In this paper, instead of following the traditional path to detect noisy samples according to their loss values and then suppress them, we view noisy label learning from a new feature-learning perspective and propose a novel framework to deal with all the aforementioned defects. We find that the FER model remembers noisy samples by focusing on a part of the features that can be considered related to the noisy labels, shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The image in the first column is labeled as sad, while its latent truth is surprise. SCN <ref type="bibr" target="#b37">[38]</ref> remembers this noisy sample by focusing on the frown feature which can be considered related to the noisy label of the sad expression. However, it neglects the open mouth feature, which is vital for the correct classification as an open mouth combined with a frown leads to the latent truth surprise instead of the noisy label sad. From the attention regions of the noisy samples, we conclude that the FER model only observes a part of the features that can be considered related to the noisy labels to remember noisy samples. It is intuitive as remembering noisy samples by focusing on a part of the features that can be considered related to the noisy labels does not contradict the other learned features from the clean samples. Inspired by this finding, we propose to deal with noisy label FER from a new feature-learning perspective. If the model can not focus on a part of the features and always learns from the whole features, then it cannot remember the noisy samples. Learning from the whole features from all training samples also means . NL represents the noisy label, LT represents the latent truth. The prediction results are shown under the images. SCN only focuses on a part of the features that can be considered related to the noisy labels to remember the noisy samples. (b) shows SCN predicts differently on the flipped image. Our EAC forces the model to focus on similar parts before and after the flip to prevent the model from remembering noisy labels. the model does not need to filter out large-loss samples like traditional methods which might confuse useful hard samples with noisy samples.</p><p>In this paper, we use Attention Consistency to implement the consistency regularization. Attention Consistency <ref type="bibr" target="#b10">[11]</ref> assumes that the learned attention maps should follow the same transformation as the input images to achieve better multi-label classification performance. The attention maps denote the features that the model based on to make the predictions.</p><p>We find that the flip semantic consistency of facial expression images can help to detect noisy labels. Flip semantic consistency means the original image and its flipped counterpart should be classified into the same category. However, if we train a FER model with a noisy sample, the model might remember the noisy sample while it still predicts the latent truth on its flipped counterpart, shown as the images in the first row of <ref type="figure" target="#fig_0">Figure 1</ref>. Inspired by that, we propose an imbalanced framework to prevent the model from remembering noisy samples. Specifically, we only compute classification loss on the original images and compute consistency loss between the attention maps extracted from the original images and their flipped counterparts. We utilize the consistency loss to prevent the model from remembering a part of the features of the original images. Such an imbalanced framework cannot help the model totally get rid of the noisy labels as the model can still gradually overfit the attention maps of the flipped images to keep the consistency loss small, which degrades the regularization effect. We further propose Erasing Attention Consistency (EAC) to increase the performance of the imbalanced framework. Before flipping, we first randomly erase the input images during the whole training phase. During the training phase, the dynamic changing of the erased area ensures that the model can not simply remember the attention maps before and after the flip to get small consistency loss values. When the model starts to overfit the noisy original samples by focusing on a part of the features related to the noisy labels, the attention maps of the original images will deviate largely from the attention maps of their flipped counterparts, which will lead to large consistency loss values. We set the weight of the consistency loss larger enough to ensure the model first optimizes the consistency loss. Thus, to get small consistency loss values, the model will automatically quit overfitting the noisy samples.</p><p>The main contributions of our work are as follows:</p><p>1. Instead of using traditional methods which deal with noisy labels from highlevel small-loss selection, we cope with noisy labels from middle-level feature learning, which does not require the noise rate to perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">We propose a novel method named Erasing Attention Consistency (EAC)</head><p>which automatically prevents the model from memorizing noisy samples. 3. We experimentally show that EAC significantly advances state-of-the-art results on multiple FER benchmarks with different levels of label noise. EAC also generalizes well to image classification tasks with a large number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Noisy Label Learning Learning with noisy labels has been well studied <ref type="bibr">[1,13-15, 17, 19, 20, 24, 25, 29, 31-33, 37, 41-43, 45, 46, 51]</ref>. Current works can be mainly categorized into two groups: modifying the primary loss function or selecting clean samples for training.</p><p>The first type of method mainly focuses on estimating the noise transition matrix or proposing robust loss functions. Patrini et al. <ref type="bibr" target="#b31">[32]</ref> estimate the transition matrix to model the relationship between noisy labels and the latent truth to prevent the model from overfitting noisy labels. Han et al. <ref type="bibr" target="#b12">[13]</ref> propose a human-assisted approach that conveys human cognition of invalid class transitions to make estimating transition matrix easier. Both Thulasidasan et al. <ref type="bibr" target="#b36">[37]</ref> and Zhang et al. <ref type="bibr" target="#b50">[51]</ref> propose generalized cross-entropy loss functions to combat noisy labels. Xu et al. <ref type="bibr" target="#b42">[43]</ref> design a new loss function based on mutual information which is information-monotone and robust to various kinds of label noise. Although these methods have theory guarantees, they are not suitable for challenging real-world settings or handling a large number of classes. Thus, recent works usually focus on the second type of method.</p><p>The second strand of approach is based on the memorization effect that DNNs fit the underlying clean distribution before overfitting the noisy labels <ref type="bibr" target="#b1">[2]</ref>. They focus on reweighting or sample selection to suppress noisy samples. Jiang et al. <ref type="bibr" target="#b18">[19]</ref> train a mentor net using clean samples to guide the student net by weighing the samples. <ref type="bibr">Ren</ref>   <ref type="bibr" target="#b13">[14]</ref> train two models to select small loss samples for each other hoping to filter different types of error introduced by noisy labels. Malach et al. <ref type="bibr" target="#b28">[29]</ref> improve co-teaching by updating only on instances with different predictions to keep the two models diverged. Wei et al. <ref type="bibr" target="#b40">[41]</ref> train two models together and use their agreement degree to select small-loss samples. These methods select small-loss samples to eliminate the bad influence from the noisy samples. However, the useful hard samples are likely to have large loss values and might be filtered out as noisy samples. These methods also need to know the noise rate to get better performance. Different from them, our method automatically prevents the model from memorizing the noisy samples, which do not require the noise rate or selecting clean samples. Facial Expression Recognition Facial Expression Recognition (FER) aims at helping computers to understand human behavior or even interact with a human by recognizing human expression. In recent years, as the recognition accuracy is very high in the laboratory collected FER datasets, more attempts try to address the in-the-wild FER problem, which contains lots of label noise. Zeng et al. <ref type="bibr" target="#b46">[47]</ref> first consider annotation inconsistency and assign each sample with more than one label to better mine the latent truth. Wang et al. <ref type="bibr" target="#b37">[38]</ref> propose to learn an importance weight for each sample and suppress the uncertain images by relabeling. She et al. <ref type="bibr" target="#b34">[35]</ref> train multi-branch models by leaving out one class for each branch in order to find the latent truth under label noise. Zhang et al. <ref type="bibr" target="#b49">[50]</ref> propose to learn the uncertainty of different facial images by comparison and then suppress the uncertain images. They can be mainly categorized into two classes, sample selection <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b49">50]</ref> or label ensembling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b46">47]</ref>. Sample selection methods select good samples and suppress noisy samples while label ensembling methods use crowdsourcing to improve performance. However, they either require the noise rate to better filter out noisy samples or bring extra computation overhead and cannot generalize well to classification tasks with a large number of classes. Our method automatically prevents the model from overfitting the noisy samples without the noise rate and generalizes well to classification tasks with a large number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we illustrate the implementation details of our proposed Erasing Attention Consistency (EAC) method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary</head><p>Class Activation Mapping Class Activation Mapping (CAM) <ref type="bibr" target="#b52">[53]</ref> is an attention method, which allows us to visualize the predicted class scores on the given images, highlighting the discriminative parts detected by the CNN.</p><p>In the CNN trained for classification, an attention map is the weighted sum of the feature maps from the last convolutional layer with the weights from a fully connected (FC) layer. By viewing the attention maps, we can know what the model is based on to make the predictions. We denote the feature map extracted from the last convolutional layer as F ? R C?H?W , C, H, W respectively represent the number of channels, height, width of the feature map. We denote the weights of the FC layer as W ? R L?C , L represents the number of classes. The attention map computes as</p><formula xml:id="formula_0">M j (h, w) = C c=1 W(j, c)F c (h, w),<label>(1)</label></formula><formula xml:id="formula_1">M j (h, w)</formula><p>is the attention value of location (h, w) for class index j, which is the weighted sum of feature maps over different channels. In our method, we use CAM to compute the attention maps from the input images to show the features that the model attends to. Attention Consistency Attention Consistency <ref type="bibr" target="#b10">[11]</ref> is first proposed for achieving better visual perceptual plausibility and better multi-label image classification by considering visual attention consistency under spatial transforms. It assumes that the learned attention maps of the model should follow the same transformation as the input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview of Erasing Attention Consistency</head><p>In this paper, we design an imbalanced framework to help the model get rid of the negative effect of the noisy labels. We notice that the facial images before and after the flip have the same semantic meaning of the facial expression. We only compute classification loss with the original images and compute consistency loss between the attention maps of the original images and their flipped counterparts to prevent the model from remembering the original images with noisy labels. Simply using this imbalanced framework can not help the model totally get rid of the negative effect from noisy labels as the model can gradually remember the flipped images to always get small consistency loss, which degrades the regularization effect. We further propose Erasing Attention Consistency to enhance the performance of our proposed imbalanced framework. Before flipping the original images to generate their counterparts, we first randomly erase the images according to <ref type="bibr" target="#b51">[52]</ref>, which will generate different pairs of original images and their flipped counterparts during the training process. Thus, the model cannot remember the flipped images to get small consistency loss. If the model starts to remember the original images with noisy labels, the attention maps extracted from them will focus on a part of the features, which deviate largely from the flipped attention maps extracted from their flipped counterparts leading to the increase of the consistency loss. Thus, the consistency loss can prevent the model from remembering noisy samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Framework of Erasing Attention Consistency</head><p>The overall framework of our proposed EAC is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Given a batch of facial expression images, we first erase the input images according to [52] and get I. We then flip these images to get their flipped counterparts I ? . I and I ? are the input images. The feature maps are extracted from the last convolutional layer, denoted as F ? R N ?C?H?W and F ? ? R N ?C?H?W . N , C, H, W respectively represent the number of images, the number of channels, height, width of the feature maps. We only input F through the global average pooling (GAP) layer to get features f ? R N ?C?1?1 . We resize features f to N ?C and put them through fully connected (FC) layer to compute classification loss according to</p><formula xml:id="formula_2">l cls = ? 1 N N i=1 (log e Wy i fi L j e Wj fi ),<label>(2)</label></formula><p>W y i is the y i -th weight from the FC layer with y i as the given label of the i-th image. We compute attention maps M and M ? for I and I ? according to Eq. (1). Note that the weights used to compute attention maps come from the FC layer, while the FC layer only computes classification loss with the original feature maps F. We use consistency loss to minimize the distance between the feature maps M and F lip(M ? ) as</p><formula xml:id="formula_3">l c = 1 N LHW N i=1 L j=1 ||M ij ? F lip(M ? ) ij || 2 .<label>(3)</label></formula><p>The total loss is computed as follows,</p><formula xml:id="formula_4">l total = l cls + ?l c .<label>(4)</label></formula><p>? is the weight of the erasing consistency loss. The ablation study of ? is in Section 4.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first describe 3 popular in-the-wild FER benchmarks and our implementation details. We then verify the proposed EAC on the FER datasets with different levels of label noise and study why EAC works. Visualization results of the learned features, attention maps and classification loss values are displayed to provide an intuitive understanding of EAC. We carry out an ablation study and also show the generalization ability of EAC by conducting experiments on CIFAR100 <ref type="bibr" target="#b21">[22]</ref> and Tiny-ImageNet <ref type="bibr" target="#b33">[34]</ref>. Finally, we compare EAC with other state-of-the-art FER methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>RAF-DB <ref type="bibr" target="#b25">[26]</ref> is annotated with basic or compound expressions by 40 trained human coders. In our experiments, images with seven basic expressions (i.e. neutral, happy, surprise, sad, angry, disgust, fear) are used including 12,271 images for training and 3,068 images for testing. FERPlus <ref type="bibr" target="#b2">[3]</ref> is extended from FER2013 <ref type="bibr" target="#b9">[10]</ref> with finer label annotations. It is collected by the Google search engine consisting of 28,709 training images and 3,589 test images. We use the most voting category as the annotation for a fair comparison <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>AffectNet <ref type="bibr" target="#b29">[30]</ref> is by far the largest FER dataset, which is collected from the Internet by querying expression-related keywords in three search engines containing more than one million images. There are 286,564 training images and 4,000 test images manually labeled to eight classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>By default, we use ResNet-18 <ref type="bibr" target="#b15">[16]</ref> pre-trained on MS-Celeb-1M <ref type="bibr" target="#b11">[12]</ref> as the backbone network with the same routine as <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref> for fair comparisons. The facial images are aligned and cropped with three landmarks <ref type="bibr" target="#b39">[40]</ref>, resized to 224 ? 224 pixels. We only use the horizontal flip and the random erasing without any other data augmentation tricks to evaluate the effectiveness of our proposed method. During training, the batch size is 256. The initial learning rate is 0.0002. We use Adam <ref type="bibr" target="#b20">[21]</ref> optimizer with weight decay of 0.0001 and ExponentialLR <ref type="bibr" target="#b26">[27]</ref> learning rate scheduler with the gamma of 0.9 to decrease the learning rate after each epoch. The training ends at epoch 60.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of EAC on Noisy FER Datasets</head><p>We quantitatively evaluate the improvement of our proposed EAC against other state-of-the-art noisy label FER methods. We explore the robustness of EAC with three levels of label noise including the ratio of 10%, 20%, 30% on RAF-DB, FERPlus, and AffectNet datasets. We follow <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50]</ref> to generate noisy labels. As the generation of label noise is random, we re-implement other state-ofthe-art methods on our generated noisy datasets to make fair comparisons with them. We also consider the influence of the different backbones and backbones with or without pretraining. Shown in <ref type="table" target="#tab_1">Table 1</ref>, our method outperforms other state-of-the-art FER noisy label learning methods by a large margin. For example, EAC outperforms SCN under 30% label noise by 6.97%, 3.24%, 4.31% on RAF-DB, FERPlus, AffectNet respectively.</p><p>Note that, unlike SCN <ref type="bibr" target="#b37">[38]</ref> and RUL <ref type="bibr" target="#b49">[50]</ref>, EAC does not need to modify the labels of the training samples. Relabeling has the risk of changing right labels to wrong labels, which is less flexible than our method as EAC can automatically learn useful information from all training samples. EAC does not need to know the noise rate or tell apart hard samples and noisy samples, which fundamentally solves the defects of sample selection methods as sample selection methods require the noise rate to filter out large-loss samples, which might contain useful hard samples and useless noisy samples.</p><p>We also study EAC with different backbones. With different backbones, ? is set to 5 under 0 and 10% noise, 10 under 20% and 30% noise. As shown in <ref type="table" target="#tab_2">Table 2</ref>, adding EAC to MobileNet or ResNet-50 can both improve their performance. Baselines are also trained with erase and flip for a fair comparison. EAC achieves better results in all settings using ResNet-50 as backbone compared with ResNet-18 in <ref type="table" target="#tab_1">Table 1</ref>. The experiments of EAC using an unpretrained model as backbone are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Why EAC works</head><p>We evaluate the three modules of the proposed EAC to find why EAC works well under label noise. The experiment results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Several observations are concluded as follows. Without the flip attention consistency module, the model can not use the same semantic meaning from the flipped counterparts to regularize the classification loss, which is shown in the second row. Without the erasing, the model will gradually remember the attention maps from the flipped images to get small consistency loss values, which degrades the regularization effect. Without the imbalanced framework, the noisy labels will affect the images before and after the flip together. The model can remember the noisy samples before and after the flip together, making the consistency loss useless. However, when we combine the three modules, the performance skyrockets. We believe it is the dynamic erasing that prevents the model from remembering the attention maps. Thus, the model needs to learn flip consistent features to minimize the consistency loss. As we only compute the classification loss with the original images (the imbalanced framework), if the model tries to remember the noisy samples, the features learned from these samples will deviate largely from their flipped counterparts, making the consistency loss large. As we set the weight of the consistency loss large enough, the model will first minimize the consistency loss. Thus, it will quit remembering the noisy samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Whether flip and erase is sufficiently valid for EAC</head><p>We use flip because we need spatial transforms to enable attention consistency following <ref type="bibr" target="#b10">[11]</ref>. Other spatial transforms like Rotate or Scale are not very effective   <ref type="bibr" target="#b35">[36]</ref> and AutoAugment <ref type="bibr" target="#b4">[5]</ref> (AutoAug.) is compared to Erasing. AutoAugment searches and combines many kinds of augments together while it is still inferior to erasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Feature Visualization</head><p>To understand EAC intuitively, we plot the learned features of EAC trained with 30% noisy labels on RAF-DB by t-SNE <ref type="bibr" target="#b27">[28]</ref>. <ref type="figure" target="#fig_2">Figure 3 (a)</ref> is the learned features displayed with the noisy training labels. It is shown that EAC does not remember noisy labels as features with different labels are clustered together. It is shown that the features with noisy labels are close to the classification boundary which means these samples are with large classification loss values. Thus, EAC separates clean and noisy samples effectively. We also plot the same   <ref type="figure" target="#fig_2">Figure 3</ref> (a), we can draw the conclusion that EAC can automatically prevent the model from remembering noisy labels and learn useful features from both clean and noisy samples. We plot the attention maps on images before and after the flip in <ref type="figure" target="#fig_3">Figure 4</ref> to show the effectiveness of EAC. We train SCN with the original images and test on their flipped counterparts. It is shown that SCN remembers the original images to the noisy labels, while it still gets correct predictions on their flipped counterparts after training. Inspired by that, EAC uses the attention maps of the flipped ones to regularize the classification loss and get correct prdictions on both the original images and their flipped counterparts. We display more results in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Visualization of the classification loss values</head><p>We plot the distribution of classification loss values after training for 60 epochs in <ref type="figure" target="#fig_4">Figure 5</ref> under the same setting as Section 4.6. We normalize the histogram of loss values and plot it as the probability density. The baseline method overfits nearly all the noisy samples after training for 60 epochs as the loss values of all samples are around 0. SCN learns importance weights and uses relabeling to deal with noisy samples. However, lots of the noisy samples are not correctly relabeled during the training process as there are still lots of noisy samples with loss values close to 0. Our EAC prevents the model from remembering the noisy samples during the whole training process. After training for 60 epochs, the loss values of clean and noisy samples can still be separated clearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Ablation Study</head><p>We evaluate the consistency loss weight ? from 0.1 to 10.0 with different levels of label noise. The results are shown in the supplementary material. We can choose ? from a wide range to acquire state-of-the-art performance. The best value of ? is 3 under 10% and 20% noise and 5 under 30% noise on RAF-DB using ResNet-18 as backbone. For simplicity, we set ? as 5 in the noisy label experiments using ResNet-18 as backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">The generalization ability of EAC</head><p>Noisy label FER methods might not be suitable for noisy label classification tasks with a large number of classes as the class number of the facial expression is very small. For example, DMUE <ref type="bibr" target="#b34">[35]</ref> needs to train a multi-branch model whose branch number equals the class number plus 1 to mine the latent truth, which is unaffordable when the class number is very large. However, EAC can generalize well to tasks with a large number of classes.</p><p>To show the generalization ability of EAC. We carry out experiments on CIFAR100 <ref type="bibr" target="#b21">[22]</ref> and Tiny-ImageNet <ref type="bibr" target="#b33">[34]</ref>. Due to the space limitation, the implementation details are illustrated in the supplementary material. As shown in Tabel 5, our EAC consistently improves the baseline by a large margin in both top-1 and top-5 accuracy. EAC outperforms the baseline by 6.37% , 9.40%, 10.89% on CIFAR100 and 12.11%, 17.67%, 22.19% on Tiny-ImageNet in top-1 accuracy with noise ratio 10%, 20%, 30%. Although SCN <ref type="bibr" target="#b37">[38]</ref> also outperforms the baseline, it is clear that our EAC achieves much better results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10">Comparison with other state-of-the-art FER methods</head><p>EAC can also help the FER model achieve state-of-the-art performance on clean datasets as EAC encourages the model to learn flip consistent features from the input images which conforms to the human visual perceptual. The results are shown in <ref type="table" target="#tab_6">Table 6</ref>. Besides the works mentioned in Section 2, RAN <ref type="bibr" target="#b38">[39]</ref> utilizes attention weights to aggregate a varied number of face regions to recognize facial expression robustly. DACL <ref type="bibr" target="#b7">[8]</ref> adaptively selects a subset of significant feature elements for enhanced discrimination. <ref type="bibr" target="#b22">[23]</ref> utilizes a knowledgeable teacher network (KTN) and a self-taught student network (STSN) to transfer knowledge. Our EAC achieves the best performance than other state-of-the-art methods on RAF-DB and AffectNet(7 classes) while slightly lower than KTN <ref type="bibr" target="#b22">[23]</ref> under FERPlus. We do not compare with <ref type="bibr" target="#b43">[44]</ref> as it utilizes Vision Transformer <ref type="bibr" target="#b5">[6]</ref> as backbone while we use ResNet-18 <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we explore to deal with noisy label FER from a new featurelearning perspective and propose a novel and effective method named Erasing Attention Consistency (EAC). We design an imbalanced framework to utilize the erasing and flip consistency loss to prevent the model from remembering noisy labels. EAC does not require the noise rate or label ensembling. Extensive experiments verify that EAC outperforms other state-of-the-art noisy label FER methods on clean and noisy datasets. Furthermore, EAC generalizes well to noisy label classification tasks with a large number of classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(a) shows the attention regions of the noisy samples learned by SCN and EAC (Ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The framework of the Erasing Attention Consistency (EAC). EAC randomly erases input images and then gets their flipped counterparts. EAC only computes the classification loss with the original images. The classification loss with the noisy labels might cause the model to overfit the noisy samples shown as M i . EAC uses the consistency loss between the original images and their flipped counterparts to prevent the model from remembering noisy labels. The dotted lines mean no gradient propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The learned features by EAC training with noisy labels. (a) is the learned features displayed with the noisy training labels, EAC does not overfit noisy labels as different classes mixed with each other. Notice that noisy samples are pushed to the classification boundary by EAC. (b) is the same learned features with (a), but displayed with the latent truth. Though we train EAC with noisy labels, it can still learn useful features related to the latent truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The attention maps of SCN and EAC on the original images and their flipped counterparts. learned features in Figure 3 (b), but displayed with the latent truth. Compared with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>The classification loss values of different methods after training for 60 epochs with noisy samples. The baseline remembers nearly all noisy samples. SCN avoids overfitting a part of the noisy samples, while EAC can still separate clean and noisy samples apart after training for 60 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>et al. [33] reweight samples according to their gradient directions. Arazo et al. [1] model per-sample loss by a mixture model to calculate a weight for each sample. Han et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of EAC on noisy FER datasets. We re-implement other state-of-the-art methods and test all the methods with the same noisy datasets to make fair comparisons. Results are computed as the mean of the accuracy from the last 5 epochs</figDesc><table><row><cell>Method</cell><cell cols="4">Noise(%) RAF-DB(%) FERPlus(%) AffectNet(%)</cell></row><row><cell>Baseline</cell><cell>10</cell><cell>81.01</cell><cell>83.29</cell><cell>57.24</cell></row><row><cell>SCN (CVPR20)</cell><cell>10</cell><cell>82.15</cell><cell>84.99</cell><cell>58.60</cell></row><row><cell>RUL (NeurIPS21)</cell><cell>10</cell><cell>86.17</cell><cell>86.93</cell><cell>60.54</cell></row><row><cell>EAC (Ours)</cell><cell>10</cell><cell>88.02</cell><cell>87.03</cell><cell>61.11</cell></row><row><cell>Baseline</cell><cell>20</cell><cell>77.98</cell><cell>82.34</cell><cell>55.89</cell></row><row><cell>SCN (CVPR20)</cell><cell>20</cell><cell>79.79</cell><cell>83.35</cell><cell>57.51</cell></row><row><cell>RUL (NeurIPS21)</cell><cell>20</cell><cell>84.32</cell><cell>85.05</cell><cell>59.01</cell></row><row><cell>EAC (Ours)</cell><cell>20</cell><cell>86.05</cell><cell>86.07</cell><cell>60.29</cell></row><row><cell>Baseline</cell><cell>30</cell><cell>75.50</cell><cell>79.77</cell><cell>52.16</cell></row><row><cell>SCN (CVPR20)</cell><cell>30</cell><cell>77.45</cell><cell>82.20</cell><cell>54.60</cell></row><row><cell>RUL (NeurIPS21)</cell><cell>30</cell><cell>82.06</cell><cell>83.90</cell><cell>56.93</cell></row><row><cell>EAC (Ours)</cell><cell>30</cell><cell>84.42</cell><cell>85.44</cell><cell>58.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The influence of different backbones on EAC. We carry out experiments on RAF-DB. Results are computed as the mean of the accuracy from the last 5 epochs</figDesc><table><row><cell>Method</cell><cell>0 noise</cell><cell>10% noise</cell><cell>20% noise</cell><cell>30% noise</cell></row><row><cell>MobileNet</cell><cell>83.31%</cell><cell>77.80%</cell><cell>70.60%</cell><cell>62.48%</cell></row><row><cell>MobileNet + EAC</cell><cell>86.47%</cell><cell>82.63%</cell><cell>81.65%</cell><cell>79.82%</cell></row><row><cell>ResNet-50</cell><cell>88.75%</cell><cell>83.44%</cell><cell>79.11%</cell><cell>71.67%</cell></row><row><cell>ResNet-50 + EAC</cell><cell>90.35%</cell><cell>88.62%</cell><cell>87.35%</cell><cell>85.27%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of the three modules of EAC on RAF-DB with 30% label noise</figDesc><table><row><cell cols="4">flip attention consistency imbalanced framework erasing RAF-DB</cell></row><row><cell>x</cell><cell>x</cell><cell>x</cell><cell>75.50</cell></row><row><cell>x</cell><cell>?</cell><cell>?</cell><cell>78.10</cell></row><row><cell>?</cell><cell>x</cell><cell>?</cell><cell>78.29</cell></row><row><cell>?</cell><cell>?</cell><cell>x</cell><cell>76.26</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>84.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison with other augmentation methods. The experiments are carried out on noisy RAF-DB. Erasing guides the model to focus on the whole feature as the remembered feature parts might be absent during the training. Other augments can not directly solve the part-view problem and are not very effective. We test them on noisy RAF-DB. Rotate and Scale is compared to Flip. Blur</figDesc><table><row><cell>Noise Rotate</cell><cell>Scale</cell><cell>Flip</cell><cell>Blur</cell><cell cols="2">AutoAug. Erasing</cell></row><row><cell cols="4">10% 80.93% 85.98% 88.02% 86.80%</cell><cell>87.84%</cell><cell>88.02%</cell></row><row><cell cols="4">20% 79.63% 85.30% 86.05% 83.77%</cell><cell>85.82%</cell><cell>86.05%</cell></row><row><cell cols="4">30% 78.23% 82.01% 84.42% 76.92%</cell><cell>82.40%</cell><cell>84.42%</cell></row><row><cell cols="6">for FER as FER test sets are mainly frontal faces with a similar scale. We</cell></row><row><cell cols="6">utilize erasing as FER models fit noisy labels through remembering parts of</cell></row><row><cell>the features.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>CIFAR100 and Tiny-ImageNet label noise training 56/85.37 57.33/78.93 49.70/72.55 58.11/80.24 49.56/72.43 41.32/64.58 SCN [38] 65.18/86.60 60.38/82.11 56.19/78.30 62.22/85.89 55.23/80.21 47.39/72.56 EAC 70.93/90.15 66.73/87.01 60.59/82.84 70.22/90.23 67.23/89.01 63.51/87.18</figDesc><table><row><cell cols="3">CIFAR100 Noise Rate</cell><cell cols="3">Tiny-ImageNet Noise Rate</cell></row><row><cell>Methods</cell><cell>Top-1/Top-5 (%)</cell><cell></cell><cell></cell><cell>Top-1/Top-5 (%)</cell><cell></cell></row><row><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell></row><row><cell>Baseline 64.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison with other state-of-the-art results on different FER datasets. ? denotes training with both AffectNet and RAF-DB datasets. * denotes test with 7 classes on AffectNet.</figDesc><table><row><cell cols="2">RAF-DB</cell><cell cols="2">FERPlus</cell><cell cols="2">AffectNet</cell></row><row><cell>Methods</cell><cell cols="2">Acc. (%) Methods</cell><cell cols="2">Acc. (%) Methods</cell><cell>Acc. (%)</cell></row><row><cell>IPA2LT  ? [47]</cell><cell>86.77</cell><cell>IPA2LT  ? [47]</cell><cell>-</cell><cell>IPA2LT  ? [47]</cell><cell>57.31</cell></row><row><cell>RAN [39]</cell><cell>86.90</cell><cell>RAN [39]</cell><cell>88.55</cell><cell>RAN [39]</cell><cell>59.50</cell></row><row><cell>SCN [38]</cell><cell>87.03</cell><cell>SCN [38]</cell><cell>88.01</cell><cell>SCN [38]</cell><cell>60.23</cell></row><row><cell>DACL [8]</cell><cell>87.78</cell><cell>DACL [8]</cell><cell>-</cell><cell>DACL  *  [8]</cell><cell>65.20</cell></row><row><cell>KTN [23]</cell><cell>88.07</cell><cell>KTN [23]</cell><cell>90.49</cell><cell>KTN  *  [23]</cell><cell>63.97</cell></row><row><cell>DMUE [35]</cell><cell>88.76</cell><cell>DMUE [35]</cell><cell>88.64</cell><cell>DMUE [35]</cell><cell>62.84</cell></row><row><cell>RUL [50]</cell><cell>88.98</cell><cell>RUL [50]</cell><cell>88.75</cell><cell>RUL [50]</cell><cell>61.43</cell></row><row><cell>EAC (Ours)</cell><cell>89.99</cell><cell>EAC (Ours)</cell><cell>89.64</cell><cell>EAC  *  (Ours)</cell><cell>65.32</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Training deep networks for facial expression recognition with crowd-sourced label distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Label distribution learning on auxiliary label space graphs for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Autoaugment: Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning discriminative representation for facial expression recognition from uncertainties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Facial expression recognition in the wild via deep attentive center loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Farzaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Noisy annotations robust consensual collaborative affect expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balasubramanian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Challenges in representation learning: A report on three machine learning contests</title>
		<imprint>
			<publisher>ICONIP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visual attention consistency under image transforms for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">O2u-net: A simple noisy label detection approach for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Boosting facial expression recognition by a semi-supervised progressive teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Nlnl: Negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptively learning facial expression representation via cf labels and distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07394</idno>
		<title level="m">Dividemix: Learning with noisy labels as semisupervised learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning from noisy data with robust representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An exponential learning rate schedule for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07454</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P N</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01842</idno>
		<title level="m">Self: Learning to filter noisy labels with self-ensembling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dive into ambiguity: latent distribution mining and pairwise uncertainty estimation for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Towards universal representation learning for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mohd-Yusof</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10964</idno>
		<title level="m">Combating label noise in deep learning using abstention</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Suppressing uncertainties for largescale facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Region attention networks for pose and occlusion robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Adaptive wing loss for robust face alignment via heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Partial multi-label learning with noisy label identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">L_dmi: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Transfer: Learning relation-aware facial expression representations with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Purifynet: A robust person re-identification model with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIFS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Facial expression recognition with inconsistently annotated datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Understanding deep learning (still) requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weakly-supervised facial expression recognition in the wild with noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Relative uncertainty learning for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
