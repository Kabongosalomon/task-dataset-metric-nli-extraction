<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Capsule Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-08-26">26 Aug 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Graph Capsule Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-08-26">26 Aug 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Neural Networks (GCNNs) are the most recent exciting advancement in deep learning field and their applications are quickly spreading in multi-cross-domains including bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. In this paper, we expose and tackle some of the basic weaknesses of a GCNN model with a capsule idea presented in <ref type="bibr" target="#b13">(Hinton et al., 2011)</ref> and propose our Graph Capsule Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve especially graph classification problem which current GCNN models find challenging. Through extensive experiments, we show that our proposed Graph Capsule Network can significantly outperforms both the existing state-of-art deep learning methods and graph kernels on graph classification benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graphs are one of the most fundamental structures that have been widely used for representing many types of data. Learning on graphs such as graph semi-supervised learning, graph classification or graph evolution have found wide applications in domains such as bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. With remarkable successes of deep learning approaches in image classification and object recognition that attain "superhuman" performance, there has been a surge of research interests in generalizing convolutional neural networks (CNNs) to structures beyond regular grids, i.e., from 2D/3D images to arbitrary structures such as graphs <ref type="bibr" target="#b2">(Bruna et al., 2013;</ref><ref type="bibr" target="#b12">Henaff et al., 2015;</ref><ref type="bibr" target="#b6">Defferrard et al., 2016;</ref><ref type="bibr" target="#b15">Kipf &amp; Welling, 2016</ref>). These convolutional networks on graphs are now commonly known as Graph Convolutional Neural Networks (GCNNs). The principal idea behind graph convolution has been derived from the graph signal processing domain <ref type="bibr" target="#b32">(Shuman et al., 2013)</ref>, which has since been extended in different ways for a variety of purposes <ref type="bibr" target="#b7">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b11">Gilmer et al., 2017;</ref><ref type="bibr" target="#b20">Kondor et al., 2018)</ref>.</p><p>In this paper, we expose three major limitations of the standard GCNN model commonly used in existing deep learning approaches on graphs, especially when applied to the graph classification problem, and explore ways to overcome these limitations. In particular, we propose a new model, referred to as Graph Capsule Convolution Neural Networks <ref type="bibr">(GCAPS-CNN)</ref>. It is inspired by the notion of capsules developed in <ref type="bibr" target="#b13">(Hinton et al., 2011)</ref>: capsules are new types of neurons which encapsulate more information in a local pool operation (e.g., a convolution operation in a CNN) by computing a small vector of highly informative outputs rather than just taking a scalar output. Our graph capsule idea is quite general and can be employed in any version of GCNN model either design for solving graph semi-supervised problem or doing sequence learning on graphs via Graph Convolution Recurrent Neural Network models (GCRNNs).</p><p>The first limitation of the standard GCNN model is due to the basic graph convolution operation which is defined -in its purest form -as the aggregation of node values in a local neighborhood corresponding to each feature (or channel). As such, there is a potential loss of information associated with the basic graph convolution operation. This problem has been noted before <ref type="bibr" target="#b13">(Hinton et al., 2011)</ref>, but has not attracted much attention until recently <ref type="bibr" target="#b29">(Sabour et al., 2017)</ref>. To address this limitation, we propose to improve upon the basic graph convolution operation by introducing the notion of graph capsules which encapsulate more information about nodes in a local neighborhood, where the local neighborhood is defined in the same way as in the standard GCCN model. Similar to the original capsule idea proposed in <ref type="bibr" target="#b13">(Hinton et al., 2011)</ref>, this is achieved by replacing the scalar output of a graph convolution operation with a small vector output containing higher order statistical information per feature. Another source of inspiration for our proposed GCAPS-CNN model comes from one of the most successful graph kernels -the Weisfeiler-Lehman (WL)-subtree graph kernel <ref type="bibr" target="#b31">(Shervashidze et al., 2011)</ref> de-signed specifically for solving the graph classification problem. In WL-subtree graph kernel, node labels (features) are collected from neighbors of each node in a local neighborhood and compressed injectively to form a new node label in each iteration. The histogram of these new node labels are concatenated in each iteration to serve as a graph invariant feature vector. The important point to notice here is that due to the injection process, one can recover the exact node labels of local neighbors in each iteration without losing track of them. In contrast, this is not possible in the standard GCNN model as the input feature values of node neighbors are lost after the graph convolution operation.</p><p>The second major limitation of the standard GCNN model is specific to its (in)ability in tackling the graph classification problem. GCNN models cannot be applied directly because they are equivariant (not invariant) with respect to the node order in a graph. To be precise, consider a graph G with Laplacian L ? R N ?N and node feature matrix X ? R N ?d . Let f (X, L) ? R N ?h be the output function of a GCNN model where N, d, h are the number of nodes, input dimension and hidden dimension of node features, respectively. Then, f (X, L) is a permutation equivariant function, i.e., for any P permutation matrix f (PX, PLP T ) = Pf (X, L). This specific permutation equivariance property prevent us from directly applying GCNN to a graph classification problem, since it cannot provide any guarantee that the outputs of any two isomorphic graphs are always the same. Consequently, a GCNN architecture needs an additional graph permutation invariant layer in order to perform the graph classification task successfully. This invariant layer also needs to be differentiable for end-to-end learning.</p><p>Very limited amount of efforts has been devoted to carefully designing such an invariant GCNN model for the purpose of graph classification. Currently the most common method for achieving graph permutation invariance is performing aggregation (i.e., summing) over all graph node values <ref type="bibr" target="#b0">(Atwood &amp; Towsley, 2016;</ref><ref type="bibr" target="#b4">Dai et al., 2016;</ref><ref type="bibr" target="#b37">Zhao et al., 2018;</ref><ref type="bibr" target="#b33">Simonovsky &amp; Komodakis, 2017)</ref>. Though simple and fast, it can again incur significant loss of information. Likewise, using a max-pooling layer to achieve graph permutation invariance encounters similar issues. A few attempts have been made <ref type="bibr" target="#b20">Kondor et al., 2018</ref>) that go beyond aggregation or maxpooling in designing graph permutation invariant GCNNs. In  the authors propose a global ordering of nodes by sorting them according to their values in the last hidden layer. This type of invariance is based on creating an order among nodes and has also been explored before in <ref type="bibr" target="#b26">(Niepert et al., 2016)</ref>. However, as discussed in Section 4.1, we show that there are some issues with this type of approach. A more tangential approach has been adopted in <ref type="bibr" target="#b20">(Kondor et al., 2018)</ref> based on group theory to design transformation operations and tensor aggregation rules that results in permutation invariant outputs. However, this approach relies on computing high order tensors which are computationally expensive in many cases. To that end, we propose a novel permutation invariant layer based on computing the covariance of the data whose output does not depend upon the order of nodes in the graph. It is also fast to compute since it requires only a single dense-matrix multiplication operation.</p><p>Our last concern with the standard GCNN model is their limited ability in exploiting global information for the purpose of graph classification. The filters employed in graph convolutions are in essence local in nature and hence can only provide an "average/aggregate view" of the local data. This shortcoming poses a serious difficulty in handling graphs where node labels are not present; approaches which initialize (node) feature values using, e.g., node degree, are not much helpful in this respect. We propose to utilize global features (features that account for the full graph structure) using a family of graph spectral distances as proposed in <ref type="bibr" target="#b34">(Verma &amp; Zhang, 2017)</ref> to remedy this problem.</p><p>In summary, the major contributions of our paper are:</p><p>? We propose a novel Graph Capsule Convolution Neural Network model based on the capsule idea to capture highly informative output in a small vector in place of a scaler output currently employed in GCNN models.</p><p>? We develop a novel graph permutation invariant layer based on computing the covariance of data to solve graph classification problem. We show that it is a better choice than performing node aggregation or doing max pooling and at the same time it can be computed efficiently.</p><p>? Lastly, we advocate explicitly including global graph structure features at each graph node to enable the proposed GCAPS-CNN model to exploit them for graph learning tasks.</p><p>We organize our paper into five sections. We start with the related work on graph kernels and GCNNs in Section 2, and present our core idea behind graph capsules in Section 3.</p><p>In Section 4, we focus on building a graph permutation invariant layer especially for solving the graph classification problem. In Section 5, we propose to equip our GCAPS-CNN model with enhanced global features to exploit the full graph structure for learning on graphs. Lastly in Section 6 we conduct experiments and show the superior performance of our proposed GCAPS-CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are three main approaches for solving the graph classification problem. The most common approach is concerned with building graph kernels. In graph kernels, a graph G is decomposed into (possibly different) {G s } sub-structures. The graph kernel K(G 1 , G 2 ) is defined based on the frequency of each sub-structure appeared in G 1 and G 2 , respectively. Namely, K(G 1 , G 2 ) = f Gs 1 , f Gs 2 , where f Gs is the vector containing frequencies of {G s } sub-structures, and , is an inner product in an appropriately defined normed vector space. Much of work has been devoted to deciding on which sub-structures are more suitable than others. Among the existing graph kernels, popular ones are graphlets <ref type="bibr" target="#b28">(Pr?ulj, 2007;</ref><ref type="bibr" target="#b30">Shervashidze et al., 2009</ref>), random walk and shortest path kernels <ref type="bibr" target="#b14">(Kashima et al., 2003;</ref><ref type="bibr" target="#b1">Borgwardt &amp; Kriegel, 2005)</ref>, and Weisfeiler-Lehman subtree kernel <ref type="bibr" target="#b31">(Shervashidze et al., 2011)</ref>. Furthermore, deep graph kernels <ref type="bibr" target="#b35">(Yanardag &amp; Vishwanathan, 2015)</ref>, graph invariant kernels <ref type="bibr" target="#b27">(Orsini et al., 2015)</ref>, optimal assignment graph kernels <ref type="bibr" target="#b21">(Kriege et al., 2016)</ref> and multiscale laplacian graph kernel <ref type="bibr" target="#b18">(Kondor &amp; Pan, 2016)</ref> have been proposed with the goal to re-define kernel functions to appropriately capture sub-structural similarity at different levels. Another line of research in this area focuses on efficiently computing these kernels either through exploiting certain structure dependency, or via approximation or randomization <ref type="bibr" target="#b8">(Feragen et al., 2013;</ref><ref type="bibr" target="#b5">de Vries, 2013;</ref><ref type="bibr" target="#b25">Neumann et al., 2012)</ref>.</p><p>The second category involves constructing explicit graph features such as FGSD features in <ref type="bibr" target="#b34">(Verma &amp; Zhang, 2017)</ref> which is based on a family of graph spectral distances. It comes with certain theoretical guarantees. The Skew Spectrum of Graphs <ref type="bibr" target="#b16">(Kondor &amp; Borgwardt, 2008</ref>) based on group-theoretic approaches is another example in this category. Graphlet spectrum <ref type="bibr" target="#b19">(Kondor et al., 2009)</ref> improves upon this work by including labeled information; it also accounts for the relative position of subgraphs within a graph. However, the main concern with graphlet spectrum or skew spectrum is its computational O(N 3 ) complexity.</p><p>The third -more recent and perhaps more promising -approach to the graph classification is on developing convolutional neural networks (CNNs) for graphs. The original idea of defining graph convolution operations comes from the graph signal processing domain <ref type="bibr" target="#b32">(Shuman et al., 2013)</ref>, which has since been recognized as the problem of learning filter parameters that appear in the graph fourier transform in the form of a graph Laplacian <ref type="bibr" target="#b2">(Bruna et al., 2013;</ref><ref type="bibr" target="#b12">Henaff et al., 2015)</ref>.</p><p>Various GCNN models such a <ref type="bibr" target="#b15">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b0">Atwood &amp; Towsley, 2016;</ref><ref type="bibr" target="#b7">Duvenaud et al., 2015)</ref> have been proposed, where traditional graph filters are replaced by a self-loop graph adjacency matrix and the outputs of each neural network layer output are computed using a propagation rule while updat-ing the network weights. The authors in <ref type="bibr" target="#b6">(Defferrard et al., 2016)</ref> extend such GCNN models by utilizing fast localized spectral filters and efficient pooling operations. A very different approach is proposed in <ref type="bibr" target="#b26">(Niepert et al., 2016)</ref> where a set of local nodes are converted into a sequence in order to create receptive fields which are then fed into a 1D convolutional neural network.</p><p>Another popular name for GCNNs is message passing neural networks (MPNNs) <ref type="bibr" target="#b22">(Lei et al., 2017;</ref><ref type="bibr" target="#b11">Gilmer et al., 2017;</ref><ref type="bibr" target="#b4">Dai et al., 2016;</ref><ref type="bibr" target="#b9">Garc?a-Dur?n &amp; Niepert, 2017)</ref> . Though the authors in <ref type="bibr" target="#b11">(Gilmer et al., 2017)</ref> suggests that GCNNs are a special case of MPNNs, we believe that both are equivalent models in a certain sense; it is simply a matter of how the graph convolution operation is defined. In MPNNs the hidden states of each node is updated based on messages received from its neighbors as well as the values of the previous hidden states in each iteration. This is made possible by replacing traditional neural networks in GCNN with a small recurrent neural network (RNN) with the same weight parameters shared across all nodes in the graph. Note that here the number of iterations in MPNNs can be related to the depth of a GCNN model. In <ref type="bibr" target="#b33">(Simonovsky &amp; Komodakis, 2017)</ref> the authors propose to condition the learning parameters of filters based on edges rather than on traditional nodes. This approach is similar to some instances of MPNNs such as in <ref type="bibr" target="#b11">(Gilmer et al., 2017)</ref> where learning parameters are also associated with edges. All the above MPNNs models employ aggregation as the graph permutation invariant layer for solving the graph classification problem. In contrast, the authors in <ref type="bibr" target="#b20">Kondor et al., 2018)</ref> employs a max-sort pooling layer and group theory to achieve graph permutation invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph Capsule CNN Model</head><p>Basic Setup and Notations: Consider a graph G = (V, E, A) of size N = |V |, where V is the vertex set, E the edge set (with no self-loops) and A = [a ij ] the weighted adjacency matrix. The standard graph Laplacian is defined</p><formula xml:id="formula_0">as L = D ? A ? R N ?N , where D is the degree matrix. Let X ? R N ?d be the node feature matrix, where d is the input dimension.</formula><p>When used, we will use h to denote the dimension of hidden (latent) variables/feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General GCNN Model:</head><p>We start by describing a general GCNN model before presenting our Graph Capsule CNN model. Let G be a graph with graph Laplacian L and X ? R N ?d be a node feature matrix. Then the most general form of a GCNN layer output function f (X, L) ? R N ?h equipped with polynomial filters is given by Equation <ref type="formula" target="#formula_3">(1)</ref>,</p><formula xml:id="formula_1">0 1 2 3 [x ?=0 0 ] [x 2 ] [x 1 ] [x 3 ] 0 1 2 3 [x 2 ] [x 1 ] [x 3 ]</formula><p>Applying Graph Capsule Function at node 0 A Capsule Vector (for example containing moments)</p><formula xml:id="formula_2">[x ?=1 0 ] = 1 |N 0 | ? ? ? ? ? ? ? ? k a 0k x k (mean) k a 0k (x k ? ?) 2 (std.) k a 0k ( x k ?? ? ) 3 (skewness) . . . ? ? ? ? ? ? ? ? Figure 1.</formula><p>Above figure shows that the graph capsule function at node 0 computes a capsule vector which encodes higherorder statistical information about its local neighboorhood (per feature). Here {x 0 , x 1 , x 2 , x 3 } are respective node feature values. For example, when a node has no more than two neighbors then it is possible to recover back the input node neighbors values from the very first three statistical moments.</p><formula xml:id="formula_3">f (X, L) = ? X LX . . . L k X g(X,L) ? ? ? ? ? W 1 W 2 . . . W k ? ? ? ? ? learning weight parameters = ? K k=0 L k XW k<label>(1)</label></formula><p>In Equation <ref type="formula" target="#formula_3">(1)</ref>, g(X, L) ? R N ?kd is defined as a graph convolution filter of polynomial form with degree k. While</p><formula xml:id="formula_4">[W 1 , W 2 , ..., W k ] are learning weight parameters where each W k ? R d?h .</formula><p>Note that g(X, L) = [X, LX, ..., L K X] ? R N ?kd can be seen as a new node feature matrix with extended dimension kd 1 . Furthermore, L can be replaced by any other suitable filter matrix as discussed in <ref type="bibr" target="#b23">(Levie et al., 2017;</ref><ref type="bibr" target="#b15">Kipf &amp; Welling, 2016)</ref>.</p><p>A GCNN model with a depth of L layers can be expressed recursively as,</p><formula xml:id="formula_5">f (?) (X, L) = ? g(f (??1) (X, L), L)W (?)<label>(2)</label></formula><p>where W ? ? R kd?h is the weight parameter matrix for the ? th ?layer, 1 ? l ? L.</p><p>One can notice that in any layer the basic computation expression involve is [L k f (??1) (X, L)] ij . This expression represents that the new j th feature value of i th node (associated with the i th row) is yielded out as a single <ref type="formula">(scalar)</ref> 1 Also referred to as the breadth of a GCNN layer .</p><p>aggregated value based on its local-hood neighbors. This particular operation can incur significant loss of information. We aim to remedy this issue by introducing our novel GCAPS-CNN model based on the fundamental capsule idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Capsule Networks</head><p>The core idea behind our proposed graph capsule convolutional neural network is to capture more information in a local node pool beyond what is captured by aggregation, the graph convolution operation used in a standard GCCN model. This new information is encapsulated in so-called instantiation parameters described in <ref type="bibr" target="#b13">(Hinton et al., 2011)</ref> which forms a capsule vector of highly informative outputs.</p><p>The quality of these parameters are determined by their ability to encode the node feature values in a local neighborhood of each node as well decode (i.e., to reconstruct) them from the capsule vector. For instance, one can take the histogram of neighborhood feature values as the capsule vector. If histogram bandwidth is sufficiently small, we can guarantee to recover back all the original input node values. This strategy has been used in constructing a successful graph kernel. However, as histogram is not a continuous differentiable function, it cannot be employed in backpropagation for end-to-end deep learning.</p><p>Beside seeking representative instantiation parameters, we further impose two more constraints on a graph capsule function. First, we want our graph capsule function to be permutation invariant (unlike equivariant as discussed in <ref type="bibr" target="#b13">(Hinton et al., 2011)</ref>) with respect to the input node order since we are interested in a model that can produce the same output for isomorphic graphs. Second, we would like to be able to compute these parameters efficiently.</p><p>Graph Capsule Function: To describe a general graph capsule function, consider an i th node with x 0 value and the set of its neighborhood node values as N (i) = {x 0 , x 1 , x 2 , ..., x k } including itself. In the standard graph convolution operation, the output is a scalar function f : R k ? R which takes k input neighbors at the i th node and yields an output given by</p><formula xml:id="formula_6">f i (x 0 , x 1 , ..., x k ) = 1 |N (i)| k?N (i) a ik x k<label>(3)</label></formula><p>where a ik represents edge weights between nodes i and k.</p><p>In our graph capsule network, we replace f (x 0 , ..., x k ) with a vector-valued capsule function f : R k ? R p . For example, consider a capsule function that captures higherorder statistical moments as follows (for simplicity, we omit the mean and standard deviation),</p><formula xml:id="formula_7">f i (x 0 , ..., x k ) = 1 |N (i)| ? ? ? ? ? ? ? ? ? k?N (i) a ik x k k?N (i) a ik x 2 k . . . k?N (i) a ik x p k ? ? ? ? ? ? ? ? ?</formula><p>(4) <ref type="figure">Figure 1</ref> shows an instance of applying our graph capsule function on a specific node. Consequently, for an input feature matrix X ? R N ?d , our graph capsule network will produce an output f (X, L) ? R N ?h?p where p is the number of instantiation parameters.</p><p>Managing Graph Capsule Vector Dimension: In the first layer, our graph capsule network receives an input X ? R N ?d and produces a non-linear output f (1) (X, L) ? R N ?h1?p . Since our graph capsule function produces a vector of p dimension (for each input d dimension), the feature dimension of the output in subsequent layers can quickly blow up to an unmanageable value. To keep it in check, we restrict the feature dimension of the output f (?) (X, L) to be always ? R N ?h ? ?p at any middle ? th ?layer of a GCAP-CNN (here h ? represents the hidden dimension of that layer). This can be accomplished in two ways 1) either by flattening the last two dimension of f (X, L) and carrying out graph convolution in usual way (see Equation 5 for an example) 2) or by taking the weighted combination of p?dimension capsule vectors (this is similar to performing attention mechanism) at each node as performed in <ref type="bibr" target="#b29">(Sabour et al., 2017)</ref>. We leave the second approach for our future work. Thus in a nutshell, our graph capsule network in ? th ?layer (? &gt; 1) receives an input f (??1) (X, L) ? R N ?h ??1 ?p and produces an output f (?) (X, L) ? R N ?h ? ?p .</p><p>Graph Capsule Function with Statistical Moments: In this paper, we consider higher-order statistical moments as instantiation parameters because they are permutationally invariant and can nicely be computed through matrixmultiplication operations in a fast manner. To see exactly how, let f p (X, L) be the output matrix corresponding to p th dimension. Then, we can compute f (?) p (X, L) containing statistical moments as instantiation parameters as follows,</p><formula xml:id="formula_8">f (?) p (X, L) = ? K k=0 L k (f (??1) F (X, L) ? ... ? f (??1) F (X, L) p times )W (?) pk<label>(5)</label></formula><p>where ? is a hadamard product. Here to keep the feature dimensions in check from growing, we flatten the last two dimension of the input as f (??1)</p><p>F lat (X, L) ? R N ?h ??1 p and performs usual graph convolution operation followed by a linear transformation with W (?) pk ? R h ??1 p?h ? as the learning weight parameter. Note that here p is used to denote both the capsule dimension as well the order of statistical moments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Capsule Function with Polynomial Coefficients:</head><p>As mentioned earlier, the quality of instantiation parameters depend upon their capability to encode and decode the input values. Therefore, we seek capsule functions which are bijective in nature i.e., guaranteed to preserve everything about the local neighborhood. For instance, one consider coefficients of polynomial as instantiation parameters by taking the set of local node feature values as roots,</p><formula xml:id="formula_9">f i (?) = 1 |N (i)| ? ? ? ? ? ? ? ? ? ? ? k?N (i) x k k1,k2?N (i) x k1 x k2 k1,k2,k3?N (i) x k1 x k2 x k3 . . . x 0 x 1 . . . x k?1 x k ? ? ? ? ? ? ? ? ? ? ?<label>(6)</label></formula><p>One can show that from a given full set of polynomial coefficients, we are guaranteed to recover back all the original node values (upto permutation). However, the first issue with this approach is that they are expensive to compute at each node. Specifically, a combinatorial algorithm without fast fourier transform takes O(k 2 ) complexity to compute where k is the number of roots. Also, there is numerical instability issue associated with computing polynomial coefficients. There are ways to deal with these kind issues but we leave pursuing this direction for our future work.</p><p>In short, our graph capsule idea is powerful and can be employed in any type of GCNN model for either solving graph semi-supervised learning problem or performing sequence learning on graphs using Graph Recurrent Neural Network models (GCRNNs) or doing link prediction via Graph Autoencoders (GAEs) or/and for generating synthetic graphs through Graph Generative Adversarial models (GGANs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Designing Graph Permutation Invariant Layer</head><p>In this section, we focus on the second limitation of GCNN model regarding achieving permutation invariance for graph classification purpose. Before presenting our novel invariant layer in GCAPS-CNN model, we first discuss the shortcomings of Max-Sort Pooling Layer which is the next popular choice after aggregation for achieving invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problems with Max-Sort Pooling Layer</head><p>We design a test to determine whether the invariant graph feature constructed by a model has any degree of certainty to produce the same output for sub-graph isomers or not.</p><p>Sub-Graph Isomorphism Feature Test: Consider two graphs G 1 = (V 1 , E 1 ) and G 2 = (V 2 , E 2 ) such that G 1 is isomorphic to a sub-graph of G 2 . Let f 1 , f 2 ? R k be the invariant feature vector (w.r.t. to graph isomorphism) of G 1 , G 2 respectively. Then, we define sub-graph isomorphism feature test as a criteria providing guarantee that each elements of f 1 and f 2 are comparable under certain notion i.e., f 1i ? f 2i for any i ? [1, k]. Here ? represents a comparison operator defined in a sensible way. Satisfying this test is very desirable for graph classification problem since it is quite likely that sub-graph isomers of a graph belong to the same class label. This property helps the model to learn w i weight parameter appropriately which is shared across the same input place i.e., f 1i and f 2i .</p><p>Proposition 1 Let f 1 , f 2 ? R k be the feature vectors containing top k ? max node values in sorted order for graphs G 1 , G 2 respectively and given G 1 is sub-graph isomorphic to G 2 . Then the Max-Sort Pooling Layer fails the Subgraph Isomorphism Feature Test owing to the comparison done with respect to node ordering.</p><p>Remarks: Max-Sort Pooling layer fails the test because it does not guarantee that f 1i ? f 2i for any i ? [1, k]. Here ? (not comparable) operator represents that the node corresponding to values f 1i and f 2i may not be the same in sub-graph isomers. Even including a single node (value) in f 2 vector which is not present in G 1 can mess up the whole comparision order of f 1 and f 2 elements. As a result, in Max-Sort Pooling layer the comparison is not always guar-anteed to be sensible which makes the problem of learning weight parameters harder. In general, any invariant graph feature vector that relies on node ordering will fail this test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Covariance as Permutation Invariant Layer</head><p>Our novel idea of permutation invariant features in GCAPS-CNN model is computing the covariance of f (X, L) layer output given as follows,</p><formula xml:id="formula_10">C(f (X, L)) = 1 N (f (X, L) ? ?) T (f (X, L) ? ?) (7)</formula><p>Here ? is the mean of f (X, L) output and C(?) is a covariance function. Since covariance function is differentiable and does not depends upon the order of row elements, it can serve as a permutation invariant layer in GCAPS-CNN model. Also, it is fast in computation due to a single matrix-multiplication operation. Note that we flatten the last two dimension of GCAPS-CNN layer output f (X, L) ? R N ?h?p in order to compute the covariance.</p><p>Moreover, covariance provides much richer information about the data by including shapes, norms and angles (between node hidden features) information rather than just providing the mean of data. Infact in multivariate normal distribution, it is used as a statistical parameter to approximate the normal density and thus also reflects information about the data distribution. This particular property along with invariance has been exploited before in <ref type="bibr" target="#b17">(Kondor &amp; Jebara, 2003)</ref> for computing similarity between two set of vectors. One can also think about fitting multivariate normal distribution on f (X, L) but it involves computing inverse of covariance matrix which is computationally expensive.</p><p>Since each element of covariance matrix is invariant to node orders, we can flatten the symmetric covariance matrix C ? R hp?hp to construct the graph invariant feature vector f ? R (hp+1)hp/2 . On an another positive note, here the output dimension of f does not depend upon N number of nodes and can be adjusted according to computational constraints.</p><p>Proposition 2 Let f 1 , f 2 ? R k be the feature vectors containing covariance elements of node feature matrices for graphs G 1 , G 2 respectively and given G 1 is sub-graph isomorphic to G 2 . Then the covariance invariant layer pass the Sub-Graph Isomorphism Feature Test owing to the comparison done with respect to feature dimensions.</p><p>Remarks: It is quite straightforward to see that the feature dimension order of a node does not depend upon the graph node ordering and hence the order is same across all graphs. As a result, each elements of f 1 and f 2 are always compara-ble. To be more specific, covariance output compares both the norms sand angles between the corresponding pairs of feature dimension vectors in two graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Designing GCAP-CNN with Global Features</head><p>Besides guaranteeing permutation invariance in GCAP-CNN model, another important desired characteristic of graph classification model is to capture global structure (or features) of a graph. For instance, considering only node degree (as a node feature) is a local information and not much helpful towards solving graph classification problem.</p><p>On the other hand, considering spectral embedding as a node feature takes global piece of information into account and have been proven successful in serving as a node vector for problems dealing with graph semi-supervised learning. We define global features that takes full graph structure into account during their computation. While local features only depend upon some (at-most) k?hop node neighbors.</p><p>Unfortunately, the basic design of GCNN model can only capture local structure information of the graph at each node. We make this loose statement more concrete with the following theorem.</p><p>Theorem 1 : Let G be a graph with L ? R N ?N graph Laplacian and X ? R N ?d node feature matrix. Let f (?) (X, L) be the output function of a ? th GCNN layer equipped with polynomial filters of degree k. Then [f (?) (X, L)] i output at i th node (i.e., i th row in f (?) (?)) depends upon "only" on the input values of neighbors distant at most "k??hops" away.</p><p>Proof: We can proof this statement by mathematical induction. It is easy to see that the base case ? = 1 holds true. Lets assume it also holds true for f (??1) (X, L) i.e., i th node output depends upon neighbors distant upto k ? (? ? 1) hop away. Then in f (?) (X, L) = ? g f (??1) (X, L), L W (?) we focus on the term,</p><formula xml:id="formula_11">g(X, L) = [f (??1) (X, L), . . . , L k f (??1) (X, L)] (8)</formula><p>particularly the last term involving L k f (??1) (X, L). Matrix multiplication of L k with f (??1) (X, L) will result in i th node to include all node information which are at-most k?hop distance away. But since a node in f (??1) (X, L) at a distance k?hops (from i th node) can contain information upto k ? (? ? 1) hops, we have i th node containing information at-most k + k(? ? 1) = k? hops distance away.</p><p>Remarks: Above theorem 1 establishes that GCNN model with ? layers can capture only k??hop local-hood structure information at each node. Thus, employing GCNN for graph classification with say aggregation layer can capture only average variation of k??hop local-hood information over the whole graph. To include more global information about the graph one can either increase k (i.e, choose higher order graph convolution filters) or ? (i.e, the depth of GCNN model). Both these choices increases model complexity and thus would require more data samples to reach satisfying results. However among the two, we prefer increasing the depth of GCNN model because the first choice leads to increase in the breadth of the GCNN layer (see footnote 1 about g(X, L) in Section 3) and based on the current understanding of deep learning theory, increasing the depth is favored more over the breadth.</p><p>For cases where graph node features are missing, it is a common practice to take node degree as a node feature. Such practices can work for problems like graph semisupervised where local-structure information drives node output labels (or classes). But in graph classification global features governs the output labels and hence taking node degree is not sufficient. Of course, we can go for a very deep GCNN model that will allows us to exploit more global information but requires higher sample complexity to achieve satisfying results.</p><p>To balance the two (model complexity with depth vs. required sample complexity), we propose to incorporate FGSD features in our GCAP-CNN model computed at each node. As shown in <ref type="bibr" target="#b34">(Verma &amp; Zhang, 2017</ref>) FGSD features capture global information about the graph and can also be computed in fast manner. Specifically, at each i th node FGSD features are computed as the histogram of the multiset formed by taking the harmonic distance between all nodes and the i th node. It is given by,</p><formula xml:id="formula_12">S(x, y) = N ?1 n=0 1 ? n (? n (x) ? ? n (y)) 2<label>(9)</label></formula><p>where S(x, y) is the harmonic distance, x, y are any graph nodes and ? n , ? n (?) is the n th eigenvalue and eigenvector respectively.</p><p>In our experiments, we employ these features only for datasets where node feature are missing (specifically for social network datasets in our case). Although this strategy can always be used by concatenating FGSD features with original node feature values to capture more global information. Further inspired from Weisfeiler-lehman graph kernel <ref type="bibr" target="#b31">(Shervashidze et al., 2011)</ref> which also concatenate features in each labeling iteration, we also propose to pass concatenated outputs from intermediate layers to our covariance and fully connected layers. Finally, our whole end-to-end GCAP-CNN learning model is guaranteed to produce the same output for isomorphic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiment and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCAPS-CNN Model Configuration:</head><p>We build ? layer GCAPS-CNN with following configuration:</p><formula xml:id="formula_13">Input ? GC(h, p) ? ? ? ? ? GC(h, p) ? [M, C(?)] ? F C(h) ? F C(h) ? Sof tmax.</formula><p>Here GC(h, p) represents a Graph Capsule CNN layer with h hidden dimensions and p instantiation parameters. As mentioned earlier, we take the intermediate output of each GC(h, p) layers and form a concatenated tensor which is subsequently pass through [M, C(?)] layer which computes mean and covariance of the input. Output of [M, C(?)] layer is then passed to two fully connected F C layers with again h output dimensions and finally connects to a softmax layer for computing class probabilities. In between intermediate layers, we use batch normalization and dropout technique to prevent overfitting along with L2 norm regularization. We set ? ? {2, 3, 4} depending upon the dataset size (towards higher for larger dataset) and h ? {32, 64, 128} for setting hidden dimension. We restrict p ? [1, 4] for computing higher-order statistical moments due to computational constraints. Further, we employ ADAM optimization technique with initial learning rate chosen from the set {10 ?1 , . . . , 10 ?7 } with a decaying factor of 0.1 after every few epochs. Batch size is set according to the given dataset size and memory requirements. Number of epochs are chosen from the set {100, 200, 500, 1000}. All the above mentioned hyperparameters are tuned based on the training loss. Average classification accuracy based on 10?fold cross validation error is reported for each dataset. Our GCAPS-CNN code and data will be made available at Github 2 .</p><p>Datasets: To evaluate our GCAPS-CNN model, we perform graph classification tasks on variety of benchmark datasets. In first round, we used 6 bioinformatics datasets namely: PTC, PROTEINS, NCI1, NCI109, D&amp;D, and ENZYMES. In second round, we used 5 social network datasets namely: COLLAB, IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY and REDDIT-MULTI-5K. D&amp;D dataset contains 691 enzymes and 587 nonenzymes proteins structures. For other datasets details can be found in <ref type="bibr" target="#b35">(Yanardag &amp; Vishwanathan, 2015)</ref>. Also for each dataset number of graphs, maximum and average number of nodes is shown in the <ref type="table" target="#tab_2">Table 1 and Table 2</ref>.</p><p>Experimental Set-up: All experiments were performed on a single machine loaded with recently launched 2?NVIDIA TITAN VOLTA GPUs and 64 GB RAM. We compare our method with both deep learning models and graph kernels.</p><p>Deep Learning Baselines: For deep learning approaches, we adopted 4 recently proposed state-of-art graph convolutional neural networks namely: PATCHY-SAN (PSCN) <ref type="bibr" target="#b26">(Niepert et al., 2016)</ref>, Diffusion CNNs (DCNN) [ <ref type="bibr" target="#b0">(Atwood &amp; Towsley, 2016)</ref>], Dynamic Edge CNN (ECC) <ref type="bibr" target="#b33">(Simonovsky &amp; Komodakis, 2017)</ref> and Deep Graph CNN (DGCNN) .</p><p>Graph Kernel Baselines: We adopted 6 state-ofart graphs kernels for comparison namely: Random Walk (RW) <ref type="bibr" target="#b10">(G?rtner et al., 2003)</ref>, Shortest Path Kernel (SP) <ref type="bibr" target="#b1">(Borgwardt &amp; Kriegel, 2005)</ref>, Graphlet Kernel (GK) , Weisfeiler-Lehman Sub-tree Kernel (WL) <ref type="bibr" target="#b31">(Shervashidze et al., 2011)</ref>, Deep Graph Kernels (DGK) <ref type="bibr" target="#b35">(Yanardag &amp; Vishwanathan, 2015)</ref> and Multiscale Laplacian Graph Kernels (MLK) <ref type="bibr" target="#b18">(Kondor &amp; Pan, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines Settings:</head><p>We adopted the same procedure from previous works <ref type="bibr" target="#b26">(Niepert et al., 2016;</ref><ref type="bibr" target="#b35">Yanardag &amp; Vishwanathan, 2015;</ref><ref type="bibr" target="#b36">Zhang et al., 2018)</ref> to make a fair comparison and used 10-fold cross validation with LIBSVM <ref type="bibr" target="#b3">(Chang &amp; Lin, 2011)</ref> library to report the classification performance for graph kernels. Parameters of SVM are independently tuned using training folds data and best average classification accuracies are reported for each method. For Random-Walk (RW) kernel, decay factor is chosen from {10 ?6 , 10 ?5 ..., 10 ?1 }. For Weisfeiler-Lehman (WL) kernel, we chose height of subtree kernel from h ? {2, 3, 4}. For graphlet kernel (GK), we chose graphlets size {3, 5, 7} and for deep graph kernels (DGK), we report the best classification accuracy obtained among: deep graphlet kernel, deep shortest path kernel and deep Weisfeiler-Lehman kernel. For Multiscale Laplacian Graph (MLG) kernel, we chose ? and ? parameter of the algorithm from {0.01, 0.1, 1}, radius size from {1, 2, 3, 4}, and level number from {1, 2, 3, 4}. For diffusion-convolutional neural networks (DCNN), we chose number of hops from {2, 5}. For the rest, best reported results were borrowed from papers PATCHY-SAN (k = 10) <ref type="bibr" target="#b26">(Niepert et al., 2016)</ref>, ECC <ref type="bibr" target="#b33">(Simonovsky &amp; Komodakis, 2017</ref>) (without edge labels since all other methods also relies on only node labels) and DGCNN (with sorting layer) , since the experimental setup was the same and a fair comparison can be made. In short, we follow the same procedure as mentioned in previous papers. Note: some results are not present because either they are not previously reported or source code not available to run them.  with graph kernels. '&gt; 1 day' represents that the computation exceed more than 24hrs. 'OMR' is out of memory error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Classification Results: From</head><p>Again, this trend is continued to be the same on social network datasets as shown in <ref type="table" target="#tab_2">Table 2</ref>. Here, we were able to achieve upto 4% accuracy gain on COLLAB dataset and rest were around 1% gain with consistency when compared against other deep learning approaches.</p><p>Our GCAPS-CNN is also very competitive with state-ofart graph kernel methods. It again show a consistent performance gain of 1% ? 3% accuracy (highest being on PTC dataset) on many bioinformatic datasets when compared against with strong graph kernels. While other considered deep learning methods are not even close enough to beat graph kernels on many of these datasets. It is worth mentioning that the most deep learning models (like ours) are also scalable while graph kernels are more fine tuned towards handling small graphs.</p><p>For social network datasets, we have a significant gain of atleast 4% ? 9% accuracy (highest being on REDDIT-MULTI dataset) against graph kernels as observed in <ref type="table" target="#tab_2">Table 2</ref>. But this is expected as deep learning methods tend to do better with the large amount of data available for training on social networks datasets. Altogether, our GCAPS-CNN model shows very promising results against both the current state-of-art deep learning methods and graph kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion &amp; Future Work</head><p>In this paper, we present a novel Graph Capsule Network (GCAPS-CNN) model based on the fundamental capsule idea to address some of the basic weaknesses of existing GCNN models. Our graph capsule network model by design captures more local structure information than traditional GCNN and can provide much richer representation of individual graph nodes or for the whole graph. For our purpose, we employ a capsule function that preserves statistical moments formation since they are faster to compute.</p><p>Furthermore, we propose a novel permutation invariant layer based on computing covariance in our GCAPS-CNN architecture to deal with graph classification problem which most GCNN models find challenging. This covariance can again be computed in a fast manner and has shown to be better than adopting aggregation or max-sort pooling layer. On the top, we also propose to equip our GCAPS-CNN model with FGSD features explicitly to capture more global information in absence of node features. This is essential to consider since non-deep GCNN models are not capable enough to exploit global information implicitly. Finally, we show GCAPS-CNN superior performance on many bioinformatics and social network datasets in comparison with existing deep learning methods as well as strong graph kernels and set the current state-of-the-art.</p><p>Our general idea of graph capsule is quite rich and can taken to another level by designing more sophisticated capsule functions that are capable of preserving more information in a local pool. In our future work, we will investigate various other capsule functions such as polynomial coefficients (as instantiation parameters) which comes with theoretical guarantees. Another choice, we will investigate is performing kernel density estimation technique in end-toend deep learning framework and understanding their theoretical significance. Lastly, we will also explore the other approach of managing the graph capsule vector dimension as discussed in <ref type="bibr" target="#b29">(Sabour et al., 2017)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Department of Computer Science, Univeristy of Minnesota Twin Cities USA. Correspondence to: Saurabh Verma &lt;verma076@cs.umn.edu&gt;, Zhi-Li Zhang &lt;zhzhang@cs.umn.edu&gt;. Joint ICML and IJCAI Workshop on Computational Biology, Stockholm, Sweden, 2018.</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 Table 1 .</head><label>11</label><figDesc>, it is clear that our GCAPS-CNN model consistently outperforms most of the considered deep learning methods on bioinformatics datasets (except on D&amp;D dataset) with a significant margin of 1% ? 6% classification accuracy gain (highest being on NCI1 dataset). Classification accuracy on bioinformatics datasets. Result in bold indicates the best reported classification accuracy. Top half of the table compares results with various deep learning approaches while bottom half compares results with graph kernels. '&gt; 1 day' represents that the computation exceed more than 24hrs. 'OMR' is out of memory error.</figDesc><table><row><cell>Dataset</cell><cell>PTC</cell><cell></cell><cell>PROTEINS</cell><cell>NCI1</cell><cell></cell><cell>NCI109</cell><cell>D &amp; D</cell><cell>ENZYMES</cell></row><row><cell>(No. Graphs)</cell><cell>344</cell><cell></cell><cell>1113</cell><cell>4110</cell><cell></cell><cell>4127</cell><cell>1178</cell><cell>600</cell></row><row><cell>(Max. Graph Size)</cell><cell>109</cell><cell></cell><cell>620</cell><cell>111</cell><cell></cell><cell>111</cell><cell>5748</cell><cell>126</cell></row><row><cell>(Avg. Graph Size)</cell><cell>25.56</cell><cell></cell><cell>39.06</cell><cell>29.80</cell><cell></cell><cell>29.60</cell><cell>284.32</cell><cell>32.60</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Deep Learning Methods</cell></row><row><cell>DCNN[2016]</cell><cell>56.60 ? 2.89</cell><cell></cell><cell>61.29 ? 1.60</cell><cell cols="2">56.61 ? 1.04</cell><cell cols="2">57.47 ? 1.22</cell><cell>58.09 ? 0.53</cell><cell>42.44 ? 1.76</cell></row><row><cell>PSCN[2016]</cell><cell>62.29 ? 5.68</cell><cell></cell><cell>75.00 ? 2.51</cell><cell cols="2">76.34 ? 1.68</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ECC[2017]</cell><cell>-</cell><cell></cell><cell>-</cell><cell>76.82</cell><cell></cell><cell>75.03</cell><cell>72.54</cell><cell>45.67</cell></row><row><cell>DGCNN[2018]</cell><cell>58.59 ? 2.47</cell><cell></cell><cell>75.54 ? 0.94</cell><cell cols="2">74.44 ? 0.47</cell><cell cols="2">75.03 ? 1.72</cell><cell>79.37 ? 0.94</cell><cell>51.00 ? 7.29</cell></row><row><cell>GCAPS-CNN</cell><cell>66.01 ? 5.91</cell><cell cols="2">76.40 ? 4.17</cell><cell cols="2">82.72 ? 2.38</cell><cell cols="2">81.12 ? 1.28</cell><cell>77.62 ? 4.99</cell><cell>61.83 ? 5.39</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Graph Kernels</cell><cell></cell></row><row><cell>RW[2003]</cell><cell>57.85 ? 1.30</cell><cell></cell><cell>74.22 ? 0.42</cell><cell cols="2">&gt; 1 Day</cell><cell>&gt; 1 Day</cell><cell>&gt; 1 Day</cell><cell>24.16 ? 1.64</cell></row><row><cell>SP[2005]</cell><cell>58.24 ? 2.44</cell><cell></cell><cell>75.07 ? 0.54</cell><cell cols="2">73.00 ? 0.24</cell><cell cols="2">73.00 ? 0.21</cell><cell>&gt; 1Day</cell><cell>40.10 ? 1.50</cell></row><row><cell>GK[2009]</cell><cell>57.26 ? 1.41</cell><cell></cell><cell>71.67 ? 0.55</cell><cell cols="2">62.28 ? 0.29</cell><cell cols="2">62.60 ? 0.19</cell><cell>78.45 ? 1.11</cell><cell>26.61 ? 0.99</cell></row><row><cell>WL [2011]</cell><cell>57.97 ? 0.49</cell><cell></cell><cell>74.68 ? 0.49</cell><cell cols="2">82.19 ? 0.18</cell><cell cols="2">82.46 ? 0.24</cell><cell>79.78 ? 0.36</cell><cell>52.22 ? 1.26</cell></row><row><cell>DGK[2015]</cell><cell>60.08 ? 2.55</cell><cell></cell><cell>75.68 ? 0.54</cell><cell cols="2">80.31 ? 0.46</cell><cell cols="2">80.32 ? 0.33</cell><cell>73.50 ? 1.01</cell><cell>53.43 ? 0.91</cell></row><row><cell>MLG[2016]</cell><cell>63.26 ? 1.48</cell><cell></cell><cell>76.34 ? 0.72</cell><cell cols="2">81.75 ? 0.24</cell><cell cols="2">81.31 ? 0.22</cell><cell>78.18 ? 2.56</cell><cell>61.81 ? 0.99</cell></row><row><cell>GCAPS-CNN</cell><cell>66.01 ? 5.91</cell><cell></cell><cell>76.40 ? 4.17</cell><cell cols="2">82.72 ? 2.38</cell><cell cols="2">81.12 ? 1.28</cell><cell>77.62 ? 4.99</cell><cell>61.83 ? 5.39</cell></row><row><cell>Dataset</cell><cell>COLLAB</cell><cell></cell><cell cols="2">IMDB-BINARY</cell><cell cols="2">IMDB-MULTI</cell><cell>REDDIT-BINARY</cell><cell>REDDIT-MULTI</cell></row><row><cell>(No. Graphs)</cell><cell>5000</cell><cell></cell><cell>1000</cell><cell></cell><cell></cell><cell>1500</cell><cell>2000</cell><cell>5000</cell></row><row><cell>(Max. Graph Size)</cell><cell>492</cell><cell></cell><cell>136</cell><cell></cell><cell></cell><cell>89</cell><cell>3783</cell><cell>3783</cell></row><row><cell>(Avg. Graph Size)</cell><cell>74.49</cell><cell></cell><cell>19.77</cell><cell></cell><cell></cell><cell>13.00</cell><cell>429.61</cell><cell>508.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Deep Learning Methods</cell></row><row><cell>DCNN[2016]</cell><cell>52.11 ? 0.71</cell><cell></cell><cell cols="2">49.06 ? 1.37</cell><cell cols="2">33.49 ? 1.42</cell><cell>OMR</cell><cell>OMR</cell></row><row><cell>PSCN[2016]</cell><cell>72.60 ? 2.15</cell><cell></cell><cell cols="2">71.00 ? 2.29</cell><cell cols="2">45.23 ? 2.84</cell><cell>86.30 ? 1.58</cell><cell>49.10 ? 0.70</cell></row><row><cell>DGCNN[2018]</cell><cell>73.76 ? 0.49</cell><cell></cell><cell cols="2">70.03 ? 0.86</cell><cell cols="2">47.83 ? 0.85</cell><cell>76.02 ? 1.73</cell><cell>48.70 ? 4.54</cell></row><row><cell>GCAPS-CNN</cell><cell cols="2">77.71 ? 2.51</cell><cell cols="2">71.69 ? 3.40</cell><cell cols="2">48.50 ? 4.10</cell><cell>87.61 ? 2.51</cell><cell>50.10 ? 1.72</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Graph Kernels</cell><cell></cell></row><row><cell>GK[2009]</cell><cell>72.84 ? 0.28</cell><cell></cell><cell cols="2">65.87 ? 0.98</cell><cell cols="2">43.89 ? 0.38</cell><cell>77.34 ? 0.18</cell><cell>41.01 ? 0.17</cell></row><row><cell>DGK[2015]</cell><cell>73.09 ? 0.25</cell><cell></cell><cell cols="2">66.96 ? 0.56</cell><cell cols="2">44.55 ? 0.52</cell><cell>78.04 ? 0.39</cell><cell>41.27 ? 0.18</cell></row><row><cell>GCAPS-CNN</cell><cell cols="2">77.71 ? 2.51</cell><cell cols="2">71.69 ? 3.40</cell><cell cols="2">48.50 ? 4.10</cell><cell>87.61 ? 2.51</cell><cell>50.10 ? 1.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy on social network datasets. Result in bold indicates the best reported classification accuracy. Top half of the table compares results with various deep learning approaches while bottom half compares results</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/vermaMachineLearning/Graph-Capsule-CNN-Networks/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The research was supported in part by US DoD DTRA grants HDTRA1-09-1-0050 and HDTRA1-14-1-0040, ARO MURI Award W911NF-12-1-0385 and NSF grants CNS 1618339 and CNS 1617729.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shortestpath kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, Fifth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Cun</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Yann. Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung And</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast approximation of the weisfeiler-lehman graph kernel for rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Gerben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="606" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aasa</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niklas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marleen</forename><surname>Bruijne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03059</idno>
		<title level="m">Learning graph representations with embedding propagation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The skew spectrum of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A kernel between sets of vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML-03)</title>
		<meeting>the 20th International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The multiscale laplacian graph kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2982" to="2990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The graphlet spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hy</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02144</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre-Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wengong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09037</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cayleynets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07664</idno>
		<title level="m">Graph convolutional neural networks with complex rational spectral filters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning invariant representations of molecules for atomization energy prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fazli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siamac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franziska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ziehe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><forename type="middle">V</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient graph kernels by randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Novi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="378" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual international conference on machine learning</title>
		<meeting>the 33rd annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph invariant kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3756" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nata?a</forename><surname>Pr?ulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Jan Van, Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending highdimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svn</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Substructure assembling network for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ziyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
