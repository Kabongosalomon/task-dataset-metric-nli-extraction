<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Sampling Based Deep Metric Learning for Generalizable Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
							<email>scliao@ieee.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@ieee.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI) Masdar City</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Terminus Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Sampling Based Deep Metric Learning for Generalizable Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at https://github.com/ShengcaiLiao/QAConv. * Shengcai Liao is the corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies show that, both explicit deep feature matching as well as large-scale and diverse training data can significantly improve the generalization of person reidentification. However, the efficiency of learning deep matchers on large-scale data has not yet been adequately studied. Though learning with classification parameters or class memory is a popular way, it incurs large memory and computational costs. In contrast, pairwise deep metric learning within mini batches would be a better choice. However, the most popular random sampling method, the well-known PK sampler, is not informative and efficient for deep metric learning. Though online hard example mining has improved the learning efficiency to some extent, the mining in mini batches after random sampling is still limited. This inspires us to explore the use of hard example mining earlier, in the data sampling stage. To do so, in this paper, we propose an efficient mini-batch sampling method, called graph sampling (GS), for large-scale deep metric learning. The basic idea is to build a nearest neighbor relationship graph for all classes at the beginning of each epoch. Then, each mini batch is composed of a randomly selected class and its nearest neighboring classes so as to provide informative and challenging examples for learning. Together with an adapted competitive baseline, we improve the state of the art in generalizable person re-identification significantly, by 25.1% in Rank-1 on MSMT17 when trained on RandPerson. Besides, the proposed method also outperforms the competitive baseline, by 6.8% in Rank-1 on CUHK03-NP when trained on MSMT17. Meanwhile, the training time is significantly reduced, from 25.4 hours to 2 hours when trained on RandPerson with 8,000 identities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification is a popular computer vision task, where the goal is to find a person, given in a query image, from the search over a large set of gallery images. In the last two years, generalizable person re-identification has gain increasing attention due to both its research and practical value <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>. This task studies the generalizability of a learned person re-identification model in unseen scenarios, and employs direct cross-dataset evaluation <ref type="bibr">[10,</ref><ref type="bibr" target="#b38">39]</ref> for performance benchmarking.</p><p>For deep metric learning, beyond feature representation learning and loss designs, explicit deep feature matching schemes are shown to be effective for matching person images <ref type="bibr">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b28">29]</ref>, due to the advantages in addressing pose and viewpoint changes, occlusions, and misalignments. In particular, a recent method, called query-adaptive convolution (QAConv) <ref type="bibr" target="#b17">[18]</ref>, has proved that explicit convolutional matching between gallery and query feature maps is quite effective for generalizable person re-identification. However, these methods all require more computational costs compared to conventional feature learning methods.</p><p>Beyond novel generalizable algorithms, another way to improve generalization is to enlarge the scale and diversity of the training data. For example, a recent dataset called RandPerson <ref type="bibr" target="#b33">[34]</ref> synthesized 8,000 identities, while <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr">[2]</ref> both collected 30K persons for re-identification training. These studies all observed improved generalization ability for person re-identification. However, the efficiency of deep metric learning from large-scale data has not yet been adequately studied in person re-identification.</p><p>There are some popular ways of learning deep person re-identification models, including classification (with the ID loss <ref type="bibr" target="#b43">[44]</ref>), metric learning (with a pairwise loss <ref type="bibr">[5,</ref><ref type="bibr" target="#b38">39]</ref> or triplet loss <ref type="bibr">[9]</ref>), and their combinations (e.g. ID + triplet loss). Using an ID loss is convenient for classification learning. However, in large-scale deep learning, involving classifier parameters incurs large memory and computational costs in both the forward and backward passes. Similarly, arXiv:2104.01546v4 [cs.CV] 6 Apr 2022 (a) PK sampler (b) GS sampler <ref type="bibr">Figure 1</ref>. Two different sampling methods: (a) PK sampler; and (b) the proposed GS sampler. Different shapes indicate different classes, while different colors indicate different batches. GS constructs a graph for all classes and always samples nearest neighboring classes.</p><p>involving class signatures for metric learning in a global view is also not efficient. For example, QAConv in <ref type="bibr" target="#b17">[18]</ref> is difficult to scale up for large-scale training, because a class memory module is designed, where full feature maps are stored for all classes as signatures, and they are required for cross feature map convolutional matching during training.</p><p>Therefore, involving class parameters or signatures in either classification or metric learning is not efficient for large-scale person re-identification training. In contrast, we consider that pairwise deep metric learning between samples in mini batches is better suited for this task. Accordingly, the batch sampler plays an important role for efficient learning <ref type="bibr">[9,</ref><ref type="bibr" target="#b37">38]</ref>. The well-known PK sampler <ref type="bibr">[9,</ref><ref type="bibr" target="#b22">23]</ref> is the most popular random sampling method in person reidentification. It first randomly selects P classes, and then randomly samples K images per class to construct a mini batch of size B = P ? K. Since this is performed randomly, the sampled instances within a mini batch are uniformly distributed across the whole dataset (see <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>), and might therefore not be informative and efficient for deep metric learning. To address this, an online hard example mining method was proposed in <ref type="bibr">[9]</ref>, which improved the learning efficiency to some extent. However, the mining is performed online on the already sampled mini batches. Therefore, this method is still limited by the fully random PK sampler, because the mini batches obtained by this sampler do not consider the sample relationship information.</p><p>To address this, we propose to shift the hard example mining earlier to the data sampling stage. Accordingly, we propose an efficient mini-batch sampling method, called graph sampling (GS), for large-scale deep metric learning. The basic idea is to build a nearest neighbor relationship graph for all classes at the beginning of each epoch. Then, the mini-batch sampling is performed by randomly selecting a class as anchor, and its top-k nearest neighboring classes, with the same K instances per class, as shown in <ref type="figure">Fig. 1 (b)</ref>. This way, instances within a sampled mini batch are mostly similar to each other, so as to provide informative and challenging examples for discriminant learning. From face recognition loss function studies <ref type="bibr">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref>, it is known that focusing on boundary (hard) examples helps improving the discriminant ability of the learned model, and helps resulting in compact representations that generalize well beyond the training data. The GS sampler shares a similar idea in focusing on nearest neighboring classes, and thus has a potential of improving the discrimination and generalization ability of the learned model.</p><p>In summary, the contributions of this paper include: (1) We propose a new mini-batch sampling method, termed GS, and prove that it enables more efficient learning than the well-known PK sampler; <ref type="bibr">(2)</ref> We improve a very competitive baseline by 6.8% in Rank-1 with MSMT17 ? CUHK03-NP, and reduce the training time significantly, from 25.4 hours to 2 hours on RandPerson with 8,000 identities; and (3) Together with the baseline, we improve the state of the art in generalizable person re-identification significantly, by 20.6% in Rank-1 with Market-1501 ? MSMT17, and by 25.1% in Rank-1 with RandPerson ? MSMT17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Metric learning approaches have been widely studied in the early stage of person re-identification. Many algorithms have been proposed, such as the well-known PRDC <ref type="bibr" target="#b44">[45]</ref>, KISSME <ref type="bibr" target="#b13">[14]</ref>, and XQDA <ref type="bibr" target="#b16">[17]</ref>, to name a few. In recent years, deep metric learning in particular has become popular and been extensively studied. Beyond feature representation learning, specific deep metric learning can be roughly classified in terms of loss function designs and deep feature matching schemes. For loss function designs, pairwise loss functions <ref type="bibr">[5,</ref><ref type="bibr" target="#b38">39]</ref>, classification or identification loss <ref type="bibr" target="#b43">[44]</ref>, and triplet loss <ref type="bibr">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45]</ref>   <ref type="bibr" target="#b28">[29]</ref>. Liao and Shao proposed the query-adaptive convolution (QAConv) for explicit deep feature matching, and proved its effectiveness for generalizable person reidentification <ref type="bibr" target="#b17">[18]</ref>. They also proposed a Transformer based method, TransMatcher <ref type="bibr" target="#b18">[19]</ref>, for improved performance.</p><p>Generalizable person re-identification was initially studied in <ref type="bibr">[10,</ref><ref type="bibr" target="#b38">39]</ref>, where direct cross-dataset evaluation was proposed to benchmark algorithms. With advancements in deep learning, this task has gained increasing attention in recent years. For example, Song et al. <ref type="bibr" target="#b26">[27]</ref> proposed a domain-invariant mapping network with a meta-learning pipeline. Jia et al. <ref type="bibr" target="#b11">[12]</ref> adopted both instance and feature normalization to alleviate both style and content variances across datasets. Zhou et al. proposed a new backbone network called OSNet <ref type="bibr" target="#b46">[47]</ref>, and further demonstrated its advantages in generalizing deep models <ref type="bibr" target="#b46">[47]</ref>. Jin et al. proposed a style normalization and restitution module, which shows good generalizability <ref type="bibr" target="#b12">[13]</ref>. Yuan et al. proposed an adversarial domain-invariant feature learning network (ADIN), which explicitly learns to separate identityrelated features from challenging variations <ref type="bibr" target="#b39">[40]</ref>. Zhuang et al. proposed a camera-based batch normalization (CBN) method for domain-invariant representation learning <ref type="bibr" target="#b48">[49]</ref>. Recently, meta-learning has also been shown to be effective for learning generalizable models.  <ref type="bibr">[2]</ref>. In addition to the above, Wang et al. proposed a large-scale synthetic person re-identification dataset, called RandPerson, and proved that models learned from synthesized data generalize well to real-world datasets <ref type="bibr" target="#b33">[34]</ref>.</p><p>However, the generalization of current methods is still far from satisfactory for practical person re-identification. Taking face recognition as a good example in practice, future directions may gradually be learning from more largescale data for better performance. However, the efficiency of large-scale learning has been inadequately studied in person re-identification. As basic as the mini-batch sampler, though it plays an important role in deep metric learning <ref type="bibr">[9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, it still has not yet been much studied.</p><p>Beyond online hard example mining within mini batches <ref type="bibr">[9]</ref>, several methods have been proposed for hard example mining during data sampling for mini batches. Suh et al. <ref type="bibr" target="#b27">[28]</ref> proposed a stochastic class-based hard example mining for deep metric learning. It uses learnable class signatures to find nearest classes, and further performs an instance-level refined search within the subset of classes found in the first stage for hard example mining. Besides, the Doppelganger <ref type="bibr" target="#b25">[26]</ref> also relies on classification layers for doppelganger mining from the predicted classification scores. However, these methods require classification parameters to be learned for class mining, which is intractable for large-scale classes and complex non-Euclidean matchers (e.g. QAConv). In <ref type="bibr" target="#b30">[31]</ref>, all training classes are divided into subspaces by clustering on averaged class representations, and then mini batches are sampled within each subspace. This method requires a full forward pass of all the training data, and the clustering operation cannot easily be scaled up to large-scale classes. In <ref type="bibr">[7]</ref>, SmartMining was proposed, which builds an approximate nearest neighbor graph for all training samples after a full forward pass of the training data for feature extraction. However, this instance-level mining can be very expensive in computation, and even infeasible for complex non-Euclidean metric layers. In contrast, we propose and prove that sampling one example per class for class mining works well for large-scale deep metric learning without classification or instance-level mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Metric Learning</head><p>There are two popular ways for learning deep person reidentification neural networks. The first one is the classification based method <ref type="bibr" target="#b43">[44]</ref>, also known as using the identification loss, or ID loss. This is a straightforward extension from general image classification. Since person reidentification is an open-class problem, the learned classifier is usually dropped after training. The last feature embedding layer is usually adopted instead (known as the identity embedding, or IDE <ref type="bibr" target="#b43">[44]</ref>), and the Euclidean or cosine distance is applied to measure the distance between two person images. The second one is the triplet loss based method <ref type="bibr">[9,</ref><ref type="bibr" target="#b23">24]</ref>, which is usually combined with the ID loss. Together with the online hard example mining, the triplet loss is a very useful auxiliary loss function for enhancing the discriminability of the learned model. However, the above methods always require classifier parameters, which incur large memory and computational costs in both the forward and backward passes of large-scale deep learning. When dot products are employed for classification this is still acceptable to some extent. However, with more complex modules, e.g. QAConv <ref type="bibr" target="#b17">[18]</ref> where a full feature map convolution is required for matching, learning with class signatures is difficult to scale up.</p><p>Therefore, for large-scale deep metric learning, we consider removing classification layers. Accordingly, pairwise matching between mini-batch samples is another solution <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>. We adopt QAConv as our baseline method, which is the recent state of the art for generalizable person reidentification. It constructs query adaptive convolutional kernels on the fly for image matching, which suits pairwise learning. However, the original design of QAConv learning is based on the so-called class memory, which stores one feature map for each class for image-to-class matching, instead of using pairwise matching between mini-batch sam-ples. Considering the matching complexity of the QAConv layer, this is not efficient in large-scale learning. Therefore, we only consider pairwise matching between mini-batch samples for QAConv, and remove its class memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Graph Sampling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motivation</head><p>As discussed, for deep metric learning, the well-known PK sampler <ref type="bibr">[9]</ref> is typically used to provide mini-batch samples. However, its random nature makes the sampled instances not informative enough for discriminant learning.</p><p>In the PK sampler, as shown in <ref type="figure">Fig. 1</ref> (a), P classes and K images per class are randomly sampled for each mini batch. Though an online hard example mining (OHEM) was further proposed in <ref type="bibr">[9]</ref> to find informative instances within a mini batch, the PK sampler itself is still not efficient, as it provides limited hard examples for OHEM to mine.</p><p>Therefore, the sampling method itself needs to be improved so as to provide informative samples for mini batches. Instead of using fully random sampling, the relationships among classes need to be considered. Thus, we construct a graph for all classes at the beginning of each epoch, and always sample nearest neighboring classes in a mini batch so as to enable discriminant learning. We call this idea graph sampling (GS), which is detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">GS Sampler</head><p>At the beginning of each epoch, we utilize the latest learned model to evaluate the distances or similarities between classes, and then construct a graph for all classes. This way, the relationships between classes can be used for informative sampling. Specifically, we randomly select one image per class to construct a small sub-dataset. Then, the feature embeddings with the current network are extracted, denoted as X ? R C?d , where C is the total number of classes for training, and d is the feature dimension. Next, pairwise distances between all the selected samples are computed, e.g. by QAConv. As a result, a distance matrix dist ? R C?C for all classes is obtained.</p><p>Then, for each class c, the top P ? 1 nearest neighboring classes can be retrieved, denoted by N (c) = {x i |i = 1, 2, . . . , P ? 1}, where P is the number of classes to sample in each mini batch. Accordingly, a graph G = (V, E) can be constructed, where V = {c|c = 1, 2, . . . , C} represents the vertices, with each class being one node, and E = {(c 1 , c 2 )|c 2 ? N (c 1 )} represents the edges.</p><p>Finally, for the mini-batch sampling, for each class c as anchor, we retrieve all its connected classes in G. Then, together with the anchor class c, we obtain a set A = {c} {x|(c, x) ? E}, where |A| = P . Next, for each class in A, we randomly sample K instances per class to generate a mini batch of B = P ? K samples for training. A pseudocode of the GS sampler is shown in Appendix A.</p><p>Note that, different from other mini-batch sampling methods, for the GS sampler the number of mini batches or iterations per epoch is always C, which is independent to the parameters B, P , and K. Nevertheless, the parameter B still affects the computational load of each mini batch. Besides, one may worry that the GS sampler will be computationally expensive. However, note that, firstly, only one image per class is randomly sampled for the graph construction; and, secondly, the above computation is performed only once per epoch. In practice, we find that the GS sampler with QAConv, which is already a heavy matcher compared to the mainstream Euclidean distance, only requires tens of seconds for thousands of identities. Details will be presented in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Loss Function</head><p>With mini batches provided by the GS sampler, we apply QAConv to compute similarity values between each pair of images, and formulate a triplet-based ranking learning problem within mini batches. Accordingly, we compute the batch OHEM triplet loss <ref type="bibr">[9]</ref> alone for metric learning:</p><formula xml:id="formula_0">(?; X) = P i=1 K a=1 [m? min p=1...K s(f ? (x a i ), f ? (x p i )) + max j=1...P j =i n=1...K s(f ? (x a i ), f ? (x n j ))] + ,<label>(1)</label></formula><p>where X = {x a i , i ? [1, P ], a ? [1, K]} contains the minibatch samples, ? is the network parameter, f ? is the feature extractor, s(?, ?) is the similarity, and m is the margin.</p><p>Note that Eq. (1) is usually used as an auxiliary to the ID loss, but not alone in person re-identification. This is probably because random samplers including PK cannot provide informative mini batches for OHEM to mine, which makes Eq. (1) very small or even zero, and so the learning is not efficient. In contrast, with the proposed GS sampler, we prove that the OHEM triplet loss works well by itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Gradient Clipping</head><p>Note that the GS sampler already provides almost the hardest mini batches, and the batch OHEM triplet loss further finds the hardest triplets within a mini batch for training. As a result, the model may suffer optimization difficulty, which in turn may impact convergence during training. In practice, we find that limiting K = 2 alleviates this problem significantly. Or otherwise, the binary crossentropy loss for pairwise matching can be a more stable alternative to the OHEM triplet loss (see Appendix B).</p><p>Furthermore, to stabilize the training with the GS sampler and the hard triplet loss, we clip the gradient norm during the backward propagation. Specifically, let g be the gradient of all parameters, and g be its norm. The gradient will be clipped as g ? min(1, T g ) ? g, where T is a predefined threshold. That is, if the gradient norm is larger than T then clip it to be T . Note that GS and OHEM provide the hardest examples, which facilitates discriminant learning. However, this may also lead to overfitting. Therefore, besides stabilizing the training, the gradient clipping operation is also useful to regularize noisy gradients to avoid overfitting on source domain, and, in turn, improving the generalization performance. The effect of this gradient clipping will be analyzed in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Our implementation is adapted from the official PyTorch code of QAConv <ref type="bibr" target="#b17">[18]</ref> (MIT license). We first build an improved baseline based on QAConv. Specifically, ResNet-50 <ref type="bibr">[8]</ref> is used as the backbone, with IBN-b layers appended, following several recent studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>. The layer3 feature map is used, with a neck convolution of 128 channels appended as the final feature map. The input image size is 384 ? 128 (see Appendix E for results with 256 ? 128). Several commonly used data augmentation methods are applied, including random cropping, flipping, occlusion <ref type="bibr" target="#b17">[18]</ref>, and color jittering. The batch size is set to 64. The SGD optimizer is adopted to train the network, with a learning rate of 0.0005 for the backbone, and 0.005 for newly added layers. The maximal learning epochs are 60. When the initial loss is reduced as a factor of 0.7, the learning rates are decayed by 0.1, and an early stopping is triggered after a further half of the already learned epochs. Gradient clipping is applied with T = 8. Automatic Mixed Precision (AMP) in PyTorch is applied to accelerate training. When the proposed GS sampler is further applied (denoted by QAConv-GS), we use the hard triplet loss (m=16), instead of the class memory based loss proposed in <ref type="bibr" target="#b17">[18]</ref>, and the default parameters for GS are B=64, and K=2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets</head><p>Four large-scale person re-identification datasets, CUHK03 <ref type="bibr" target="#b14">[15]</ref>, Market-1501 <ref type="bibr" target="#b42">[43]</ref>, MSMT17 <ref type="bibr" target="#b34">[35]</ref>, and RandPerson <ref type="bibr" target="#b33">[34]</ref>  identities. This dataset is only used for large-scale training and generalization testing. Cross-dataset evaluation <ref type="bibr">[10,</ref><ref type="bibr" target="#b38">39]</ref> is performed on all datasets, by training on the training subset of one dataset (except that with MSMT17 we further used an additional setting with all images for training), and evaluating on the test subset of another dataset. Rank-1 and mean average precision (mAP) are used as the performance metrics, evaluated under single-query evaluation protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison to the State of the Art</head><p>A comparison to the state of the art (SOTA) in generalizable person re-identification is shown in <ref type="table" target="#tab_7">Table 1</ref>, where three datasets are used for training, and three others are used for testing. Note that, with MSMT17 as the training set, one setting is to use all images for training, regardless of its subset splits. This is denoted by MSMT17 (all). Several generalizable person re-identification methods published recently are compared, including OSNet-IBN <ref type="bibr" target="#b46">[47]</ref>, OSNet-AIN <ref type="bibr" target="#b47">[48]</ref>, MuDeep <ref type="bibr" target="#b21">[22]</ref>, SNR <ref type="bibr" target="#b12">[13]</ref>, QAConv <ref type="bibr" target="#b17">[18]</ref>, CBN <ref type="bibr" target="#b48">[49]</ref>, ADIN <ref type="bibr" target="#b39">[40]</ref>, and M 3 L [42]. <ref type="table" target="#tab_7">Table 1</ref> shows that QAConv-GS significantly improves the previous SOTA. For example, with Market-1501 ? CUHK03, the Rank-1 and mAP are improved by 8.8% and 9.0%, respectively. With Market-1501 ? MSMT17, they are improved by 20.6% and 7.7%, respectively. With MSMT17 (all) ? Market-1501, the improvements are 9.8% for Rank-1 and 13.8% for mAP. With RandPerson as the training data, the improvements on Market-1501 are 12% for Rank-1 and 7.4% for mAP, while the improvements on MSMT17 are 25.1% for Rank-1 and 8.7% for mAP. Though RandPerson is synthetic, the results show that models learned on it generalize quite well to realworld datasets, which confirms the findings in <ref type="bibr" target="#b33">[34]</ref>.</p><p>Note that, M 3 L [42] uses a different evaluation protocol, and thus the results are not directly comparable. Specifically, M 3 L is trained on three datasets selected from CUHK03, Market-1501, DukeMTMC-reID 1 , and MSMT17, while the other is held for testing. Impressive results are obtained by M 3 L on CUHK03-NP, which, though not directly comparable, exceed all our results, including those trained with all MSMT17 images. However, on Market-1501, the proposed method trained on MSMT17 outperforms M 3 L in Rank-1 by 3.2%, while the mAPs are comparable. Furthermore, on MSMT17, the proposed method trained on Market-1501 significantly outperforms M 3 L, with 9% gain in Rank-1 and 2.5% in mAP. This is quite encouraging, since in both cases our training dataset is a subset of that used by M 3 L.  <ref type="table" target="#tab_7">Table 2</ref> shows a comparison among different variations of QAConv: the original QAConv <ref type="bibr" target="#b17">[18]</ref> (denoted as Ori), the competitive QAConv baseline we adapted (denoted as Base), and the proposed QAConv-GS. It shows that, beyond the successful learning scheme of the class memory module proposed in QAConv, QAConv-GS with the proposed GS sampler is also very effective in learning discriminant models. QAConv-GS outperforms the competitive baseline for all experiments, by 6.8% and 5.4% in Rank-1, respectively, on CUHK03 and Market-1501 when trained on MSMT17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Comparison of QAConv variants</head><p>Interestingly, <ref type="table" target="#tab_7">Table 2</ref> also shows that the within-dataset evaluation results are also improved by QAConv-GS compared to the baseline. However, improving performance on a single dataset does not always lead to better generalization, since it may also overfit a dataset, as can be observed in <ref type="table" target="#tab_7">Table 1</ref>. Therefore, we suggest a focus on generalization since it is more critical for practical applications.</p><p>Furthermore, we also compare the training time of QA-Conv (with class memory) and QAConv-GS. Both methods are tested on a single NVIDIA V100 GPU. From the comparison shown in <ref type="table" target="#tab_7">Table 2</ref> slow when trained on large-scale datasets, such as the full MSMT17 or RandPerson. This is not surprising, because in each mini-batch iteration, the QAConv with class memory needs to compute matching scores between mini-batch samples and the feature map memory of all classes; and the number of classes is 4,101 in MSMT17, and 8,000 in Rand-Person. In contrast, the proposed pairwise learning with the GS sampler is much more efficient because it avoids matching all classes in each iteration. As can be seen from <ref type="table" target="#tab_7">Table  2</ref>, the training time of the baseline QAConv can be reduced from 25.4 hours to 2 hours when trained on RandPerson with 8,000 identities, which is a significant achievement. In addition, we also evaluate the sampling efficiency of the proposed GS sampler. As stated earlier, it constructs a graph at the beginning of each epoch. We evaluate the running time of all the computations in GS. The results are 4 seconds on Market-1501, 9 seconds on the MSMT17 training subset, 40 seconds on the full MSMT17 dataset, and 138 seconds on RandPerson with 8,000 identities. Therefore, the GS sampler is in fact efficient, despite being incorporated into QAConv, which is a heavy matcher compared to the mainstream Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Comparison of different sampling methods</head><p>In <ref type="table" target="#tab_5">Table 3</ref>, using the same QAConv and hard triplet loss, we compare three mini-batch sampling methods, including PK, a clustering based method <ref type="bibr" target="#b30">[31]</ref> (denoted as Cluster), and GS. Besides, the implementation of PK and Cluster follows GS, where the number of batches per epoch is determined by the number of classes. For <ref type="bibr" target="#b30">[31]</ref>, since k-means does not support non-Euclidean metric, we replace it with spectral clustering. The subspace parameter M in <ref type="bibr" target="#b30">[31]</ref> is set to 10, after an optimization in <ref type="bibr">[5,</ref><ref type="bibr">50]</ref>. From <ref type="table" target="#tab_5">Table 3</ref>, we can see that PK performs the worst, due to its fully random nature, which does not provide enough hard examples in mini batches. Besides, we can see that, with the subspace clustering method proposed in <ref type="bibr" target="#b30">[31]</ref>, the performance is generally improved, thanks to the more informative mini batches sampled within each cluster. However, feature extraction from the whole training set and clustering of all classes are time consuming. In contrast, the proposed GS sampler is more efficient, since it only considers one example per class for the graph construction. Furthermore, GS also achieves the best performance, with improvements over Cluster of up to 4.7% in Rank-1, and 3.4% in mAP. We believe that clustering is less effective than graph based GS due to two reasons. First, only cluster centers may be surrounded by their dense neighbors, while others, especially boundary points (classes), may not be always with their full set of neighbors in the same cluster. Second, mini-batch classes need to be randomly sampled within a cluster, of which the operation may further miss out some nearest neighbors of each class.</p><p>Furthermore, in Appendix C, applications to two other baselines, OSNet <ref type="bibr" target="#b46">[47]</ref> and TransMatcher <ref type="bibr" target="#b18">[19]</ref>, also verify the generality of GS's advantage over PK. Besides, application to unsupervised domain adaptation (UDA) with GS in SpCL <ref type="bibr">[6]</ref> is discussed in Appendix D, and a variant of GS using class centers is analyzed in Appendix E.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Parameter analysis</head><p>In <ref type="figure">Fig. E, we</ref> show the performance of the proposed method with different batch sizes and margin parameters. The training is performed on MSMT17. For ease and reliable comparison, we report the average (denoted by mAcc) of all Rank-1 and mAP results on all test sets over four random runs. We observe that, generally, the accuracy increases with increasing batch size B, but saturates at 64. As for the margin parameter m, note that the QAConv similarity score s(?, ?) used in Eq. (1) ranges in (-?, +?). <ref type="figure">Fig. E(b)</ref> shows <ref type="bibr">Figure 3</ref>. Influence of gradient clipping, trained on three datasets. that the performance slightly improves with increasing m due to the increased discriminability, and achieves the best with m=16. However, after m=32, the performance drops significantly, due to intractable learning difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Effect of gradient clipping</head><p>Next, we study the effect of gradient clipping on the learning of QAConv-GS. The results are shown in <ref type="figure">Fig. 3</ref>. Interestingly, when trained on MSMT17, the performance is less affected without gradient clipping (Inf). Specifically, with gradient clipping, only a slight improvement can be obtained, but too small threshold T even prevent effective model learning. This is because, in our experiments, MSMT17 is the most comprehensive dataset. It provides large-scale and diverse training examples, which prevents overfitting in the view of "regularization from data". However, with the small-scale training dataset Market-1501, and the quite different synthetic dataset RandPerson, gradient clipping does provide useful regularization for model training, and improves the generalization performance. Therefore, a reasonable value of T =8 is considered as a trade-off. As can be observed, the GS sampler is indeed able to find similar classes as hard examples to challenge the learning. For example, it identifies similar kinds of clothes, colors, patterns, and accessories. These confusing examples help a lot in learning discriminative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5">Visualization of GS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>With this study, we show that the popular PK sampler is not efficient in deep metric learning, and thus we propose a new batch sampler, called the graph sampler, to help learning discriminant models more efficiently. This is achieved  by constructing a nearest neighbor graph of all classes for informative sampling. Together with a competitive baseline, we achieve the new state of the art in generalizable person re-identification with a significant improvement. Meanwhile, the training time is much reduced by removing the classification parameters and only using the pairwise distances between mini batches for loss computation. We believe the proposed technique is general and may also be applied in other fields, such as image retrieval, and face recognition, among others. Moreover, we discuss social impacts and some limitations of this research in Appendix G and H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Alternative Loss Function and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Binary Cross Entropy Loss</head><p>Note that the batch hard triplet loss (Eq. (1) in the main paper) is usually used as an auxiliary to the classification loss, but not alone, in person re-identification. This is probably because random samplers including PK cannot provide informative mini batches for OHEM to mine, which makes Eq. (1) very small or even zero, and so the learning is not efficient. In contrast, with the proposed GS sampler, we prove that the OHEM triplet loss works well by itself with K = 2. We use this loss function alone because, as motivated in the main paper, we aim at removing classification layers for large-scale metric learning.</p><p>However, note that the GS sampler already provides almost the hardest mini batches, and the batch hard triplet loss further finds the hardest triplets within a mini batch for training. As a result, the model may suffer optimization difficulty, which in turn may impact convergence during training. In practice, we find that limiting K = 2 alleviates this problem significantly, while K &gt; 2 usually makes the learning not able to converge.</p><p>Alternatively, pairwise verification or binary classification is another solution <ref type="bibr">[3,</ref><ref type="bibr">9]</ref> for pairwise matching or metric learning within mini batches. Specifically, we apply QA-Conv to compute similarity values between a pair of images, and formulate a pairwise verification or binary classification problem in mini-batch based learning. Accordingly, we compute the binary cross entropy loss as follows.</p><formula xml:id="formula_1">(?) = ? 1 B B i=1 j =i y ij log(p ij (?))+(1?y ij )log(1?p ij (?)),</formula><p>(B) where B is the mini-batch size, ? is the network parameter, p ij ? [0, 1] is the QAConv similarity indicating binary classification probability, and y ij = 1 indicates a positive pair, while a negative pair otherwise. By default, we choose B = 64 and K = 4 for this loss. <ref type="table" target="#tab_7">Table D</ref> shows a comparison between the hard triplet loss and the binary cross entropy loss for QAConv-GS. Results shown in the table indicate that, the hard triplet loss performs better than the binary cross entropy loss for all datasets, thanks to OHEM which further mines hard examples within mini batches provided by GS. However, the hard triplet loss used alone in the proposed pipeline is sensitive to K values as discussed. In contrast, the binary cross entropy loss is a more stable alternative, working well with different B and K values. This will be analyzed in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Experimental Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Parameter analysis</head><p>When the binary cross entropy loss is applied, in <ref type="figure">Fig. E</ref>, we show the performance with different parameter configurations of the GS sampler, trained on Market-1501. We observe that for the batch size ( <ref type="figure" target="#fig_1">Fig. E (a)</ref>), generally the accuracy increases with the increasing batch size (thus increasing P ), but saturates at about 64. It is understood that mini batches with larger batch size provides more comprehensive data for learning, however, at the cost of enlarged computation time, recalling that the number of iterations per epoch is fixed as C for the GS sampler. For example,  <ref type="figure">Fig. E (b)</ref>. Interestingly, larger K leads to gradually better performance on the CUHK03-NP, however, it degrades the performance significantly on MSMT17. It appears that K = 4 is a reasonable trade-off.</p><p>Since the hard triplet loss performs better, in the following, by default we still use this loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Application to Other Baselines</head><p>Furthermore, to show the generality of the proposed graph sampling method, we apply it to two other algorithms, namely OSNet <ref type="bibr">[10]</ref> and TransMatcher <ref type="bibr">[4]</ref>.</p><p>The official code of OSNet 2 (MIT License) is used. We used its osnet ibn x1 0 config, with softmax+triplet loss and the PK sampler (RandomIdentitySampler) for the best performance, denoted by OSNet-IBN + PK. This combination of softmax+triplet loss and the PK sampler is also the most popular setting in person re-identification for strong baselines. Then, upon this baseline, we apply the proposed graph sampling to replace the PK sampler, denoted by OSNet-IBN + GS. The training is performed on the MSMT17 (all), as in <ref type="bibr">[10]</ref>, and the learned models are evaluated on the CUHK03-NP and Market-1501 datasets. The results are shown in <ref type="table" target="#tab_7">Table E</ref>. From the comparison it can be seen that the proposed GS sampler can also improve other strong baselines in replacing the popular PK sampler. Therefore, it is proved to be general and may also be applied to other methods. Furthermore, with a very recent method TransMatcher  [4], we also compare the PK and GS samplers. The official code of TransMatcher 3 (MIT License) is used, with its de-fault settings. The results are shown in <ref type="table" target="#tab_7">Table F</ref>. It can also be observed that on average the proposed GS sampler performs much better than the PK sampler, verifying again the generality of GS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Application to Unsupervised Domain Adaptation</head><p>As a new scenario, we tried unsupervised domain adaptation (UDA) by replacing PK with GS in the source domain of SpCL <ref type="bibr">[1]</ref>. The Rank1/mAP results for PK and GS are 86.1/70.9 and 87.3/71.5, respectively, for CUHK03-NP?Market-1501. Slight improvements can be observed. However, for the time being, it is still not yet straightforward to apply GS for pseudo labeled samples by clustering on the target domain. This may be because pseudo labels could be noisy, and GS may be aggressive in finding hard negative samples that could possibly be positive. To address this, further developments may be required in considering how to handle noisy samples, which is quite interesting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Further Ablation Studies</head><p>In this paper, all experiments are with images of 384 ? 128 as inputs. To understand the influence of image size, we also conduct experiments with images of 256 ? 128 as inputs. The results are shown in <ref type="table" target="#tab_7">Table G</ref>. It can be seen that the results are quite close to each other on Market-1501 and MSMT17, though results with 384 ? 128 are clearly better than that of 256 ? 128 on CUHK03. Note that our results with 256 ? 128 still achieve the state of the art compared to existing methods in <ref type="table" target="#tab_7">Table 1</ref> of the main paper.</p><p>In the proposed GS, one example per class is sampled for the graph construction, which is efficient. Alternatively, class centers can also be considered for graph construction. In fact, class centers are used in <ref type="bibr">[6]</ref> for clustering based batch sampling, and we show better performance of GS in <ref type="table" target="#tab_5">Table 3</ref> of the main paper. To further understand this, we use class centers to construct graphs for GS. This is denoted by QAConv-GS-Center. The comparison results are shown in <ref type="table" target="#tab_7">Table G</ref>. It is clear that GS performs better.</p><p>There might be two problems with class centers. First, it lacks flexibility of sample relationships, since many classes may have large distribution variances. This is also discussed in <ref type="bibr">[5]</ref>. Second, computing class centers requires feature extraction of all training samples, which hinders large-scale learning. The average training time increases from 1.68 hours of QAConv-GS to 2.27 hours of QAConv-GS-Center. Especially, QAConv-GS costed 3.4 hours to train MSMT17 (all), but QAConv-GS-Center costed 5.4 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visualization of GS</head><p>Finally, we show some examples for the nearest neighboring classes generated by the GS sampler in <ref type="figure">Fig. F</ref>. It can be observed that, the GS sampler is indeed able to find similar classes as hard examples to challenge the learning. For example, similar kind of clothes, similar colors, patterns, and accessories. These confusing examples helps a lot in learning discriminative models. Besides, it seems that in early epochs, the model tends to evaluate similarity with visual appearance, regardless of the influence of foreground and background. However, in late epochs, the model learns to remove the influence of background, and learns higher level of abstraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Limitations</head><p>The proposed method, despite achieving very good results, may have two limitations. First, GS requires additional computation for mini batch sampling. We design two ways to reduce the computation, that is, employing GS only at the beginning of each epoch, and randomly sampling only one sample per class for the distance computation and graph construction. As a result, the additional running time introduced by GS is still acceptable, as reported in Section 5.4.1 of the main paper. Besides, note that with GS the number of training epochs is generally reduced. For example, with GS the proposed method usually requires less than 20 epochs for training, while existing methods typically require 60 epochs or more to train. Therefore, GS deserves the additional computational costs. However, in our experiments, the maximal number of classes is only 8,000. GS may still have a big limitation with millions of identities, which need further investigation.</p><p>Second, as discussed, GS provides challenging examples for training, and so the default hard triplet loss only works well with K = 2. Otherwise, the training is too difficult to converge. Nevertheless, as discussed in Section B, this limitation can be solved by employing the binary cross entropy loss as an alternative, though with inferior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Social Impacts</head><p>Person re-identification is a technique to automatically search persons from a large amount of videos. It has potential social values in some practical applications, such as person image retrieval of suspects, character recognition in movies <ref type="bibr">[2]</ref>, and so on. For example, it is very useful to reduce large amount of human labors and greatly advance the effort in criminal investigation. Accordingly, person re-identification methods are actively studied. The person re-identification technique, however, may also be used by company for a surveillance of employees, or by malls for tracking of daily visitors. Therefore, it requires effective legislation to avoid abuse of this technique. This paper focuses on foundational research; it is not tied to particular applications, let alone deployments.</p><p>Besides, the research and developments of such technique are often with datasets collected from surveillance videos that may contain personally identifiable information. To address this, a positive action is to remove such information, as done in MSMT17v2 <ref type="bibr">[8]</ref> with facial areas masked. More promisingly, a better way recently demonstrated is to use synthesized data, as done in RandPerson <ref type="bibr">[7]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For example, Zhao et al. proposed memory-based multi-source meta-learning (M 3 L) for generalizing to unseen domains [42]. Choi et al. proposed the MetaBIN algorithm for meta-training the batch-instance normalization nettwork [3]. Bai et al. proposed a dual-meta generalization network and a large-scale dataset called Person30K for person re-identification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( a )</head><label>a</label><figDesc>Effect of batch size B (b) Effect of margin m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>mAcc (%) performance with (a) different batch sizes, and (b) different margin parameters, trained on MSMT17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. F shows</head><label></label><figDesc>some examples (more in Appendix F) of the nearest neighboring classes generated by the GS sampler.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Groups of examples of the nearest neighboring classes generated by the GS sampler when trained on (a) Market-1501 and (b) MSMT17. In each group, the upper left image is the center class, and other images are the top-7 nearest neighboring classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2</head><label></label><figDesc>https : / / github . com / KaiyangZhou / deep -personreid (a) Effect of batch size (b) Effect of K Figure E. Performance with different parameter configurations of the GS sampler when the binary cross entropy loss is applied, trained on Market-1501. (a) with varying batch size under fixed K = 4; and (b) with varying K under fixed B = 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>For example, in the upper right group, the similarity is less affected by bicycles in background with epoch 15. In the first group of MSMT17, the similarity is less affected by trees in background with later epochs. With epoch 15 of the first group, the model learns the concept of security guards. In the upper right group, with epoch 15 the model learns the concept of girls with short skirts. In the last group of Market-1501, with epoch 15 the clothes are more consistent in style and color. In the upper right group of MSMT17, with epoch 15 in GS the model correctly retrieves red coats. In the last group of MSMT17, with epoch 15 in GS the model correctly retrieves pink coats as well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>are the most popular. For deep feature matching schemes, a number of methods have been proposed in the literature. For example, Ahmed et al. proposed a deep convolutional architecture with layers specifically designed for local neighborhood matching [1]. Li et al. proposed a novel filter pairing neural network (FPNN) to jointly handle several known challenges, such as misalignment and occlusions [15]. Shen et al. proposed an</figDesc><table /><note>end-to-end deep Kronecker-Product Matching (KPM) net- work [25] for softly aligned matching. Suh et al. proposed a deep neural network to learn part-aligned bilinear repre- sentations</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>are used in our experiments. The CUHK03 dataset contains 1,360 persons and 13,164 images. The most challenging subset named detected is used for our experiments. Besides, the CUHK03-NP protocol [46] is adopted, with 767 and 700 subjects used for training and testing, respectively. The Market-1501 dataset includes 32,668 images of 1,501 identities captured from six cameras. The training subset contains 12,936 images from 751 identities, while the test subset includes 19,732 images from 750 identities. The MSMT17 dataset contains 4,101 identities and 126,441 images captured from 15 cameras. It is divided into a training set of 32,621 images from 1,041 identities, and a test set with the remaining images from 3,010 identities. The RandPerson dataset is a recently released synthetic person re-identification dataset. It contains 8,000 persons and 1,801,816 images. We use the subset including 132,145 images of the 8,000</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Comparison of different sampling methods. MS-all:</figDesc><table /><note>MSMT17 (all). RP: RandPerson. Time is with seconds per epoch.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table E .</head><label>E</label><figDesc>Direct cross-dataset evaluation results (%) with different baselines trained on MSMT17 (all).</figDesc><table><row><cell>Method</cell><cell cols="2">CUHK03 R1 mAP</cell><cell>Market R1 mAP</cell></row><row><cell>OSNet-IBN [10]</cell><cell>-</cell><cell>-</cell><cell>66.5 37.2</cell></row><row><cell cols="4">OSNet-IBN + PK 23.4 23.6 67.9 39.6</cell></row><row><cell cols="4">OSNet-IBN + GS 24.5 24.9 71.3 42.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table G .</head><label>G</label><figDesc>Table F. Comparison of direct cross-dataset evaluation results (%) using TransMatcher[4]  with PK and GS. MSMT17 (all) means all images are used for training, regardless of subset splits. Comparison of the direct cross-dataset evaluation results (%) for different variants of QAConv-GS. QAConv-GS-Center is based on selecting class centers for graph construction for GS. MSMT17 (all) means all images are used for training, regardless of subset splits.</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell cols="7">CUHK03-NP Rank-1 mAP Rank-1 mAP Rank-1 mAP Market-1501 MSMT17</cell></row><row><cell>TransMatcher-PK</cell><cell>Market-1501</cell><cell></cell><cell>22.9</cell><cell>21.5</cell><cell>-</cell><cell>-</cell><cell>45.6</cell><cell>17.8</cell></row><row><cell>TransMatcher-GS</cell><cell>Market-1501</cell><cell></cell><cell>22.2</cell><cell>21.4</cell><cell>-</cell><cell>-</cell><cell>47.3</cell><cell>18.4</cell></row><row><cell>TransMatcher-PK</cell><cell>MSMT17</cell><cell></cell><cell>23.6</cell><cell>22.9</cell><cell>78.3</cell><cell>51.7</cell><cell>-</cell><cell>-</cell></row><row><cell>TransMatcher-GS</cell><cell>MSMT17</cell><cell></cell><cell>23.7</cell><cell>22.5</cell><cell>80.1</cell><cell>52.0</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">TransMatcher-PK MSMT17 (all)</cell><cell></cell><cell>30.7</cell><cell>29.5</cell><cell>79.9</cell><cell>55.7</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">TransMatcher-GS MSMT17 (all)</cell><cell></cell><cell>31.9</cell><cell>30.7</cell><cell>82.6</cell><cell>58.4</cell><cell>-</cell><cell>-</cell></row><row><cell>TransMatcher-PK</cell><cell>RandPerson</cell><cell></cell><cell>18.0</cell><cell>16.5</cell><cell>73.3</cell><cell>45.3</cell><cell>40.6</cell><cell>14.1</cell></row><row><cell>TransMatcher-GS</cell><cell>RandPerson</cell><cell></cell><cell>17.1</cell><cell>16.0</cell><cell>77.3</cell><cell>49.1</cell><cell>48.3</cell><cell>17.7</cell></row><row><cell>Method</cell><cell>Training</cell><cell></cell><cell cols="6">CUHK03-NP Rank-1 mAP Rank-1 mAP Rank-1 mAP Market-1501 MSMT17</cell></row><row><cell>QAConv-GS</cell><cell>Market-1501</cell><cell></cell><cell>19.1</cell><cell>18.1</cell><cell>-</cell><cell>-</cell><cell>45.9</cell><cell>17.2</cell></row><row><cell>QAConv-GS (256 ? 128)</cell><cell>Market-1501</cell><cell></cell><cell>16.9</cell><cell>17.2</cell><cell>-</cell><cell>-</cell><cell>45.4</cell><cell>17.1</cell></row><row><cell>QAConv-GS-Center</cell><cell>Market-1501</cell><cell></cell><cell>15.4</cell><cell>14.7</cell><cell>-</cell><cell>-</cell><cell>45.0</cell><cell>15.7</cell></row><row><cell>QAConv-GS</cell><cell>MSMT17</cell><cell></cell><cell>20.9</cell><cell>20.6</cell><cell>79.1</cell><cell>49.5</cell><cell>-</cell><cell>-</cell></row><row><cell>QAConv-GS (256 ? 128)</cell><cell>MSMT17</cell><cell></cell><cell>18.6</cell><cell>19.8</cell><cell>77.9</cell><cell>49.6</cell><cell>-</cell><cell>-</cell></row><row><cell>QAConv-GS-Center</cell><cell>MSMT17</cell><cell></cell><cell>15.3</cell><cell>16.1</cell><cell>73.9</cell><cell>41.5</cell><cell>-</cell><cell>-</cell></row><row><cell>QAConv-GS</cell><cell cols="2">MSMT17 (all)</cell><cell>27.6</cell><cell>28.0</cell><cell>82.4</cell><cell>56.9</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">QAConv-GS (256 ? 128) MSMT17 (all)</cell><cell>24.3</cell><cell>25.6</cell><cell>81.5</cell><cell>55.3</cell><cell>-</cell><cell>-</cell></row><row><cell>QAConv-GS-Center</cell><cell cols="2">MSMT17 (all)</cell><cell>25.2</cell><cell>24.6</cell><cell>78.6</cell><cell>51.2</cell><cell>-</cell><cell>-</cell></row><row><cell>QAConv-GS</cell><cell>RandPerson</cell><cell></cell><cell>18.4</cell><cell>16.1</cell><cell>76.7</cell><cell>46.7</cell><cell>45.1</cell><cell>15.5</cell></row><row><cell>QAConv-GS (256 ? 128)</cell><cell>RandPerson</cell><cell></cell><cell>16.2</cell><cell>14.4</cell><cell>74.7</cell><cell>45.5</cell><cell>45.0</cell><cell>15.8</cell></row><row><cell>QAConv-GS-Center</cell><cell>RandPerson</cell><cell></cell><cell>17.4</cell><cell>15.4</cell><cell>76.8</cell><cell>47.0</cell><cell>44.3</cell><cell>15.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is no longer available, so we do not use it in our experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https : / / github . com / ShengcaiLiao / QAConv / tree / master/projects/transmatcher</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Pseudocode of the GS sampler</head><p>A pseudocode of the GS sampler is shown in Algorithm 1. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ejaz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3908" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person30k: A dual-meta generalization network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jile</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuetao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta batch-instance normalization for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokeon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minki</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungseob</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Selfpaced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2821" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross dataset person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV Workshop on Human Identification for Surveillance (HIS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="650" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Movienet: A holistic dataset for movie understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>2020. 14</idno>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Frustratingly easy person re-identification: Generalizing person re-id in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqi</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Style Normalization and Restitution for Generalizable Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep-ReID: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TransMatcher: Deep Image Matching Through Transformers for Generalizable Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhiksha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Leader-based Multi-Scale Attention Deep Architecture for Person Re-identification. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end deep kronecker-product matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6886" to="6895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Doppelganger mining for face representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Smirnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Melnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Novoselov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Luckyanets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galina</forename><surname>Lavrentyeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1916" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generalizable person reidentification by domain-invariant mapping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stochastic class-based hard example mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How to train triplet networks with 100k identities?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="1907" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly supervised person re-id: Differentiable graphical learning and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xujie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengtao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2142" to="2156" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Surpassing Real-World Source Training Data: Random 3D Characters for Generalizable Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th ACM International Conference on Multimedia (ACMMM)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep Learning for Person Re-identification: A Survey and Outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04193</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Calibrated Domain-Invariant Learning for Highly Generalizable Large Scale Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>WACV</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unrealperson: An adaptive pipeline towards costless person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11506" to="11515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to generalize unseen domains via memory-based multi-source metalearning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Person re-identification in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Person reidentification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning generalisable omni-scale representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the Distribution Gap of Person Re-identification with Camera-based Batch Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="140" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Selfpaced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Movienet: A holistic dataset for movie understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>2020. 14</idno>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">TransMatcher: Deep Image Matching Through Transformers for Generalizable Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Stochastic class-based hard example mining for deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The first two rows are from the training on Market-1501, while the last two rows are from that of MSMT17. In each group, three sets of images are shown, corresponding to epoch 2, 8, and 15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Figure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Eight groups of examples for the nearest neighboring classes generated by the GS sampler. In each set, the upper left image is the center class, and other images are the top-7 nearest neighboring classes to the center class. metric learning</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">How to train triplet networks with 100k identities?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="1907" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Surpassing Real-World Source Training Data: Random 3D Characters for Generalizable Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th ACM International Conference on Multimedia (ACMMM)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Biography Shengcai Liao is the Director of Research and Developments (Acting) in the Inception Institute of Artificial Intelligence (IIAI)</title>
	</analytic>
	<monogr>
		<title level="j">Chinese Academy of Sciences</title>
		<imprint>
			<publisher>CA</publisher>
		</imprint>
	</monogr>
	<note>He is a Senior Member of IEEE. Previously, he was an Associate Professor in the Institute of Automation</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">He serves as Program Chair for IJCB 2022. He served as Area Chairs for ICPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cvpr</forename><surname>Eccv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neurips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaai</forename><surname>Iclr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tpami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ijcv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tip</surname></persName>
		</author>
		<ptr target="https://liaosc.wordpress.com/" />
	</analytic>
	<monogr>
		<title level="m">ICB 2006, ICB 2015, and CCBR 2016, and the Best Paper in ICB 2007. He was also awarded the IJCB 2014 Best Reviewer and CVPR 2019/2021 Outstanding Reviewer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>His team was the Winner of the CVPR 2017 Detection in Crowded Scenes Challenge and ICCV 2019 NightOwls Pedestrian Detection Challenge</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
