<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid Spectrogram and Waveform Source Separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hybrid Spectrogram and Waveform Source Separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>License Authors of papers retain copyright and release the work under a Creative Commons Attribution 4.0 International License (CC BY 4.0). In partnership with</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Source separation models either work on the spectrogram or waveform domain. In this work, we show how to perform end-to-end hybrid source separation, letting the model decide which domain is best suited for each source, and even combining both. The proposed hybrid version of the Demucs architecture (D?fossez et al., 2019) won the Music Demixing Challenge 2021 organized by Sony. This architecture also comes with additional improvements, such as compressed residual branches, local attention or singular value regularization. Overall, a 1.4 dB improvement of the Signal-To-Distortion (SDR) was observed across all sources as measured on the MusDB HQ dataset <ref type="bibr" target="#b22">(Rafii et al., 2019)</ref>, an improvement confirmed by human subjective evaluation, with an overall quality rated at 2.83 out of 5 (2.36 for the non hybrid Demucs), and absence of contamination at 3.04 (against 2.37 for the non hybrid Demucs and 2.44 for the second ranking model submitted at the competition).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Work on music source separation has recently focused on the task of separating 4 well defined instruments in a supervised manner: drums, bass, vocals and other accompaniments. Recent evaluation campaigns <ref type="bibr" target="#b27">(St?ter et al., 2018)</ref> have focused on this setting, relying on the standard MusDB benchmark <ref type="bibr" target="#b21">(Rafii et al., 2017)</ref>. In 2021, Sony organized the Music Demixing Challenge (MDX) <ref type="bibr" target="#b16">(Mitsufuji et al., 2021)</ref>, an online competition where separation models are evaluated on a completely new and hidden test set composed of 27 tracks.</p><p>The challenge featured a number of baselines to start from, which could be divided into two categories: spectrogram or waveform based methods. The former consists in models that are fed with the input spectrogram, either represented by its amplitude, such as Open-Unmix  and its variant CrossNet Open-Unmix <ref type="bibr" target="#b24">(Sawata et al., 2020)</ref>, or as the concatenation of its real and imaginary part, a.k.a Complex-As-Channels (CAC) <ref type="bibr" target="#b1">(Choi et al., 2020)</ref>, such as LaSAFT <ref type="bibr" target="#b10">(Choi et al., 2021)</ref>. Similarly, the output can be either a mask on the input spectrogram, complex modulation of the input spectrogram <ref type="bibr" target="#b10">(Kong et al., 2021)</ref>, or the CAC representation.</p><p>On the other hand, waveform based models such as Demucs <ref type="bibr" target="#b4">(D?fossez et al., 2019)</ref> are directly fed with the raw waveform, and output the raw waveform for each of the source. Most of those methods will perform some kind of learnt time-frequency analysis in its first layers through convolutions, such as Demucs and Conv-TasNet <ref type="bibr" target="#b13">(Luo and Mesgarani, 2019)</ref>, although some will not rely at all on convolutional layers, like Dual-Path RNN <ref type="bibr">(Luo et al., 2020)</ref>.</p><p>Theoretically, there should be no difference between spectrogram and waveform models, in particular when considering CaC (complex as channels), which is only a linear change of base for the input and output space. However, this would only hold true in the limit of having an infinite amount of training data. With a constrained dataset, such as the 100 songs of MusDB, inductive bias can play an important role. In particular, spectrogram methods varies by more than their input and output space. For instance, with a notion of frequency, it is possible to apply convolutions along frequencies, while waveform methods must use layers that are fully connected with respect to their channels. The final test loss being far from zero, there will also be artifacts in the separated audio. Different representations will lead to different artifacts, some being more noticeable for the drums and bass (phase inconsitency for spectrogram methods will make the attack sounds hollow), while others are more noticeable for the vocals (vocals separated by Demucs suffer from crunchy static noise)</p><p>In this work, we extend the Demucs architecture to perform hybrid waveform/spectrogram domain source separation. The original U-Net architecture <ref type="bibr" target="#b23">(Ronneberger et al., 2015)</ref> is extended to provide two parallel branches: one in the time (temporal) and one in the frequency (spectral) domain. We bring other improvements to the architecture, namely compressed residual branches comprising dilated convolutions <ref type="bibr">(Yu and Koltun, 2016)</ref>, LSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref> and attention <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> with a focus on local content. We measure the impact of those changes on the MusDB benchmark and on the MDX hidden test set, as well as subjective evaluations. Hybrid Demucs ranked 1st at the MDX competition when trained only on MusDB, with 7.32 dB of SDR, and 2nd with extra training data allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>There exist a number of spectrogram based music source separation architectures. Open-Unmix  is based on fully connected layers and a bi-LSTM. It uses multi-channel Wiener filtering <ref type="bibr" target="#b17">(Nugraha et al., 2016)</ref> to reduce artifacts. While the original Open-Unmix is trained independently on each source, a multi-target version exists <ref type="bibr" target="#b24">(Sawata et al., 2020)</ref>, through a shared averaged representation layer. D3Net  is another architecture, based on dilated convolutions connected with dense skip connections. It was before the competition the best performing spectrogram architecture, with an average SDR of 6.0 dB on MusDB. Unlike previous methods which are based on masking, LaSAFT <ref type="bibr" target="#b10">(Choi et al., 2021)</ref> uses Complex-As-Channels <ref type="bibr" target="#b1">(Choi et al., 2020)</ref> along with a U-Net <ref type="bibr" target="#b23">(Ronneberger et al., 2015)</ref> architecture. It is also single-target, however its weights are shared across targets, using a weight modulation mechanism to select a specific source.</p><p>Waveform domain source separation was first explored by <ref type="bibr" target="#b12">Llu?s et al. (2018)</ref>, as well as <ref type="bibr" target="#b9">Jansson et al. (2017)</ref> and <ref type="bibr" target="#b25">Stoller et al. (2018)</ref> with Wave-U-Net. However, those methods were lagging in term of quality, almost 2 dB behind their spectrogram based competitors. Demucs <ref type="bibr" target="#b4">(D?fossez et al., 2019)</ref> was built upon Wave-U-Net, using faster strided convolutions rather than explicit downsampling, allowing for a much larger number of channels, but potentially introducing aliasing artifacts as noted by <ref type="bibr">Pons et al. (2021)</ref>, and extra Gated Linear Unit layers <ref type="bibr" target="#b3">(Dauphin et al., 2017)</ref> and biLSTM. For the first time, waveform domain methods surpassed spectrogram ones when considering the overall SDR (6.3 dB on MusDB), although its performance is still inferior on the other and vocals sources. Conv-Tasnet <ref type="bibr" target="#b13">(Luo and Mesgarani, 2019)</ref>, a model based on masking over a learnt time-frequency representation using dilated convolutions, was also adapted to music source separation by <ref type="bibr" target="#b4">D?fossez et al. (2019)</ref>, but suffered from more artifacts and lower SDR.</p><p>To the best of our knowledge, no other work has studied true end-to-end hybrid source separation, although other teams in the MDX competition used model blending from different domains as a simpler post-training alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>In this Section we present the structure of Hybrid Demucs, as well as the other additions that were added to the original Demucs architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Demucs</head><p>The original Demucs architecture <ref type="bibr" target="#b4">(D?fossez et al., 2019</ref>) is a U-Net <ref type="bibr" target="#b23">(Ronneberger et al., 2015)</ref> encoder/decoder structure. A BiLSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref> is applied between the encoder and decoder to provide long range context. The encoder and decoder have a symetric structure. Each encoder layer is composed of a convolution with a kernel size of 8, stride of 4 and doubling the number of channels (except for the first layer, which sets it to a fix value, typically 48 or 64). It is followed by a ReLU, and a so called 1x1 convolution with Gated Linear Unit activation <ref type="bibr" target="#b3">(Dauphin et al., 2017)</ref>, i.e. a convolution with a kernel size of 1, where the first half of the channels modulates the second half through a sigmoid. The 1x1 convolution double the channels, and the GLU halves them, keeping them constant overall. Symetrically, a decoder layer sums the contribution from the U-Net skip connection and the previous layer, apply a 1x1 convolution with GLU, then a transposed convolution that halves the number of channels (except for the outermost layer), with a kernel size of 8 and stride of 4, and a ReLU (except for the outermost layer). There are 6 encoder layers, and 6 decoder layers, for processing 44.1 kHz audio. In order to limit the impact of aliasing from the outermost layers, the input audio is upsampled by a factor of 2 before entering the encoder, and downsampled by a factor of 2 when leaving the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid Demucs</head><p>Overall architecture Hybrid Demucs extends the original architecture with multi-domain analysis and prediction capabilities. The model is composed of a temporal branch, a spectral branch, and shared layers. The temporal branch takes the input waveform and process it like the standard Demucs. It contains 5 layers, which are going to reduce the number of time steps by a factor of 4 5 = 1024. Compared with the original architecture, all ReLU activations are replaced by Gaussian Error Linear Units (GELU) <ref type="bibr" target="#b6">(Hendrycks and Gimpel, 2016)</ref>.</p><p>The spectral branch takes the spectrogram obtained from a STFT over 4096 time steps, with a hop length of 1024. Notice that the number of time steps immediately matches that of the output of the encoder of the temporal branch. In order to reduce the frequency dimension, we apply the same convolutions as in the temporal branch, but along the frequency dimension. Each layer reduces by a factor of 4 the number of frequencies, except for the 5th layer, which reduces by a factor of 8. After being processed by the spectral encoder, the signal has only one "frequency" left, and the same number of channels and sample rate as the output of the temporal branch. The temporal and spectral representations are then summed before going through a shared encoder/decoder layer which further reduces by 2 the number of time steps (using a kernel size of 4). Its output serves both as the input of the temporal and spectral decoder. Hybrid Demucs has a dual U-Net structure, with the temporal and spectral branches having their respective skip connections.</p><p>The output of the spectral branch is inversed with the ISTFT, and summed with the temporal branch output, giving the final model prediction. Due to this overall design, the model is free to use whichever representation is most conveniant for different parts of the signal, even within one source, and can freely share information between the two representations. The hybrid architecture is represented on 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Padding for easy alignment</head><p>One difficulty was to properly align the spectral and temporal representations for any input length. For an input length L, kernel size K, stride S and padding on each side P , the output of a convolution is of length (L ? K + 2 * P )/S + 1. Following the practice from models like MelGAN <ref type="bibr" target="#b11">(Kumar et al., 2019)</ref> we pad by P = (K ? S)/2, giving an output of L/S, so that matching the overall stride is now sufficiant to exactly match the length of the spectral and temporal representations. We apply this padding both for the STFT, and convolution layers in the temporal encoders.  ZDecoder 2 (C in = 96, C out = 48) T/1024 time steps, 512 freq.</p><p>ZDecoder 2 (C in = 48, C out = 4 ? 2 ? 2) T/1024 time steps, 2048 freq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ISTFT</head><p>TDecoder 5 (C in = 768, C out = 386) T/1024 time steps . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T/256 time steps</head><p>TDecoder 2 (C in = 96, C out = 48)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T/4 time steps</head><p>TDecoder 1 (C in = 48, C out = 4 ? 2 ? 2) T time steps. + <ref type="figure" target="#fig_1">Figure 1</ref>: Hybrid Demucs architecture. The input waveform is processed both through a temporal encoder, and first through the STFT followed by a spectral encoder. The two representations are summed when their dimensions align. The decoder is built symmetrically. The output spectrogram go through the ISTFT and is summed with the waveform outputs, giving the final model output. The Z prefix is used for spectral layers, and T prefix for the temporal ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frequency-wise convolutions</head><p>In the spectral branch, we use frequency-wise convolutions, dividing the number of frequency bins by 4 with every layer. For simplicity we drop the highest bin, giving 2048 frequency bins after the STFT. The input of the 5th layer has 8 frequency bins, which we reduce to 1 with a convolution with a kernel size of 8 and no padding. It has been noted that unlike the time axis, the distribution of musical signals is not truely invariant to translation along the frequency axis. Instruments have specific pitch range, vocals have well defined formants etc. To account for that, <ref type="bibr" target="#b8">Isik et al. (2020)</ref> suggest injecting an embedding of the frequency before applying the convolution. We use the same approach, with the addition that we smooth the initial embedding so that close frequencies have similar embeddings. We inject this embedding just before the second encoder layer. We also investigated using specific weights for different frequency bands. This however turned out more complex for a similar result.</p><p>Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spectrogram representation</head><p>We investigated both with representing the spectrogram as complex numbers <ref type="bibr" target="#b1">(Choi et al., 2020)</ref> or as amplitude spectrograms. We also experimented with using Wiener filtering <ref type="bibr" target="#b17">(Nugraha et al., 2016)</ref>, using Open-Unmix differentiable implementation , which uses an iterative procedure. Using more iterations at evaluation time is usually optimal, but sadly doesn't work well with the hybrid approach, as changing the spectrogram output, without the waveform output being able to adapt will drastically reduce the SDR, and using a high number of iterations at train time is prohibitively slow. In all cases, we differentiably transform the spectrogram branch output to a waveform, summed to the waveform branch output, and the final loss is applied in the waveform domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compressed residual branches</head><p>The original Demucs encoder layer is composed of a convolution with kernel size of 8 and stride of 4, followed by a ReLU, and of a convolution with kernel size of 1 followed by a GLU. Between those two convolutions, we introduce two compressed residual branches, composed of dilated convolutions, and for the innermost layers, a biLSTM with limited span and local attention. Remember that after the first convolution of the 5th layer, the temporal and spectral branches have the same shape. The 5th layer of each branch actually only contains this convolution, with the compressed residual branch and 1x1 convolution being shared.</p><p>Inside a residual branch, all convolutions are with respect to the time dimension, and different frequency bins are processed separately. There are two compressed residual branch per encoder layer. Both are composed of a convolution with a kernel size of 3, stride of 1, dilation of 1 for the first branch and 2 for the second, and 4 times less output dimensions than the input, followed by layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016</ref>) and a GELU activation.</p><p>For the 5th and 6th encoder layers, long range context is processed through a local attention layer (see definition hereafter) as well as a biLSTM with 2 layers, inserted with a skip connection, and with a maximum span of 200 steps. In practice, the input is splitted into frames of 200 time steps, with a stride of 100 steps. Each frame is processed concurrently, and for any time step, the output from the frame for which it is the furthest away from the edge is kept.</p><p>Finally, and for all layers, a final convolution with a kernel size of 1 outputs twice as many channels as the input dimension of the residual branch, followed by a GLU. This output is then summed with the original input, after having been scaled through a LayerScale layer <ref type="bibr" target="#b29">(Touvron et al., 2021)</ref>, with an initial scale of 1e?3. A complete representation of the compressed residual branches is given on 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local attention</head><p>Local attention builds on regular attention <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> but replaces positional embedding by a controllable penalty term that penalizes attending to positions that are far away. Formally, the attention weights from position i to position j is given by</p><formula xml:id="formula_0">w i,j = softmax(Q T i K j ? 4 k=1 k? i,k |i ? j|)</formula><p>where Q i are the queries and K j are the keys. The values ? i,k are obtained as the output of a linear layer, initialized so that they are initially very close to 0. Having multiple ? i,k with different weights k allows the network to efficiently reduce its receptive field without requiring ? i,k to take large values. In practice, we use a sigmoid activation to derive the values ? i,k .</p><p>Interestingly, a similar idea has been developed in NLP <ref type="bibr" target="#b20">(Press et al., 2021)</ref>, although with a fixed penalty rather than a dynamic and learnt one done here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stabilizing training</head><p>We observed that Demucs training could be unstable, especially as we added more layers and increased the training set size with 150 extra songs. Loading the model just before its divergence point, we realized that the weights for the innermost encoder and decoder layers would get very large eigen values.</p><p>A first solution is to use group normalization (with 4 groups) just after the non residual convolutions for the layers 5 and 6 of the encoder and the decoder. Using normalization on all layers deteriorates performance, but using it only on the innermost layers seems to stabilize training without hurting performance. Interestingly, when the training is stable (in particular when trained only on MusDB), using normalization was at best neutral with respect to the separation score, but never improved it, and considerably slowed down convergence during the first half of the epochs. When the training was unstable, using normalization would improve the overall performance as it allows the model to train for a larger number of epochs.</p><p>A second solution we investigated was to use singular value regularization <ref type="bibr" target="#b32">(Yoshida and Miyato, 2017)</ref>. While previous work used the power method iterative procedure, we obtained better and faster approximations of the largest singular value using a low rank SVD method <ref type="bibr" target="#b5">(Halko et al., 2011)</ref>. This solution has the advantage of always improving generalization, even when the training was already stable. Sadly, it was not sufficient on its own to remove entirely instabilities, but only to reduce them. Another down side was the longer training time due to the extra low rank SVD evaluation. In the end, in order to both achieve the best performance and remove entirely training instabilities, the two solutions were combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Important: The results presented in this section are results obtained as part of the MDX challenge. We provide easier to reproduce results and detailed ablation that were conducted after the challenge in the Section "Reproducibility and Ablation" hereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>The 2021 MDX challenge <ref type="bibr" target="#b16">(Mitsufuji et al., 2021)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Realistic remix of tracks</head><p>We achieved further gains, by fine tuning the models on a specifically crafted dataset, and with longer training samples (30 seconds instead of 10). This dataset was built by combining stems from separate tracks, while respecting a number of conditions, in particular beat matching and pitch compatibility. Note that training from scratch on this dataset led to worse performance, likely because the model could rely too much on melodic structure, while random remixing forces the model to separate without this information.</p><p>We use librosa <ref type="bibr" target="#b15">(McFee et al., 2015)</ref> for both beat tracking and tempo estimation, as well as chromagram estimation. Beat tracking is applied only on the drum source, while chromagram estimation is applied on the bass line. We aggregate the chromagram over time to a single chroma distribution and find the optimal pitch shift between two stems to maximize overlap (as measured by the L1 distance). We assume that the optimal shift for the bass line is the same for the vocals and accompaniments. Similarly, we align the tempo and first beat. In order to limit artifacts, we only allow two stems to blend if they require less than 3 semi-tones of shift and 15% of tempo change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>The MDX challenge introduced a novel Signal-To-Distortion measure. Another SDR measure existed, as introduced by <ref type="bibr" target="#b31">Vincent et al. (2006)</ref>. The advantage of the new definition is its simplicty and fast evaluation. The new definition is simply defined as SDR = 10 log 10 n s</p><formula xml:id="formula_1">(n) 2 + n s(n) ??(n) 2 + ,<label>(1)</label></formula><p>where s is the ground truth source,? the estimated source, and n the time index. In order to reliably compare to previous work, we will refer to this new SDR definition as nSDR, and to the old definition as SDR. Note that when using nSDR on the MDX test set, the metric is defined as the average across all songs. The evaluation on the MusDB test set follows the traditional median across the songs of the median over all 1 second segments of each song.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>The model submitted to the competitions were actually bags of 4 models. For Track A, we had to mix hybrid and non hybrid Demucs models, as the hybrid ones were having worse performance on the bass source. On Track B, we used only hybrid models, as the extra training data allowed them to perform better for all sources. Note that a mix of Hybrid models using CaC or real masking were used, mostly because it was too costly to reevaluate all models for the competition. For details on the exact architecture and hyper-parameter used, we refer the reader to our Github repository facebookresearch/demucs.</p><p>For the baselines, we report the numbers from the top participants at the MDX competition <ref type="bibr" target="#b16">(Mitsufuji et al., 2021)</ref>. We focus particularly on the KUIELAB-MDX-Net model, which came in second. This model builds on <ref type="bibr" target="#b1">(Choi et al., 2020)</ref> and combines a pure spectrogram model with the prediction from the original Demucs <ref type="bibr" target="#b4">(D?fossez et al., 2019)</ref> model for the drums and bass sources. When comparing models on MusDB, we also report the numbers for some of the best performing methods outside of the MDX competition, namely D3Net  and ResUNetDecouple+ <ref type="bibr" target="#b10">(Kong et al., 2021)</ref>, as well as the original Demucs model <ref type="bibr" target="#b4">(D?fossez et al., 2019)</ref>. Note that those models were evaluated on MusDB (not HQ) which lacks the frequency content between 16 kHz and 22kHz. This can bias the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on MDX</head><p>We provide the results from the top participants at the MDX competition on <ref type="table" target="#tab_1">Table 1</ref> for the track A (trained on MusDB HQ only) and on <ref type="table" target="#tab_2">Table 2</ref> for track B (any training data). We also report for track A the metrics for the Demucs architecture improved with the residual branches, but without the spectrogram branch. The hybrid approach especially improves the nSDR on the Other and Vocals source. Despite this improvement, the Hybrid Demucs model is still performing worse than the KUIELAB-MDX-Net on those two sources. On Track B, we notice again that the Hybrid Demucs architecture is very strong on the Drums and Bass source, while lagging behind on the Other and Vocals source.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on MusDB</head><p>We show on <ref type="table" target="#tab_3">Table 3</ref> the SDR metrics as measured on the MusDB dataset. Again, Hybrid Demucs achieves the best performance for the Drums and Bass source, while improving quite a lot over waveform only Demucs for the Other and Vocals, but not enough to surpasse KUIELAB-MDX-Net, which is purely spectrogram based for those two sources. Interestingly, the best performance on the Vocals source is also achieved by ResUNetDecouple+ <ref type="bibr" target="#b10">(Kong et al., 2021)</ref>, which uses a novel complex modulation of the input spectrogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human evaluations</head><p>We also performed Mean Opinion Score human evaluations. We re-use the same protocol as in <ref type="bibr" target="#b4">(D?fossez et al., 2019)</ref>: we asked human subjects to evaluate a number of samples based on two criteria: the absence of artifacts, and the absence of bleeding (contamination). Both are evaluated on a scale from 1 to 5, with 5 being the best grade. Each subject is tasked with evaluating 25 samples of 12 seconds, drawn randomly from the 50 test set tracks of MusDB. All subjects have a strong experience with music (amateur and professional musicians, sound engineers etc). The results are given on <ref type="table" target="#tab_4">Table 4</ref> for the quality, and 5 for the bleeding. We observe strong improvements over the original Demucs, although we observe some regression on the bass source when considering quality. The model KUIELAb-MDX-Net that came in second at the MDX competition performs the best on vocals. The Hybrid Demucs architecture however reduces by a large amount bleeding across all sources.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility and Ablation</head><p>In this section, we provide ablation of the performance of the model, as well as a simpler setup for reproducing the performance of the model submitted to MDX Track A. Note that the numbers and analysis presented here might differ slightly from the ones presented up to now, and should be preferred when referring to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>The model submitted to the MDX competition Track A used heterogeneous configurations, as we used any model that was sufficiently trained at any given time. This led to a complex bag of 4 models, some of which were used only for some sources. While suitable for a competition, such a complex model does get in the way of reproducing easily the performance achieved.</p><p>The training grids for all the models presented in this section can be found on GitHub repository facebookresearch/demucs, in the file demucs/grids/repro.py, and demucs/grids/repro_ ft.py for the fine tuned models.</p><p>We reproduced the performance of the model submitted to Track A by training two time domain Demucs (with the residual branches depicted on <ref type="figure" target="#fig_2">Figure 2</ref>), and two hybrid Demucs using Complex-As-Channels representation. All four models were trained for 600 epochs, using the SVD penalty and exponential moving average on the weights. Within each domain, only the random seed changes between the two models. The four models were fined tuned on the realistic remix of tracks dataset. The predictions of the four models are averaged into the final prediction, with equal weights over all sources.</p><p>As a first ablation, we also form one bag composed of the two time domain only models, and another bag with the two hybrid models only. For fairness, we evaluate each model twice with a random shift, which is known to improve the performance <ref type="bibr" target="#b4">(D?fossez et al., 2019)</ref>. We also compare to the original Demucs model, retrained on MusDB HQ, using 10 random shifts, as done in <ref type="bibr" target="#b4">(D?fossez et al., 2019)</ref>.</p><p>The results are presented on <ref type="table" target="#tab_6">Table 6</ref>. We reach an overall SDR of 7.64 dB, just 0.04 dB of the model submitted to MDX Track A. We notice a difference in performance between time only, and hybrid only bags only for the bass source and the other source. We still decided to use the same weights over all sources for each type of model for simplicity. We can see the advantage of averaging multiple models, as the combination of both time only and hybrid only model surpasses either ones individually for instance on the drums or the vocals sources. Note however that when training with extra training data, e.g. for the MDX Track B models, the hybrid models were always better than the time only ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation</head><p>We report on <ref type="table" target="#tab_7">Table 7</ref> a short ablation study of the model. We start from a time only improved Demucs, e.g. trained with residual branches, local attention and svd penalty. We can first oberve the effect of fine tuning on a set of realistic remixes, which improves by 0.3 dB the SDR overall. Further gains are achieved using the bagging. Using Exponential Moving Average on the weights improves the SDR by 0.2dB. The effect of the SVD penalty is more contrasted, with on overall gain of 0.1dB, mainly due to the improved vocals (+0.7 dB), but with a deterioration on the drums source (-0.4 dB).</p><p>Finally, removing the LSTM or the local attention in the residual branches lead to a strong decrease of the SDR. Interestingly, the local attention is the most important, despite the absence of positional embedding. One decision taken during the challenge was to switch to Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.</p><p>GELU instead of ReLU. The ablation indicates that no real gain is achieved here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We introduced a number of architectural changes to the Demucs architecture that greatly improved the quality of source separation for music. On the MusDB HQ benchark, the gain is around 1.4 dB. Those changes include compressed residual branches with local attention and chunked biLSTM, and most importantly, a novel hybrid spectrogram/temporal domain U-Net structure, with parallel temporal and spectrogram branches, that merge into a common core.</p><p>Those changes allowed to achieve the first rank at the 2021 Sony Music DemiXing challenge, and translated into strong improvements of the overall quality and absence of bleeding between sources as measured by human evaluations. For all its gain, one limitation of our approach is the increased complexity of the U-Net encoder/decoder, requiring careful alignmement of the temporal and spectral signals through well shaped convolutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Hybrid</head><label></label><figDesc>Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>STFT TEncoder 1</head><label>1</label><figDesc>(C in = 2, C out = 48) T time steps T/4 time steps TEncoder 2 (C in = 48, C out = 96) T/16 time steps . . . T/256 time steps TEncoder 5 (C in = 384, C out = 768) T/1024 time steps ZEncoder 1 (C in = 2 ? 2, C out = 48) T /1024 time steps 2048 freq. T/1024 time steps, 512 freq. ZEncoder 2 (C in = 48, C out = 96) T/1024 time steps, 256 freq. . . . T/1024 time steps, 8 freq. ZEncoder 5 (C in = 384, C out = 768) T/1024 time steps, 1 freq. + Encoder 6 (C in = 768, C out = 1586) T/2048 time steps Decoder 6 (C in = 1586, C out = 768) T/1024 time steps ZDecoder 5 (C in = 768, C out = 386) T/1024 time steps, 8 freq. . . . T/1024 time steps, 256 freq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>HybridFigure 2 :</head><label>2</label><figDesc>Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.GELU(Conv1d(C in , C out , K = 8, S = 4)) GELU(LN(Conv1d(C out , C out /4, K = 3, D = 1))) BiLSTM(layers = 2, span = 200)LocalAttention(heads = 4) if i ? {5, 6} GLU(LN(Conv1d(C out /4, 2 ? C out , K = 1))) LayerScale(init = 1e?3)GELU(LN(Conv1d(C out , C out /4, K = 3, D = 2))) BiLSTM(layers = 2, span = 200)LocalAttention(heads = 4) if i ? {5, 6}GLU(LN(Conv1d(C out /4, 2 ? C out , K = 1)))LayerScale(init = 1e?3) GLU(Conv1d(C out , 2 ? C out , K = 1, S = 1Representation of the compressed residual branches that are added to each encoder layer. For the 5th and 6th layer, a BiLSTM and a local attention layer are added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>offered two tracks: Track A, where only MusDB HQ (Rafii et al., 2019) could be used for training, and Track B, where any data could be used. MusDB HQ, released under mixed licensing 1 is composed of 150 tracks, including 86 for the train set, 14 for the valid, and 50 for the test set. For Track B, we additionally trained using 150 tracks for an internal dataset, and repurpose the test set of MusDB as training data, keeping only the original validation set for model selection. Models are evaluated either through the MDX AI Crowd API 2 , or on the MusDB HQ test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of Hybrid Demucs on the MDX test set, when trained only on MusDB (track A) using the nSDR metric.</figDesc><table><row><cell>Method</cell><cell>All</cell><cell cols="4">Drums Bass Other Vocals</cell></row><row><cell>Hybrid Demucs</cell><cell cols="2">7.33 8.04</cell><cell cols="2">8.12 5.19</cell><cell>7.97</cell></row><row><cell cols="3">KUIELAB-MDX-Net 7.24 7.17</cell><cell>7.23</cell><cell>5.63</cell><cell>8.90</cell></row><row><cell>Music_AI</cell><cell cols="2">6.88 7.37</cell><cell>7.27</cell><cell>5.09</cell><cell>7.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of Hybrid Demucs on the MDX test set, when trained with extra training (track B) using the nSDR metric.</figDesc><table><row><cell>Method</cell><cell>All</cell><cell cols="4">Drums Bass Other Vocals</cell></row><row><cell>Hybrid Demucs</cell><cell cols="2">8.11 8.85</cell><cell cols="2">8.86 5.98</cell><cell>8.76</cell></row><row><cell cols="3">KUIELAB-MDX-Net 7.37 7.55</cell><cell>7.50</cell><cell>5.53</cell><cell>8.89</cell></row><row><cell>AudioShake</cell><cell cols="2">8.33 8.66</cell><cell>8.34</cell><cell>6.51</cell><cell>9.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison on the MusDB (HQ for Hybrid Demucs) test set, using the original SDR metric.</figDesc><table><row><cell cols="7">This includes methods that did not participate in the competition. "Mode" indicates if the waveform</cell></row><row><cell cols="7">(W) or spectrogram (S) domain is used. Model with a "*" were evaluated on MusDB HQ.</cell></row><row><cell>Method</cell><cell cols="2">Mode All</cell><cell cols="4">Drums Bass Other Vocals</cell></row><row><cell>Hybrid Demucs*</cell><cell cols="3">S+W 7.68 8.24</cell><cell cols="2">8.76 5.59</cell><cell>8.13</cell></row><row><cell>Demucs v2</cell><cell>W</cell><cell cols="2">6.28 6.86</cell><cell>7.01</cell><cell>4.42</cell><cell>6.84</cell></row><row><cell cols="4">KUIELAB-MDX-Net* S+W 7.47 7.20</cell><cell>7.83</cell><cell>5.90</cell><cell>8.97</cell></row><row><cell>D3Net</cell><cell>S</cell><cell cols="2">6.01 7.01</cell><cell>5.25</cell><cell>4.53</cell><cell>7.24</cell></row><row><cell>ResUNetDecouple+</cell><cell>S</cell><cell cols="2">6.73 6.62</cell><cell>6.04</cell><cell>5.29</cell><cell>8.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Mean Opinion Score results when asking to rate the quality and absence of artifacts in the generated samples, from 1 to 5 (5 being the best grade). Standard deviation is around 0.15.</figDesc><table><row><cell>Method</cell><cell>All</cell><cell cols="4">Drums Bass Other Vocals</cell></row><row><cell>Ground Truth</cell><cell cols="2">4.12 4.12</cell><cell>4.25</cell><cell>3.92</cell><cell>4.18</cell></row><row><cell>Hybrid Demucs</cell><cell cols="2">2.83 3.18</cell><cell>2.58</cell><cell>2.98</cell><cell>2.55</cell></row><row><cell cols="3">KUIELAB-MDX-Net 2.86 2.70</cell><cell>2.68</cell><cell>2.99</cell><cell>3.05</cell></row><row><cell>Demucs v2</cell><cell cols="2">2.36 2.62</cell><cell cols="2">2.89 2.31</cell><cell>1.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Mean Opinion Score results when asking to rate the absence of bleeding between the sources, from 1 to 5 (5 being the best grade). Standard deviation is around 0.15.</figDesc><table><row><cell>Method</cell><cell>All</cell><cell cols="4">Drums Bass Other Vocals</cell></row><row><cell>Ground Truth</cell><cell cols="2">4.40 4.51</cell><cell>4.52</cell><cell>4.13</cell><cell>4.43</cell></row><row><cell>Hybrid Demucs</cell><cell cols="2">3.04 2.95</cell><cell cols="2">3.25 3.08</cell><cell>2.88</cell></row><row><cell cols="3">KUIELAB-MDX-Net 2.44 2.23</cell><cell>2.19</cell><cell>2.64</cell><cell>2.66</cell></row><row><cell>Demucs v2</cell><cell cols="2">2.37 2.24</cell><cell>2.96</cell><cell>1.99</cell><cell>2.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison on the MusDB HQ test set, using the original SDR metric of different bags of models, as well as with the original Demucs v2 model retrained on MusDB HQ. "Mode" indicates if the waveform (W) or spectrogram (S) domain is used.</figDesc><table><row><cell>Method</cell><cell cols="2">Mode All</cell><cell cols="4">Drums Bass Other Vocals</cell></row><row><cell cols="4">Bag time + hybrid S+W 7.64 8.12</cell><cell cols="2">8.43 5.65</cell><cell>8.35</cell></row><row><cell>Bag time only</cell><cell>W</cell><cell cols="2">7.27 7.57</cell><cell cols="2">8.38 5.17</cell><cell>7.96</cell></row><row><cell>Bag hybrid only</cell><cell cols="3">S+W 7.34 7.96</cell><cell>7.85</cell><cell>5.63</cell><cell>7.95</cell></row><row><cell>Demucs v2 HQ</cell><cell>W</cell><cell cols="2">6.17 6.54</cell><cell>7.08</cell><cell>4.21</cell><cell>6.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation study, all models are trained and evaluated on MusDB HQ. The base model is a time only improved Demucs, with local attention, residual branches and svd penalty. Note that we report single model performance instead of bags of model.</figDesc><table><row><cell>Model</cell><cell cols="4">All Drums Bass Other Vocals</cell></row><row><cell>Improved Time Demucs</cell><cell>6.83 7.06</cell><cell>7.78</cell><cell>4.81</cell><cell>7.65</cell></row><row><cell>+ fine tuning</cell><cell>7.11 7.42</cell><cell>8.18</cell><cell>5.08</cell><cell>7.75</cell></row><row><cell cols="2">+ fine tuning and bagging 7.27 7.57</cell><cell>8.38</cell><cell>5.17</cell><cell>7.96</cell></row><row><cell>-LSTM in branch</cell><cell>6.44 6.66</cell><cell>6.68</cell><cell>4.89</cell><cell>7.54</cell></row><row><cell>-Local Attention</cell><cell>6.29 6.39</cell><cell>6.76</cell><cell>4.68</cell><cell>7.33</cell></row><row><cell>-SVD penalty</cell><cell>6.73 7.45</cell><cell>8.01</cell><cell>4.48</cell><cell>6.98</cell></row><row><cell>-EMA on weights</cell><cell>6.63 6.99</cell><cell>7.36</cell><cell>4.74</cell><cell>7.43</cell></row><row><cell>-GELU + ReLU</cell><cell>6.84 7.19</cell><cell>7.81</cell><cell>4.73</cell><cell>7.63</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/sigsep/website/blob/master/content/datasets/assets/tracklist.csv 2 https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021 Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Investigating u-nets with various intermediate blocks for spectrogram-based singing voice separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehwa</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daewon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soonyoung</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR, editor, 21th International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lasaft: Latent source attentive frequency transformation for conditioned source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehwa</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soonyoung</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Per-Gunnar Martinsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="288" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Poconet: Better speech enhancement with frequency-positional embeddings, semi-supervised conversational data, and biased loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neerad</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Helwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04470</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Singing voice separation with deep u-net convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tillman</forename><surname>Weyde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decoupling magnitude and phase estimation with deep resunet for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22th International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Melgan: Generative adversarial networks for conditional waveform synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Thibault De Boissiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Zhen</forename><surname>Gestin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>De Br?bisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">End-to-end music source separation: is it possible in the waveform domain?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Llu?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<idno>arXiv:1810.12</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcvicar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Fabbro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>St?ter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13559</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Music demixing challenge 2021. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multichannel music separation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Arie</forename><surname>Nugraha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hybrid Spectrogram and Waveform Source Separation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MDX Workshop</title>
		<meeting>the MDX Workshop</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Upsampling artifacts in neural audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Cengarle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The musdb18 corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafar</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>St?ter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stylianos Ioannis Mimilakis, and Rachel Bittner</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MUSDB18-HQ -an uncompressed version of musdb18</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafar</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>St?ter</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3338373</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3338373" />
	</analytic>
	<monogr>
		<title level="m">Stylianos Ioannis Mimilakis, and Rachel Bittner</title>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">All for one and one for all: Improving music separation by bridging networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Sawata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusuke</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Wave-u-net: A multi-scale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03185</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Open-unmix -a reference implementation for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.01667</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobutaka</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">D3net: Densely connected multidilated densenet for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10941</idno>
		<title level="m">Spectral norm regularization for improving the generalizability of deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2016. Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
