<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ConvMLP: Hierarchical Convolutional MLPs for Vision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ConvMLP: Hierarchical Convolutional MLPs for Vision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>MLP-based architectures, which consist of a sequence of consecutive multi-layer perceptron blocks, have recently been found to reach comparable results to convolutional and transformer-based methods. However, most adopt spatial MLPs which take fixed dimension inputs, therefore making it difficult to apply them to downstream tasks, such as object detection and semantic segmentation. Moreover, single-stage designs further limit performance in other computer vision tasks and fully connected layers bear heavy computation. To tackle these problems, we propose Con-vMLP: a hierarchical Convolutional MLP for visual recognition, which is a light-weight, stage-wise, co-design of convolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1 accuracy on ImageNet-1k with 9M parameters and 2.4 GMACs (15% and 19% of MLP-Mixer-B/16, respectively). Experiments on object detection and semantic segmentation further show that visual representation learned by ConvMLP can be seamlessly transferred and achieve competitive results with fewer parameters. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Convolutional-MLPs</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image classification is a fundamental problem in computer vision, and most milestone solutions in the past five years have been dominated by deep convolutional neural networks. Since late 2020 and the rise of Vision Transformer <ref type="bibr" target="#b4">[5]</ref>, researchers have not only been applying Transformers <ref type="bibr" target="#b35">[36]</ref> to image classification, but explored more meta-models other than convolutional neural networks. MLP-Mixer <ref type="bibr" target="#b32">[33]</ref> proposes token-mixing and channel-mixing MLPs to allow communication between spatial locations and channels. ResMLP <ref type="bibr" target="#b33">[34]</ref> uses crosspatch and cross-channel sublayers as the building block, following design of ViT. gMLP <ref type="bibr" target="#b21">[22]</ref> connects channel MLPs by adding spatial gating units. In essence, MLP-based models show that simple feed-forward neural networks can compete with operators like convolution and attention on image classification. However, using MLPs to encode spatial information requires fixing dimension of inputs, which makes it difficult to be deployed on downstream computer vision tasks -such as object detection and semantic segmentation -since they usually require arbitrary resolutions of input sizes. Furthermore, single-stage design, following ViT, may constrain performances on object detection and semantic segmentation since they make predictions based on feature pyramids. Large consecutive MLPs also bring heavy computation burden and more parameters, with high dimension of hidden layers. MLP-Mixer was only able to slightly surpass ViT-Base with its large variant, which is over twice as large and twice as expensive in terms of computation. Similarly, ResMLP suffers from over 30% more parameters and complexity, compared to a transformer-based model of similar performance.</p><p>Based on these observations, we propose ConvMLP: A Hierarchical Convolutional MLP backbone for visual recognition, which is a combination of convolution layers and MLP layers for image classification and can be seamlessly used for downstream tasks like object detection and segmentation as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. To remove constraints on input dimension in other MLP-like frameworks, In conclusion, our contributions are as follows:</p><p>? We analyze the constraints of current MLP-based models for image classification, which only take inputs of fixed dimensions and are difficult to be used in downstream computer vision tasks as backbones. Singlestage design and large computation burden further limit their applications.</p><p>? We propose ConvMLP: a Hierachical Convolutional MLP backbone for visual recognition with co-design of convolution and MLP layers. It is scalable and can be seamlessly deployed on downstream tasks like object detection and semantic segmentation.</p><p>? We conduct extensive experiments on ImageNet-1k for image classification, Cifar and Flowers-102 for transfer learning, MS COCO for object detection and ADE20K for semantic segmentation to evaluate the effectiveness of our ConvMLP model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Convolutional Methods Image classification has been dominated by convolutional neural networks for almost a decade, since the rise of AlexNet <ref type="bibr" target="#b17">[18]</ref>, which introduced a convolutional neural network for image classification, and won the 2012 ILSRVC. Following that, VGGNet <ref type="bibr" target="#b30">[31]</ref> proposed larger and deeper network for better performance. ResNet <ref type="bibr" target="#b8">[9]</ref> introduced skip connections to allow training even deeper networks, and DenseNet <ref type="bibr" target="#b13">[14]</ref> proposed densely connected convolution layers. In the meantime, researchers explored smaller and more light-weight models that would be deployable to mobile devices. MobileNet <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref> consisted of depth-wise and point-wise convolutions, which reduced the number of parameters and computation required. ShuffleNet <ref type="bibr" target="#b25">[26]</ref> found channel shuffling to be effective, and EfficientNet <ref type="bibr" target="#b31">[32]</ref> further employs model scaling to width, depth and resolution for better model scalability.</p><p>Transformer-based Methods Transformer <ref type="bibr" target="#b35">[36]</ref> was proposed for machine translation and has been widely adopted in most natural language processing. Recently, researchers in computer vision area adopt transformer to image classification. They propose ViT <ref type="bibr" target="#b4">[5]</ref> that reshapes image to patches for feature extraction by transformer encoder, which achieves comparable results to CNN-based models. DeiT <ref type="bibr" target="#b34">[35]</ref> further employs more data augmentation and makes ViT comparable to CNN-based model without ImageNet-22k or JFT-300M pretraining. DeiT also proposes an attention-based distillation method, which is used for student-teacher training, leading to even better performance. PVT <ref type="bibr" target="#b36">[37]</ref> proposes feature pyramids for vi- Scale sion transformers, making them more compatible for downstream tasks. Swin Transformer <ref type="bibr" target="#b23">[24]</ref> uses patch-level multiheaded attention and stage-wise design, which also increase transferability to downstream tasks. Shuffle Swin Transformer <ref type="bibr" target="#b14">[15]</ref> proposes shuffle multi-headed attention to augment spatial connection between windows. CCT <ref type="bibr" target="#b6">[7]</ref> proposes a convolutional tokenizer and compact vision transformers, leading to better performance on smaller datasets training from scratch, with fewer parameters compared with ViT. TransCNN <ref type="bibr" target="#b22">[23]</ref> also proposes a co-design of convolutions and multi-headed attention to learn hierarchical representations.</p><formula xml:id="formula_0">C 3 = 256, R = 2 C 3 = 256, R = 3 C 3 = 384, R = 3 Conv-MLP Channel MLP 3?3 DW Conv Channel MLP ? 2 Channel MLP 3?3 DW Conv Channel MLP ? 3 Channel MLP 3?3 DW Conv Channel MLP ? 3 Scale C 4 = 512, R = 2 C 4 = 512, R = 3 C 4 = 768, R = 3</formula><p>MLP-based Methods MLP-Mixer <ref type="bibr" target="#b32">[33]</ref> was recently proposed as a large scale image classifiers that was neither convolutional nor transformer-based. At its core, it consisted of basic matrix multiplications, data layout changes and scalar nonlinearities. ResMLP <ref type="bibr" target="#b33">[34]</ref> followed a ResNet-like structure with MLP-based blocks instead of convolutional ones. Following that, gMLP <ref type="bibr" target="#b21">[22]</ref> proposed a Spatial Gating Unit to process spatial features. S 2 -MLP <ref type="bibr" target="#b38">[39]</ref> adopts shifted spatial feature maps to augment information communication.</p><p>ViP <ref type="bibr" target="#b10">[11]</ref> employs linear projection on the height, width and channel dimension separately. All these methods have MLPs on fixed spatial dimensions which make it hard to be used in downstream tasks since the dimensions of spatial MLPs are fixed. Cycle MLP <ref type="bibr" target="#b1">[2]</ref> and AS-MLP <ref type="bibr" target="#b18">[19]</ref> are concurrent works. The former replaces the spatial MLPs with cycle MLP layers and the latter with axial shifted MLPs, which make the model more flexible for varying inputs sizes. They reach competitive results on both image classification and other downstream tasks. Hire-MLP <ref type="bibr" target="#b5">[6]</ref> is another concurrent work that uses Hire-MLP blocks to learn hierarchical representations and achieves comparable result to transformer-based model on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ConvMLP</head><p>In this section, we first introduce overall design and framework of our ConvMLP. Then, we follow that design pattern including convolutional tokenizer, convolution stage and Conv-MLP Stage. We also explain how model scaling is applied to ConvMLP on convolution and Conv-MLP stages.   <ref type="bibr" target="#b9">[10]</ref> and dropout. We then apply global average pooling across to the output feature map, F 4 , and send it through the classification head. When applying ConvMLP to downstream tasks, the feature maps F 1 , F 2 , F 3 and F 4 can be used to generate feature pyramids with no constraints on input size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convolutional Tokenizer</head><p>As stated, we replace the original patch tokenizer with a convolutional tokenizer. It includes three convolutional blocks, each consisting of a 3x3 convolution, batch normalization and ReLU activation. The tokenizer is also appended with a max pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convolution Stage</head><p>In order to augment spatial connections, we adopt a fully-convolutional first stage. It consists of multiple blocks, where each block is comprised of two 1x1 convolution layers with a 3x3 convolution in between.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Conv-MLP Stage</head><p>To reduce constraints on input dimension, we replace all spatial MLPs with channel MLPs. Since channel MLP only share weights across channels which lacks spatial interac-tions, we make up it by adding convolution layers in early stage, down-sampling and MLP blocks. Convolutional Downsampling In the baseline model, we follow Swin Transformer <ref type="bibr" target="#b23">[24]</ref> that uses a patch merging method based on linear layers to down-sample feature maps. To augment adjacent spatial intersection, we replace patch merging with a 3x3 convolution layer under stride 2. It improves the classification accuracy while only brings a few more parameters. Convolution in MLP block We further add a depth-wise convolution layer between two channel MLPs in one MLP block and name it Conv-MLP block. It is a 3x3 convolution layer with the same channel to the two channel MLPs, which is also used in recent Shuffle Swin Transformer <ref type="bibr" target="#b14">[15]</ref> to augment neighbor window connections. It makes up the deficiency of removing spatial MLPs, which improves the performance by a large margin while only brings few parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Model Scaling</head><p>To make ConvMLP scalable, we scale up ConvMLP on both width and depth of convolution stages and Conv-MLP stages. We present 3 ConvMLP variants. Our smallest ConvMLP-S starts with only a two convolutional blocks, and has 2, 4 and 2 Conv-MLP blocks in the three Conv-MLP stages respectively. ConvMLP-M and ConvMLP-L start with three convolutional blocks. ConvMLP-M has 3, 6 and 3, and ConvMLP-L has 4, 8 and 3 Conv-MLP blocks in the three Conv-MLP stages. Details are also presented in <ref type="table" target="#tab_2">Table 1</ref>. Experiments show that the performance of image classification and downstream tasks improves consistently with model scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we mainly introduce our experiments on ImageNet-1k, CIFAR, Flowers-102, MS COCO and ADE20K benchmark. We first show ablation studies on different convolution modules in our ConvMLP framework to evaluate their effectiveness. Then, we compare ConvMLP to other state-of-the-art models on ImageNet-1k. We then    show transferring ability on CIFAR and Flowers-102. On MS COCO and ADE20K benchmark, we use ConvMLP as backbones of RetinaNet, Mask R-CNN, Semantic FPN and it shows consistent improvements on these different downstream models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet-1k</head><p>ImageNet-1k [18] contains 1.2M training images and 50k images on 1000 categories for evaluating performances of classifiers. We follow standard practice provided by timm <ref type="bibr" target="#b37">[38]</ref> toolbox. We use RandAugment <ref type="bibr" target="#b3">[4]</ref> Mixup <ref type="bibr" target="#b41">[42]</ref>, and CutMix <ref type="bibr" target="#b40">[41]</ref> for data augmentation. AdamW <ref type="bibr" target="#b24">[25]</ref> is adopted as optimizer with momentum of 0.9 and weight de-cay of 0.05. The initial learning rate is 0.0005 with batch size of 128 on each GPU card. We use 8 NVIDIA RTX A6000 GPUs to train all models for 300 epochs and the total batch size is 1024. All other training settings and hyperparameters are adopted from Deit <ref type="bibr" target="#b34">[35]</ref> for fair comparisons. For those results in ablation study, we train these models for 100 epochs with batch size 256 on each GPU and use 4 GPUs with learning rate at 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Our baseline model Pure-MLP Baseline is composed of one patch converter and a sequence of channel MLPs in following stages. In <ref type="table" target="#tab_4">Table 2</ref>, the baseline model reaches 63.29% top-1 accuracy on ImageNet-1k and we first replace the first stage of MLPs into a convolution stage. Then, we replace the down-sampler from patch merging into a single 3 ? 3 convolution layer with stride 2, which further improves top-1 accuracy to 69.56%. To further augment spatial information communication, we add 3 ? 3 depth-wise convolution between two channel MLPs and extends training epochs to 300. Finally, we modify the convolution stage with successive 1 ? 1, 3 ? 3, 1 ? 1 convolution blocks and builds ConvMLP-S model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with SOTA</head><p>In <ref type="table" target="#tab_6">Table 3</ref>, we compare ConvMLP to other state-ofthe-art image classification models on ImageNet-1k. We include Convolution-based, Transformer-based and MLPbased methods under different scales. We also present number of parameters and GMACs of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Transfer learning</head><p>Dataset We use CIFAR-10/CIFAR-100 <ref type="bibr" target="#b16">[17]</ref> and Flowers-102 <ref type="bibr" target="#b26">[27]</ref> to evaluate transferring ability of ImageNetpretrained ConvMLP variants. Each model was fine-tuned for 50 epochs with a learning rate of 3e-4 (with cosine scheduler), weight decay of 5e-2, 10 warmup and cooldown epochs. We used the same training script and therefore augmentations as the ImageNet-1k experiments. We also resized all images to 224?224. Results The results are presented in <ref type="table" target="#tab_7">Table 4</ref>. We report results from ResMLP, ViT and DeiT as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Object Detection</head><p>Dataset MS COCO <ref type="bibr" target="#b20">[21]</ref> is a widely-used benchmark for evaluating object detection model. It has 118k images for training and 5k images for evaluating performances of object detectors. We follow standard practice of Reti-naNet <ref type="bibr" target="#b19">[20]</ref> and Mask R-CNN <ref type="bibr" target="#b7">[8]</ref> with ResNet as backbones in mmdetection <ref type="bibr" target="#b0">[1]</ref>. We replace ResNet backbones with ConvMLP and adjust the dimension of convolution layers in feature pyramids accordingly. We also replace SGD optimizer with AdamW and adjust learning rate to 0.0001 with weight decay at 0.0001, which follows the configs in PVT <ref type="bibr" target="#b36">[37]</ref>. We train both RetinaNet and Mask R-CNN for 12 epochs on 8 GPUs with total batch size of 16. Results We transfer ResNet, Pure-MLP and ConvMLP variants to object detection on MS COCO and the results are presented in <ref type="figure" target="#fig_1">Figure 3</ref>. It can be observed that Con-vMLP achieves better performance on object detection and instance segmentation consistently as backbones of Reti-naNet and Mask R-CNN compared with Pure-MLP and ResNet. More details of the results are presented in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Semantic Segmentation</head><p>Dataset ADE20K <ref type="bibr" target="#b42">[43]</ref> is a widely-used dataset for semantic segmentation, which has 20k images for training and 2k images for evaluating the performance of semantic segmentation models. We employ standard practice of Semantic FPN <ref type="bibr" target="#b15">[16]</ref> implemented based on mmsegmentation <ref type="bibr" target="#b2">[3]</ref>. Following PVT in semantic segmentation, we train ConvMLP-based Semantic FPN on 8 GPUs with total batch size of 16 for 40k iterations. We also replace optimizer from SGD to AdamW with learning rate at 0.0002 and weight decay at 0.0001. The learning rate decays with polynomial rate at 0.9 and input images are randomly resized and cropped to 512 ? 512. Results All experimental results on ADE20K are presented in <ref type="figure" target="#fig_1">Figure 3</ref>. Similar to the results of object detection, it shows that visual representations learned by ConvMLP can be transferred to pixel-level prediction task like semantic segmentation. More details of the results can be found in Appendix.  <ref type="figure" target="#fig_2">Figure 4</ref> to analyze the differences in visual representations learned by these models, and similar feature maps of transformer-based model are presented in T2T-ViT <ref type="bibr" target="#b39">[40]</ref>. We observe that representations learned by ConvMLP involve more low-level features like edges or textures compared with ResNet and more semantics compared with Pure-MLP Baseline .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we analyze the constraints of current MLPbased models for visual representation learning: 1. Spatial MLPs only take inputs with fixed resolutions, making transfer to downstream tasks, such as object detection and segmentation, difficult. 2. Single-stage design and fully connected layers further constrain usage due to the added complexity. To tackle these problems, we propose Con-vMLP: a Hierarchical Convolutional MLP for visual representation learning through combining convolutional layers and MLPs. The architecture can be seamlessly prepended to downstream networks like RetinaNet, Mask R-CNN and Semantic FPN. Experiments further show that it can achieve competitive results on different benchmarks with fewer parameters compared to other methods. The main limitation of ConvMLP is that ImageNet performance scales slower with model size. We leave this to be explored in future works.    patterns worsen with the larger models, despite that they achieve significantly higher accuracy scores. We believe that this type of analysis suggests that these networks show significantly different biases. We also believe that this analysis supports claims that MLP-Mixer is overfitting the dataset and that scale is not all one needs to perform well. Rather that scale harms performance, but not in the way that we are evaluating models. With this analysis we believe that there is significant evidence that MLP-Mixer and ResMLP overfit the ImageNet dataset and that accuracy cannot be the only score used to evaluate a model's performance. With this analysis we encourage the reader to use Occam's razor when selecting models and encourage practitioners to perform similar and more inclusive analyses when evaluating models. This analysis highlights that just because one does well on accuracy does not mean that is has low bias or will generalize well to the real world. With this it becomes important to analyze models and understand the biases that they have as well as the biases within the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Detection &amp; Semantic Segmentation</head><p>We provide details of results on MS COCO and ADE20K benchmarks as reference to <ref type="figure" target="#fig_1">Figure 3</ref>. For MS COCO, we use ResNet, Pure-MLP and ConvMLP as backbone of RetinaNet and Mask R-CNN. The results are shown in <ref type="table" target="#tab_9">Table 5</ref> and <ref type="table" target="#tab_10">Table 6</ref>. For ADE20k, we use ResNet, Pure-MLP and ConvMLP as backbone of Semantic FPN and the result is shown in <ref type="table" target="#tab_11">Table 7</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparing MLP-Mixer to ConvMLP. ConvMLP adopts a simple hierarchical multi-stage co-design of convolutions and MLPs and achieves both more suitable representations as well as better accuracy vs computation trade-offs for visual recognition tasks including classification, detection and segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparisons between ConvMLP, Pure-MLP and ResNet as backbones of RetinaNet, Mask R-CNN on MS COCO and Semantic FPN on ADE20K. ConvMLP-based models show consistent improvements under different evaluation metrics and tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of feature maps in different stages of ResNet50, MLP-Mixer, Pure-MLP Baseline and ConvMLP-M. Visual representations learned by ConvMLP-M show both semantic and low-level information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>We visualize feature maps of ResNet50, MLP-Mixer-B/16, Pure-MLP Baseline and ConvMLP-M under (1024, 1024) input size (MLP-Mixer-B/16 under (224, 224) due to dimension constraint) in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Salient Maps of selected ImageNet images, comparing MLP-Mixer-B/16, ResMLP-B24, and ConvMLP-L. The labels at the top represent the ground truth label and the smaller labels below the images show the network's prediction. RetinaNet Backbone # Params AP b AP b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Detailed model architecture of ConvMLP in different scales. R denotes scaling ratio of hidden layers in MLP.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Conv Stage Conv Downsampling Depth-Wise Conv Epochs # Params (M) GMACs Top-1 Acc (%)</figDesc><table><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>100</cell><cell>7.88</cell><cell>1.47</cell><cell>63.29</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>100</cell><cell>7.89</cell><cell>1.59</cell><cell>66.69</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>100</cell><cell>8.71</cell><cell>1.65</cell><cell>69.56</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>100</cell><cell>7.91</cell><cell>1.59</cell><cell>73.84</cell></row><row><cell></cell><cell></cell><cell></cell><cell>100</cell><cell>8.73</cell><cell>1.65</cell><cell>74.04</cell></row><row><cell></cell><cell></cell><cell></cell><cell>300</cell><cell>8.73</cell><cell>1.65</cell><cell>76.33</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>300</cell><cell>9.02</cell><cell>2.40</cell><cell>76.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on ImageNet-1k validation set. All experiments are based on ConvMLP-S. ? denotes slightly modified Conv Stage with improved accuracy in the long run which is used in our final ConvMLP-S model.3.1. Overall DesignThe overall framework of ConvMLP is illustrated inFigure 2. Unlike other MLP-based models, we use a convolutional tokenizer to extract the initial feature map F 1 ( H 4 ? W 4 ? C 1 dimensional). To reduce computation and improve spatial connections, we follow tokenization with a pure convolutional stage, producing feature map F 2 ( H 8 ? W 8 ? C 2 dimensional). Then we place 3 Conv-MLP stages, generating 2 feature maps F 3 and F 4 ( H 16 ? W 16 ?C 3 and H 32 ? W 32 ?C 4 dimensional respectively). Each Conv-MLP stage includes multiple Conv-MLP blocks and each Conv-MLP block has one channel MLP followed by a depth-wise convolutional layer, succeeded by another channel MLP. Similar to previous works, we include residual connections and Layer Normalization applied to inputs in the block. Each channel MLP consists of two fully connected layers with a GeLU activation</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>ImageNet-1k validation top-1 accuracy comparison between ConvMLP and state-of-the-art models. Comparing to other MLP-based methods, ConvMLP achieved the best Acc/GMACs and Acc/MParams in different model size ranges. ?: reported from DeiT for fairer comparison; ViT-S was not proposed in the original paper. ? specifies image resolution, if different from 224?224.</figDesc><table><row><cell>Model</cell><cell cols="5"># Params (M) ImageNet-1k (%) CIFAR-10 (%) CIFAR-100 (%) Flowers-102 (%)</cell></row><row><cell>ConvMLP-S</cell><cell>9.0</cell><cell>76.8</cell><cell>98.0</cell><cell>87.4</cell><cell>99.5</cell></row><row><cell>ResMLP-S12 [34]</cell><cell>15.4</cell><cell>76.6</cell><cell>98.1</cell><cell>87.0</cell><cell>97.4</cell></row><row><cell>ConvMLP-M</cell><cell>17.4</cell><cell>79.0</cell><cell>98.6</cell><cell>89.1</cell><cell>99.5</cell></row><row><cell>ResMLP-S24 [34]</cell><cell>30.0</cell><cell>79.4</cell><cell>98.7</cell><cell>89.5</cell><cell>97.4</cell></row><row><cell>ConvMLP-L</cell><cell>42.7</cell><cell>80.2</cell><cell>98.6</cell><cell>88.6</cell><cell>99.5</cell></row><row><cell>ViT-B [5]</cell><cell>86.6</cell><cell>81.8</cell><cell>99.1</cell><cell>90.8</cell><cell>98.4</cell></row><row><cell>DeiT-B [35]</cell><cell>86.6</cell><cell>83.4</cell><cell>99.1</cell><cell>91.3</cell><cell>98.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Top-1 accuracy when pre-trained on ImageNet-1k and fine-tuned on CIFAR-10, CIFAR100 and Flowers-102. It reaches best performance on Flowers-102 among different model sizes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison between ConvMLP and ResNet as RetinaNet backbones on MS COCO. Mask R-CNN Backbone # Params AP b AP b</figDesc><table><row><cell>50</cell><cell>AP b 75</cell><cell>AP m AP m 50</cell><cell>AP m 75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Comparison between ConvMLP and ResNet as Mask R-CNN backbones on MS COCO.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparison between ConvMLP and ResNet as Semantic FPN backbones on ADE20k.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Salient Maps, Bias, and OverFitting</head><p>In an effort to analyze the differences in ConvMLP we investigate the salient maps of MLP-Mixer, ResMLP, and ConvMLP. We created salient maps based on the final output. We then selected a random sample of images from the network and investigated what they looked like. For the most part images had fairly similar maps, but in some images there were stark differences that we believe highlight major differences in the networks. While these images are hand selected we note that there are some counter examples. Though the counter examples exist we found that there exists a trend within the biases, which we try to highlight here. We show these selected images in <ref type="figure">Figure 5</ref>. To the left we label each network. At the top of the column we provide the ground truth label for the image and underneath each image we provide the network's corresponding predicted label. Specifically we show salient maps where the MLP-Mixer model contains 59.9M params, ResMLP contains 30M params, and we use ConvMLP-L with 42.7M params. Additionally, we note that the MLP-Mixer and ResMLP models are both versions that were trained from scratch on ImageNet, making for a more fair comparison. The first aspect that should be noted is that MLP-Mixer and ResMLP have more "pixelated" looking salient maps. We can clearly see the effect of ResMLP's smaller patches and subsequently finer resolutions to where the network sees information.</p><p>Starting with the left most image, Binoculars, we notice some interesting things. For one, a human would likely not label this image as an example of binoculars. We should note that several labels within the dataset exist within this image. The second thing to notice is the salient maps. MLP-Mixer pays attention to almost everything except the car and predicts the correct label. ResMLP highlights the hares and a little bit of the person (near the face) but predicts the incorrect label of car wheel. Lastly, ConvMLP also pays attention to the hares but incorrectly classifies the images as hare. This presents an interesting phenomena when analyzing results. We have a clue that MLP-Mixer and ResMLP might be overfitting the data. While ConvMLP makes the misclassification, we note that it is at least paying attention to the part of the image that closely corresponds to the label that it predicts. MLP-Mixer and ResMLP offers more suspicious behavior as they pay attention to different parts of the image than what they predict. This may suggest overfitting.</p><p>In the second image all three networks correctly identify the image as a ballpoint pen, but there are interesting things to note in the salient maps. MLP-Mixer and ConvMLP both ignore the rubix cube and we can see their salient maps outline the pens (possibly pencils). Between MLP-Mixer and ConvMLP we notice that ConvMLP pays slightly more attention to the pens, though MLP-Mixer pays more attention to the rest of the scene. Additionally, we note that ResMLP pays more attention to the rubix cube that the other two networks.</p><p>In the third image we see a worrying problem in MLP-Mixer. Ribeiro et al <ref type="bibr" target="#b28">[29]</ref> showed how huskies can be misclassified as wolves due to the presence of snow within the image. That is, if an image contains snow then it would be far more likley to be classified as a wolf, showing the bias that the network learned. What is worrying here is that the areas MLP-Mixer pays significant attention don't contain dogs, such as the bottom center image and center left. This actually gets worse with the larger Mixer model, not shown, and it exclusively pays attention to sub-images without dogs. This suggests that the network may be biased from the background in the image, which is likely given that images of malamutes are likely to have snow within the image. Again, this suggests potential for overfitting. ResMLP and ConvMLP do not make these same mistakes. Both ResMLP and ConvMLP pay more attention to the dogs in the right most column, which are more centered in their respective sub-pictures, but ConvMLP also pays attention to the dogs in the top row. Neither of these networks pay any significant attention to sub-images without dogs in them, which is a good sign. Despite ResMLP and ConvMLP misclassifying the image, the labels they provide are acceptable, being very close to the true labels. Both Canadian Eskimo Dogs and Malamutes look very similar in appearance and humans commonly call both Huskies. We note here that ImageNet contains exclusively images of Canadian Eskimo Dogs and does not contain images of American Eskimo Dogs, which are more easily distinguishable.</p><p>With the last column we see a similar story. None of the networks get the classification correct. MLP-Mixer again pays significant attention to the majority of the image and specifically the background. While all three models pay attention to the top left background of the image it is clear that MLP-Mixer is focused less on the animals than the other two networks. Again we note that Gazelles, Impalas, and Hartebeests have very similar appearances, and that all three networks predict reasonable labels.</p><p>We found many similar examples of these patterns while analyzing salient maps for these networks and saw these</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cyclemlp: A mlp-like architecture for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoufa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10224</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation,2020.8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hire-mlp: Vision mlp via hierarchical rearrangement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13341</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Escaping the big data paradigm with compact transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulikemu</forename><surname>Abuduweili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05704</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Vision permutator: A permutable mlp-like architecture for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12368</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Shuffle transformer: Rethinking spatial shuffle for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youcheng</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03650</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Asmlp: An axial shifted mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08391</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajad</forename><surname>Chhatkuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03180</idno>
		<title level="m">Transformer in convolutional neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">why should I trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco T?lio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
		<idno>abs/1602.04938</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mlpmixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">S 2 -mlp: Spatial-shift mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07477</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resmlp-B24</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
