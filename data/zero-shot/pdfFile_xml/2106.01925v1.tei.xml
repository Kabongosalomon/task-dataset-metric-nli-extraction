<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
							<email>lbqin@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxuan</forename><surname>Wei</surname></persName>
							<email>fuxuanwei@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Xie</surname></persName>
							<email>tianbaoxie@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-intent SLU can handle multiple intents in an utterance, which has attracted increasing attention. However, the state-of-the-art joint models heavily rely on autoregressive approaches, resulting in two issues: slow inference speed and information leakage. In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate. Specifically, we propose a Global-Locally Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intentslot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance. Experimental results on two public datasets show that our framework achieves state-of-the-art performance while being 11.5 times faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spoken Language Understanding (SLU) <ref type="bibr" target="#b38">(Young et al., 2013</ref>) is a critical component in spoken dialog systems, which aims to understand user's queries. It typically includes two sub-tasks: intent detection and slot filling <ref type="bibr" target="#b30">(Tur and De Mori, 2011)</ref>.</p><p>Since intents and slots are closely tied, dominant single-intent SLU systems in the literature <ref type="bibr" target="#b7">(Goo et al., 2018;</ref><ref type="bibr" target="#b20">Liu et al., 2019b;</ref><ref type="bibr" target="#b2">E et al., 2019;</ref><ref type="bibr" target="#b23">Qin et al., 2019;</ref><ref type="bibr" target="#b29">Teng et al., 2021;</ref><ref type="bibr">Qin et al., 2021b,c)</ref> adopt joint models to consider the correlation between the two tasks, which have obtained remarkable success.</p><p>Multi-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end, * Corresponding author. Xu and Sarikaya (2013) and <ref type="bibr" target="#b12">Kim et al. (2017)</ref> begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot filling task. Recently, Gangadharaiah and Narayanaswamy (2019) make the first attempt to propose a multi-task framework to joint model the multiple intent detection and slot filling.  further propose an adaptive interaction framework (AGIF) to achieve fine-grained multi-intent information integration for slot filling, obtaining state-of-the-art performance. Though achieving the promising performance, the existing multi-intent SLU joint models heavily rely on an autoregressive fashion, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a), leading to two issues:</p><p>? Slow inference speed. The autoregressive models make the generation of slot outputs must be done through the left-to-right pass, which cannot achieve parallelizable, leading to slow inference speed.</p><p>? Information leakage. Autoregressive models predict each word slot conditioned on the previously generated slot information (from leftto-right), resulting in leaking the bidirectional context information.</p><p>In this paper, we explore a non-autoregressive framework for joint multiple intent detection and slot filling, with the goal of accelerating inference speed while achieving high accuracy, which is shown in <ref type="figure" target="#fig_0">Figure 1</ref> <ref type="bibr">(b)</ref>. To this end, we propose a Global-Locally Graph-Interaction Network (GL-GIN) where the core module is a proposed local slot-aware graph layer and global intent-slot interaction layer, which achieves to generate intents and slots sequence simultaneously and nonautoregressively. In GL-GIN, a local slot-aware graph interaction layer where each slot hidden states connect with each other is proposed to explicitly model slot dependency, in order to alleviate uncoordinated slot problem (e.g., B-singer followed by I-song) <ref type="bibr" target="#b34">(Wu et al., 2020)</ref> due to the non-autoregressive fashion. A global intent-slot graph interaction layer is further introduced to perform sentence-level intent-slot interaction. Unlike the prior works that only consider the tokenlevel intent-slot interaction, the global graph is constructed of all tokens with multiple intents, achieving to generate slots sequence in parallel and speed up the decoding process.</p><p>Experimental results on two public datasets MixSNIPS <ref type="bibr" target="#b1">(Coucke et al., 2018)</ref> and Mix-ATIS <ref type="bibr" target="#b9">(Hemphill et al., 1990)</ref> show that our framework not only obtains state-of-the-art performance but also enables decoding in parallel. In addition, we explore the pre-trained model (i.e., Roberta <ref type="bibr" target="#b21">(Liu et al., 2019c)</ref>) in our framework.</p><p>In summary, the contributions of this work can be concluded as follows: (1) To the best of our knowledge, we make the first attempt to explore a non-autoregressive approach for joint multiple intent detection and slot filling; (2) We propose a global-locally graph-interaction network, where the local graph is used to handle uncoordinated slots problem while a global graph is introduced to model sequence-level intent-slot interaction; (3) Experiment results on two benchmarks show that our framework not only achieves the state-of-theart performance but also considerably speeds up the slot decoding (up to ?11.5); (4) Finally, we explore the pre-trained model in our framework. With the pre-trained model, our model reaches a new state-of-the-art level.</p><p>For reproducibility, our code for this paper is publicly available at https://github.com/ yizhen20133868/GL-GIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition</head><p>Multiple Intent Detection Given input sequence x = (x 1 , . . . , x n ), multiple intent detection can be defined as a multi-label classification task that outputs a sequence intent label o I = (o I 1 , . . . , o I m ), where m is the number of intents in given utterance and n is the length of utterance.</p><p>Slot Filling Slot filling can be seen as a sequence labeling task that maps the input utterance x into a slot output sequence o S = (o S 1 , . . . , o S n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), we describe the proposed framework, which consists of a shared selfattentive encoder ( ?3.1), a token-level intent detection decoder ( ?3.2) and a global-local graphinteraction graph decoder for slot filling ( ?3.3). Both intent detection and slot filling are optimized simultaneously via a joint learning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-attentive Encoder</head><p>Following <ref type="bibr" target="#b23">Qin et al. (2019)</ref>, we utilize a selfattentive encoder with BiLSTM and self-attention mechanism to obtain the shared utterance representation, which can incorporate temporal features within word orders and contextual information.</p><p>BiLSTM The bidirectional LSTM (BiL-STM) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref> have been successfully applied to sequence labeling tasks <ref type="bibr" target="#b15">(Li et al., 2020</ref><ref type="bibr" target="#b16">(Li et al., , 2021</ref>. We adopt BiLSTM to read the input sequence {x 1 , x 2 , . . . , x n } forwardly and backwardly to produce contextsensitive hidden states H = {h 1 , h 2 , . . . , h n }, by repeatedly applying the</p><formula xml:id="formula_0">h i = BiLSTM (? emb (x i ), h i?1 , h i+1 ),</formula><p>where ? emb is embedding function.</p><p>Self-Attention Following <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>, we map the matrix of input vectors X ? R n?d (d represents the mapped dimension) to queries Q, keys K and values V matrices by using different linear projections. Then, the self-attention output C ? R n?d is a weighted sum of values:</p><formula xml:id="formula_1">C = softmax QK ? d k V .<label>(1)</label></formula><p>Self-Attentive Encoder  We concatenate the output of BiLSTM and selfattention as the final encoding representation:</p><formula xml:id="formula_2">E = H || C,<label>(2)</label></formula><p>where E = {e 1 , . . . , e n } ? R n?2d and || is concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Token-Level Intent Detection Decoder</head><p>Inspired by <ref type="bibr" target="#b23">Qin et al. (2019)</ref>, we perform a tokenlevel multi-label multi-intent detection, where we predict multiple intents on each token and the sentence results are obtained by voting for all tokens. Specifically, we first feed the contextual encoding E into an intent-aware BiLSTM to enhance its task-specific representations:</p><formula xml:id="formula_3">h I t = BiLSTM e t , h I t?1 , h I t+1 .<label>(3)</label></formula><p>Then, h I t is used for intent detection, using:</p><formula xml:id="formula_4">I t = ?(W I (LeakyReLU(W h h I t +b h ))+b I ),<label>(4)</label></formula><p>where I t denotes the intent results at the t-th word; ? denotes the sigmoid activation function; W h and W I are the trainable matrix parameters. Finally, the sentence intent results o I k can be obtained by:</p><formula xml:id="formula_5">o I = {o I k |( n i=1 1[I (i,k) &gt; 0.5]) &gt; n/2},<label>(5)</label></formula><p>where I (i,k) represents the classification result of token i for o I k . We predict the label as the utterance intent when it gets more than half positive predictions in all n tokens. For example, if I 1 = {0.9, 0.8, 0.7, 0.1},</p><formula xml:id="formula_6">I 2 = {0.8, 0.2, 0.7, 0.4}, I 3 = {0.9, 0.3, 0.2, 0.3},</formula><p>from three tokens, we get {3, 2, 1, 0} positive votes (&gt; 0.5) for four intents respectively. Thus the index where more than half of the votes ( &gt; 3/2 ) were obtained was o I 1 and o I 3 , we predict intents</p><formula xml:id="formula_7">o I = {o I 1 , o I 3 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Slot Filling Decoder</head><p>One main advantage of our framework is the proposed global-locally graph interaction network for slot filling, which is a non-autoregressive paradigm, achieving the slot filling decoding in parallel. In the following, we first describe the slot-aware LSTM ( ?3.3.1) to obtain the slot-aware representations, and then show how to apply the global-locally graph interaction layer ( ?3.3.2) for decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Slot-aware LSTM</head><p>We utilize a BiLSTM to produce the slot-aware hidden representation S = (s 1 , . . . , s n ). At each decoding step t, the decoder state s t calculating by:</p><formula xml:id="formula_8">s t = BiLSTM I t || e t , s t?1 , s t+1 ,<label>(6)</label></formula><p>where e t denotes the aligned encoder hidden state and I t denotes the predicted intent information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Global-locally Graph Interaction Layer</head><p>The proposed global-locally graph interaction layer consists of two main components: one is a local slot-aware graph interaction network to model dependency across slots and another is the proposed global intent-slot graph interaction network to consider the interaction between intents and slots.</p><p>In this section, we first describe the vanilla graph attention network. Then, we illustrate the local slot-aware and global intent-slot graph interaction network, respectively.</p><p>Vanilla Graph Attention Network A graph attention network (GAT) <ref type="bibr" target="#b32">(Veli?kovi? et al., 2018)</ref> is a variant of graph neural network, which fuses the graph-structured information and node features within the model. Its masked self-attention layers allow a node to attend to neighborhood features and learn different attention weights, which can automatically determine the importance and relevance between the current node with its neighborhood.</p><p>In particular, for a given graph with N nodes, one-layer GAT take the initial node featuresH = {h 1 , . . . ,h N },h n ? R F as input, aiming at producing more abstract representation,H = {h 1 , . . . ,h N },h n ? R F , as its output. The attention mechanism of a typical GAT can be summarized as below:</p><formula xml:id="formula_9">h i = || K k=1 ? j?N i ? k ij W k hh j ,<label>(7)</label></formula><formula xml:id="formula_10">? ij = exp(LeakyReLU(a [W hhi W hhj ])) j ?N i exp (LeakyReLU a [W hhi W hh j ] ) ,<label>(8)</label></formula><p>where W h ? R F ?F and a ? R 2F are the trainable weight matrix; N i denotes the neighbors of node i (including i); ? ij is the normalized attention coefficients and ? represents the nonlinearity activation function; K is the number of heads.</p><p>Local Slot-aware Graph Interaction Layer Given slot decode hidden representations S = (s 1 , . . . , s n ), we construct a local slot-aware graph where each slot hidden node connects to other slots. This allows the model to achieve to model the dependency across slots, alleviating the uncoordinated slots problem. Specifically, we construct the graph G = (V, E) in the following way,</p><p>Vertices We define the V as the vertices set. Each word slot is represented as a vertex. Each vertex is initialized with the corresponding slot hidden representation. Thus, the first layer states vector for all nodes is S 1 = S = (s 1 , . . . , s n ).</p><p>Edges Since we aim to model dependency across slots, we construct a slot-aware graph interaction layer so that the dependency relationship can be propagated from neighbor nodes to the current node. Each slot can connect other slots with a window size. For node S i , only {S i?m , . . . , S i+m } will be connected where m is a hyper-parameter denotes the size of sliding window that controls the length of utilizing utterance context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Aggregation</head><p>The aggregation process at l-th layer can be defined as:</p><formula xml:id="formula_11">s l+1 i = ? j?N i ? ij W l s l j ,<label>(9)</label></formula><p>where N i is a set of vertices that denotes the connected slots. After stacking L layer, we obtain the contextual slot-aware local hidden features S L+1 ={s L+1 1 , . . . , s L+1 n }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Slot-Intent Graph Interaction Layer</head><p>To achieve sentence-level intent-slot interaction, we construct a global slot-intent interaction graph where all predicted multiple intents and sequence slots are connected, achieving to output slot sequences in parallel. Specifically, we construct the graph G = (V, E) in the following way, Vertices As we model the interaction between intent and slot token, we have n + m number of nodes in the graph where n is the sequence length and m is the number of intent labels predicted by the intent decoder. The input of slot token feature is G [S,1] = S L+1 ={s L+1 1 , . . . , s L+1 n } which is produced by slot-aware local interaction graph network while the input intent feature is an embedding G <ref type="bibr">[I,1]</ref> </p><formula xml:id="formula_12">= {? emb (o I 1 ), . . . , ? emb (o I m )}</formula><p>where ? emb is a trainable embedding matrix. The first layer states vector for slot and intent nodes is</p><formula xml:id="formula_13">G 1 = {G [I,1] , G [S,1] } = {? emb (o I 1 ), . . . , ? emb (o I m ), s L+1 1 , . . . , s L+1 n }</formula><p>Edges There are three types of connections in this graph network.</p><p>? intent-slot connection: Since slots and intents are highly tied, we construct the intent-slot connection to model the interaction between the two tasks. Specifically, each slot connects all predicted multiple intents to automatically capture relevant intent information.</p><p>? slot-slot connection: We construct the slotslot connection where each slot node connects other slots with the window size to further model the slot dependency and incorporate the bidirectional contextual information.</p><p>? intent-intent connection: Following , we connect all the intent nodes to each other to model the relationship between each intent, since all of them express the same utterance's intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Aggregation</head><p>The aggregation process of the global GAT layer can be formulated as:</p><formula xml:id="formula_14">g [S,l+1] i = ?( j?G S ? ij W g g [S,l] j + j?G I ? ij W g g [I,l] j ),<label>(10)</label></formula><p>where G S and G I are vertices sets which denotes the connected slots and intents, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Slot Prediction</head><p>After L layers' propagation, we obtain the final slot representation G [S,L+1] for slot prediction.</p><formula xml:id="formula_15">y S t = softmax W s g [S,L+1] t ,<label>(11)</label></formula><formula xml:id="formula_16">o S t = arg max(y S t ),<label>(12)</label></formula><p>where W s is a trainable parameter and o S t is the predicted slot if the t-th token in an utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Joint Training</head><p>Following Goo et al. <ref type="formula" target="#formula_1">(2018)</ref>, we adopt a joint training model to consider the two tasks and update parameters by joint optimizing. The intent detection objective is:</p><formula xml:id="formula_17">CE(?, y) =? log (y) + (1 ??) log (1 ? y) , (13) L 1 ? n i=1 N I j=1 CE(? (j,I) i , y (j,I) i</formula><p>) . <ref type="formula" target="#formula_1">(14)</ref> Similarly, the slot filling task objective is:</p><formula xml:id="formula_18">L 2 ? n i=1 N S j=1? (j,S) i log y (j,S) i ,<label>(15)</label></formula><p>where N I is the number of single intent labels and N S is the number of slot labels. The final joint objective is formulated as:</p><formula xml:id="formula_19">L = ?L 1 + ?L 2 ,<label>(16)</label></formula><p>where ? and ? are hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct experiments on two publicly available multi-intent datasets. 1 One is the Mix-ATIS <ref type="bibr" target="#b9">(Hemphill et al., 1990;</ref>, which includes 13,162 utterances for training, 756 utterances for validation and 828 utterances for testing. Another is MixSNIPS <ref type="bibr" target="#b1">(Coucke et al., 2018;</ref>, with 39,776, 2,198, 2,199 utterances for training, validation and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>The dimensionality of the embedding is 128 and 64 on ATIS and SNIPS, respectively. The dimensionality of the LSTM hidden units is 256. The batch size is 16. The number of the multi head is 4 and 8 on MixATIS and MixSNIPS dataset, respectively. All layer number of graph attention network is set to 2. We use Adam <ref type="bibr" target="#b13">(Kingma and Ba, 2015)</ref> to optimize the parameters in our model. For all the experiments, we select the model which works the best on the dev set and then evaluate it on the test set. All experiments are conducted at GeForce RTX 2080Ti and TITAN Xp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare our model with the following best baselines: (1) Attention BiRNN. <ref type="bibr" target="#b18">Liu and Lane (2016)</ref>  Gangadharaiah and Narayanaswamy (2019) propose a multi-task framework with slot-gated mechanism for multiple intent detection and slot filling; (7) AGIF  proposes an adaptive interaction network to achieve the fine-grained multi-  intent information integration, achieving state-ofthe-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>Following Goo et al. <ref type="formula" target="#formula_1">(2018)</ref> and , we evaluate the performance of slot filling using F1 score, intent prediction using accuracy, the sentence-level semantic frame parsing using overall accuracy. Overall accuracy measures the ratio of sentences for which both intent and slot are predicted correctly in a sentence. <ref type="table" target="#tab_1">Table 1</ref> shows the results, we have the following observations: (1) On slot filling task, our framework outperforms the best baseline AGIF in F1 scores on two datasets, which indicates the proposed local slot-aware graph successfully models the dependency across slots, so that the slot filling performance can be improved. (2) More importantly, compared with the AGIF, our framework achieves +2.7% and 1.2% improvements for Mix-ATIS and MixSNIPS on overall accuracy, respectively. We attribute it to the fact that our proposed global intent-slot interaction graph can better capture the correlation between intents and slots, improving the SLU performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Speedup</head><p>One of the core contributions of our framework is that the decoding process of slot filling can be significantly accelerated with the proposed non-autoregressive mechanism. We evaluate the speed by running the model on the MixATIS test data in an epoch, fixing the batch size to 32. The comparison results are shown in Table 2. We observe that our model achieves the ?8.2, ?10.8 and ?11.5 speedup compared with SOTA models stack-propagation, Joint Multiple ID-SF and AGIF. This is because that their model utilizes an autoregressive architecture that only performs slot filling word by word, while our non-autoregressive framework can conduct slot filling decoding in parallel. In addition, it's worth noting that as the batch size gets larger, GL-GIN can achieve better acceleration where our model could achieve ?17.2 speedup compared with AGIF when batch size is 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Effectiveness of the Local Slot-aware Graph Interaction Layer</head><p>We study the effectiveness of the local slot-aware interaction graph layer with the following ablation. We remove the local graph interaction layer and directly feed the output of the slot LSTM to the global intent-slot graph interaction layer. We refer it to w/o local GAL in Tabel 3. We can clearly observe that the slot F1 drops by 1.5% and 1.2% on MixATIS and MixSNIPS datasets. We attribute this to the fact that local slot-aware GAL can capture the slot dependency for each token, which helps to alleviate the slot uncoordinated problems. A qualitative analysis can be founded at Section 4.5.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Effectiveness of Global Slot-Intent Graph Interaction Layer</head><p>In order to verify the effectiveness of slot-intent global interaction graph layer, we remove the global interaction layer and utilizes the output of local slot-aware GAL module for slot filling. It is named as w/o Global Intent-slot GAL in <ref type="table" target="#tab_4">Table 3</ref>. We can observe that the slot f1 drops by 0.9%, 1.3%, which demonstrates that intent-slot graph in-   teraction layer can capture the correlation between multiple intents, which is beneficial for the semantic performance of SLU system. Following , we replace multiple LSTM layers (2-layers) as the proposed globallocally graph layer to verify that the proposed global-locally graph interaction layer rather than the added parameters works. <ref type="table" target="#tab_4">Table 3</ref> (more parameters) shows the results. We observe that our model outperforms more parameters by 1.6% and 2.4% overall accuracy in two datasets, which shows that the improvements come from the proposed Global-locally graph interaction layer rather than the involved parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Effectiveness of the Global-locally Graph Interaction Layer</head><p>Instead of using the whole global-locally graph interaction layer for slot filling, we directly leverage the output of slot-aware LSTM to predict each token slot to verify the effect of the global-locally graph interaction layer. We name the experiment as w/o Global-locally GAL in Tabel 3. From the results, We can observe that the absence of global GAT module leads to 3.0% and 5.2% overall accuracy drops on two datasets. This indicates that the global-locally graph interaction layer encourages our model to leverage slot dependency and intent information, which can improve SLU performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.5">Visualization</head><p>To better understand how global-local graph interaction layer affects and contributes to the final result, we visualize the attention value of the Global intent-slot GAL. As is shown in <ref type="figure" target="#fig_3">Figure 3</ref>, we visualize the dependence of the word "6" on context and intent information. We can clearly observe that token "6" obtains information from all contextual tokens. The information from "and 10" helps to predict the slot, where the prior autoregressive models cannot be achieved due to the generation word by word from left to right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.6">Qualitative analysis</head><p>We conduct qualitative analysis by providing a case study that consists of two sequence slots which are generated from AGIF and our model. From <ref type="table" target="#tab_6">Table 4</ref>, for the word "6", AGIF predicts its slot label as "O" incorrectly. This is because that AGIF only models its left information, which makes it hard to predict "6" is a time slot. In contrast, our model predicts the slot label correctly. We attribute this to the fact that our proposed global intent-slot interaction layer can model bidirectional contextual information. In addition, our framework predicts the word slot "am" correctly while AGIF predicts it incorrectly (I-airport name follows Bdepart time), indicating that the proposed local slot-   <ref type="formula" target="#formula_1">(2019)</ref>, we explore the pretrained model in our framework. We replace the self-attentive encoder by Roberta <ref type="bibr" target="#b21">(Liu et al., 2019c)</ref> with the fine-tuning approach. We keep other components identical to our framework and follow <ref type="bibr" target="#b23">Qin et al. (2019)</ref> to consider the first subword label if a word is broken into multiple subwords. <ref type="figure" target="#fig_4">Figure 4</ref> gives the result comparison of AGIF, GL-GIN and two models with Roberta on two datasets. We have two interesting observations. First, the Roberta-based model remarkably well on two datasets. We attribute this to the fact that pre-trained models can provide rich semantic features, which can help SLU. Second, GL-GIN + Roberta outperforms AGIF+Roberta on both datasets and reaches a new state-of-the-art performance, which further verifies the effectiveness of our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Slot Filling and Intent Detection Recently, joint models <ref type="bibr" target="#b40">(Zhang and Wang, 2016;</ref><ref type="bibr" target="#b8">Hakkani-T?r et al., 2016;</ref><ref type="bibr" target="#b7">Goo et al., 2018;</ref><ref type="bibr" target="#b35">Xia et al., 2018;</ref><ref type="bibr" target="#b2">E et al., 2019;</ref><ref type="bibr" target="#b20">Liu et al., 2019b;</ref><ref type="bibr" target="#b23">Qin et al., 2019;</ref><ref type="bibr" target="#b39">Zhang et al., 2019;</ref><ref type="bibr" target="#b34">Wu et al., 2020;</ref><ref type="bibr" target="#b26">Qin et al., 2021b;</ref><ref type="bibr">Ni et al., 2021)</ref> are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success. Compared with their work, we focus on jointly modeling multiple intent detection and slot filling while they only consider the single-intent scenario.</p><p>More recently, multiple intent detection can handle utterances with multiple intents, which has attracted increasing attention. To the end, <ref type="bibr" target="#b36">Xu and Sarikaya (2013)</ref> and <ref type="bibr" target="#b12">Kim et al. (2017)</ref> begin to explore the multiple intent detection. <ref type="bibr" target="#b6">Gangadharaiah and Narayanaswamy (2019)</ref> first apply a multi-task framework with a slot-gate mechanism to jointly model the multiple intent detection and slot fill-ing.  propose an adaptive interaction network to achieve the fine-grained multiple intent information integration for token-level slot filling, achieving the state-of-the-art performance. Their models adopt the autoregressive architecture for joint multiple intent detection and slot filling. In contrast, we propose a non-autoregressive approach, achieving parallel decoding. To the best of our knowledge, we are the first to explore a non-autoregressive architecture for multiple intent detection and slot filling.</p><p>Graph Neural Network for NLP Graph neural networks that operate directly on graph structures to model the structural information, which has been applied successfully in various NLP tasks. <ref type="bibr" target="#b17">Linmei et al. (2019)</ref> and <ref type="bibr" target="#b11">Huang and Carley (2019)</ref> explore graph attention network (GAT) <ref type="bibr" target="#b32">(Veli?kovi? et al., 2018)</ref> for classification task to incorporate the dependency parser information. <ref type="bibr" target="#b0">Cetoli et al. (2017)</ref> and <ref type="bibr" target="#b19">Liu et al. (2019a)</ref> apply graph neural network to model the non-local contextual information for sequence labeling tasks. <ref type="bibr" target="#b37">Yasunaga et al. (2017)</ref> and <ref type="bibr" target="#b3">Feng et al. (2020a)</ref> successfully apply a graph network to model the discourse information for the summarization generation task, which achieved promising performance. Graph structure are successfully applied for dialogue direction <ref type="bibr" target="#b4">(Feng et al., 2020b;</ref><ref type="bibr" target="#b5">Fu et al., 2020;</ref><ref type="bibr" target="#b24">Qin et al., , 2021a</ref>. In our work, we apply a global-locally graph interaction network to model the slot dependency and interaction between the multiple intents and slots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we investigated a non-autoregressive model for joint multiple intent detection and slot filling. To this end, we proposed a global-locally graph interaction network where the uncoordinatedslots problem can be addressed with the proposed local slot-aware graph while the interaction between intents and slots can be modeled by the proposed global intent-slot graph. Experimental results on two datasets show that our framework achieves state-of-the-art performance with ?11.5 times faster than the prior work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Autoregressive model generates outputs word by word from left-to-right direction. The gray color denotes the unseen information when model decodes for the word Denver. (b) Non-autoregressive model can produce outputs in parallel. AN denotes airport name.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overflow of model architecture (a) and global-locally graph interaction layer (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>propose an alignment-based RNN for joint slot filling and intent detection; (2) Slot-Gated Atten. Goo et al. (2018) propose a slot-gated joint model, explicitly considering the correlation between slot filling and intent detection; (3) Bi-Model. Wang et al. (2018) propose the Bi-model to model the bi-directional between the intent detection and slot filling; (4) SF-ID Network. E et al. (2019) proposes the SF-ID network to establish a direct connection between the two tasks; (5) Stack-Propagation. Qin et al. (2019) adopt a stack-propagation framework to explicitly incorporate intent detection for guiding slot filling; (6) Joint Multiple ID-SF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization. We use the green color to indicate the attention value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Overall accuracy Performances with Roberta.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Main results. The numbers with * indicate that the improvement of our framework over all baselines is statistically significant with p &lt; 0.05 under t-test.</figDesc><table><row><cell>Model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Speed comparison. Speedup is based on the ratio of the time taken by the slot decoding part of different models to run an epoch on the MixATIS dataset with batch size set to 32.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation Experiment.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Case study. Predicted slots sequence about utterance "What airlines off from LOVE field between 6 and 10 am on June sixth" aware graph layer has successfully captured the slot dependency.</figDesc><table><row><cell>4.5.7 Effect of Pre-trained Model</cell></row><row><cell>Following Qin et al.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We adopt the cleaned verison that removes the repeated sentences in original dataset, which is available at https:// github.com/LooperXX/AGIF.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Key R&amp;D Program of China via grant 2020AAA0106501 and the National Natural Science Foundation of China (NSFC) via grant 61976072 and 61772153. This work was also supported by the Zhejiang Lab's International Talent Fund for Young Professionals.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Cetoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Bragaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Harney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sloan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories</title>
		<meeting>the 16th International Workshop on Treebanks and Linguistic Theories<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?odore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10190</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel bi-directional interrelated model for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haihong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1544</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5467" to="5471" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dialogue discourseaware graph convolutional networks for abstractive meeting summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiachong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Incorporating commonsense knowledge into abstractive dialogue summarization via heterogeneous graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiachong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10044</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DRTS parsing with structureaware encoding and decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6818" to="6828" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint multiple intent detection and slot labeling for goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Gangadharaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1055</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="564" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Slot-gated modeling for joint slot filling and intent prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chih-Wen Goo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Kai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Li</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Chieh</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2118</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Short Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="753" to="757" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-domain joint semantic frame parsing using bi-directional rnn-lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung Vivian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 17th Annual Meeting of the International Speech Communication Association</title>
		<meeting>The 17th Annual Meeting of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The ATIS spoken language systems pilot corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">T</forename><surname>Hemphill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley</title>
		<meeting><address><addrLine>Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-06-24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Syntaxaware aspect level sentiment classification with graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binxuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Carley</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1549</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5469" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-stage multi-intent detection for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghan</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary Geunbae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="11377" to="11390" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A selfattentive model with gate mechanism for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Qi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1417</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3824" to="3833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Handling rare entities for neural sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.574</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6441" to="6451" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Empirical analysis of unlabeled entity problem in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention networks for semi-supervised short text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchi</forename><surname>Hu Linmei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houye</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1488</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4821" to="4830" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2016-1352</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="685" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contextualized non-local neural networks for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaichen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016762</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6762" to="6769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CM-net: A novel collaborative memory network for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1097</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1051" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<idno>abs/1907.11692</idno>
		<title level="m">Roberta: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fuzhao Xue, Vinay Adiga, and Erik Cambria. 2021. Recent advances in deep learning based dialogue systems: A systematic survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjie</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Pandelea</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A stack-propagation framework with token-level intent detection for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1214</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2078" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowing where to leverage: Context-aware graph convolutional network with an adaptive fusion layer for contextual spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1280" to="1289" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Co-gat: A co-interactive graph attention network for joint dialog act recognition and sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A cointeractive transformer for joint slot filling and intent detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tailu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sendong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">Wanxiang Che, and Ting Liu. 2021c. A survey on spoken language understanding: Recent advances and new frontiers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">AGIF: An adaptive graph-interactive framework for joint multiple intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.163</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1807" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Injecting word information with multi-level word adapter for chinese spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dechuang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sendong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Spoken language understanding: Systems for extracting semantic information from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Accepted as poster</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A bimodel based RNN semantic frame parsing model for intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2050</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Short Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SlotRefine: A fast non-autoregressive model for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1932" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Zero-shot user intent detection via capsule neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1348</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3090" to="3099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular crf for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph-based neural multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitijh</forename><surname>Meelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1045</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Ga?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint slot filling and intent detection via capsule neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5259" to="5267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A joint model of intent determination and slot filling for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2993" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
