<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Guidelines for the Regularization of Gammas in Batch Normalization for Deep Residual Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bum</forename><forename type="middle">Jun</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>POSTECH</roleName><forename type="first">Hyeyeon</forename><surname>Choi</surname></persName>
							<email>hyeyeon@postech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>POSTECH</roleName><forename type="first">Hyeonah</forename><surname>Jang</surname></persName>
							<email>hajang@postech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Postech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>POSTECH</roleName><forename type="first">Wonseok</forename><surname>Jeong</surname></persName>
							<email>wonseok.jeong@postech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Woo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Postech</surname></persName>
							<email>postechkmbmjn@postech.edu</email>
						</author>
						<title level="a" type="main">Guidelines for the Regularization of Gammas in Batch Normalization for Deep Residual Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>L 2 regularization for weights in neural networks is widely used as a standard training trick. However, L 2 regularization for ?, a trainable parameter of batch normalization, remains an undiscussed mystery and is applied in different ways depending on the library and practitioner. In this paper, we study whether L 2 regularization for ? is valid. To explore this issue, we consider two approaches: 1) variance control to make the residual network behave like identity mapping and 2) stable optimization through the improvement of effective learning rate. Through two analyses, we specify the desirable and undesirable ? to apply L 2 regularization and propose four guidelines for managing them. In several experiments, we observed the increase and decrease in performance caused by applying L 2 regularization to ? of four categories, which is consistent with our four guidelines. Our proposed guidelines were validated through various tasks and architectures, including variants of residual networks and transformers.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have exhibited remarkable performance in various fields. Previously, large neural networks with deep and wide architectures were considered difficult to train. However, various regularization techniques such as L 2 regularization <ref type="bibr" target="#b25">[26]</ref>, batch normalization (BN) <ref type="bibr" target="#b11">[12]</ref>, and residual learning <ref type="bibr" target="#b7">[8]</ref> have made it possible to train large neural networks, leading to successful performance.</p><p>Since the classic machine learning era, L 2 regularization has been applied as a restriction on the weights W of neural networks and has not been applied to bias b. However, trainable parameters in modern neural networks are not limited to weights W and bias b. BN outputs ? ixi + ? i from normalized featurex, where ? and ? are also trainable parameters. Because ? plays a similar role to bias b, we ignore L 2 regularization of ?. However, ? controls the scale of the feature map and can be viewed as a special case of weights (Section 3). Despite the similar roles of ? and weights, L 2 regularization of the ? parameters of BN remains an undiscussed mystery. Moreover, each deep learning library and practice provides a different method:</p><p>TensorFlow and Keras To implement L 2 regularization, a regularizer option needs to be set for each layer. For example, kernel_regularizer can be applied to the convolution and fully connected layer to implement L 2 regularization on weight W . L 2 regularization of ? is functionally supported through gamma_regularizer, which may be set to the BN layer. In practice, however, gamma_regularizer is rarely used. To our knowledge, in the TensorFlow and Keras official tutorials and code, practical use of gamma_regularizer does not exist. In other words, L 2 regularization in TensorFlow and Keras generally means using kernel_regularizer on weights, and L 2 regularization has not been applied to ?.</p><p>PyTorch L 2 regularization is implemented by applying a weight decay at the optimizer level. However, the weight decay of PyTorch is applied to all trainable parameters, including all W , b, ?, and ?. Although the validity of weight decay for b and ? is also arguable, we focus on whether PyTorch's practice of applying weight decay to ? is valid.</p><p>Practice and empirical observation In several work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6]</ref>, L 2 regularization was not applied ? and ?. However, <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref> used L 2 regularization on ? and ?, while <ref type="bibr" target="#b21">[22]</ref> turned off it during fine-tuning. Reference <ref type="bibr" target="#b17">[18]</ref> empirically observed that L 2 regularization on ? and ? often improves performance depending on the architecture. Because these practices lack theoretical analysis, in this paper, we explore whether it is desirable to apply L 2 regularization to the ? parameter of BN. First, we claim that ? plays a role in controlling variance in residual networks. We show that the variance of the features in the residual block is either accumulated or reset according to the layer arrangement. For better behavior in residual networks, we propose a strategy of making the accumulated variance small and the reset variance large (Section 2). Second, we present an analysis of the effective learning rate from optimization perspective of ? (Section 3). Through our theoretical analysis, we present four guidelines for managing ? ( <ref type="table" target="#tab_0">Table 1</ref>).</p><p>The validity of the four guidelines is confirmed through several experiments (Section 4). We demonstrate a performance decrease due to incorrect L 2 regularization on ? and a performance increase due to correct L 2 regularization on ?. We observed this phenomenon in various residual networks and transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Variance analysis in residual networks</head><p>In this paper, we target residual networks because they are widely used as a standard architecture and involve many variants. In residual networks, an input image is first passed through the early stage. Then, four stages are applied, and each stage is composed of several residual blocks. Each residual block consists of a skip connection and a residual branch, in which several weight, BN, and ReLU layers are placed in the residual branch.</p><p>Our assumption is that the condition when the skip connection dominates over the residual branch, which makes the residual block behave more like an identity mapping, is advantageous for residual network training. In fact, in the original ResNet paper, residual learning was introduced with the intention of modelling identity mappings <ref type="bibr" target="#b7">[8]</ref>. Reference <ref type="bibr" target="#b5">[6]</ref> reported that initializing ? = 0 in the last BN causes the residual block to behave like an identity function and eases optimization. Reference <ref type="bibr" target="#b2">[3]</ref> found that a residual block behaves like an identity mapping because the variance of the skip connection is larger than the variance of the residual branch. In their paper, the ? parameter of BN was assumed to be 1, but we show here that variance in the residual block varies according to ?. We consider the bias parameter b in the weight layers and ? in the BN layers to be 0. L 2 regularization of b or ? is ignored in this paper. In BN, we assume that the mini-batch size is sufficiently large, and each feature map is normalized to N (0, 1) and then rescaled using ?. In other words, the i-th</p><formula xml:id="formula_0">element of BN with ? i outputs B i (x), where E[B i (x)] = 0 and V ar[B i (x)] = ? 2 i .</formula><p>The term "weight layers" used below indicates convolution layers in ResNet, but will be described by generalizing to fully connected layers. We assume that each element of the weights comes from He initialization N (0, 2/f an in ) <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 1. Basic block in PreActResNet</head><p>PreActResNet <ref type="bibr" target="#b8">[9]</ref> is a modified version of the original ResNet <ref type="bibr" target="#b7">[8]</ref>, and is also called ResNetV2. The residual branch of PreActResNet consists of two or three [BN-ReLU-Weight] blocks <ref type="figure" target="#fig_0">(Figure 1</ref>, Left). The l-th residual block outputs x l+1 , which is the sum of the residual branch f l (x l ) and skip connection x l , i.e., x l+1 = x l + f l (x l ). Because the covariance between the residual branch and the skip connection is zero <ref type="bibr" target="#b2">[3]</ref>, we have V ar[</p><formula xml:id="formula_1">x l+1 ] = V ar[x l ] + V ar[f l (x l )].</formula><p>Here, we examine the variance of the residual branch V ar[f l (x l )]. Consider the residual branch with two [BN-ReLU-Weight] blocks. Because of the normalization in the second BN, the ? of the first BN and W 1 do not affect the variance of the residual branch. Therefore, the variance of the residual branch is determined by the second [BN-ReLU-Weight] block. By the mean and variance of W from He initialization and BN, the variance of the i-th element of the residual branch is</p><formula xml:id="formula_2">V ar[f l,i (x l )] = f anin j V ar[W 2,ij,l ] ? E[ReLU (B 2,l,j (x l )) 2 ]</formula><p>(1)</p><formula xml:id="formula_3">= 2 ? E[ReLU (B 2,l,i (x l )) 2 ] (2) = E[B 2,l,i (x l ) 2 ] = V ar[B 2,l,i (x l )] (3) = ? 2 2,l,i .<label>(4)</label></formula><p>The third equality holds because</p><formula xml:id="formula_4">E[ReLU (X) 2 ] = 1 2 E[X 2 ] for normalized X [7]. Thus, V ar[x l+1,i ] = V ar[x l,i ] + ? 2 2,l,i .<label>(5)</label></formula><p>Starting from the s-th residual block, imagine that the variance of the residual branch is accumulated in a row. The variance of the l-th residual block is</p><formula xml:id="formula_5">V ar[x l,i ] = V ar[x s,i ] + l?1 m=s ? 2 2,m,i ,<label>(6)</label></formula><p>where x s denotes the first input feature map for each stage of ResNet. As such, in the residual block, the variance of the residual branch is accumulated. Here, we want to make the residual block satisfy V ar[x l,i ] &gt; V ar[f l,i (x l )] so that the skip connection dominates over the residual branch. From Eq. 4 and Eq. 6, the inequality V ar[x s,i ] + l?1 m=s ? 2 2,m,i &gt; ? 2 2,l,i requires two conditions:</p><formula xml:id="formula_6">1. ? 2 2,l,i should be small for all l; 2. V ar[x s,i ] should be large.</formula><p>The variance at the stage starting point in the second condition is discussed later in Cases 3 and 4. According to the first condition, ? in the second BN in all residual blocks should be small to reduce the variance of the residual branch. Similarly, when the residual branch consists of three [BN-ReLU-Weight] blocks, the ? in the third BN should be small. Thus, in order for the skip connection to dominate over the residual branch, we should control ? in the last BN in the residual branch. To ensure that those called ? last are small during training, we recommend applying L 2 regularization on ? last . In summary, we present the following guideline: Guideline 1. Because residual block plays the role of variance accumulation, we should decay ? last .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 2. Basic block in the original ResNet</head><p>We check whether Guideline 1 is also applicable to the original ResNet. The original ResNet, also called ResNetV1, constitutes a residual branch with two or three [Weight-BN-ReLU] blocks, but the last ReLU is deployed after addition <ref type="figure" target="#fig_0">(Figure 1</ref>, Right). First, we examine the variance of residual branch V ar[f l (x l )]. Because the residual branch ends with BN, the variance of the residual branch is equal to the variance of the last BN, i.e., V ar[f l,i (x l )] = ? 2 last,l,i . Contrary to PreActResNet, here, because ReLU is applied after addition,</p><formula xml:id="formula_7">x l+1 = ReLU (x l + f l (x l )).</formula><p>This ReLU after addition makes a different variance accumulation.</p><formula xml:id="formula_8">Ignoring E[x l,i ], we have V ar[x l+1,i ] = 1 2 E[(x l,i + f l,i (x l )) 2 ]<label>(7)</label></formula><formula xml:id="formula_9">= 1 2 (E[x 2 l,i ] + E[f l,i (x l ) 2 ]) (8) = 1 2 (V ar[x l,i ] + V ar[f l,i (x l )]) (9) = 1 2 (V ar[x l,i ] + ? 2 last,l,i ).<label>(10)</label></formula><p>Thus, the added variance is halved. Here, we introduce the following substitutions: a l = 2 l V ar[x l,i ] and b l = 2 l ? 2 last,l,i . Rewriting Eq. 10, we have a l+1 = a l + b l , and thus a l = a s +</p><formula xml:id="formula_10">l?1 m=s b m . Therefore, we obtain V ar[x l,i ] = 1 2 l?s V ar[x s,i ] + l?1 m=s 1 2 l?m ? 2 last,m,i .<label>(11)</label></formula><p>As such, the variance of the residual branch is accumulated here as well, but because the variance of the last ReLU is halved, the variance of the previous block decays. We call this half variance accumulation. For the skip connection to dominate, the condition V ar[x l,i ] &gt; V ar[f l,i (x l )] requires that 1) ? 2 last,l,i should be small and 2) V ar[x s,i ] should be large. Therefore, it is desirable to decay ? last , and thus, Guideline 1 is valid for the original ResNet as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 3. Downsampling block in ResNet</head><p>Convolutional neural networks downsample high-dimensional image features to low-dimensional ones using pooling layers <ref type="bibr" target="#b13">[14]</ref>. For ResNet, downsampling is performed using strided convolution <ref type="bibr" target="#b16">[17]</ref> as well as pooling. For the original ResNet, at the end of each stage, a downsampling block <ref type="figure" target="#fig_1">(Figure 2, Right)</ref> is applied, where the strided convolution is used in both the first weight layers in the residual branch and the skip path. Here, the skip path of the downsampling block is composed of strided convolution and BN rather than identity. We denote the skip path output as g l (x l ) = B down,l (W down,l x l ). Because each branch ends with BN, the variance of each branch is determined by the last BN as follows:</p><formula xml:id="formula_11">V ar[g l,i (x l )] = ? 2 down,l,i ,<label>(12)</label></formula><p>V ar[f l,i (x l )] = ? 2 last,l,i .</p><p>From x l+1 = ReLU (g l (x l ) + f l (x l )), the variance after the downsampling block is</p><formula xml:id="formula_13">V ar[x l+1,i ] = 1 2 (V ar[g l,i (x l )] + V ar[f l,i (x l )])<label>(14)</label></formula><formula xml:id="formula_14">= 1 2 (? 2 down,l,i + ? 2 last,l,i ).<label>(15)</label></formula><p>Note that the output variance of the downsampling block is not affected by the input variance and is newly determined by ? down and ? last . We call this variance reset.</p><p>Again, it is desirable to decay ? last so that the residual branch does not dominate. However, it is not desirable to decay ? down . There are two reasons for this. First, in order for the skip path in the downsampling block to dominate, we should decay ? last and should not decay ? down . Second, it is desirable to have a large reset variance. The output variance of the downsampling block becomes the input variance of the successive (l + 1)-th residual block, which is the variance at the stage starting point. In Cases 1 and 2, we confirmed that a large V ar[x s,i ] is favorable. Thus, a large output variance of the downsampling block is desirable. In other words, if we decay both ? down and ? last , the reset variance decreases, which is undesirable. Therefore, we claim that ? down should not be subjected to L 2 regularization to ensure that the reset variance appears dominant afterward. In this regard, we present the following guideline:</p><formula xml:id="formula_15">Guideline 2.</formula><p>Because the downsampling block plays the role of variance reset, we should not decay ? down .  Now, we check whether Guideline 2 is also applicable to the downsampling block of PreActResNet. In the downsampling block of PreActRes-Net, before starting both the residual branch and skip path, block input x l is subjected to the first BN and ReLU <ref type="figure" target="#fig_1">(Figure 2</ref>, Left). The first BN does not affect the variance of the residual branch but does affect the variance of the skip path. For skip path g l (x l ) = W down,l ReLU (B 1,l (x l )), we have V ar[g l,i (x l )] = 2 ? E[ReLU (B 1,l,i (x l )) 2 ] = ? 2 1,l,i . For the residual branch, similar to Case 1, we have V ar[f l,i (x l )] = ? 2 2,l,i . Thus, the variance after residual block is</p><formula xml:id="formula_16">V ar[x l+1,i ] = V ar[g l,i (x l )] + V ar[f l,i (x l )] (16) = ? 2 1,l,i + ? 2 2,l,i .<label>(17)</label></formula><p>Thus, the downsampling block of PreActResNet plays the role of variance reset as well. To ensure that the reset variance is dominant afterward, we should not decay ? 1 at the first BN. For convenience, we also call this parameter ? down .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 4. Early stage in ResNet</head><p>The input variance of the first residual block is determined by the early stage of ResNet before the residual block starts. The early stage of ResNet, which is also called the stem, consists of convolution, BN, ReLU, and pooling layers without a skip connection in the original ResNet <ref type="figure" target="#fig_2">(Figure 4</ref>, Right). Ignoring pooling, from the output of early stage</p><formula xml:id="formula_17">x 1 = ReLU (B 0 (W 0 x 0 )), we have V ar[x 1,i ] = 1 2 E[B 0,i (W 0 x 0 )) 2 ]<label>(18)</label></formula><formula xml:id="formula_18">= 1 2 ? 2 0,i .<label>(19)</label></formula><p>Therefore, ? 0 sets the output variance of the early stage. The output of the early stage becomes the input of the residual block of the first stage. Because a large variance at the stage starting point is favorable, ? 0 should be large. In this regard, we present the following guideline:</p><p>Guideline 3. Because the early stage plays the role of variance set, we should not decay ? 0 .</p><p>In case of PreActResNet, BN is not applied in the early stage <ref type="figure" target="#fig_2">(Figure 4, Left)</ref>. Thus, we do not have to consider ? 0 .</p><p>Finally, in the residual branch, additional ? parameters exist in the BN before the last BN for both PreActResNet and the original ResNet. We call those parameters ? others . Because they do not affect the variance of the residual branches, our variance analysis cannot explain their role. The ? others parameters are discussed again in Section 3; here, we simply present the conclusion:</p><p>Guideline 4. Though other BNs do not determine the variance, L 2 regularization on ? others helps optimization by improving the effective learning rate. Thus, we should decay ? others . Further, using ResNet-50, we measured the variance of each feature map ( <ref type="figure" target="#fig_4">Figure 5</ref>). First, the variance is (re)set at layer 1-0, and then the variance decreases at layer 1-1, which is expected from the half variance accumulation (Eqs. 10 and 11). In layer 2-0, where the subsequent stage begins, the variance is reset to a new larger value and decreases again from layer 2-1 to layer 2-3. As such, both the variance reset to a larger value and half variance accumulation with decreasing value appear in each residual block, which agrees with our claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 5. Transformer block</head><p>The transformer is attracting attention for its high performance in various tasks, including computer vision <ref type="bibr" target="#b4">[5]</ref> and natural language processing <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref>. The transformer uses a series of transformer blocks, which consists of residual branches with self-attention and multilayer perceptron blocks. In contrast to ResNet, the transformer uses LayerNorm (LN) <ref type="bibr" target="#b0">[1]</ref>. However, LN also rescales normalized features using ? and ?. Therefore, our variance analysis can explain how the ? parameter of LN affects the variance within the transformer block. See the Appendix for the mathematical details regarding the roles of ? 1 and ? 2 in the first and second LNs. Here, we simply present the following guideline for the transformer block:</p><p>Guideline 1T. Because a transformer block accumulates the variance, we should decay ? 1 and ? 2 .</p><p>Note that the transformer has no downsampling block (and hence there is no Guideline 2T). Further, some transformers such as bidirectional encoder representations from transformers (BERT) <ref type="bibr" target="#b3">[4]</ref> have LN in the early stage, whose ? 0 should not be subjected to the L 2 regularization (Guideline 3T).</p><p>Finally, some transformers <ref type="bibr" target="#b19">[20]</ref> have another LN between the final transformer block and the head layer. The role of the ? other parameters of LN cannot be explained by our variance analysis, but in the same context as Guideline 4, L 2 regularization on ? other helps optimization (Guideline 4T).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Effective learning rate analysis</head><p>In this section, we explore why ? others parameters should be decayed even though their scale does not affect the BN output. Reference <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref> argued that in a neural network with BN, the weight scale does not affect the BN output. They introduced the effective learning rate to claim that a smaller weight scale is advantageous for optimization, which can be achieved through L 2 regularization on weights. In our paper, we note that ? is a trainable parameter in a neural network similar to weights. Extending their claim, we investigate the effective learning rate for the optimization of ?.</p><p>BN is composed of a normalization step N (?) and linear scaling step L(?). From a series of [Weight-BN-ReLU] blocks, we decompose BN into [N-L] and investigate the intermediate [L-ReLU-Weight-N] block. In the l-th intermediate block, weight layer holds W l and L step holds ? l , where ? l is a diagonal matrix with (? l ) i,i = ? i and (? l ) i,j = 0 for i = j. From the input feature map x l , the intermediate block outputs</p><formula xml:id="formula_19">x l+1 = y(x l ; W l , ? l ) = N [W l ReLU (? l x l )].</formula><p>Here, we decompose the diagonal matrix into its norm and direction, i.e., ? l = ? l ? l . Then,</p><formula xml:id="formula_20">y(x l ; W l , ? l ) = N [W l ReLU ( ? l ? l x l )] = N [W l ? l ReLU (? l x l )]<label>(20)</label></formula><formula xml:id="formula_21">= N [W l ReLU (? l x l )].<label>(21)</label></formula><p>Thus, the scale of ? in BN does not affect the intermediate block output. If we investigate the gradient of y(x l ; W l , ? l ) with respect to ? l , we obtain</p><formula xml:id="formula_22">? ? l y(x l ; W l , ? l ) = 1 ? l ?? l y(x l ; W l ,? l ).<label>(22)</label></formula><p>This scale-variant gradient property was used in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11]</ref> to claim that L 2 regularization on weights helps optimization even in neural networks with BN. Thus, effective learning rate analysis is expected to be applicable to ? in BN. From Eq. 22, we have ? ? l L(? l ) = 1 ? l ?? l L(? l ) for loss function L. First, gradient descent with respect to ? l is in the form of ? l,t+1 = ? l,t ? ?? ? l,t L(? l,t ).</p><p>If we investigate gradient descent with respect to? l , we have, <ref type="figure">Figure 6</ref>: Norm of the first update by the initial value of ?.</p><formula xml:id="formula_24">? l,t+1 =? l,t ? ? ? l,t ?2 ?? l,t L(? l,t ).<label>(24)</label></formula><p>Here, ? ? l,t ?2 is called the effective learning rate <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11]</ref>. If the scale of ? in BN decreases, the effective learning rate increases, which prevents the optimization from saturating due to the large weight norm. Although we followed the derivation in <ref type="bibr" target="#b18">[19]</ref>, the derivation in <ref type="bibr" target="#b10">[11]</ref> shows similar results. Thus, although L 2 regularization on ? others does not affect the variance, it improves the effective learning rate and optimization. In this regard, we advocate applying L 2 regularization to ? others . Now, we empirically validate Eq. 24. Using ResNet-18, we varied the initial value of ? and measured the norm of the first update, ? l,1 ?? l,0 ( <ref type="figure">Figure 6</ref>). From their log values, a regression fit yielded a coefficient near ?2, which agrees with ? l,t+1 ?? l,t ? ? l,t ?2 .</p><p>Note that this effective learning rate analysis is applicable to the ? parameters of all four categories. Although applying L 2 regularization on ? down and ? 0 is harmful to variance control, it could be advantageous for improving optimization with respect to the effective learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image classification tasks</head><p>We tested whether Guidelines 1-4 are valid in practical tasks. First, in the training task of ResNet and its variants <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23]</ref>, we measured the performance of applying L 2 regularization to the weights and ? of each group with 1 2 ?( W   <ref type="table" target="#tab_1">Table 2</ref> shows image classification accuracy on the Oxford-IIIT PET dataset <ref type="bibr" target="#b15">[16]</ref> for different L 2 regularization setups. First, when the ? last parameters were subjected to L 2 regularization, the test accuracy increased approximately 1%-4% compared to the case where L 2 regularization was not applied to them. This is consistent with Guideline 1, which states that we should decay the ? last . However, when L 2 regularization was applied to ? down , the test accuracy decreased approximately 1%-3%. This is consistent with Guideline 2, which states that L 2 regularization for ? down is undesirable. Similarly, for ? 0 and ? others , the experimental results agree with Guidelines 3 and 4.</p><p>Limitations However, for ? 0 , an increase in performance was observed in some models. When the effect of improving the effective learning rate is more dominant than the variance control, it can result in performance improvement. Because the first stage is relatively shallow compared to other stages, the variance from ? 0 would have a minor effect, and improvement of the effective learning rate may become dominant. The coexistence of variance control and improved effective learning rate requires further research. See the Appendix for more results on other datasets, including NABirds, Food-101, and CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Natural language processing tasks</head><p>We also tested whether our proposed guidelines are valid for transformers on natural language processing tasks. First, we visited a machine translation task using a transformer. The BLEU score <ref type="bibr" target="#b14">[15]</ref> was measured for the German to English translation task using the IWSLT-14 dataset <ref type="bibr" target="#b1">[2]</ref>  <ref type="table" target="#tab_2">(Table  3</ref>). We measured the performance before and after applying L 2 regularization to ? 1 and ? 2 . For both cases, an increase in the BLEU score was observed. This result is consistent with Guideline 1T, which states that small ? is desirable for transformer blocks because they accumulate variance. Our claim was also verified in the text classification task using BERT for the SST-2 task from GLUE <ref type="bibr" target="#b20">[21]</ref>. Following Guideline 1T improved test accuracy here as well <ref type="table" target="#tab_3">(Table 4</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we studied L 2 regularization for the ? parameters of BN. We theoretically showed that this ? should be controlled so that the residual blocks behave similarly to identity mapping. Specifically, we discussed the cases in which it is desirable to decay the ? parameters of BN and the cases in which it is not, presenting four guidelines for their management. The proposed guidelines were verified in experiments on several variants of residual networks and transformer models. We confirmed that the performance increases when the guidelines are followed and decreases when the guidelines are violated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Basic block in (left) PreActResNet and (right) original ResNet (Cases 1 and 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Downsampling block in (left) PreActResNet and (right) original ResNet (Case 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Early stage in (left) PreActResNet and (right) original ResNet (Case 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Performance degradation caused by L 2 regularization on ? down with various ? for ResNet-152 on the PET dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Variance of each feature map of ResNet-50 pre-trained on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 2 + ? 2 2</head><label>22</label><figDesc>). For all the experiments, we report the average results from three experiments. For hyperparameters and training details, refer to the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Preview of our guidelines on L 2 regularization.</figDesc><table><row><cell></cell><cell>TensorFlow</cell><cell cols="2">PyTorch Our Guidelines</cell></row><row><cell>W</cell><cell cols="2">Applied Applied</cell><cell>Applicable</cell></row><row><cell>? last</cell><cell cols="2">Rarely applied Applied</cell><cell>Applicable</cell></row><row><cell>? down</cell><cell cols="2">Rarely applied Applied</cell><cell>Inapplicable</cell></row><row><cell>? 0</cell><cell cols="2">Rarely applied Applied</cell><cell>Inapplicable</cell></row><row><cell>?</cell><cell></cell><cell></cell></row></table><note>others Rarely applied Applied Applicable</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy (%) for the PET dataset.</figDesc><table><row><cell>L 2 L 2 L 2 Regularization</cell><cell>W W W</cell><cell cols="2">W, ? last W, ? last W, ? last W, ? down W, ? down W, ? down</cell><cell cols="2">W, ? 0 W, ? 0 W, ? 0 W, ? others W, ? others W, ? others</cell></row><row><cell>ResNet-18</cell><cell>83.5733</cell><cell>85.4100</cell><cell>82.5800</cell><cell>82.8800</cell><cell>84.6000</cell></row><row><cell></cell><cell></cell><cell cols="4">(+1.8367) (-0.9933) (-0.6933) (+1.0267)</cell></row><row><cell>ResNet-50</cell><cell>82.9433</cell><cell>85.6500</cell><cell>80.0833</cell><cell>82.4000</cell><cell>83.5133</cell></row><row><cell></cell><cell></cell><cell cols="4">(+2.7067) (-2.8600) (-0.5433) (+0.5700)</cell></row><row><cell>ResNet-101</cell><cell>82.0100</cell><cell>84.4200</cell><cell>78.9400</cell><cell>81.3767</cell><cell>83.1200</cell></row><row><cell></cell><cell></cell><cell cols="4">(+2.4100) (-3.0700) (-0.6333) (+1.1100)</cell></row><row><cell>ResNet-152</cell><cell>80.9867</cell><cell>85.7400</cell><cell>79.2100</cell><cell>82.3700</cell><cell>82.9100</cell></row><row><cell></cell><cell></cell><cell cols="4">(+4.7533) (-1.7767) (+1.3833) (+1.9233)</cell></row><row><cell>Wide ResNet-50-2</cell><cell>82.9433</cell><cell>85.6500</cell><cell>80.0833</cell><cell>82.4000</cell><cell>83.5133</cell></row><row><cell></cell><cell></cell><cell cols="4">(+2.7067) (-2.8600) (-0.5433) (+0.5700)</cell></row><row><cell>Wide ResNet-101-2</cell><cell>82.4600</cell><cell>85.1400</cell><cell>81.2000</cell><cell>83.4533</cell><cell>84.6300</cell></row><row><cell></cell><cell></cell><cell cols="4">(+2.6800) (-1.2600) (+0.9933) (+2.1700)</cell></row><row><cell>ResNeXt-50-32x4d</cell><cell>84.4167</cell><cell>85.6200</cell><cell>84.1133</cell><cell>84.6000</cell><cell>85.7400</cell></row><row><cell></cell><cell></cell><cell cols="4">(+1.2033) (-0.3033) (+0.1833) (+1.3233)</cell></row><row><cell cols="2">ResNeXt-101-32x8d 84.0233</cell><cell>85.4100</cell><cell>83.6067</cell><cell>82.9700</cell><cell>85.2900</cell></row><row><cell></cell><cell></cell><cell cols="4">(+1.3867) (-0.4167) (-1.0533) (+1.2667)</cell></row><row><cell>Our Guidelines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>BLEU scores (%) for the IWSLT-14 dataset.</figDesc><table><row><cell>L 2 L 2 L 2 Regularization</cell><cell>W W W</cell><cell>W, ? 1 W, ? 1 W, ? 1</cell><cell>W, ? 2 W, ? 2 W, ? 2</cell></row><row><cell>Transformer</cell><cell>34.7894</cell><cell>34.9063</cell><cell>35.1385</cell></row><row><cell></cell><cell></cell><cell cols="2">(+0.1170) (+0.3491)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy (%) for the GLUE SST-2 task.</figDesc><table><row><cell>L 2 L 2 L 2 Regularization</cell><cell cols="3">W W W W, ? 1 , ? 2 W, ? 1 , ? 2 W, ? 1 , ? 2 Difference</cell></row><row><cell>BERT</cell><cell>91.8807</cell><cell>92.0872</cell><cell>+0.2064</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer Normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Report on the 11th IWSLT evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign</title>
		<meeting>the 11th International Workshop on Spoken Language Translation: Evaluation Campaign</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bag of Tricks for Image Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Norm matters: efficient and accurate normalization schemes in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangzihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haidong</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiegang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Chu</surname></persName>
		</author>
		<title level="m">Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes. CoRR, abs/1807.11205</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Striving for Simplicity: The All Convolutional Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Four things everyone should know to improve batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Dinneen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">L2 Regularization versus Batch and Weight Normalization. CoRR, abs/1706.05350</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Twan Van Laarhoven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Group Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wide Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Three Mechanisms of Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
