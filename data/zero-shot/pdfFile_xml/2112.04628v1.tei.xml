<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
							<email>xuenan@whu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
							<email>tianfuwu@ncsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular 3D object detection aims to localize 3D bounding boxes in an input single 2D image. It is a highly challenging problem and remains open, especially when no extra information (e.g., depth, lidar and/or multi-frames) can be leveraged in training and/or inference. This paper proposes a simple yet effective formulation for monocular 3D object detection without exploiting any extra information. It presents the MonoCon method which learns Monocular Contexts, as auxiliary tasks in training, to help monocular 3D object detection. The key idea is that with the annotated 3D bounding boxes of objects in an image, there is a rich set of well-posed projected 2D supervision signals available in training, such as the projected corner keypoints and their associated offset vectors with respect to the center of 2D bounding box, which should be exploited as auxiliary tasks in training. The proposed MonoCon is motivated by the Cram?r-Wold theorem in measure theory at a high level. In implementation, it utilizes a very simple end-to-end design to justify the effectiveness of learning auxiliary monocular contexts, which consists of three components: a Deep Neural Network (DNN) based feature backbone, a number of regression head branches for learning the essential parameters used in the 3D bounding box prediction, and a number of regression head branches for learning auxiliary contexts. After training, the auxiliary context regression branches are discarded for better inference efficiency. In experiments, the proposed MonoCon is tested in the KITTI benchmark (car, pedestrian and cyclist). It outperforms all prior arts in the leaderboard on the car category and obtains comparable performance on pedestrian and cyclist in terms of accuracy. Thanks to the simple design, the proposed MonoCon method obtains the fastest inference speed with 38.7 fps in comparisons.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>3D object detection is a critical component in many computer vision applications in practice, such as autonomous driving and robot navigation. High performing methods often require more costly system setups such as Lidar sensors <ref type="bibr" target="#b61">(Yan, Mao, and Li 2018;</ref><ref type="bibr" target="#b19">Lang et al. 2019;</ref><ref type="bibr" target="#b40">Qi et al. 2019;</ref>) for precise depth measurements or stereo cameras <ref type="bibr" target="#b22">(Li, Chen, and Shen 2019;</ref><ref type="bibr" target="#b41">Qin, Wang, and Lu 2019;</ref><ref type="bibr"></ref>  MonoRCNN <ref type="bibr">(Shi et al. 2021)</ref> DDMP3D <ref type="bibr" target="#b52">(Wang et al. 2021a)</ref> CaDDN <ref type="bibr" target="#b42">(Reading et al. 2021)</ref> MonoEF  MonoFlex <ref type="bibr" target="#b65">(Zhang, Lu, and Zhou 2021)</ref> GUPNet  MonoCon (Ours) <ref type="figure">Figure 1</ref>: Performance comparisons on the car category in the KITTI 3D object detection benchmark. The proposed MonoCon shows consistently better performance. See text for detail. <ref type="bibr" target="#b60">Xu et al. 2020;</ref><ref type="bibr" target="#b50">Sun et al. 2020)</ref> for stereo depth estimation, and are often more computationally expensive. To alleviate those "burden" and due to the potential prospects of reduced cost and increased modular redundancy, monocular 3D object detection that aims to localize 3D object bounding boxes from an input 2D image has emerged as a promising alternative approach with much attention received in the computer vision and AI community <ref type="bibr" target="#b7">(Chen et al. 2016;</ref><ref type="bibr" target="#b36">Manhardt, Kehl, and Gaidon 2019;</ref><ref type="bibr" target="#b48">Simonelli et al. 2019;</ref><ref type="bibr" target="#b1">Brazil and Liu 2019;</ref><ref type="bibr" target="#b56">Wang et al. 2020;</ref><ref type="bibr" target="#b63">Ye et al. 2020;</ref><ref type="bibr" target="#b45">Shi, Chen, and Kim 2020;</ref><ref type="bibr" target="#b32">Luo et al. 2021;</ref><ref type="bibr" target="#b18">Kumar, Brazil, and Liu 2021;</ref><ref type="bibr">Wang et al. 2021c,d)</ref>. In addition to potential advantages in practice, developing powerful monocular 3D object detection systems will facilitate addressing a fundamental question in computer vision, that is whether it is possible to recover 3D structures from only 2D images which have lost the depth information in the first place. This paper is interested in 3D object detection in the autonomous driving application. The objective is to estimate the 3D bounding box for each object instance such as a car in a 2D image. In the KITTI benchmark <ref type="bibr" target="#b13">(Geiger et al. 2013</ref>), a 3D bounding box is parameterized by: (i) the 3D center location (x, y, z) in the camera 3D coordinates (in meters),  <ref type="figure">Figure 2</ref>: Illustration of the proposed MonoCon method for monocular 3D object detection without exploiting any extra information. It seeks a minimally-simple design. Given an input RGB image of dimensions 3 ? H ? W , a convolution neural network feature backbone computes the output feature map of dimensions D ? h ? w, where D is the output feature map dimension, h = H/s and w = W/s with s the overall stride/sub-sampling rate of the feature backbone (e.g., s = 4). Then, light-weight regression head branches are used in a direct and straightforward way, including one set of the regression head branches for the essential parameters (3D locations, shape dimensions and observation angles) which will be used in inferring the 3D bounding box, and the other set for the auxiliary contexts. Only the heatmap of 2D bounding box centers is class specific, and the others are class-agnostic. The proposed MonoCon is trained end-to-end and the auxiliary branches will be discarded in testing. In the right-top, the intermediate results for three regression branches are shown (note that the depth map will only be used sparsely based on the detected 2D bounding box centers). Best viewed in color and magnification. See text for detail. camera based on the vector from the camera center to the 3D object center, and (iii) the shape dimensions (h, w, l), i.e., height, width and length (in meters). Based on the extensive and insightful analyses made by the MonoDLE method  in the KITTI benchmark, one main challenge of improving the overall performance in monocular 3D object detection lies in inferring the 3D center location with high accuracy. To address the challenge, there are two main types of settings in state-of-the-art monocular 3D object detection, depending on whether there are extra information (Lidar depth measurements, monocular depth estimation results by a separately trained model or multi-frames) leveraged in training and/or inference. In practice, the 3D center location (x, y, z) is often decomposed to the projected 3D center in the image plane (x c , y c ) and the object depth z. With the camera intrinsic matrix assumed to be known in both training and inference, the 3D location can be recovered with the inferred projected 3D center and object depth.</p><p>This paper focuses on end-to-end monocular 3D object detection without exploiting any extra information. It adopts the anchor-offset formulation proposed in the Cen-terNet <ref type="bibr" target="#b69">(Zhou, Wang, and Kr?henb?hl 2019)</ref> in learning the projected 3D center based on the 2D bounding box center (i.e., the anchor), and proposes a simple yet effective method that facilitates better overall performance <ref type="figure">(Fig. 1)</ref>.</p><p>The key idea is to leverage Monocular Contexts as auxiliary learning tasks in training to improve the performance <ref type="figure">(Fig. 2)</ref>. The underlying rationale is that with the annotated 3D bounding boxes of objects in an image, there is a rich set of well-posed projected 2D supervision signals available in training, such as the projected corner keypoints and their associated offset vectors with respect to the anchor. They should be exploited in training to induce more expressive representations for monocular 3D object detection. The proposed method is thus dubbed as MonoCon.</p><p>Statistically speaking, the monocular contexts can be treated as marginal random variables in the image plane, which are projected from the 3D bounding box random variables. In measure theory, the Cram?r-Wold theorem <ref type="bibr" target="#b11">(Cram?r and Wold 1936)</ref> states that a Borel probability measure on R k is uniquely determined by the totality of its onedimensional projections. Motivated by the Cram?r-Wold theorem, the proposed MonoCon method introduces monocular projections as auxiliary tasks in training to learn more effective representations for monocular 3D object detection. In the meanwhile, it seeks a minimally-simple design of the overall detection system to justify the effectivenss of the underlying rationale and the high-level motivation.</p><p>In implementation, the proposed MonoCon utilizes a very simple design consisting of three components <ref type="figure">(Fig. 2)</ref>: a Deep Neural Network (DNN) based feature backbone, a number of regression head branches for learning the essential parameters used in the 3D bounding box prediction, and a number of regression head branches for learning auxiliary contexts. After training, the auxiliary context regression branches are discarded. In experiments, the proposed Mono-Con is tested in the KITTI benchmark (car, pedestrian and cyclist) <ref type="bibr" target="#b13">(Geiger et al. 2013)</ref>. It outperforms prior arts (including methods that use lidar, depth or multi-frame extra information) in the leaderboard on the car category and obtains comparable performance on pedestrian and cyclist in terms of accuracy. Thanks to the simple design, the proposed MonoCon obtains the fastest speed with 38.7 fps (on a single NVIDIA 2080Ti GPU card) in comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work and Our Contributions</head><p>Auxiliary tasks and auxiliary learning: In machine learning, auxiliary tasks refer to tasks which are leveraged in training with the sole goal of better performing the primary tasks of interest in inference. The learning procedure is thus called auxiliary learning, in contrast to multitask learning, for which all tasks in training will be of interest in inference too. Auxiliary tasks and auxiliary learning have shown many successful applications in a wide range of fields including computer vision <ref type="bibr" target="#b67">(Zhang et al. 2014;</ref><ref type="bibr" target="#b38">Mordan et al. 2018;</ref><ref type="bibr" target="#b28">Liu, Davison, and Johns 2019;</ref><ref type="bibr" target="#b62">Ye et al. 2021;</ref><ref type="bibr" target="#b51">Valada, Radwan, and Burgard 2018)</ref>, natural language processing <ref type="bibr" target="#b37">(Mikolov et al. 2013</ref>) and reinforcement learning <ref type="bibr" target="#b17">(Jaderberg et al. 2016;</ref><ref type="bibr" target="#b3">Burda et al. 2018)</ref>. Although simple, exploiting comprehensive 2D auxiliary tasks has not been studied well in monocular 3D object detection. The proposed MonoCon showcases the advantage of auxiliary learning for both performance (on the car category) and inference efficiency in the KITTI benchmark <ref type="bibr" target="#b13">(Geiger et al. 2013)</ref>.</p><p>Monocular 3D detection with extra information: Monocular 3D detection falls behind lidar-based and stereoimage based counterparts significantly due to its ill-posed nature. Therefore, many monocular 3D detection methods seek solutions with the help of extra information, such as lidar data <ref type="bibr" target="#b6">(Chen et al. 2021;</ref><ref type="bibr" target="#b42">Reading et al. 2021)</ref>, off-theshelf monocular depth estimation modules (pretrained using dense depth map) <ref type="bibr" target="#b59">(Xu and Chen 2018;</ref><ref type="bibr" target="#b12">Ding et al. 2020;</ref><ref type="bibr" target="#b57">Wang et al. 2019;</ref><ref type="bibr" target="#b33">Ma et al. 2020</ref><ref type="bibr" target="#b34">Ma et al. , 2019</ref><ref type="bibr" target="#b52">Wang et al. 2021a</ref>), multi-frames <ref type="bibr" target="#b2">(Brazil et al. 2020)</ref>, or CAD models <ref type="bibr" target="#b58">(Xiang et al. 2015;</ref><ref type="bibr" target="#b5">Chabot et al. 2017;</ref><ref type="bibr" target="#b14">He and Soatto 2019)</ref>, etc. Although these methods have shown promising results, however, most of these models heavily rely on extra modules (i.e. depth estimation modules, etc.), which entails extra computation cost. As a result, these methods are usually slow in inference (less than 10 fps), severely hindering their applications in real-time autonomous driving. The proposed MonoCon method does not use any extra information and seeks a minimally-simple design with very promising performance and real-time inference speed achieved. One motivation is that before exploiting the more computationally expensive settings using multi-view images or more costly settings with more sensors, we want to understand the "true" limit of purely monocular 3D detection methods.</p><p>Monocular 3D detection without extra information: Since the seminal work of Deep3DBox <ref type="bibr" target="#b39">(Mousavian et al. 2017</ref>), many efforts <ref type="bibr" target="#b4">Cai et al. 2020;</ref><ref type="bibr" target="#b10">Chen et al. 2020;</ref><ref type="bibr" target="#b0">Bao, Yu, and Kong 2020;</ref><ref type="bibr">Shi et al. 2021;</ref><ref type="bibr" target="#b29">Liu, Yixuan, and Liu 2021;</ref><ref type="bibr" target="#b31">Lu et al. 2021;</ref><ref type="bibr" target="#b65">Zhang, Lu, and Zhou 2021)</ref> have been proposed to utilize 2D-3D geometric constraints to improve 3D detection performance, which are often posed as multi-task learning, rather than auxiliary learning. For example, in the RTM3D method ), all of the learned 2D tasks are used as optimization terms to calculate the 3D location of cars in the post-processing (using the PnP method) in inference. In the MonoRCNN method <ref type="bibr">(Shi et al. 2021)</ref>, one 2D task (i.e., 2D box) is used. The 2D box prediction is used to calculate the depth together with 3D box size in inference. The SMOKE method (Liu, Wu, and T?th 2020) does not learn any 2D task. One main claim in SMOKE is that 2D tasks will interfere the learning of 3D tasks. In exploiting 2D-3D geometric constraints, existing work compute 3D locations explicitly based on 2D predictions, and thus often suffer from the well-known error amplification effect. So, more recent work try to use uncertainty modeling (e.g., the GUPNet ), or sophisticated model ensemble (e.g., in MonoFlex <ref type="bibr" target="#b65">(Zhang, Lu, and Zhou 2021)</ref>). The goal of the proposed MonoCon is to investigate the effects of 2D auxiliary tasks in training, and to eliminate the potential error amplification effect in inference, with improved performance obtained. More recently, there are efforts exploring how to generate extrinsic-invariant  or distance-invariant <ref type="bibr" target="#b49">(Simonelli et al. 2020)</ref> representations to improve 3D detection performance. The proposed MonoCon is complementary to aforementioned methods by leveraging well-posed 2D contexts projected from 3D bounding boxes as auxiliary learning tasks. It has the potential to be easily extended using aforementioned methods with performance further improved.</p><p>Our contributions: This paper makes three main contributions to monocular 3D object detection as follows: (i) It presents a simple yet surprisingly effective method, Mono-Con for purely monocular 3D object detection by learning auxiliary monocular contexts. At a high level, the proposed MonoCon formulation can be explained by the Cram?r-Wold theorem <ref type="bibr" target="#b11">(Cram?r and Wold 1936)</ref> in measure theory. (ii) It shows state-of-the-art performance on the car category in the KITTI 3D object detection benchmark, outperforming prior arts by a large margin. It obtains comparable performance on the pedestrian and cyclist categories. It can run at a speed of 38.7 fps, faster than prior arts. (iii) It sheds light on developing more powerful and efficient monocular 3D object detection systems by exploring and exploiting even more auxiliary contexts in general applications going beyond autonomous driving (e.g., robot navigation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach Problem Definition</head><p>Let ? be the image lattice (e.g. 384 ? 1280 in the KITTI benchmark), and I ? an image defined on the lattice. As aforementioned, the objective of monocular 3D object detection is to infer the label (e.g., car, pedestrian and cyclist) and the 3D bounding box for each object instance in I ? . The 3D bounding box is parameterized by the 3D center location (x, y, z) in meters, the shape dimensions (h, w, l) in meters and the observation angle ? ? [??, ?], all measured in the camera coordinate system. The observation angle is used in prediction due to its underlying stronger relationship with image appearance. The camera intrinsic matrix is assumed to be known in both training and inference.</p><p>Challenges. Typically, both the shape dimensions and the orientation are directly regressed using features computed by a feature backbone such as a Convolutional Neural Network (CNN). The direct regression methods also have shown good performance for them individually. In the meanwhile, the overall 3D bounding box prediction performance (e.g., the Average Precision (AP) based on the intersectionover-union) is relatively less sensitive to the shape dimensions and the orientation, in the sense that if the 3D center location can be inferred with high accuracy, the AP will not decrease dramatically even if the shape dimensions and the orientation are not accurately predicted. By contrast, even with very accurate estimate of shape dimensions and orientations, the AP will drop catastrophically if the 3D center location is perturbed. The underlying reason is the significant gap between the shape dimensions (roughly between 1 and 3 meters) and the 3D location (roughly between 1 and 60 meters), and the uncertainty measured for both of them in monocular images will cause dramatically different effects for the overall AP.</p><p>There are two different formulations in learning the projected 3D center (x c , y c ): One is to directly predict it by learning a heatmap representation, for which the projected centers falling outside the image plane are either simply discarded in training <ref type="bibr" target="#b30">(Liu, Wu, and T?th 2020;</ref><ref type="bibr" target="#b35">Ma et al. 2021)</ref> or cleverly handled with the help from the intersection point between the image edge and the line from the center of 2D bounding box to the outside projected 3D center <ref type="bibr" target="#b65">(Zhang, Lu, and Zhou 2021)</ref>. The other is to further decompose a projected 3D center into the center of 2D bounding box (x b , y b ) (i.e., the anchor inside the image plane) and an offset/displacement vector (?x, ?y) with x c = x b + ?x and y c = y b + ?y, following the CenterNet <ref type="bibr" target="#b69">(Zhou, Wang, and Kr?henb?hl 2019)</ref> formulation. Due to the large variation of the offset vectors, it is difficult to learn them. Thus, the latter is often inferior to the former in terms of the overall performance <ref type="bibr" target="#b65">(Zhang, Lu, and Zhou 2021;</ref><ref type="bibr" target="#b35">Ma et al. 2021)</ref>, albeit it is an intuitively simple and generic representation for learning the projected 3D center. The proposed method shows that the latter can work well when sufficient monocular contexts are exploited in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Proposed MonoCon Method</head><p>As illustrated in <ref type="figure">Fig. 2</ref>, the proposed MonoCon method is simple by design, consisting of three components:</p><p>Feature Backbone. Given an input RGB image I ? of dimensions 3 ? H ? W , a feature backbone f (?; ?) is used to compute the output feature map F of dimensions D ?h?w,</p><formula xml:id="formula_1">F = f (I ? ; ?),<label>(1)</label></formula><p>where ? collects all the learnable parameters, D is the output feature map dimension (e.g., D = 512), and h and w are determined by the overall stride/sub-sampling rate s in the backbone (e.g. s = 4). We use the DLA network <ref type="bibr" target="#b64">(Yu et al. 2018</ref>) (DLA-34) that is widely used in monocular 3D object detection for fair comparisons in experiments.</p><p>The 3D Bounding Box Regression Heads. We adopt the anchor-offset formulation in learning the projected 3D bounding box center (x c , y c ) in the image plane. A regression head is used to compute the class-specific heatmap H b of dimensions c ? h ? w for the 2D bounding box center (x b , y b ) for each of the c classes (e.g., c = 3 representing car, pedestrian and cyclist in the KITTI benchmark),</p><formula xml:id="formula_2">H b = g(F ; ? b ), (2) where g(?; ? b ) is realized by a light-weight module with the learnable parameters ? b , F Conv+AN +ReLU = =========== ? d?3?3?D F d?h?w Conv = ===== ? c?1?1?d H b c?h?w ,<label>(3)</label></formula><p>where the first convolution also reduce the feature dimension to d (e.g., d = 64) to be light-weight, and AN represents the Attentive Normalization (AN) <ref type="bibr" target="#b24">(Li, Sun, and Wu 2020)</ref>, which is a light-weight module integrating feature normalization (e.g., BatchNorm <ref type="bibr" target="#b16">(Ioffe and Szegedy 2015)</ref>) and channel-wise feature attention (e.g. the Squeeze-Excitation module <ref type="bibr" target="#b15">(Hu, Shen, and Sun 2018)</ref>). Thanks to its mixture modeling formulation of the affine transformation in recalibrating the features after standardization, it is adopted in the regression head for learning more expressive latent feature representations F d?h?w . The light-weight module architecture g(?) (Eqn. 3) is used by all regression heads with different instantiations (i.e., different learnable parameters).</p><p>The regression head of computing the offset vector</p><formula xml:id="formula_3">(?x c b , ?y c b ) from the 2D bounding box center (x b , y b ) to the projected 3D bounding box center (x c , y c ) is defined by, O c 2?h?w = g(F ; ? bc ).</formula><p>(4) Similarly, the depth and the shape dimensions are regressed respectively as follows,</p><formula xml:id="formula_4">Z 1?h?w = 1 Sigmoid (g (F ; ? Z ) [0]) + ? 1,<label>(5)</label></formula><formula xml:id="formula_5">? Z 1?h?w = g(F ; ? Z )[1],<label>(6)</label></formula><formula xml:id="formula_6">S 3D 3?h?w = g(F ; ? S 3D ),<label>(7)</label></formula><p>where g(F ; ? Z ) estimates the depth and its uncertainty. The inverse sigmoid transformation is applied to handle the unbounded output of g(F ; ? Z )[0], as done in <ref type="bibr" target="#b69">(Zhou, Wang, and Kr?henb?hl 2019)</ref>, and is a small positive constant to ensure numeric stability. ? Z is used to model the heteroscedastic aleatoric uncertainty in the depth estimation as done in <ref type="bibr" target="#b65">Zhang, Lu, and Zhou 2021;</ref><ref type="bibr" target="#b35">Ma et al. 2021)</ref>.</p><p>For the observation angle ?, the multi-bin setting proposed by <ref type="bibr" target="#b39">(Mousavian et al. 2017</ref>) is used. The angle range [??, ?] is divided evenly into a predefined number of b nonoverlapping bins (e.g., b = 12). The observation angle regression head is defined by,</p><formula xml:id="formula_7">A 2b?h?w = g(F ; ? A ),<label>(8)</label></formula><p>where the observation angle ? is predicted by computing its bin index, ? i ? {0, 1, ? ? ? 11} from the first b channels (using arg max after softmax along the b channels) and the corresponding angle residual, ? r in the second b channels of A, together with proper conversions to ensure ? ? [??, ?].</p><p>Computing the predicted 3D bounding box. Based on the peaks in each channel of the heatmap H b (Eqn. 2) after non-maximum suppression (NMS) and thresholding with a threshold ? (e.g., ? = 0.2), a set of 2D bounding box centers are detected for each class. Without loss of generality, consider a detected 2D bounding box center (x b , y b ) for a car, the offset vector is retrieved from O c (Eqn. 4),</p><formula xml:id="formula_8">(?x b , ?y b ) = O c (x b , y b ).</formula><p>Then, the projected 3D center for the car is predicted by (x c , y c ) = (x b +?x b , y b +?y b ). The corresponding depth is predicted by z = Z(x b , y b ). With the camera intrinsic matrix, the 3D location (x, y, z) will be computed in a straightforward way. Similarly, the shape dimensions (h, w, l) and the observation angle ? can be predicted for the car. With all these parameter inferred, the 3D bounding box is predicted.</p><p>The Auxiliary Context Regression Heads. The pro-posed MonoCon method exploits four types of projection information from 3D bounding boxes as auxiliary learning tasks.</p><p>i) The heatmaps of the projected keypoints. As done in computing the 2D bounding box center heatmap H b (Eqn. 2), the first type of auxiliary contexts is the heatmaps of the 9 projected keypoints consisting of the projected 8 corner points and the projected center of the 3D bounding box, and we have, H k 9?h?w = g(F ; ? k ).</p><p>(9) ii) The offset vectors for the 8 projected corner points. In addition to the offset vector from the 2D bounding box center to the projected 3D bounding box center, O c <ref type="figure" target="#fig_1">(Eqn. 4)</ref>, the second type of auxiliary contexts is the offset vectors from the 2D bounding box center to the 8 projected corner points of the 3D bounding box, and we have,</p><formula xml:id="formula_9">O k 16?h?w = g(F ; ? b k ).</formula><p>(10) Note that this is combined with Eqn. 4 with the first convolution block in g(?) shared in implementation.</p><p>iii) The 2D bounding box size. This is as done in the Cen-terNet <ref type="bibr" target="#b69">(Zhou, Wang, and Kr?henb?hl 2019)</ref>. The height and width of the 2D bounding box are regressed, S 2D 2?h?w = g(F ; ? S 2D ).</p><p>(11) iv) The quantization residual of a keypoint location. Due to the overall stride s (typically s &gt; 1) in the feature backbone, there is a residual between the pixel location in the original input image I ? and its corresponding pixel location in the output feature map F after multiplying the stride s. Consider the 2D bounding box center (x * b , y * b ) of a car in the original image, its pixel location in the feature map F is</p><formula xml:id="formula_10">(x b = x * b s , y b = y * b</formula><p>s ), and the residual is defined by,</p><formula xml:id="formula_11">?x b = x * b ? x b , ?y b = y * b ? y b .</formula><p>(12) We model the residual of the 2D bounding box center (x b , y b ) and that of the 9 projected keypoints (x k , y k ) separately to account for the underlying difference of the nature of those points. The latter is modeled in a keypoint-agnostic way as shown in <ref type="figure">Fig. 2</ref> for simplicity. We have,</p><formula xml:id="formula_12">R b 2?h?w = g(F ; ? R b ),<label>(13)</label></formula><formula xml:id="formula_13">R k 2?h?w = g(F ; ? R k ).<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions</head><p>We use five loss functions which are widely used in monocular 3D object detection, consisting of (i) the Gaussian kernel weighted focal loss <ref type="bibr" target="#b25">(Lin et al. 2017;</ref><ref type="bibr" target="#b20">Law and Deng 2018)</ref> function for the heatmaps (Eqn. 2 and Eqn. 9) as used in the CenterNet (Zhou, Wang, and Kr?henb?hl 2019), (ii) the Laplacian aleatoric uncertainty loss function for the depth estimation (Eqn. 5 and Eqn. 6), (iii) the dimension-aware L1 loss function for shape dimensions (Eqn. 7), (iv) the standard cross-entropy loss function for the bin index in observation angles (Eqn. 8), and (v) the standard L1 loss function for offset vectors (Eqn. 4 and Eqn. 10), the intra-bin angle residual in observation angles (Eqn. 8), 2D bounding box sizes (Eqn. 11) and the quantization residual (Eqn. 13 and Eqn. 14). We briefly discuss the first three as follows.</p><p>i) The Gaussian kernel weighted focal loss function for heatmaps <ref type="bibr" target="#b25">(Lin et al. 2017;</ref><ref type="bibr" target="#b20">Law and Deng 2018;</ref><ref type="bibr" target="#b69">Zhou, Wang, and Kr?henb?hl 2019)</ref>. Without loss of generality, consider a regressed heatmap H 1?h?w (e.g., the 2D bounding box centers of cars), the ground-truth heatmap H * 1?h?w is also generated at the resolution of the regressed heatmap. For each ground-truth center point (x * b , y * b ) ? P in the original image, its location in the ground-truth heatmap is</p><formula xml:id="formula_14">(x b = x * b s , y b = y * b s ) (where s is the overall stride of the feature backbone). A Gaussian kernel G(x, y) = exp (? (x?x b ) 2 +(y?y b ) 2 2?? 2 b</formula><p>) is used to model the center point, where ? b is a predefined object-size-adaptive standard deviation as used in <ref type="bibr" target="#b20">(Law and Deng 2018)</ref>. If two Gaussian kernels overlap, the element-wise maximum is kept. All the G(?, ?)'s are then collapsed to form the ground-truth heatmap H * . The loss function is defined by,</p><formula xml:id="formula_15">L(H, H * ) = ?1 N (x,y) (1 ? H xy ) ? log(H xy ), if H * xy = 1, (1 ? H * xy ) ? (H xy ) ? log(1 ? H xy ),<label>(15)</label></formula><p>where N = |P| is the number of ground-truth points. ? and ? are hyper-parameters (e.g., ? = 4.0 and ? = 2.0).</p><p>ii) The Laplacian aleatoric uncertainty loss function for depth <ref type="bibr" target="#b35">Ma et al. 2021;</ref><ref type="bibr" target="#b65">Zhang, Lu, and Zhou 2021)</ref>. Denote by Z * 1?h?w the ground-truth (sparse) depth map in which the ground-truth depth of an annotated 3D bounding box is assigned to the corresponding groundtruth 2D bounding box center location in the lattice of h?w, i.e., Z * (x b , y b ) (with the same inverse sigmoid transformation applied as in Eqn. 5). The Laplace distribution is used in modeling the uncertainty ? Z (Eqn. 6). For the prediction depth Z (Eqn. 5), the loss function is defined by,</p><formula xml:id="formula_16">L(Z, Z * ) = 1 |P| (x b ,y b )?P ? 2 ? Z b |z b ? z * b | + log(? Z b ),<label>(16)</label></formula><p>where P is the set of ground-truth 2D bounding box center points,</p><formula xml:id="formula_17">? Z b = ? Z (x b , y b ), z b = Z(x b , y b ) and z * b = Z * (x b , y b ).</formula><p>iii) The dimension-aware L1 loss function for shape dimensions , which is motivated by the IoU oriented optimization <ref type="bibr" target="#b43">(Rezatofighi et al. 2019</ref>) and realizes a re-distribution of the standard L1 loss. Similarly, let S 3D * be the ground-truth map of shape dimensions assigned to the ground-truth 2D bounding box center locations in the lattice of h ? w. For the predicted shape dimensions S 3D <ref type="figure">(Eqn. 7)</ref>, the loss function is defined by,</p><formula xml:id="formula_18">L(S 3D , S 3D * ) = ? ? || S 3D ? S 3D * S 3D || 1 ,<label>(17)</label></formula><p>where ? is the compensation weight to ensure the dimension-aware L1 loss has the same value as the standard L1 loss, which is by definition the ratio (without gradients in training) between the standard L1 loss and the dimensionaware loss before applying the compensation weight.</p><p>The Overall Loss is simply the sum of all loss terms each of which has a trade-off weight parameter. For simplicity, we use 1.0 for all loss terms except for the 2D size L1 loss which uses 0.1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we test the proposed MonoCon in the widely used and challenging KITTTI 3D object detection benchmark <ref type="bibr" target="#b13">(Geiger et al. 2013)</ref>. We first present comparisons with prior arts in the leaderboard, and then analyze the proposed MonoCon method using ablation studies. Data. The KITTI dataset consists of 7,481 images for training and 7,518 images for testing. There are three categories of interest: car, pedestrian and cyclist. The ground truth for the test set is reserved for evaluation on the test server. In comparison with prior arts, we train our Mono-Con on all 7,481 images and the performance is evaluated by the KITTI official server. For ablation studies, we follow the protocol used by prior works <ref type="bibr" target="#b7">(Chen et al. 2016</ref><ref type="bibr" target="#b8">(Chen et al. , 2015</ref><ref type="bibr" target="#b9">(Chen et al. , 2017</ref> to split the provided whole training data into a training subset (3,712 images) and a validation subset <ref type="bibr">(3,769 images)</ref>.</p><p>Evaluation Metrics. We follow the protocol provided in the KITTI benchmark. The detection is evaluated by the average precision (AP) of 3D bounding boxes AP 3D|R40 and the AP of bird's eye view (AP BEV |R40 ), both with 40 re-call positions (R40) used and under three difficulty settings (easy, moderate, and hard). The moderate difficulty level is used to rank methods in the KITTI leaderboard. The APs are computed with the intersection-over-union (IoU) threshold 0.7, 0.5 and 0.5 for car, pedestrian and cyclist respectively. Implementation Details. Our MonoCon is trained on a single GPU with a batch size of 8 in an end-to-end way for 200 epochs. The AdamW optimizer is used with (? 1 , ? 2 ) = (0.95, 0.99) and weight decay 0.00001 (not applying to feature normalization layers and bias parameters). The initial learning rate is 2.25e ? 4, and the cyclic learning rate scheduler is used (1 cycle), which first gradually increases the learning rate to 2.25e ? 3 with the step ratio 0.4, and then gradually drops to 2.25e ? 4 ? 1.0e ? 4 (i.e., the target ratio is (10, 1.0e ? 4)). The cyclic scheduler is also applied for the momentum with the target ratio (0.85/0.95, 1) and the same step ratio 0.4. Due to the auxiliary context regression heads in training, the complexity of training our MonoCon is greater in terms of training time and memory footprint. After training, the auxiliary components will be discarded, resulting in faster speed in inference than prior arts. We adopt the commonly used data augmentation methods such as photometric distortion and random horizontal flipping following <ref type="bibr" target="#b69">(Zhou, Wang, and Kr?henb?hl 2019;</ref><ref type="bibr" target="#b35">Ma et al. 2021;</ref><ref type="bibr" target="#b65">Zhang, Lu, and Zhou 2021)</ref>. We also utilize random shifting to augment cropped instances at the edge of images.</p><p>Comparisons with State-of-the-Art Methods <ref type="table" target="#tab_2">Table 1 and Table 2</ref> show the quantitative comparisons of our MonoCon with state-of-the-art methods. We provide qualitative results in the supplementary materials.</p><p>Comparisons on the car category. Cars are the dominant objects in the KITTI 3D object detection benchmark, and of the most interest in evaluation. Our MonoCon consistently outperforms all prior arts. In terms of the KITTI ranking protocol based on the AP 3D|R40 under the moderate difficulty setting, our MonoCon achieves significant improvement by 1.44% absolute increase against the runner-up method, the GUPNet . It also runs faster than prior arts. The improvement justify the effectiveness of learning more auxiliary monocular contexts in monocular 3D object detection for the autonomous driving applications. Our MonoCon also consistently outperform prior arts in the validation set with the results provided in the supplementary materials due to the space limit.</p><p>Comparisons on the pedestrian and cyclist categories. Our MonoCon shows inferior performance than some of the prior arts. On the pedestrian category, our MonoCon shows 1.35% drop of AP 3D|R40 under the moderate setting comparing with the best model, the GUPNet , but outperforms all other methods in comparisons. On the cyclist category, our MonoCon shows 1.29% drop comparing with the best purely monocular model, the MonoDLE method . Overall, our MonoCon is less effective on the cyclist category among the three categories. We observe that the 3D bounding boxes of pedestrian and cyclist are much smaller than those of car, and the projected monocular contexts are often in the very close proximity in the feature map (of the h ? w lattice). The close proximity may affect the learnability and effectiveness of the auxiliary contexts. One potential solution is to randomly sample a subset of auxiliary contexts that are spatially separate from each other, which we leave to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>We investigate the effects of learning auxiliary contexts and of the class-agnostic settings for regression heads in <ref type="figure">Fig. 2</ref>.</p><p>The Importance of Learning Auxiliary Contexts and the Attentive Normalization (AN). <ref type="table">Table 3</ref> shows the comparisons which show the effectiveness of the proposed MonoCon and justify the importance of the design choices. One the one hand, without the auxiliary components, our MonoCon is most similar to the MonoDLE method . We retrain an enhanced MonoDLE model which obtains significantly better performance than the vanilla MonoDLE. Our MonoCon still outperforms the enhanced MonoDLE by a large margin. On the other hand, we test 7 variants of our MonoCon model from (a) to (g). The auxiliary contexts are significantly more important than the Attentive Normalization: (g) vs (a) with a 8.49% absolution increase under the moderate difficulty settings, which clearly shows the significance of the proposed formulation against some implementation tuning. From (b) to (f), we rank the importance of the auxiliary contexts based on the performance of the model trained without them: the lower the performance is, the more important the context(s) are.</p><p>The effects of class-agnostic settings in regression heads and of training settings. <ref type="table" target="#tab_5">Table 4</ref> shows the comparisons. On the one hand, using the class-agnostic design shows better performance for the car and cyclist categories,  <ref type="table">Table 3</ref>: Ablation studies on different auxiliary contexts (see <ref type="figure">Fig. 2</ref> and Eqn. 9 to Eqn. 14) and the Attentive Normalization <ref type="bibr" target="#b24">(Li, Sun, and Wu 2020)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper proposes a simple yet effective formulation for monocular 3D object detection without exploiting any extra information. It presents the MonoCon method which learns auxiliary monocular contexts that are projected from the 3D bounding boxes in training. The proposed MonoCon utilizes a simple design in implementation consisting of a ConvNet feature backbone and a list of regression heads with the same module architecture for the essential parameters and the auxiliary contexts. In experiments, the proposed MonoCon is tested in the KITTI 3D object detection benchmark with state-of-the-art performance on the car category and comparable performance on the pedestrian and cyclist categories. At a high level, the effectiveness of the proposed MonoCon can be explained by the Cram?r-Wold theorem in measure theory. Ablation studies are performed to investigate, and the results support, the effectiveness of the proposed MonoCon method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials Quantitative and Qualitative Results on KITTI Validation Set</head><p>Quantitative Results: We present our quantitative 3D detection results on KITTI validation set <ref type="bibr" target="#b7">(Chen et al. 2016)</ref> in Tab. 5. It shows that our MonoCon method achieves significantly improvements on validation set for the car category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Extra AP 3D|R40|IoU ?0.7 AP BEV |R40|IoU ?0.7 Easy Mod. Hard Easy Mod. Hard Kinematic3D <ref type="bibr" target="#b2">(Brazil et al. 2020)</ref> Multi  Qualitative Results: We show more qualitative results on the validation set in <ref type="figure">Fig. 3</ref>. It shows that our MonoCon method achieves high localization performance on close&amp;mid-distance, not heavily occluded instances. For the instances that are heavily occluded and far away from the viewpoint, it would be still challenging for both our MonoCon and other monocular 3D object detectors <ref type="bibr" target="#b65">Zhang, Lu, and Zhou 2021;</ref><ref type="bibr" target="#b31">Lu et al. 2021)</ref>. Furthermore, we compare our MonoCon with our implemented enhanced MonoDLE  in <ref type="figure" target="#fig_1">Fig. 4</ref> to demonstrate the advantages of our MonoCon. The results shows that our MonoCon achieves better performance on truncated, occluded and far instances than the best-performing prior art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Qualitative Results on KITTI Test Set</head><p>We present our MonoCon's qualitative 3D detection performance in <ref type="figure" target="#fig_2">Fig. 5</ref>. Qualitative result shows that our model achieves high 3D detection precision, i.e. our model's predicted 3D box encompasses instances' point cloud well. Besides, our model even predicts two close objects (i.e. a person next to a car), which is usually hard for prior arts. <ref type="figure">Figure 3</ref>: Qualitative results of our MonoCon on KITTI validation set <ref type="bibr" target="#b7">(Chen et al. 2016</ref>). In the front view image, our prediction result is shown in blue, while the ground truth is shown in orange. In the lidar view image, our prediction result is shown in green. The ground truth 3D box is shown in blue.  In the lidar view image, our prediction result is shown in green.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of our MonoCon with enhanced MonoDLE* on KITTI validation set<ref type="bibr" target="#b7">(Chen et al. 2016</ref>). Our prediction result is shown in blue. The MonoDLE's prediction is shown in orange.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of our MonoCon on KITTI test set. In the front view image, our prediction result is shown in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>In both training and testing Groundtruth for regression In training only Loss Functions Auxiliary Monocular Contexts</head><label></label><figDesc>the observation angle ? of the object with respect to the arXiv:2112.04628v1 [cs.CV] 9 Dec 2021</figDesc><table><row><cell></cell><cell>Projected keypoints Heatmap</cell><cell cols="2">Offset Vectors</cell><cell>2D Bbox Size</cell><cell>2D Bbox Center Quantization Residual</cell><cell>(projected keypoint Projected keypoints Quantization Residual</cell><cell>2D Bbox Center Heatmap -Car</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>agnostic)</cell></row><row><cell>Annotated 3D bboxes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>An example of offset vector -Car</cell></row><row><cell></cell><cell></cell><cell cols="2">Offset Vector</cell><cell></cell><cell></cell></row><row><cell>Input image</cell><cell>2D Bbox Center Heatmap</cell><cell></cell><cell></cell><cell>Depth &amp; Uncertainty</cell><cell>Shape Dimensions</cell><cell>Observation angle</cell><cell>Estimated depth map</cell></row><row><cell></cell><cell>(per class)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Multibin regression)</cell></row><row><cell></cell><cell cols="2">Projected 3D Bbox Center</cell><cell cols="2">Camera intrinsic matrix</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3D location:</cell><cell></cell><cell>Predicted 3D bboxes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Comparisons with state-of-the-art methods on the car category in the KITTI official test set. Following the KITTI protocol, methods are ranked by their performance under the moderate difficulty setting. The best results are listed in bold and the second place in blue. The runtime of our MonoCon is measured using a single 2080Ti GPU card. The KITTI leaderboard entry of our MonoCon is at this link. AP 3D|R40|IoU ?0.5 Cyc., AP 3D|R40|IoU ?0.5</figDesc><table><row><cell cols="3">Methods, Publication Venues</cell><cell></cell><cell></cell><cell cols="2">Extra Info.</cell><cell>Runtime? (ms)</cell><cell>AP BEV |R40|IoU ?0.7 ? Easy Mod. Hard</cell><cell>AP 3D|R40|IoU ?0.7 ? Easy Mod. Hard</cell></row><row><cell cols="4">PatchNet, ECCV20 (Ma et al. 2020)</cell><cell></cell><cell></cell><cell></cell><cell>400</cell><cell>22.97 16.86 14.97 15.68 11.12 10.17</cell></row><row><cell cols="4">D4LCN, CVPR20 (Ding et al. 2020)</cell><cell></cell><cell></cell><cell>Depth</cell><cell>200</cell><cell>22.51 16.02 12.55 16.65 11.72</cell><cell>9.51</cell></row><row><cell cols="5">DDMP-3D, CVPR21 (Wang et al. 2021a)</cell><cell></cell><cell></cell><cell>180</cell><cell>28.08 17.89 13.44 19.71 12.78</cell><cell>9.80</cell></row><row><cell cols="5">Kinematic3D, ECCV20 (Brazil et al. 2020)</cell><cell cols="2">Multi-frames</cell><cell>120</cell><cell>26.69 17.52 13.10 19.07 12.72</cell><cell>9.17</cell></row><row><cell cols="5">MonoRUn, CVPR21 (Chen et al. 2021) CaDDN, CVPR21 (Reading et al. 2021)</cell><cell></cell><cell>Lidar</cell><cell>70 630</cell><cell>27.94 17.34 15.24 19.65 12.30 10.58 27.94 18.91 17.19 19.17 13.41 11.46</cell></row><row><cell cols="4">RTM3D, ECCV20 (Li et al. 2020)</cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell>19.17 14.20 11.99 14.41 10.34</cell><cell>8.77</cell></row><row><cell cols="5">Movi3D, ECCV20 (Simonelli et al. 2020)</cell><cell></cell><cell></cell><cell>45</cell><cell>22.76 17.03 14.85 15.19 10.90</cell><cell>9.26</cell></row><row><cell cols="4">IAFA, ACCV20 (Zhou et al. 2020)</cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell>25.88 17.88 15.35 17.81 12.01 10.61</cell></row><row><cell cols="4">MonoDLE, CVPR21 (Ma et al. 2021)</cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell>24.79 18.89 16.00 17.23 12.26 10.29</cell></row><row><cell cols="5">MonoRCNN, ICCV21 (Shi et al. 2021)</cell><cell></cell><cell></cell><cell>70</cell><cell>25.48 18.11 14.10 18.36 12.65 10.03</cell></row><row><cell cols="6">Ground-Aware, RAL21 (Liu, Yixuan, and Liu 2021)</cell><cell>None</cell><cell>50</cell><cell>29.81 17.98 13.08 21.65 13.25</cell><cell>9.91</cell></row><row><cell cols="3">PCT, -(Wang et al. 2021b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>45</cell><cell>29.65 19.03 15.92 21.00 13.37 11.31</cell></row><row><cell cols="4">MonoGeo, -(Zhang et al. 2021)</cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>25.86 18.99 16.19 18.85 13.81 11.52</cell></row><row><cell cols="4">MonoEF, CVPR21 (Zhou et al. 2021)</cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell>29.03 19.70 17.26 21.29 13.87 11.71</cell></row><row><cell cols="6">MonoFlex, CVPR21 (Zhang, Lu, and Zhou 2021)</cell><cell></cell><cell>35</cell><cell>28.23 19.75 16.89 19.94 13.89 12.07</cell></row><row><cell cols="4">GUPNet, ICCV21 (Lu et al. 2021)</cell><cell></cell><cell></cell><cell></cell><cell>34</cell><cell>30.29 21.19 18.20 22.26 15.02 13.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>None</cell><cell>25.8</cell><cell>31.12 22.10 19.00 22.50 16.46 13.95</cell></row><row><cell cols="3">MonoCon (Ours), AAAI22</cell><cell></cell><cell></cell><cell cols="2">Improvement</cell><cell>v.s. Depth v.s. Multi-frames +4.43 +4.58 +5.90 +3.43 +3.74 +4.78 +3.04 +4.21 +4.03 +2.79 +3.68 +3.78 v.s. LiDAR +3.18 +3.19 +1.81 +2.85 +3.05 +2.49</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>v.s. None</cell><cell>+0.83 +0.91 +0.80 +0.24 +1.44 +0.83</cell></row><row><cell>Table 1: Methods</cell><cell>Extra Info.</cell><cell cols="2">Ped., Easy Mod.</cell><cell>Hard</cell><cell>Easy Mod.</cell><cell>Hard</cell></row><row><cell>DDMP-3D</cell><cell>Depth</cell><cell>4.93</cell><cell>2.55</cell><cell>3.01</cell><cell>4.18 2.50</cell><cell>2.32</cell></row><row><cell>CaDDN</cell><cell>Lidar</cell><cell cols="2">12.87 8.14</cell><cell>6.76</cell><cell>7.00 3.41</cell><cell>3.30</cell></row><row><cell>MonoDLE</cell><cell></cell><cell>9.64</cell><cell>6.55</cell><cell>5.44</cell><cell>4.59 2.66</cell><cell>2.45</cell></row><row><cell>MonoGeo</cell><cell></cell><cell>8.00</cell><cell>5.63</cell><cell>4.71</cell><cell>4.73 2.93</cell><cell>2.58</cell></row><row><cell>MonoEF MonoFlex</cell><cell>None</cell><cell>4.27 9.43</cell><cell>2.79 6.31</cell><cell>2.21 5.26</cell><cell>1.80 0.92 4.17 2.35</cell><cell>0.71 2.04</cell></row><row><cell>GUPNet</cell><cell></cell><cell cols="2">14.95 9.76</cell><cell>8.41</cell><cell>5.58 3.21</cell><cell>2.66</cell></row><row><cell>MonoCon (Ours)</cell><cell></cell><cell cols="2">13.10 8.41</cell><cell>6.94</cell><cell>2.80 1.92</cell><cell>1.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with state-of-the-art methods on the pedestrian category and the cyclist category in the KITTI official test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>AP R40 , Car O k R b S 2D H k R k</figDesc><table><row><cell></cell><cell></cell><cell cols="3">2D Context Heads</cell><cell></cell><cell>AN</cell><cell>Val, Easy Mod. Hard</cell></row><row><cell>MonoDLE</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>17.45 13.66 11.68</cell></row><row><cell>MonoDLE  *</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>22.96 16.76 14.85</cell></row><row><cell>(a)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>16.57 10.20 8.14</cell></row><row><cell>(b)</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>17.65 11.42 8.71</cell></row><row><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>21.76 16.09 13.34</cell></row><row><cell>(d)</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>22.72 16.68 13.88</cell></row><row><cell>(e)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>23.51 17.76 15.03</cell></row><row><cell>(f)</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.11 18.28 15.35</cell></row><row><cell>(g)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>25.37 18.69 15.67</cell></row><row><cell>MonoCon (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.33 19.01 15.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>(AN in Eqn. 3)  in our MonoCon. * For fair comparisons and to justify our method's effectiveness, we re-implement and train a modified and enhanced version of the vanilla Mono-DLE with the AN added and the exactly same training settings as our MonoCon. Note that the 2D size is used in both training and inference in MononDLE.while the class-specific design is significantly better for the pedestrian category. On the other hand, jointly training the three categories is beneficial, which indicates that some inter-category synergy may exist.</figDesc><table><row><cell cols="3">Training Data Car Ped. Cyc.</cell><cell>Class-Agnostic</cell><cell cols="8">Val, APR40, Car Easy Mod. Hard Easy Mod. Hard Easy Mod. Hard Val, APR40, Ped. Val, APR40, Cyc.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">26.33 19.01 15.98 1.46 1.31</cell><cell cols="3">0.99 7.60 4.35</cell><cell>3.55</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell cols="5">24.69 18.53 15.49 9.21 6.85</cell><cell cols="3">5.49 3.44 1.50</cell><cell>1.50</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>N/A</cell><cell cols="3">24.60 18.15 15.36</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell>N/A</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">5.10 4.13</cell><cell>3.10</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>-</cell><cell>-</cell><cell></cell><cell>N/A</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">2.98 1.66</cell><cell>1.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on the design of regression heads (classagnostic vs class-specific in Eqn. 4 to Eqn. 14) and the training settings (joint vs separate training of car, pedestrian and cyclist).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>-frames 19.76 14.10 10.47 27.83 19.72 15.10 CaDDN<ref type="bibr" target="#b42">(Reading et al. 2021)</ref> </figDesc><table><row><cell></cell><cell>Lidar</cell><cell cols="2">23.57 16.31 13.84</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MonoDIS (Simonelli et al. 2019)</cell><cell></cell><cell>11.06 7.60</cell><cell cols="3">6.37 18.45 12.58 10.66</cell></row><row><cell>M3D-RPN (Brazil and Liu 2019)</cell><cell></cell><cell cols="4">14.53 11.07 8.65 20.85 15.62 11.88</cell></row><row><cell>MonoPair (Chen et al. 2020)</cell><cell></cell><cell cols="4">16.28 12.30 10.42 24.12 18.17 15.76</cell></row><row><cell>MonoDLE (Ma et al. 2021) MonoRCNN (Shi et al. 2021)</cell><cell>None</cell><cell cols="4">17.45 13.66 11.68 24.97 19.33 17.01 16.61 13.19 10.65 25.29 19.22 15.30</cell></row><row><cell>MonoGeo (Zhang et al. 2021)</cell><cell></cell><cell cols="4">18.45 14.48 12.87 27.15 21.17 18.35</cell></row><row><cell>MonoFlex (Zhang, Lu, and Zhou 2021)</cell><cell></cell><cell cols="2">23.64 17.51 14.83</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GUPNet (Lu et al. 2021)</cell><cell></cell><cell cols="4">22.76 16.46 13.72 31.07 22.94 19.75</cell></row><row><cell>MonoCon (Ours)</cell><cell>None</cell><cell cols="4">26.33 19.01 15.98 34.65 25.39 21.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Quantitative performance of the Car category on the KITTI validation set. Method are ranked by moderate settings based on 3D detection performance following KITTI leaderboard within each group. We highlight the best results in bold and the second place in blue.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements X. Liu and T. Wu   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object-Aware Centroid Voting for Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<idno>IEEE. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2197" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kinematic 3d object detection in monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large-scale study of curiosity-driven learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04355.3</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monocular 3D object detection with decoupled structured polygon estimation and height-guided depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>10478-10485. 3</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiview 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Some theorems on distribution functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the London Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mono3d++: Monocular 3d vehicle detection with two-scale 3d hypotheses and task priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8409" to="8416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR. 4</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05397.3</idno>
		<title level="m">Reinforcement learning with unsupervised auxiliary tasks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8973" to="8983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>734-750. 5</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rtm3d: Realtime monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attentive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1057" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reinforced axial refinement network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="540" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Self-supervised generalisation with meta auxiliary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08933.3</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ground-aware monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yixuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Smoke: Single-stage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>T?th</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13774.1</idno>
		<title level="m">Geometry Uncertainty Projection Network for Monocular 3D Object Detection</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">M3DSSD: Monocular 3D single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6145" to="6154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking pseudo-lidar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via colorembedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6851" to="6860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Delving into Localization Errors for Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>2, 4, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2069" to="2078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Revisiting multi-task learning with rock: a deep residual auxiliary block for visual detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<idno>NeurIPS. 3</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Triangulation learning network: from monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7615" to="7623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Categorical depth distribution network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reading</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>TPAMI. 1</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distancenormalized unified representation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="91" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<idno type="arXiv">arXiv:2104.03775.1</idno>
		<title level="m">Geometry-based Distance Decomposition for Monocular 3D Object Detection</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards generalization across depth for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10548" to="10557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep auxiliary learning for visual localization and odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno>IEEE. 3</idno>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6939" to="6946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05793.6</idno>
		<title level="m">Progressive Coordinate Transforms for Monocular 3D Object Detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10956.1</idno>
		<title level="m">FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Probabilistic and Geometric Depth: Detecting Objects in Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14160.1</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Task-aware monocular depth estimation for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12257" to="12264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Datadriven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Zoomnet: Part-aware adaptive zooming neural network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12557" to="12564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno>3337. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04112.3</idno>
		<title level="m">Auxiliary Tasks and Exploration Enable ObjectNav</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection via feature domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="17" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Objects are Different: Flexible Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Learning Geometry-Guided Depth via Projective Modeling for Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13931.3</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">IAFA: Instance-aware Feature Aggregation for 3D Object Detection from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ACCV. 6</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Monocular 3D Object Detection: An Extrinsic Parameter Free Approach</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
