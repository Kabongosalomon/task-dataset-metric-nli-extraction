<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Co-Attention Network for Fine-Grained Visual Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Intelligent System Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Intelligent System Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Intelligent System Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Intelligent System Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive Co-Attention Network for Fine-Grained Visual Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Fine-grained visual classification</term>
					<term>channel inter- action</term>
					<term>attention mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained visual classification aims to recognize images belonging to multiple sub-categories within a same category. It is a challenging task due to the inherently subtle variations among highly-confused categories. Most existing methods only take an individual image as input, which may limit the ability of models to recognize contrastive clues from different images. In this paper, we propose an effective method called progressive co-attention network (PCA-Net) to tackle this problem. Specifically, we calculate the channel-wise similarity by encouraging interaction between the feature channels within same-category image pairs to capture the common discriminative features. Considering that complementary information is also crucial for recognition, we erase the prominent areas enhanced by the channel interaction to force the network to focus on other discriminative regions. The proposed model has achieved competitive results on three fine-grained visual classification benchmark datasets: CUB-200-2011, Stanford Cars, and FGVC Aircraft.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In the past few years, convolutional neural networks (CNNs) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref> have achieved remarkable success in general image classification tasks. However, recognizing fine-grained object categories (e.g., bird species <ref type="bibr" target="#b4">[5]</ref>, car <ref type="bibr" target="#b5">[6]</ref> and aircraft <ref type="bibr" target="#b6">[7]</ref> models) is still a challenging task due to high intra-class variances and low inter-class variances, which attracts extensive research attentions.</p><p>The research of fine-grained visual classification has changed from multi-stage framework with hand-crafted features <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref> to multi-stage framework based on CNN features and then to end-to-end methods. Some works design a localization sub-network to locate key parts, and then a classification sub-network follows for identification. Such as STN <ref type="bibr" target="#b11">[12]</ref>, RA-CNN <ref type="bibr" target="#b12">[13]</ref>, NTS-Net <ref type="bibr" target="#b13">[14]</ref>. They first detected the local areas, and then cropped the detected areas on the original image to learn the key parts. However, little or no effort has been made to guarantee maximum discriminability in these areas <ref type="bibr" target="#b14">[15]</ref> and the training procedures of these methods are sophisticated due to the complex architecture designs <ref type="bibr" target="#b15">[16]</ref>. Others directly learn a more discriminative feature representation by developing powerful deep models <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b20">[21]</ref>. Among them, the most representative method is bilinear CNNs <ref type="bibr" target="#b16">[17]</ref>, * Corresponding author CNN CNN <ref type="figure">Fig. 1</ref>. The motivation of the proposed method. Most previous methods only took an individual image as input, and the relationship between images was not explored. In our work, we input a pair of same-category images and model the channel interaction between them to capture their common features. which used two deep convolutional networks to fuse output features, so that it could encode high-order statistics of convolutional activation. More recent advances reduce high feature dimensions <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> or use kernel methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> to extract higher order information. However, these works do not have an effective design for fine-grained visual classification in terms of the relationship between categories and the number of key parts. Meanwhile, most of the methods mentioned above only take an individual image as input, which will limit their ability to identify contrastive clues from different images for fine-grained visual classification. Generally, humans often recognize fine-grained objects by comparing image pairs, instead of checking single image alone <ref type="bibr" target="#b21">[22]</ref>. As shown in <ref type="figure">Figure 1</ref>, by comparing a pair of same-category images, we can easily capture their common and prominent features, such as head and wings. In this way, we can learn the discriminative features precisely.</p><p>Inspired by this observation, we propose a co-attention module (CA-Module) to model the channel interaction between a pair of same-category images. By capturing the contrastive features between channels, the model can better learn the commonality of same-category images, thereby force the network to focus on the common discriminative features. However, only focusing on common features of same-category images will cause the network to ignore complementary features that are essential for highly-confused categories. In order to tackle this problem, we propose an attention erase module (AE-Module) to learn complementary features by erasing the most prominent area found in CA-Module. With the combination of these two modules, the proposed method can capture more relevant areas to improve the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>In this section, we present the progressive co-attention network (PCA-Net) for fine-grained visual classification, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. It is made up of the CA-Module and the AE-Module to dig distinguishing features that are both prominent and complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Co-Attention Module</head><p>Each channel of feature maps can be considered as the representation of a certain feature, and its value is the response to the current feature's strength. Convolutional layer is to model the interaction between channels within an image to generate more abundant features. Gao et al. <ref type="bibr" target="#b22">[23]</ref> proposed a self-channel interaction (SCI) module to model the interplay between different channels within an image more effectively. Since the intra-class variances of fine-grained images are very high, it is important to compare a pair of same-category images to obtain common features. Hence, unlike <ref type="bibr" target="#b22">[23]</ref> modeling selfchannel interaction, we propose a co-attention module (CA-Module) to model the interaction between channels within two same-category images, forcing the network to focus on the common areas.</p><p>Specifically, given an image pair, the two images are first processed by a convolutional network to generate a pair of feature maps F 1 , F 2 ? R c?h?w . The c, h and w indicate channel numbers, height, and width respectively. We reshape the feature maps F 1 , F 2 to F 1 , F 2 ? R c?l , l = h?w. Then the channel-wise similarity is calculated by performing a bilinear operation on F 1 and F T 2 to obtain a bilinear matrix F 1 F T 2 as M . Then we take the negative value of it and get the weight matrix through a softmax function:</p><formula xml:id="formula_0">W ij = exp(?M ij ) c k=1 exp(?M ik ) .<label>(1)</label></formula><p>Where i represents the i th channel of the first image, and j represents the j th channel of the second image. We weight the weight matrix W to the original feature maps. Then the interacted feature maps can be obtained as:</p><formula xml:id="formula_1">F W 1 = W ? F 1 ? R c?h?w , F W 2 = W ? F 2 ? R c?h?w .<label>(2)</label></formula><p>Moreover, we use the cross-entropy loss for classification based on the predictions that are generated by the features F W .</p><p>After weighting the feature maps, the channels corresponding to similar features are enhanced to find the commonality of this pair of images. The response values of similar parts increase, and the response values of other parts decrease. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Erase Module</head><p>The CA-Module aims to focus on the common features of same-category images, but it ignores other slight clues containing complementary information. Because there are subtle differences between highly-confused categories in fine-grained images, it is necessary to pay attention to subtle features. Hence, we propose an attention erase module (AE-Module) to capture the subtle complementary features by erasing the prominent areas in the image.</p><p>We perform global average pooling on the feature maps weighted by CA-Module. Then we select the channel of the feature maps corresponding to the maximum value as attention map, and up-sample it to the original image size:</p><formula xml:id="formula_2">A(x, y) = U psample(max m [F W (m, x, y)]).<label>(3)</label></formula><p>Where m denotes the channel index, x and y denote the spatial indexs.</p><p>We obtain a drop mask M by setting the elements A(x, y) larger than threshold ? to 0, and setting other elements to 1:</p><formula xml:id="formula_3">M (x, y) = 0, if A(x, y) &gt; ? 1, else .<label>(4)</label></formula><p>Overlay the drop mask on the original image to obtain a new image with partial areas erased:</p><formula xml:id="formula_4">I e = I(x, y) ? M (x, y),<label>(5)</label></formula><p>where ? denotes element-wise multiplication, I denotes the original image, I e denotes the erased image.</p><p>As the prominent areas of the image are erased, the attention is distracted and the network is forced to learn discriminative information from other areas. It can also reduce the dependence on training samples to improve the robustness of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fine-Grained Feature Learning</head><p>In order to make the learned features more aggregated in embedding space, we introduce the center loss <ref type="bibr" target="#b30">[31]</ref>. For each category, a feature vector is calculated as the center of the corresponding category, and it will be continuously updated during training phase. By penalizing the deviation between the bilinear feature vector of each sample and the center of the corresponding category, the samples belonging to the same category are grouped together as many as possible, which enhances the discrimination of the learned features.</p><p>Benefiting from global and local informative features of an image, we define the feature representations for each image:  </p><formula xml:id="formula_5">F J = {F O , F W , F E },</formula><formula xml:id="formula_6">L = i?I L cls (Y i , Y * ) + ?L cen (Y i , Y c ),<label>(6)</label></formula><p>where L cls denotes the cross-entropy loss, L cen denotes the center loss and ? denotes the weight of L cen . Y i is the predicted bilinear label vector based on features F O , F W and F E . Y * is the ground-truth label vector and Y c is the learned center vector of category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND DISCUSSIONS</head><p>We evaluate the proposed approach on three publicly competitive fine-grained visual classification datasets, including CUB-200-2011 <ref type="bibr" target="#b4">[5]</ref>, Stanford Cars <ref type="bibr" target="#b5">[6]</ref>, and FGVC Aircraft <ref type="bibr" target="#b6">[7]</ref>. The detailed statistics with category numbers and the standard training/testing splits are summarized in <ref type="table" target="#tab_0">Table I</ref>. We employ top-1 accuracy as evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>In all experiments, we use ResNet-50 <ref type="bibr" target="#b3">[4]</ref> and ResNet-101 <ref type="bibr" target="#b3">[4]</ref> as our base networks. We remove the last pooling layer and fully-connected layer, and add a billinear pooling <ref type="bibr" target="#b16">[17]</ref> to generate a more powerful feature representation. Then send the feature representation to a new fully-connected layer. The <ref type="figure">Fig. 3</ref>. The first line is the original images, the second line is the features learned by the base model, and the third line is the features learned by the proposed method. It can be observed that the proposed method has learned more feature information than the base model. convolutional layers are pretrained on ImageNet <ref type="bibr" target="#b31">[32]</ref> and the fully connected layer is randomly initialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours Original</head><p>The input image size of all experiments is 448 ? 448, which is the same as the most state-of-the-art fine-grained classification approaches <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b27">[28]</ref>. In order to improve the generalization ability of the model, we implement data augmentation including random cropping and random horizontal flipping during training phase. Only center cropping is involved in inference phase. All experimental models are trained for 180 epochs, using SGD as optimizer. The initial learning rate is set to 0.01, which annealed by 0.9 every 2 epoches. We use a batch size of 32, and the last 16 samples belong to the same category as the first 16 samples. The weight decay is set to 10 ?5 and the momentum is set to 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental results</head><p>The proposed method is compared with several representative methods without extra annotations on three fine-grained benchmark datasets of CUB-200-2011, Stanford Cars, and FGVC Aircraft, as shown in <ref type="table" target="#tab_0">Table II</ref>. The proposed method performs competitive results compared with the state of-the-art methods.</p><p>Specifically, the top-1 accuracy of the proposed method on CUB-200-2011 dataset is 88.9%, which is better than 88.6% of API-Net <ref type="bibr" target="#b21">[22]</ref>. API-Net performs well because it also takes a pair of images as input, and learns a mutual feature vector by modeling the interaction between them to capture the semantic difference in the input pair. Our method is to capture the semantically similarity of a pair of same-category images, and disperse the network's attention to each discriminative area of the image.</p><p>On the other two datasets, our method also performs well, which achieves the result with 94.6% top-1 accuracy for Stanford Cars dataset and 92.8% top-1 accuracy for FGVC-Aircraft dataset. However, our method does not obtain the best results on these two datasets. Since cars and aircrafts are rigid objects, the intra-class variances of them are not assignificant as birds. And our method works better for objects that greatly vary within the category. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation studies</head><p>In order to verify the effectiveness of each component in the model, we conduct ablation studies on CUB-200-2011 dataset using ResNet-50 with some of components removed for better understanding the behavior of the model, as shown in <ref type="table" target="#tab_0">Table III</ref>.</p><p>It can be observed that CA-Module generates weighted feature maps through modeling the channel-wise interaction of same-category images, so that the network can learn their common features. AE-Module erases the most prominent area based on the weighted feature maps, making the informative areas learned by the network diversified. The center loss makes the extracted features more discriminative by constraining the distance between the feature vector and the center vector of the corresponding category. Each component of the model contributes to performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualizations</head><p>In order to further evaluate the effectiveness of our method, we apply Grad-CAM <ref type="bibr" target="#b32">[33]</ref> to visualize the images of the CUB-200-2011 dataset. Grad-CAM is formed by weighted summation of feature maps, which can show the importance of each area to its classification. We compare the visualization results of our method with the base model (ResNet-50), as shown in <ref type="figure">Figure 3</ref>. It can be observed that the base model only learns the most prominent area of the image, such as the bird's beak. Our method can learn more abundant and discriminative features, including wings and claws. This is because that our method can distribute attention to each area to make the prediction more comprehensive, which can not only focus on the salient features, but also capture the subtle and fine-grained features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSIONS</head><p>We propose an effective fine-grained visual classification method, namely progressive co-attention network. Among them, the co-attention module learns discriminative features by comparing same-category images, and the attention erase module learns subtle complementary features of images by erasing the most prominent area. We have conducted experiments on CUB-200-2011, Stanford Cars, and FGVC Aircraft datasets, which are superior to most existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>where F O denotes the feature maps extracted from the original image, F W denotes the weighted The framework of the progressive co-attention network (PCA-Net). The CA-Module can model the channel-wise interaction within a pair of samecategory images to focus on the prominent areas with common characteristics. The AE-Module can erase the images to distract attention to other areas to capture complementary features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I STATISTICS</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">OF BENCHMARK DATASETS</cell></row><row><cell>Datesets</cell><cell cols="3">Class Training Testing</cell></row><row><cell>CUB-200-2011</cell><cell>200</cell><cell>5997</cell><cell>5794</cell></row><row><cell>Stanford Cars</cell><cell>196</cell><cell>8144</cell><cell>8041</cell></row><row><cell>FGVC Aircraft</cell><cell>100</cell><cell>6667</cell><cell>3333</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARATIVE</head><label>II</label><figDesc>EXPERIMENT BETWEEN RECENT METHODS WITH OUR METHOD ON CUB-200-2011, STANFORD CARS, AND FGVC AIRCRAFT. THE BEST RESULT IS COLORED IN RED, AND THE SECOND BEST RESULT IS COLORED IN BLUE.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Input size</cell><cell cols="3">CUB-200-2011 (%) Stanford Cars (%) FGVC-Aircraft (%)</cell></row><row><cell>FT VGG-19 [24]</cell><cell>VGG-19</cell><cell>448 ? 448</cell><cell>77.8</cell><cell>84.9</cell><cell>84.8</cell></row><row><cell>RA-CNN [13]</cell><cell>VGG-19</cell><cell>448 ? 448</cell><cell>85.3</cell><cell>92.5</cell><cell>?</cell></row><row><cell>MA-CNN [25]</cell><cell>VGG-19</cell><cell>448 ? 448</cell><cell>86.5</cell><cell>92.8</cell><cell>89.9</cell></row><row><cell>FT ResNet-50 [24]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>84.1</cell><cell>91.7</cell><cell>88.5</cell></row><row><cell>RAM [26]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>86.0</cell><cell>93.1</cell><cell>-</cell></row><row><cell>DFL-CNN [24]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>87.4</cell><cell>93.1</cell><cell>91.7</cell></row><row><cell>NTS-Net [14]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>87.5</cell><cell>93.9</cell><cell>91.4</cell></row><row><cell>MC-Loss [27]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>87.3</cell><cell>93.7</cell><cell>92.6</cell></row><row><cell>TASN [28]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>87.9</cell><cell>93.8</cell><cell>-</cell></row><row><cell>Cross-X [29]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>87.7</cell><cell>94.6</cell><cell>92.6</cell></row><row><cell>ACNet [30]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>88.1</cell><cell>94.6</cell><cell>92.4</cell></row><row><cell>MAMC [16]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>86.2</cell><cell>92.8</cell><cell>-</cell></row><row><cell>MAMC [16]</cell><cell cols="2">ResNet-101 448 ? 448</cell><cell>86.5</cell><cell>93.0</cell><cell>-</cell></row><row><cell>CIN [23]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>87.5</cell><cell>94.1</cell><cell>92.6</cell></row><row><cell>CIN [23]</cell><cell cols="2">ResNet-101 448 ? 448</cell><cell>88.1</cell><cell>94.5</cell><cell>92.8</cell></row><row><cell>API-Net [22]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>87.7</cell><cell>94.8</cell><cell>93.0</cell></row><row><cell>API-Net [22]</cell><cell cols="2">ResNet-101 448 ? 448</cell><cell>88.6</cell><cell>94.9</cell><cell>93.4</cell></row><row><cell>Ours</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>88.3</cell><cell>94.3</cell><cell>92.4</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-101 448 ? 448</cell><cell>88.9</cell><cell>94.6</cell><cell>92.8</cell></row><row><cell cols="4">feature maps of the original image, and F E denotes the feature</cell><cell></cell><cell></cell></row><row><cell cols="4">maps extracted from the erased image. These features are then</cell><cell></cell><cell></cell></row><row><cell cols="4">fed to a fully-connection layer with a softmax function for the</cell><cell></cell><cell></cell></row><row><cell>final classification.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">During training phase, the whole model is optimized by</cell><cell></cell><cell></cell></row><row><cell>losses defined as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell>ABLATION STUDIES</cell></row><row><cell>CA-Module AE-Module center loss accuracy(%)</cell></row><row><cell>86.8</cell></row><row><cell>86.5</cell></row><row><cell>87.9</cell></row><row><cell>87.5</cell></row><row><cell>88.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A codebook-free and annotation-free approach for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Symbiotic segmentation and part localization for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Revisiting the fisher vector for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe-Henri</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naila</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="92" to="98" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Herv? J?gou, and Florent Perronnin</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification via progressive multi-granularity training of jigsaw patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
	<note>Compact bilinear pooling,&quot; in CVPR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for finegrained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Channel Interaction Networks for Fine-Grained Image Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic computational time for visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The devil is in the channels: Mutual-channel loss for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attention convolutional binary neural tree for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11378</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
