<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Changer: Feature Interaction is What You Need for Change Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
							<email>lizhe@sdust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Changer: Feature Interaction is What You Need for Change Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Change detection is an important tool for long-term earth observation missions. It takes bi-temporal images as input and predicts "where" the change has occurred. Different from other dense prediction tasks, a meaningful consideration for change detection is the interaction between bi-temporal features. With this motivation, in this paper we propose a novel general change detection architecture, MetaChanger, which includes a series of alternative interaction layers in the feature extractor. To verify the effectiveness of MetaChanger, we propose two derived models, ChangerAD and ChangerEx with simple interaction strategies: Aggregation-Distribution (AD) and "exchange". AD is abstracted from some complex interaction methods, and "exchange" is a completely parameter&amp;computation-free operation by exchanging bi-temporal features. In addition, for better alignment of bi-temporal features, we propose a flow dual-alignment fusion (FDAF) module which allows interactive alignment and feature fusion. Crucially, we observe Changer series models achieve competitive performance on different scale change detection datasets. Further, our proposed ChangerAD and ChangerEx could serve as a starting baseline for future MetaChanger design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Change detection is one of the most widely used fundamental technologies in earth vision. Compared to remote sensing (RS) image segmentation, change detection has two advantages in application. 1) In long-term earth observation, rather than predicting all pixels of the whole image, we often only need to focus on the land-cover category in the changed area. 2) In semi-automated applications, change detection is more tolerant of some misdetection.</p><p>Specifically, change detection is a pixel-to-pixel task, which takes bi-temporal images as input and predicts "where" the change has occurred. Driven by large amounts of RS data <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref>, change detection models based on ConvNet or Vision Transformer achieve more competitive MetaChanger is presented as a general architecture with alternative interaction layer (e.g. AD and feature "exchange" in this paper) and fusion layer (e.g. FDAF in this paper). Here, we frame the focus of this paper by dotted lines."share" denotes weight sharing.</p><p>performance in many complex scenarios. Recently, most Deep Learning (DL)-based change detection methods have been designed in close relation to segmentation models, and focus on some common problems, for instance, the misdetection caused by edges, small targets, and various scales. The goal of semantic segmentation is to determine a network to fit the target Y as much as possible, which can be described by minimizing the empirical loss as: min ? L(F ? (X), Y ). Different from this, change detection takes two inputs, i.e. the bi-temporal images X 1 and X 2 , and it can be described as min ? L(F ? (X 1 , X 2 ), Y ).</p><p>Therefore, a worthwhile consideration is whether the correlation between X 1 and X 2 should be explored, and if so, how to implement it. For the first issue, there are many studies illustrating the important effects of interaction between homo/hetero-geneous features <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b46">46]</ref>. And specifically for bi-temporal images, there are style differences between different temporal image domains, which are due to climate change, pre-processing corrections, etc. The domain differences affect the target of interest (e.g. building) and the background in different degrees, which, together with the use of siamese network, makes the understanding of the "change of interest" ambiguous for the model.</p><p>For the second issue, in this paper, firstly, we propose a general change detection architecture, MetaChanger, which aims to emphasize the effect of feature interactions during feature extraction in change detection, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Specifically, there are series of alternative interaction layers and fusion layers designed in MetaChanger.</p><p>Then, to verify MetaChanger, we try two simple interaction strategies: aggregation-distribution (AD) and "exchange". Specifically, the AD interaction is abstracted from some co-/cross-attention mechanisms, coming from tasks related to multi-modality, tracking, etc. And the "exchange" interaction is a completely parameter&amp;computation-free operation, which is achieved by exchanging bi-temporal feature maps in the spatial or channel dimension, with the exchanged features being mixed as they pass through subsequent convolution or token mixer. Astonishingly, these derived model, termed ChangerAD and ChangerEx, achieve extremely competitive performance, and even consistently outperforms other well-tuned change detection models. Moreover, we make further exploration of ChangerEx in view of its excellent performance. The results in multiple datasets show that MetaChanger, even with na?ve interaction layers, can still deliver promising performance.</p><p>In addition to the interaction during feature extraction, we propose the Flow Dual-Alignment Fusion (FDAF) module for the interactive fusion of dual-branch features to overcome the problem of side-looking and mis-alignment in multi-temporal RS images. All the interaction and fusion components abstracted in MetaChanger are not limited to these specific types. We hope our findings inspire more future research dedicated to improving MetaChanger.</p><p>Our main contributions can be summarized as follows. (1) We propose MetaChanger as a general change detection framework that focuses on a series of alternative interaction layers and can serve as a strong baseline. (2) We propose two embarrassingly simple interaction strategies, AD and feature "exchange", and extensive experiments demonstrate that they can greatly improve MetaChanger's performance, especially "exchange", even when embedded in complex networks or applied to challenging datasets. (3) We propose an interactive fusion module, called FDAF, which alleviates mis-alignment problem in bi-temporal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Binary Change Detection</head><p>We roughly divide DL-based change detection methods into two types, according to how the change map is acquired: metric-based and classification-based. In general, the metric-based method transforms the two input images into a feature space whose feature representation becomes more consistent <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">32]</ref>. In this feature space, the ultimate change map is obtained by the threshold algorithm <ref type="bibr" target="#b23">[24]</ref>. To achieve this goal, metric learning-related losses, such as contrast loss <ref type="bibr" target="#b14">[15]</ref> are leveraged to pull unchanged pairs to-gether and push changed pairs apart <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">45]</ref>.</p><p>The classification-based method, however, takes change detection as a dense classification task directly <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48]</ref>. The bi-temporal features are fused at a certain stage and the ultimate change map is generated by a classifier at the top of the network. Usually, a simple cross-entropy loss is sufficient to optimise the model stably. <ref type="bibr" target="#b11">[12]</ref> proposes three typical change detection models: FC-EF, FC-Siam-Conc and FC-Siam-Diff. Among them, FC-EF uses the early fusion strategy and the latter two use the medium fusion strategy with different fusion policies. In addition to CNN models, some transformer-based models achieve competitive performance in change detection. BiT <ref type="bibr" target="#b3">[4]</ref> builds an efficient change detection model with very limited parameters by mixing CNN and transformer. ChangeFormer <ref type="bibr" target="#b1">[2]</ref> is a pure transformer model, which is a siamese variant of SegFormer <ref type="bibr" target="#b42">[42]</ref>, and is fine-tuned in network depth. Different from the above methods, our proposed MetaChanger, shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, abstracts and simplifies the processes for change detection and especially focuses on the feature interaction between bi-temporal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature Fusion</head><p>Feature fusion is a fundamental process in many DL tasks, such as in classification <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b43">43]</ref>, segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">29]</ref>, multimodal tasks <ref type="bibr" target="#b37">[37]</ref>, etc. They include the fusion of multiple levels, multiple scales, heterogeneous features, etc.</p><p>For the specific operation of feature fusion, some simple parameter-free operations such as concat, weighted sum or bilinear pooling <ref type="bibr" target="#b22">[23]</ref> can build stable baseline performance. Additionally, several attention-based methods make feature fusion more flexible and learnable <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">49]</ref>. Moreover, alignment-based fusion methods focus on feature alignment, and they typically use flow field or deformable convolution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b50">50]</ref> to align features of different levels in the spatial dimension <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b38">38</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Feature Interaction</head><p>In some studies, feature interaction is included in feature fusion. However, here, we define feature interaction in change detection as the correlation or communication of homo/hetero-geneous features during feature extraction before fusion. Co-attention mechanism <ref type="bibr" target="#b25">[26]</ref> is frequently used in feature interaction, and similar formats have been applied with great success in many research fields like multimodal related tasks (e.g. VQA <ref type="bibr" target="#b28">[28]</ref>, RGB-D segmentation <ref type="bibr" target="#b7">[8]</ref>), Registration <ref type="bibr" target="#b41">[41]</ref>, Matching <ref type="bibr" target="#b40">[40]</ref> and network architecture design <ref type="bibr" target="#b19">[20]</ref>. This format of interaction aggregates several features and then distributes them respectively as attention maps. Hence, we abstract this format as AD interaction.</p><p>In addition, there are some other effective methods for interaction. For example, <ref type="bibr" target="#b44">[44]</ref> uses channel-wise crossattention to learn mutual information from dual branches for object tracking. <ref type="bibr" target="#b41">[41]</ref> generates the spatial affinity matrix between source and target point clouds. MixFormer <ref type="bibr" target="#b6">[7]</ref> makes bi-directional interaction across self-attention and DWConv, providing complementary cues in the channel and spatial dimensions. Different from these complex methods, Our proposed feature "exchange" method is extremely simple and does not introduce any extra computation. More relevant to feature "exchange" is CEN <ref type="bibr" target="#b39">[39]</ref>, which use channel exchange for interaction, where channels corresponding to the smaller BN scaling factors will be replaced by channels from other modalities. Yet, unlike CEN which is applied to multimodal problem, we focus on identifying positions where semantic differences exist for change detection. Hence, strict semantic maintenance and semantic correspondence must be kept between the both domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MetaChanger</head><p>MetaChanger is not divorced from classification-/metricbased models, but is a purposeful framework for exploring interaction strategies. In MetaChanger, we use CE loss like the classification-based method to avoid the extra hyperparameters in the metric-based method. In addition, to more directly demonstrate the effect of feature interactions, we use an entire siamese encoder-decoder, like the metricbased method. MetaChanger can be indicated as:</p><formula xml:id="formula_0">Y = H(D(E InterAct (X 1 )), D(E InterAct (X 2 ))) (1)</formula><p>where X 1 and X 2 indicate the bi-temporal images, Y indicate the ultimate predicted change map; E InterAct (?) and D(?) denote the interactive encoder and decoder network; H(?) denotes the fusion and projection head. To ensure the generality of MetaChanger, the encoder can be any hierarchical ConvNet or Transformer. Let F i,j denote the output of a hierarchy where i indexes the hierarchy along the encoder and j indexes the temporal dimension (i ? {0, 1, 2, 3} typically, and j ? {0, 1} for change detection). A hierarchy of MetaChanger consists of two main steps. First, multilevel features F i,j go through a network stage with sharing weights and features F i+1,j are generated. Then, F i+1,j , ?j feed into the interaction layer to get the correlated F i+1,j . The hierarchy i can be formulated as:</p><formula xml:id="formula_1">F i+1,j = Stage i (F i,j ), ?j F i+1,j = InterAct(F i+1,0 , F i+1,1 ), ?j<label>(2)</label></formula><p>where InterAct(?) refers to the interaction layer. We denote F i,j as F i+1,0 and F i+1,1 for clearer description in Eq.</p><p>(2). And to ensure MetaChanger is efficient in practice, we use a light-weight MLP decoder, like SegFormer <ref type="bibr" target="#b42">[42]</ref>.</p><formula xml:id="formula_2">F i,j = U psample(Linear i (C i , C)(F i,j )), ?i, ? F j = Linear(4C, C)(Concat(F i,j )), ?i<label>(3)</label></formula><p>where Linear(C in , C out )(?) refers to a linear layer with C in and C out as input and output dimensions respectively, and U psample(?) refers to upsampling features to 1/4th.</p><p>Here we obtain the feature mapsF 0 andF 1 from each bi-temporal image. And thenF 0 andF 1 are aggregated and projected to the ultimate change map M ? R 2?H?W , which can be formulated as:</p><formula xml:id="formula_3">F = F use(F 0 ,F 1 ) Y = P roject(F )<label>(4)</label></formula><p>where F use(?) and P roject(?) refer to the fusion layer and projection layer, respectively. In particular, the projection layer consists of two convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ChangerVanilla</head><p>For better comparisons, we first build a baseline model, ChangerVanilla. ChangerVanilla has no interaction layer and uses a simple concat operation as the fusion layer. Let x denotes the feature map, we formulate the interaction and fusion layer of ChangerVanilla as:</p><formula xml:id="formula_4">InterAct vanilla (x i ) = Identity(x i ) F use vanilla (x 0 , x 1 ) = Concat([x 0 , x 1 ])<label>(5)</label></formula><p>Then, for the specific exploration of feature interaction layers, there are many complex strategies can be adapted.</p><p>Here, however, we want to demonstrate that using only simple modules, even parameter-free interaction operations, can effectively improve the performance of change detection models, which has rarely been discussed in previous related studies. Specifically, we throw in two embarrassingly simple interaction methods: AD and feature "exchange".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ChangerAD</head><p>We abstract the aggregation-distribution style feature interaction from co-attention and some similar mechanisms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">28]</ref>, as shown in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>. We refer to this variant as ChangerAD. The basic idea of AD is to project the bitemporal features into a feature space and get the global cofeature, then, use the distributed attention maps adaptively re-weight each channel of the bi-temporal features.</p><p>Specifically, we first aggregate features from siamese branches via an element-wise summation. Then We use the global average pooling to generate global information. Furthermore, we take a MLP to extract the co-feature, and employ sigmoid to obtain the ultimate two attention maps. The interaction layer of ChangerAD is formulated as:</p><formula xml:id="formula_5">InterAct AD (x i ) = x i ? ?(x i ) x = M LP (C i , 2C i )(GAP (x 0 + x 1 ))<label>(6)</label></formula><p>where x 0 and x 1 refer to the bi-temporal features, and the M LP (C in , C out )(?) refers to a 2-layer MLP, with the first layer squeezing and the second layer expanding channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">ChangerEx</head><p>Feature "exchange" refers to: partial exchange between bi-temporal features, during feature extraction. Hence, the natural question is why exchange and why it is feasible.</p><p>Why exchange? On the one hand, contextual information of bi-temporal features can be perceived by mutual learning through feature exchange and the subsequent mix layers (e.g. convolution or token mixer). On the other hand, through feature exchange and the subsequent layers, the distribution between the features of the two branches is more similar and automatic domain adaptation between the bitemporal domains is achieved to some extent.</p><p>Why is the exchange feasible? While some studies have emphasised temporal information, we do not believe that bi-temporal change detection is strongly constrained by "time". The core of bi-temporal change learning is to train a change detector for images with the same spatial position but at different times. The temporal order is only to ensure that the appearance and disappearance of the target is interpretable, and in most definitions and applications in change detection, we are not concerned with whether it appears or disappears, but simply changes. In brief, the semantic correspondence constraint for bi-temporal images in the definition of change detection makes feature exchange feasible.</p><p>Then, an important issue is where to exchange. Because of the logical operations involved in determining whether or not to exchange certain features, we need to be cautious to avoid non-differentiable problem. For this, we considered two solutions. One is to convert the soft exchange mask to attention maps to guarantee the continuity of the gradient chain, and the other is to use an unlearnable way to exchange, i.e. predefine a hard exchange mask. In practice, we find that simple learnable exchange does not per-  form better than the parameter-free unlearnable exchange, and therefore we tend to use the latter. Let x 0 , x 1 denote the bi-temporal features. InterAct Ex for x 0/1 can be formulated as: Furthermore, we have tried feature exchange in two dimensions separately, the channel dimension and the spatial dimension, with the details given in what follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Channel Exchange</head><p>Channel exchange refers to exchanging features in channel dimension. Specifically, for Eq. <ref type="formula">(7)</ref>, the predefined mask remains spatially consistent. After channel exchange, gradients are detached from the exchanged channel and backpropagated through the other ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Spatial Exchange</head><p>Relatively, spatial exchange refers to exchanging features in spatial dimension, and the mask remains consistent on the channel. Considering that subsequent layers may be fully channel-wise MLP, a mix layer is optional. The pseudo-code of channel exchange and spatial exchange are in Algorithm 1 and Appendix Algorithm B1. Finally, we combine these two feature exchange and propose ChangerEx, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. In addition, feature exchange involves issues like how many features to exchange, at which stage to exchange, etc. which we will discuss later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Flow Dual-Alignment Fusion</head><p>Registration error is one of the most common challenges in change detection. Image registration is an essential part for change detection pre-processing. However, there are always more or less mis-alignment or side-looking problems, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Some work has tried implicit alignment, such as the use of global attention in <ref type="bibr" target="#b4">[5]</ref>. In this paper we use an explicit bi-directional alignment with optical flow. The FDAF introduces a task prior for change detection, i.e. discovering differences between images. Specifically, the bi-temporal feature maps are resampled through a deformable field to obtain their respective corrected features. Then we take the distance to the other original feature map separately and feed it into the subsequent forward propagation. In other words, the FDAF implements an explicit task transformation function where the object extraction task is converted into the change detection task. Mathematically, the features x 0 , x 1 feed into F use align can be written as: </p><formula xml:id="formula_6">x = Concat([x 0 (p + ?p 0 ) ? x 1 , x 1 (p + ?p 1 ) ? x 0 ])<label>(8)</label></formula><p>The bi-linear interpolation <ref type="bibr" target="#b17">[18]</ref> is used to compute the exact value of the features. We refer to MetaChanger with FDAF as ChangerAlign, as shown in <ref type="figure" target="#fig_1">Fig. 2(e</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>S2Looking. S2Looking dataset <ref type="bibr" target="#b31">[31]</ref> contains 5000 image pairs (1024 ? 1024, 7:1:2 for train, eval and test) and more than 65,920 annotated instances of changes extracted from side-looking rural area satellite images, which were collected by optical satellites around the world. The images span 1-3 years with a resolution of 0.5-0.8 m/pixel. LEVIR-CD. LEVIR-CD dataset <ref type="bibr" target="#b4">[5]</ref> contains 637 bitemporal RS image pairs and more than 31,333 annotated instances of changes, which were collected from Google Earth. Each image in the pairs is 1024 ? 1024 with an image resolution of 0.5 m/pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation detail</head><p>We develop a change detection toolbox, Open-CD, based on PyTorch and open-mmlab related tools <ref type="bibr" target="#b8">[9]</ref>. During training, we use the CE Loss and AdamW optimizer. The weight decay is set to 0.05 always. The poly schedule with an initial learning rate of 0.001 is adopted. We use single Tesla V100 GPU for training and the batch size is set to 8. We train all Changer models for 80k and 40k iterations for S2Looking and LEVIR-CD dataset. For data augmentation, we use random crop, flip and photometric distortion. And we randomly exchange the order of the bi-temporal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main results</head><p>In <ref type="table">Tab</ref>  <ref type="table">Table 4</ref>. Ablation study on applying spatial exchange on different stages on S2Looking. means that 1/2 spatial embedding is exchanged at this stage. are described in Appendix A. Our baseline ChangerVanilla achieves competitive performance than previous change detection methods, which demonstrates MetaChanger's effectiveness in overall architecture. The FDAF brings boost in F1-score, and its effect can also be observed in <ref type="figure" target="#fig_3">Fig. 4</ref>. ChangerAD and ChangerEx are built on top of Changer-Align and make feature interactions during feature extraction. Compared to ChangerAlign, ChangerAD and Chang-erEx achieved significant improvements with only slight or no increases in parameters and computational cost. Furthermore, the Changer models achieve more promising gains on the more challenging dataset S2Looking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>To delve into MetaChanger and its variants, especially ChangerEx, we conducted comprehensive experiments for the following questions. If not specified, ResN et18 V 1c (without pretrained) is used as the backbone network.</p><p>Which stage to exchange. We insert interaction lay-ers including AD and channel/spacial exchange at different stages. As shown in Tab. 2, Inserting AD layer in the last three stages achieves the best performance, while inserting the interaction layer in the first stage hurts the performance. A similar situation occurs in channel exchange, as shown in Tab. 3: the best results are obtained with the interaction layer in the latter two stages only. In spatial exchange, however, the situation is different, with the best two settings interacting in the earlier stages, as listed in Tab. 4. An intuition is that in the shallow layers of the network, there is a high spatial resolution and a lower channel dimension, which is more suitable for spatial interactions; and vice versa. Based on this observation, we use spatial exchange in the shallow layers and channel exchange in the deeper layers for ChangerEx, as shown in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>.</p><p>What ratio of features should be exchanged. As shown in Tab. 5, we try various exchange ratios, from 1/32 to 1/2. We find that the difference in performance of Chan-gerEx at different exchange ratios is relatively slight. Thus, in feature exchange, the ratio of features exchanged is not the determining factor; the emphasis is on the presence or absence of exchange. This ablation also suggests that feature exchange is insensitive, even when some additional hyper-parameters are introduced.</p><p>How to choose the size of the exchange window in spatial exchange. A worthwhile consideration is whether small exchange patch in spatial exchange would disrupt the original spatial structure. We therefore tried using different window sizes in spatial exchange, from 1 ? 1 to 8 ? 8, as listed in the Tab. 6. We find that the window size is a robust hyper-parameter and that a more complete original spatial structure does not result in a performance gain.</p><p>Learnable exchange. We try the learnable exchange method in the channel and spatial dimensions respectively. In our implementation, we generate an exchange map based on the distance between the soft attention maps of the two branches, with the smaller half being exchanged and the larger half retained. And the two attention maps also need to be exchanged if their corresponding features are exchanged. As Tab. 7 lists, the learnable channel exchange shows a slight improvement over the unlearnable one, but the performance of the learnable spatial exchange drops dramatically.</p><p>We have only tested simple learnable exchanges here, and this part deserves further exploration in future work.</p><p>MetaChanger with more complex backbones. To further illustrate the generalizability of ChangerEx to different network architectures, we replaced the backbone with the more complex networks, ResNeSt50 and ResNeSt101. As listed in Tab. 8, the networks with feature exchange outperform all the baselines significantly, demonstrating that the ChangerEx can generalize well on various models, especially in more challenging dataset.</p><p>We find that ChangerEx can deliver higher gains to com-   <ref type="table">Table 7</ref>. Ablation study on whether use learnable exchange for spatial/channel exchange on S2Looking.</p><p>plex models in large-scale datasets, which is promising. Specifically. ChangerEx with ResNeSt101 leads to a performance gain of 1.68 (achieving a F1-Score of 67.61).</p><p>Why exchange work? To further explore why Chang-erEx is effective, we visualize the Changer models with and without "exchange" separately, using grad-CAM <ref type="bibr" target="#b30">[30]</ref>. As shown in <ref type="figure">Fig. 5</ref>, most of the buildings in the upper half of the image disappear from t 1 to t 2 . In the t 1 -heat map with no exchange, only very few building areas are activated. In the t 1 -heat map with exchange, the situation improves but is similar overall. However, an interesting phenomenon is that the areas with buildings in t 1 are activated in the t 2 -heat map with feature exchange. In other words, the perceptual targets that are lost in t 1 are reactivated in t 2 .</p><p>Another possible explanation is that feature exchange increases the diversity of the samples, achieving a kind of intra-network data augmentation. The order of appearance and disappearance of targets of interest in the bi-temporal features is shuffled, but with still keeping the strict semantic maintenance and semantic correspondence.  <ref type="figure">Figure 5</ref>. Grad-CAM visualization results (for all of the pixels from "change" category). We compare the visualization results of ChangerAlign and ChangerEx (both with ResNeSt101). The grad-CAM visualization is calculated for the last stage outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we propose MetaChanger to explore the effect of feature interactions in change detection. To verify the effectiveness of feature interaction, we deliberately specify interaction layer as extremely simple aggregationdistribution and feature "exchange" for MetaChanger. It is found that the derived ChangerAD and ChangerEx can achieve competitive performance on multiple change detection datasets. Extensive ablation studies demonstrate the robustness and extensibility of ChangerEx.</p><p>In the future, we will further evaluate MetaChanger under more different learning settings and related tasks, such as semantic change detection. We hope this work can inspire more future research devoted to improving the MetaChanger especially the interaction methods. A. Compared methods FC-EF, FC-Siam-Conc and FC-Siam-Diff <ref type="bibr" target="#b11">[12]</ref> are three classification-based UNet-like models. FC-EF uses early fusion to directly concatenate bi-temporal images, FC-Siam-Conc and FC-Siam-Diff use siamese encoders and use concatenation and difference to fuse features respectively.</p><p>DTCDSCN [25] is a multi-task model, which can accomplish both change detection and semantic segmentation at the same time. It also introduces a dual-attention module to exploit the interdependencies between channels and spatial positions, improving the feature representation.</p><p>STANet <ref type="bibr" target="#b4">[5]</ref> is a siamese network with spatial-temporal attention designed to explore spatial-temporal relationships for change detection. It includes a base model (STANet-Base) that uses a weight-sharing CNN feature extractor, and optimise models through metric method. STANet-BAM and STANet-PAM equip the basic spatial-temporal attention module (like self-attention) and the pyramid spatialtemporal attention module on top of STANet-Base CDNet [3] is a well-tuned siamese CNN model. CDNet is used with an instance-level data augmentation, which can generate bi-temporal images that contain changes involving plenty and diverse synthesized building instances by leveraging generative adversarial training.</p><p>BiT <ref type="bibr" target="#b3">[4]</ref> is a hybrid model of CNN and transformer. It uses the convolutional blocks at the shallow layers and the transformer blocks (with cross-attention) at the deeper layers, which can effectively model contexts within the spatialtemporal domain.</p><p>ChangeFormer <ref type="bibr" target="#b3">[4]</ref> is a transformer-based siamese network. ChangeFormer combines a hierarchical transformer encoder with a MLP decoder in a siamese network to effectively render long-range details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pseudo code</head><p>The pseudo-code for spatial exchange is shown in Algorithm B1. The exchange mask is obtained in the W dimension and broadcast to the C ? H ? W , which allows more stable testing at different scales than obtaining it in the HW dimension.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>MetaChanger for change detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Component details of Changer models. ?(?) refers to Sigmoid function and GAP refers to global average pooling. The F lowN et is consisted of DWConv+IN+GELU+PWConv. (a)?(e) correspond to Section 3.1?3.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>ChangerEx model. (a) ChangerEx adopts spatial exchange in the second stage and channel exchange in the last two stage. FDAF is used to fuse features. (b) The diagram of spatial exchange. (c) The diagram of channel exchange.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Some visualization comparisons among Changer models on the S2Looking. The rendered colors represent true positives (TP), false positives (FP), and false negatives (FN). Buildings with misregistration are framed out in the first row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>x 0/ 1</head><label>1</label><figDesc>(n, c, h, w) = x 0/1 (n, c, h, w), M (n, c, h, w) = 0 x 1/0 (n, c, h, w), M (n, c, h, w) = 1 (7) where n, c and hw index the batch, channel and space respectively. M refers to the exchange mask consisting of 1 and 0, indicating exchange and non-exchange.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where p enumerates the locations in x 0 or x 1 , and the offset field ?p is obtained by the F lowN et (2-layer Conv): ?p = F lowN et(Concat([x 0 , x 1 ])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Channel Exchange for ChangerEx, PyTorch-like Code</figDesc><table><row><cell>import torch</cell></row><row><cell>import torch.nn as nn</cell></row><row><cell>class ChannelExchange(nn.Module):</cell></row><row><cell>def __init__(self, p=2):</cell></row><row><cell>super().__init__()</cell></row><row><cell>self.p = p</cell></row><row><cell>def forward(self, x1, x2):</cell></row><row><cell>N, C, H, W = x1.shape</cell></row><row><cell>exchange_mask = torch.arange(C) % self.p == 0</cell></row><row><cell>exchange_mask = exchange_mask.unsqueeze(0).expand((N, -1))</cell></row><row><cell>out_x1, out_x2 = torch.zeros_like(x1), torch.zeros_like(x2)</cell></row><row><cell>out_x1[?exchange_mask, ...] = x1[?exchange_mask, ...]</cell></row><row><cell>out_x2[?exchange_mask, ...] = x2[?exchange_mask, ...]</cell></row><row><cell>out_x1[exchange_mask, ...] = x2[exchange_mask, ...]</cell></row><row><cell>out_x2[exchange_mask, ...] = x1[exchange_mask, ...]</cell></row><row><cell>return out_x1, out_x2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>. 1, We show the results of some SOTA and typical change detection methods, including ConvNets and Vision Transformers. The details of the compared methods Ablation study on applying AD on different stages on S2Looking. means that AD is used at this stage.</figDesc><table><row><cell>Stages w/ Aggregation-Distribution Stage1 Stage2 Stage3 Stage4</cell><cell>F1</cell></row><row><cell></cell><cell>65.63</cell></row><row><cell></cell><cell>65.69</cell></row><row><cell></cell><cell>65.72</cell></row><row><cell></cell><cell>65.17</cell></row><row><cell>Stages w/ Channel Exchange Stage1 Stage2 Stage3 Stage4</cell><cell>F1</cell></row><row><cell></cell><cell>65.71</cell></row><row><cell></cell><cell>65.91</cell></row><row><cell></cell><cell>65.53</cell></row><row><cell></cell><cell>65.22</cell></row><row><cell cols="2">Table 3. Ablation study on applying channel exchange on different</cell></row><row><cell cols="2">stages on S2Looking. means that 1/2 channel is exchanged at</cell></row><row><cell>this stage.</cell><cell></cell></row><row><cell>Stages w/ Spatial Exchange Stage1 Stage2 Stage3 Stage4</cell><cell>F1</cell></row><row><cell></cell><cell>65.38</cell></row><row><cell></cell><cell>65.60</cell></row><row><cell></cell><cell>66.11</cell></row><row><cell></cell><cell>65.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Ablation study on different window size options for spatial exchange on S2Looking.</figDesc><table><row><cell cols="3">Exchange Learnable Precision Recall</cell><cell>F1</cell></row><row><cell>Channel</cell><cell>73.77 74.19</cell><cell cols="2">59.61 65.94 59.29 65.91</cell></row><row><cell>Spatial</cell><cell>73.41 74.51</cell><cell cols="2">56.53 63.88 59.41 66.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Algorithm B1 Spatial Exchange for ChangerEx, PyTorch-like Code = torch.zeros_like(x1), torch.zeros_like(x2) out_x1[...,?exchange_mask] = x1[...,?exchange_mask] out_x2[...,?exchange_mask] = x2[...,?exchange_mask] out_x1[..., exchange_mask] = x2[..., exchange_mask] out_x2[..., exchange_mask] = x1[..., exchange_mask]</figDesc><table><row><cell>import torch</cell></row><row><cell>import torch.nn as nn</cell></row><row><cell>class SpatialExchange(nn.Module):</cell></row><row><cell>def __init__(self, p=2):</cell></row><row><cell>super().__init__()</cell></row><row><cell>self.p = p</cell></row><row><cell>def forward(self, x1, x2):</cell></row><row><cell>N, C, H, W = x1.shape</cell></row><row><cell>exchange_mask = torch.arange(w) % p == 0</cell></row><row><cell>out_x1, out_x2 return out_x1, out_x2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hypertransformer: A textural and spectral feature fusion transformer for pansharpening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaminda</forename><surname>Wele Gedara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="1767" to="1777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A transformer-based siamese network for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaminda</forename><surname>Wele Gedara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial instance augmentation for building change detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficient transformer based method for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zipeng</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A spatial-temporal attentionbased method and a new dataset for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fccdn: Feature constraint network for vhr image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baipeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mixformer: Mixing features across windows and dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiman</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="5249" to="5259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bi-directional cross-modality feature propagation with separation-andaggregation gate for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">MMCV: OpenMMLab computer vision foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mmcv Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmcv" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attentional feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Gieseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Oehmcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiquan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kobus</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3560" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional siamese networks for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertr</forename><forename type="middle">Le</forename><surname>Rodrigo Caye Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Snunet-cd: A densely connected siamese network for change detection of vhr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Encoder fusion network with co-attention embedding for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15506" to="15515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fapn: Feature-aligned pyramid network for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="864" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Alignseg: Featurealigned segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="550" to="557" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic flow for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houlong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="775" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gated fully fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houlong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11418" to="11425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deep convolutional coupling network for change detection based on heterogeneous optical and radar images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoguo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puzhao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="545" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building change detection for remote sensing images using a dual-task constrained deep siamese convolutional network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqian</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Manas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gir?-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Rodriguez</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9414" to="9423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Duy-Kien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">S2looking: A satellite side-looking dataset for building change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabao</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouye</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bitao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A deeply supervised attention metric-based network and an open aerial image dataset for remote sensing change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on geoscience and remote sensing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tdan: Temporally-deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3360" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamicearthnet: Daily multi-spectral satellite dataset for semantic change segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysim</forename><surname>Toker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Kondmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Eisenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Camero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Pregel Hoderlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="21158" to="21167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Qfabric: multi-task change detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Panigrahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1052" to="1061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modality and component aware feature fusion for rgb-d scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5995" to="6004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion by channel exchanging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-modality cross attention network for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10941" to="10950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature interactive representation for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingli</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lite-hrnet: A lightweight high-resolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10440" to="10450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deformable siamese attention networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuechen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6728" to="6737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Change detection based on deep siamese convolutional network for optical aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1845" to="1849" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Lightweight and progressively-scalable networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.13600,2022.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Change is everywhere: Single-temporal supervised object change detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15193" to="15202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Changemask: Deep multi-task encodertransformer-decoder architecture for semantic change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Canet: Co-attention network for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">108468</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
