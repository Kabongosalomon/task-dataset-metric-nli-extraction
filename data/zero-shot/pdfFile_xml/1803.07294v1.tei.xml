<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
							<email>jnzhang@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
							<email>junyuanx@amazon.com</email>
							<affiliation key="aff2">
								<orgName type="department">Amazon Web Services</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
							<email>haoma@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
							<email>king@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
							<email>dyyeung@cse.ust.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new network architecture, Gated Attention Networks (GaAN), for learning on graphs. Unlike the traditional multi-head attention mechanism, which equally consumes all attention heads, GaAN uses a convolutional sub-network to control each attention head's importance. We demonstrate the effectiveness of GaAN on the inductive node classification problem. Moreover, with GaAN as a building block, we construct the Graph Gated Recurrent Unit (GGRU) to address the traffic speed forecasting problem. Extensive experiments on three real-world datasets show that our GaAN framework achieves state-of-the-art results on both tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many crucial machine learning tasks involve graph structured datasets, such as classifying posts in a social network <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref>, predicting interfaces between proteins <ref type="bibr">(Fout et al., 2017)</ref> and forecasting the future traffic speed in a road network <ref type="bibr" target="#b17">(Li et al., 2018)</ref>. The main difficulty in solving these tasks is how to find the right way to express and exploit the graph's underlying structural information. Traditionally, this is achieved by calculating various graph statistics like degree and centrality, using graph kernels, or extracting human engineered features <ref type="bibr" target="#b10">(Hamilton et al., 2017b)</ref>.</p><p>Recent research, however, has pivoted to solving these problems by graph convolution <ref type="bibr" target="#b7">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b0">Atwood and Towsley, 2016;</ref><ref type="bibr" target="#b14">Kipf and Welling, 2017;</ref><ref type="bibr">Fout et al., 2017;</ref><ref type="bibr" target="#b9">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b28">Veli?kovi? et al., 2018;</ref><ref type="bibr" target="#b17">Li et al., 2018)</ref>, which generalizes the stan- * These two authors contributed equally. dard definition of convolution over a regular grid topology <ref type="bibr" target="#b8">(Gehring et al., 2017;</ref><ref type="bibr" target="#b15">Krizhevsky et al., 2012)</ref> to 'convolution' over graph structures. The basic idea behind 'graph convolution' is to develop a localized parameter-sharing operator on a set of neighboring nodes to aggregate a local set of lower-level features. We refer to such an operator as a graph aggregator <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref> and the set of local nodes as the receptive field of the aggregator. Then, by stacking multiple graph aggregators, we build a deep neural network <ref type="bibr">(Le-Cun et al., 2015)</ref> model which can be trained end-to-end to extract the local and global features across the graph. Note that we use the spatial definition instead of the spectral definition <ref type="bibr" target="#b11">(Hammond et al., 2011;</ref><ref type="bibr" target="#b2">Bruna et al., 2014)</ref> of graph convolution because the full spectral treatment requires eigendecomposition of the Laplacian matrix, which is computationally intractable on large graphs, while the localized versions <ref type="bibr" target="#b14">Kipf and Welling, 2017)</ref> can be interpreted as graph aggregators <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref>.</p><p>Graph aggregators are the basic building blocks of graph convolutional neural networks. A model's ability to capture the structural information of graphs is largely determined by the design of its aggregators. Most existing graph aggregators are based on either pooling over neighborhoods <ref type="bibr" target="#b14">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b9">Hamilton et al., 2017a)</ref> or computing a weighted sum of the neighboring features <ref type="bibr" target="#b20">(Monti et al., 2017)</ref>. In essence, functions that are permutation invariant and can be dynamically resizing are eligible graph aggregators. One class of such functions is the neural attention network <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>, which uses a subnetwork to compute the correlation weight of the elements in a set. Among the family of attention models, the multi-head attention model has been shown to be effective for machine translation tasks <ref type="bibr" target="#b19">(Lin et al., 2017;</ref><ref type="bibr" target="#b27">Vaswani et al., 2017)</ref>. It has later been adopted as a graph aggregator to solve the node classification problem <ref type="bibr" target="#b28">(Veli?kovi? et al., 2018)</ref>. A single attention head sums the elements that are similar to the query vector in one representation subspace. Using multiple attention heads allows exploring features in different representation subspaces, which can provide more modeling power in nature. However, treating each attention head equally loses the opportunity to benefit from some attention heads which are inherently more important than others.</p><p>To this end, we propose the Gated Attention Networks (GaAN) for learning on graphs. GaAN uses a small convolutional subnetwork to compute a soft gate at each attention head to control its importance. Unlike the traditional multi-head attention that admits all attended contents, the gated attention can modulate the amount of attended content via the introduced gates. Moreover, since only a simple and light-weighted subnetwork is introduced in constructing the gates, the computational overhead is negligible and the model is easy to train. We demonstrate the effectiveness of our new aggregator by applying it to the inductive node classification problem. We also improve the sampling strategy introduced in <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref> to reduce the memory cost and increase the run-time efficiency, in order to train our model and other graph aggregators on relatively large graphs. Furthermore, since our proposed aggregator is very general, we extend it to construct a Graph Gated Recurrent Unit (GGRU), which is directly applicable for spatiotemporal forecasting problem. Extensive experiments on two node classification datasets, PPI and Reddit <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref>, and one traffic speed forecasting dataset, METR-LA <ref type="bibr" target="#b17">(Li et al., 2018)</ref>, show that GaAN consistently outperforms the baseline models and achieves the state-of-the-art performance.</p><p>In summary, our main contributions include: (a) a new multi-head attention-based aggregator with additional gates on the attention heads; (b) a unified framework for transforming graph aggregators to graph recurrent neural networks; and (c) the state-of-the-art prediction performance on three real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NOTATIONS</head><p>We denote vectors with bold lowercase letters, matrices with bold uppercase letters and sets with calligraphy letters. We denote a single fully-connected layer with a non-linear activation ?(?) as FC ? ? (x) = ?(Wx + b), where ? = {W, b} are the parameters. Also, ? with different subscripts mean different transformation parameters. For activation functions, we denote h(?) to be the LeakyReLU activation <ref type="bibr" target="#b29">(Xu et al., 2015a)</ref> with negative slope equals to 0.1 and ?(?) to be the sigmoid activation. FC ? (x) means applying no activation function after the linear transform. We denote ? as the concatenation operation and K k=1 x k as sequentially concatenating x 1 through x K . We denote the Hadamard product as '?' and the dot product between two vectors as ?, ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>In this section, we will review relevant research on learning on graphs. Our model is also related to many graph aggregators proposed by previous work. We will discuss these aggregators in Section 4.3.</p><p>Neural attention mechanism Neural attention mechanism is widely adopted in deep learning literature and many variants have been proposed <ref type="bibr" target="#b4">(Chorowski et al., 2014;</ref><ref type="bibr" target="#b30">Xu et al., 2015b;</ref><ref type="bibr" target="#b22">Seo et al., 2017;</ref><ref type="bibr" target="#b27">Vaswani et al., 2017)</ref>. Among them, our model takes inspiration from the multi-head attention architecture proposed in <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>. Given a query vector q and a set of key-value pairs {(k 1 , v 1 ), ..., (k n , v n )}, a single attention head computes a weighted combination of the value vectors n i=1 w i v i . The weights are generated by applying softmax to the inner product between the query and keys, i.e., w = softmax({q T k 1 , ..., q T k n }). In the multi-head case, the outputs of K different heads are concatenated to form an output vector with fixed dimensionality. The difference between the proposed model, GaAN, and the multi-head attention mechanism is that we compute additional gates to control the importance of each head's output.</p><p>Graph convolutional networks on large graph Applying graph convolution on large graphs is challenging because the memory complexity is proportional to the total number of nodes, which could be hundreds of thousands of nodes in large graphs <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computational cost, <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref> proposed the GraphSAGE framework that uses a sampling algorithm to select a small subset of the nodes and edges. On each iteration, GraphSAGE first uniformly samples a mini-batch of nodes. Then, for each node, only a fixed number of neighborhoods are selected for aggregation. More recently, Chen et al. <ref type="bibr" target="#b3">(Chen et al., 2018)</ref> proposed a new sampling method that randomly samples two sets of nodes according to a proposed distribution. However, this method is only applicable to one aggregator, i.e., the Graph Convolutional Network (GCN) <ref type="bibr" target="#b14">(Kipf and Welling, 2017)</ref>.</p><p>Graph convolution networks for spatiotemporal forecasting Recently, researchers have applied graph convolution, which is commonly used for learning on static graphs, to spatiotemporal forecasting. <ref type="bibr" target="#b23">(Seo et al., 2016)</ref> proposed Graph Convolutional Recurrent Neural Network (GCRNN), which replaced the fully-connected layers in LSTM <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997)</ref> with the ChebNet operator , and applied it to a synthetic video prediction task. <ref type="bibr" target="#b17">Li et al. (Li et al., 2018)</ref> proposed Diffusion Convolutional Recurrent Neural Network (DCRNN) to address the traffic forecasting problem, where the goal is to predict future traffic speeds in a sensor network given historic traffic speeds and the underlying road graph. DCRNN replaces the fully-connected layers in GRU <ref type="bibr" target="#b5">(Chung et al., 2014)</ref> with the diffusion convolution operator <ref type="bibr" target="#b0">(Atwood and Towsley, 2016)</ref>. Furthermore, DCRNN takes the direction of graph edges into account. The difference between our GGRU with GCRNN and DCRNN is that we have proposed a unified method for constructing a recurrent neural network based on an arbitrary graph aggregator rather than proposing a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GATED ATTENTION NETWORKS</head><p>In this section, we first give a generic formulation of graph aggregators followed by the multi-head attention mechanism. Then, we introduce the proposed gated attention aggregator. Finally, we review the other kinds of graph aggregators proposed by previous work and explain their relationships with ours.</p><p>Generic formulation of graph aggregators Given a node i and its neighboring nodes N i , a graph aggregator is a function ? in the form of y i = ? ? (x i , {z Ni }), where x i and y i are the input and output vectors of the center node i. z Ni = {z j |j ? N i } is the set of the reference vectors in the neighboring nodes and ? is the learnable parameters of the aggregator. In this paper, we do not consider aggregators that use edge features. However, it is straightforward to incorporate edges in our definition by defining z j to contain the edge feature vectors e i,j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MULTI-HEAD ATTENTION AGGREGATOR</head><p>We linearly project the center node feature x i to get the query vector and project the neighboring node features to get the key and value vectors. We then apply the multihead attention mechanism <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> to get the final aggregation function. The detailed formulation of the multi-head attention aggregator is as follows:</p><formula xml:id="formula_0">y i = FC ?o (x i ? K k=1 j?Ni w (k) i,j FC h ? (k) v (z j )), w (k) i,j = exp(? (k) w (x i , z j )) |Ni| l=1 exp(? (k) w (x i , z l )) , ? (k) w (x, z) = FC ? (k) xa (x), FC ? (k) za (z) .<label>(1)</label></formula><p>Here, K is the number of attention heads. w</p><p>i,j is the kth attentional weights between the center node i and the neighboring node j, which is generated by applying a softmax to the dot product values. ?</p><formula xml:id="formula_2">(k) xa , ? (k) za and ? (k) v</formula><p>are the parameters of the kth head for computing the query, key and value vectors, which have dimensions of d a , d a and d v respectively. The K attention outputs are concatenated with the input vector and pass to an output fully-connected layer parameterized by ? o to get the final output y i , which has dimension d o . The difference between our aggregator and that in GAT <ref type="bibr" target="#b28">(Veli?kovi? et al., 2018</ref>) is that we have adopted the key-value attention mechanism and the dot product attention while GAT does not compute additional value vectors and uses a fully-connected layer to compute ? (k) w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GATED ATTENTION AGGREGATOR</head><p>While the multi-head attention aggregator has the ability to explore multiple representation subspaces between the center node and its neighborhoods, not all of these subspaces are equally important; some subspaces may not even exist for certain nodes. Feeding the output of an attention head that captures a useless representation can mislead the model's final prediction.</p><p>Therefore, we compute an additional soft gate between 0 (low importance) and 1 (high importance) to assign different importance to each head. In combination with the multi-head attention aggregator, we get the formulation of the gated attention aggregator:</p><formula xml:id="formula_3">y i = FC ?o (x i ? K k=1 (g (k) i j?Ni w (k) i,j FC h ? (k) v (z j ))), g i = [g<label>(1)</label></formula><p>i , ..., g</p><formula xml:id="formula_4">(K) i ] = ? g (x i , z Ni ),<label>(2)</label></formula><p>where g Attention-based Aggregator node i. To make sure adding gates will not introduce too many additional parameters, we use a convolutional network ? g that takes the center node and neighboring node features to generate the gate values. All the other parameters have the same meanings as in Eqn.</p><p>(1).</p><p>There are multiple possible designs of the ? g network. In this paper, we combine average pooling and max pooling to construct the network. The detailed formula is given below:</p><formula xml:id="formula_6">g i = FC ? ?g (x i ? max j?Ni ({FC ?m (z j )}) ? j?Ni z j |N i | ).</formula><p>(3) Here, ? m maps the neighbor features to a d m dimensional vector before taking the element-wise max and ? g maps the concatenated features to the final K gates. By setting a small d m , the subnetwork for computing the gate will have negligible computational overhead. A visual illustration of GaAN aggregator's structure can be found in <ref type="figure" target="#fig_0">Figure 1</ref>. Also, we compare the general structures of the multi-head attention aggregator and the gated attention aggregator in <ref type="figure" target="#fig_1">Figure 2a</ref> and <ref type="figure" target="#fig_1">Figure 2b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">OTHER GRAPH AGGREGATORS</head><p>Most previous graph aggregators except attention-based aggregators can be summarized into two general categories: graph pooling aggregators and graph pairwise sum aggregators. In this section, we first describe these two types of aggregators and then explain their relationship with the attention-based aggregator. Finally, we give a list of the baseline aggregators other than the multihead attention aggregator used in the paper.</p><p>Graph pooling aggregators The main characteristic of graph pooling aggregators is that they do not consider the correlation between neighboring nodes and the center node. Instead, neighboring nodes' features are directly aggregated and the center node's feature is simply concatenated or added to the aggregated vector and then passed through an output function ? o :</p><formula xml:id="formula_7">y i = ? o (x i ? pool j?Ni (? v (z j ))).<label>(4)</label></formula><p>Here, the projection function ? v and the output function ? o can be a single fully-connected layer and the pool(?) operator can be average pooling, max pooling or sum pooling..</p><p>The majority of existing graph aggregators are special cases of the graph pooling aggregators. Some models only integrate the node features of neighborhoods <ref type="bibr" target="#b7">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b14">Kipf and Welling, 2017;</ref><ref type="bibr" target="#b9">Hamilton et al., 2017a)</ref>, while others integrated edge features as well <ref type="bibr" target="#b0">(Atwood and Towsley, 2016;</ref><ref type="bibr">Fout et al., 2017;</ref><ref type="bibr" target="#b21">Sch?tt et al., 2017)</ref>. In <ref type="figure" target="#fig_1">Figure 2c</ref>, we illustrate the architecture of the graph pooling aggregators.</p><p>Graph pairwise sum aggregators Like attentionbased aggregators, graph pairwise sum aggregators also aggregate the neighborhood features by taking K weighted sums. The difference is that the weight between node i and its neighbor j is not related to the other neighbors in N i . The formula of graph pairwise sum aggregator is given as follows:</p><formula xml:id="formula_8">y i = ? o (x i ? K k=1 j?Ni w (k) i,j ? (k) v (z j )), w (k) i,j = ? (k) w (x i , z j ).<label>(5)</label></formula><p>Here, w</p><p>i,j is only related to the pair x i and z j while in attention-based models w (k) i,j is related to features of all neighbors z Ni . Models like the adaptive forget gate strategy in Graph LSTM <ref type="bibr" target="#b18">(Liang et al., 2016)</ref> and MoNet <ref type="bibr" target="#b20">(Monti et al., 2017)</ref> employed pairwise sum aggregators with a single head or multiple heads. In <ref type="figure" target="#fig_1">Figure 2d</ref>, we illustrate the architecture of the graph pairwise sum aggregators.</p><p>Baseline aggregators To fairly evaluate the effectiveness of GaAN against previous work, we choose two representative aggregators in each category as baselines:</p><p>? Avg. pooling: y i = FC ?o (x i ?pool avg j?Ni (FC h ?v (z j ))).</p><p>? Max pooling: y i = FC ?o (x i ?pool max j?Ni (FC h ?v (z j ))).</p><p>? Pairwise + sigmoid:</p><formula xml:id="formula_10">y i = FC ?o (x i ? K k=1 j?Ni w (k) i,j FC h ? (k) v (z j )), w (k) i,j = 1 |N i | ?( FC ? (k) xa (x i ), FC ? (k)</formula><p>za (z j ) ).</p><p>? Pairwise + tanh: Replace the sigmoid activation in Pairwise + sigmoid to tanh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">INDUCTIVE NODE CLASSIFICA-TION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MODEL</head><p>In the inductive node classification setting, every node is assigned one or multiple labels. During training, the validation and testing nodes are not observable and the goal is to predict the labels of the unseen testing nodes. Our approach follows that of <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref>, where a mini-batch of nodes are sampled on each iteration during training and multiple layers of graph aggregators are stacked to compute the predictions.</p><p>With a stack of M layers of graph aggregators, we will first sample a mini-batch of nodes B 0 and then recursively expand B to be B +1 by sampling the neighboring nodes of B . After M sampling steps, we can get a hierarchy of node batches: B 1 , ..., B M . The node representations, which are initialized to be the node features, will be aggregated in reverse order from B M to B 0 . The representations of the last layer, i.e., the final representations of the nodes in B 0 , are projected to get the output. We use the sigmoid activation for multi-label classification and the softmax activation for multi-class classification. Also, we use the cross-entropy loss to train the model.</p><p>A naive sampling algorithm is to always sample all neighbors. However, it is not practical on large graphs because the memory complexity is O(|V|) and the time Note that min(|N i |, S ) is not the same for all the nodes i. Rather than padding the sampled neighborhood set to the same size, we implemented new GPU kernels that directly operate on inputs with variable lengths to accelerate computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">EXPERIMENTAL SETUP</head><p>We performed a thorough comparison of GaAN with the state-of-the-art models, five aggregator-based models in our framework and a two-layer fully connected neural network on the PPI and Reddit datasets <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref>. The five baseline aggregators include the multihead attention aggregator, two pooling based aggregators and two pairwise sum based aggregators mentioned in Section 4.3. We also conducted comprehensive ablation analysis of these two datasets.</p><p>The PPI dataset was collected from the molecular signatures database <ref type="bibr" target="#b26">(Subramanian et al., 2005)</ref>. Each node represents a protein and edges represent the interaction between proteins. Labels represent the cellular functions of each protein from gene ontology. This dataset contains 24 sub-graphs, with 20 in the training set, two in the validation set, and two in the testing set. Reddit is an online discussion forum where users can post and discuss contents on different topics. Each node represents a post and two nodes are connected if they are commented by the same user. The labels indicate which community a post belongs to. Detailed statistics of the datasets are listed in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MODEL ARCHITECTURES AND IMPLEMENTATION DETAIL</head><p>The GaAN and other five aggregator-based networks are stacked with two graph aggregators. Each aggregator is followed by the LeakyReLU activation with negative slope equals to 0.1 and a dropout layer with dropout rate set to be 0.1. The output dimension d o of all layers are fixed to be 128 except when we compare the relative performance with different output dimensions. To keep the number of parameters comparable for the multi-head models with a different number of heads, we fix the product of the dimension of the value vector and the number of heads, i.e., d v ? K to be the same when evaluating the effect of varying the number of heads. Also, the hyperparameters of the first and the second layer are assumed to be the same if no special explanation is given.</p><p>In the PPI experiments, both pooling aggregators have d v = 512, where d v means the dimensionality of the value vector projected by ? v . For the pairwise sum aggregators, the dimension of the keys d a is set to be 24, d v = 64 and K = 8. For both GaAN and the multihead attention based aggregator, d a is set to be 24 and the product d v ? K is fixed to be 256. For GaAN, we set d m to be 64 in the gate-generation network. Also, we use the entire neighborhoods in the mini-batch training algorithm.</p><p>In the Reddit experiments, both pooling aggregators have d v = 1024. For the pairwise sum aggregators, d a = 32, d v = 256 and K = 8. For the attention based aggregators, d a is set to be 32 and d v ? K is fixed to be 512. We set the gate-generation network in GaAN to have d m = 64. Also, the number of heads is fixed to 1 in the first layer for both attention-based models. The maximum number of sampled neighbors in the first and second sampling steps are denoted as S 1 and S 2 and are respectively set to be 25 and 10 in the main experiment.</p><p>In the ablation analysis, we also look at the performance when setting them to be (50, 20), (100, 40) and <ref type="bibr">(200,</ref><ref type="bibr">80)</ref>.</p><p>To illustrate the effectiveness of incorporating graph structures, we also evaluate a two-layer fully-connected <ref type="table">Table 3</ref>: Summary of different models' test micro F1 scores in the inductive node classification task. In the first block, we include the best-reported results in the previous papers. In the second block, we report the results obtained by our models. For the PPI dataset, we do not use any sampling strategies. For the Reddit dataset, we use the maximum number sampling strategy with S 1 =25 and S 2 =10.</p><p>Models / Datasets PPI Reddit GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref> (61.2) 1 95.4 GAT <ref type="bibr" target="#b28">(Veli?kovi? et al., 2018)</ref> 97.3 ? 0.2 -Fast GCN <ref type="bibr" target="#b3">(Chen et al., 2018)</ref> - We train all the aggregator-based models with Adam <ref type="bibr" target="#b13">(Kingma and Ba, 2015)</ref> and early stopping on the validation set. Besides, we use the validation set to perform learning rate decay scheduler. For Reddit, before training we normalize all the features and project all the features to a hidden dimension of 256. The initial learning rate is 0.001 and gradually decreases to 0.0001 with the decay rate of 0.5 each time the validation F1 score does not decrease in a window of 4 epochs and early stopping occurs for 10 epochs. The gradient normalization value clips no larger than 1.0. For the PPI dataset, all the input features are projected to a 64-dimension hidden state before passing to the aggregators. The learning rate begins at 0.01 and decays to 0.001 with the decay rate of 0.5 if the validation F1 score does not increase for 15 epochs and stops training for 30 epochs.</p><p>The training batch size is fixed to be 512. Also, in all experiments, we use the validation set to select the optimal hyperparameters for training. The training, validation, and testing splits are the same as that in <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref>. The micro-averaged F1 score is used to evaluate the prediction accuracy for both datasets. We repeat the training five times for Reddit and three times for PPI with different random seeds and report the average test F1 score along with the standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">MAIN RESULTS</head><p>We compare our model with the previous state-of-the-art methods on inductive node classification. This includes GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref>, GAT <ref type="bibr" target="#b28">(Veli?kovi? et al., 2018)</ref>, and FastGCN <ref type="bibr" target="#b3">(Chen et al., 2018</ref>). The GraphSAGE model used a 2-layer sample and aggregate model with a neighborhood size of S (1) = 25 and S (2) = 10 without dropout. The 3-layer GAT model consisted of 4, 4 and 6 heads in the first, second and third layer respectively. Each attention head had 256 dimensions. GAT did not use neighborhood sampling, L2 regularization, or dropout. The FastGCN model is a fast version of the 3-layer, 128-dimension GCN with sampled neighborhood size being 400, 100, and 400 for each layer and no sampling is done during testing. <ref type="table">Table 3</ref> summarizes all results of the state-of-the-art models as well as the models proposed in this paper. We denote the multi-head attention aggregator as 'Attentiononly' in the tables and figures. We find that the proposed model, GaAN, achieves the best F1 score on both benchmarks and the other baseline aggregators can also show competitive results to the state-of-the-art. We note that aggregator-based models achieve much higher F1 score than the fully-connected model, which demonstrate the effectiveness of the graph aggregators. Our max pooling and avg. pooling baselines have higher scores on Reddit than that in the original GraphSAGE paper. This mainly contributes to our usage of dropout and the LeakyReLU activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">ABLATION ANALYSIS</head><p>We ran a quantity of ablation experiments to analyze the performance of different graph aggregators when different hyperparameters were used. We also visualized the gates of the GaAN model.</p><p>Effect of the number of attention heads and the sample size We compare the performance of the aggregators when a different number of attention heads and sampling strategies are used. Results are shown in Table 4. We find that attention-based models consistently outperform pooling and pairwise sum based models with the fewer number of parameters, which demonstrates the effectiveness of the attention mechanism in this task. Moreover, GaAN consistently beats the multi-head attention model with the same number of attention heads K. This proves that adding additional gates to control the importance of the attention heads is beneficial to the final classification performance. From the last two row blocks of <ref type="table" target="#tab_4">Table 4</ref>, we note that increasing the number of attention heads will not always produce better results on Reddit. In contrast, on PPI, the larger the K, the better the prediction results.</p><p>Also, we can see steady improvement with larger sampling sizes, which is consistent with the observation in <ref type="bibr" target="#b9">(Hamilton et al., 2017a)</ref>.</p><p>Effect of output dimensions in the PPI dataset We changed the output dimension to be 64, 96 and 128 in the models for training in the PPI dataset. The test F1 score is shown in <ref type="figure" target="#fig_3">Figure 3a</ref>. All multi-head models have K=8.   We find that the performance becomes better for larger output dimensions and the proposed GaAN consistently outperforms the other models. <ref type="figure" target="#fig_3">Figure 3b</ref>, we visualized the gate values of five different nodes output by the GaAN-K8 model trained on the Reddit dataset. It illustrates the diversity of the learned gate combinations for different nodes. In most cases, the gates vary across attention heads, which shows that the gate-generation network can be learned to assign different importance to different heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of gate values In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">TRAFFIC SPEED FORECASTING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">GRAPH GRU</head><p>Following <ref type="bibr" target="#b19">(Lin et al., 2017)</ref>, we formulate traffic speed forecasting as a spatiotemporal sequence forecasting problem where the input and the target are sequences defined on a fixed spatiotemporal graph, e.g., the road network. To simplify notations, we denote Y = ? ? (X, Z; G) as applying the ? aggregator for all nodes in G, i.e., y i = ? ? (x, z Ni ). Based on a given graph aggregator ?, we can construct a GRU-like RNN structure  <ref type="figure">Figure 4</ref>: Illustration of the encoder-decoder structure used in the paper. We use two layers of Graph GRUs to predict a length-3 output sequence based on a length-2 input sequence. 'SS' denotes the scheduled sampling step.</p><p>using the following equations:</p><p>Ut =?(?? xu (Xt, Xt; G) + ?? hu (Xt ? Ht?1, Ht?1; G)), Rt =?(?? xr (Xt, Xt; G) + ?? hr (Xt ? Ht?1, Ht?1; G)),</p><formula xml:id="formula_11">H t =h(?? xh (Xt, Xt; G) + Rt ? ?? hh (Xt ? Ht?1, Ht?1; G)), Ht =(1 ? Ut) ? H t + Ut ? Ht?1.<label>(6)</label></formula><p>Here, X t ? R |V|?di are the input features and H t ? R |V|?do are the hidden states of the nodes at the tth timestamp. |V| is the total number of nodes, d i is the dimension of the input and d o is the dimension of the state. U t and R t are the update gate and reset gate that controls how H t is calculated. G is the graph that defines the connection structure between different nodes.</p><p>We refer to this RNN structure as Graph GRU (GGRU). GGRU can be used as the basic building block for RNN encoder-decoder structure <ref type="bibr" target="#b19">(Lin et al., 2017)</ref> to predict the future K steps of traffic speeds in the sensor net-workX J+1 ,X J+2 , ...,X J+K based on the previous J steps of observed traffic speeds X 1 , X 2 , ..., X J . In the decoder, we use the scheduled sampling technique described in <ref type="bibr" target="#b19">(Lin et al., 2017)</ref>. <ref type="figure">Figure 4</ref> illustrates the encoder-decoder structure in the paper. When attentionbased aggregators are used, i.e., the multi-head attention aggregator or our GaAN aggregator, the connection structure in the recurrent step will also be learned based on the attention process. This can be viewed as an extension of Trajectory GRU (TrajGRU) <ref type="bibr">(Shi et al., 2017)</ref> on irregular, graph-structured data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">EXPERIMENTAL SETUP</head><p>To evaluate the proposed GGRU model on traffic speed forecasting, we use the METR-LA dataset from <ref type="bibr" target="#b17">(Li et al., 2018)</ref>. The dataset contains traffic information of the highways of Los Angeles County. The nodes in the dataset represent sensors measuring traffic speed  <ref type="bibr" target="#b17">(Li et al., 2018)</ref> 3.44 6.30 9.6% 3.77 7.23 10.9% 4.37 8.69 13.2% 3.86 7.41 11.2% GCRNN <ref type="bibr" target="#b17">(Li et al., 2018)</ref> 2.80 5.51 7.5% 3.24 6.74 9.0% 3.81 8.16 10.9% 3.28 6.80 9.13% DCRNN ? <ref type="bibr" target="#b17">(Li et al., 2018)</ref>   and edges denote proximity between sensor pairs measured by road network distance. The sensor speeds are recorded every five minutes. Complete dataset statistics are given in <ref type="table" target="#tab_8">Table 6</ref>.</p><p>We follow <ref type="bibr" target="#b17">(Li et al., 2018)</ref>'s way to split the dataset. The first 70% of the sequences are used for training, the middle 10% are used for validation and the final 20% are used for testing. We also use the same evaluation metrics as in <ref type="bibr" target="#b17">(Li et al., 2018)</ref> for evaluation, including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE). A sequence of length 12 is used as the input to predict the future traffic speed in one hour (12 steps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">MAIN RESULTS</head><p>We compare six variations of the proposed GGRU architecture with three baseline models, including fullyconnected LSTM, GCRNN, and DCRNN <ref type="bibr" target="#b17">(Li et al., 2018)</ref>. We use the same set of six aggregators as in the inductive node classification experiment to construct the GGRU and we use two layers of GGRUs with the state dimension of 64 both in the encoder and the decoder. For attention based models, we set K = 4, d a = 16 and d v = 16. For GaAN, we set d m = 64 and only use max pooling in the gate-generation network. For pooling based aggregators, we set d v = 128. For pairwise sum aggregators, we set K = 4, d a = 32, and d v = 16.</p><p>Since the road map is directed and our model does not deal with edge information, we first convert the road map into an undirected graph and use it as the G in Eqn. <ref type="formula" target="#formula_5">(6)</ref>. All models are trained by minimizing MAE loss with Adam optimizer. The initial learning rate is set to 0.001 and the batch-size is 64. We use the same scheduled sampling strategy as in <ref type="bibr" target="#b17">(Li et al., 2018)</ref>. <ref type="table" target="#tab_1">Table 1</ref> shows the comparison of different approaches for 15 minutes, 30 minutes and 1 hour ahead forecasting on both datasets.</p><p>The scores for 15 minutes, 30 minutes, and 1 hour ahead forecasting as well as the average scores over three forecasting horizons are shown in <ref type="table" target="#tab_6">Table 5</ref>. For the average score, we can see that the proposed GGRU models consistently give better results than GCRNN, which also models the traffic network as an undirected graph. Moreover, the GaAN based GGRU model, which does not use edge information, achieves higher accuracy than DCRNN, which uses edge information in the road network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>We introduced the GaAN model and applied it to two challenging tasks: inductive node classification and traffic speed forecasting. GaAN beats previous state-of-theart algorithms in both cases. In the future, we plan to extend GaAN by integrating edge features and processing massive graphs with millions or even billions of nodes. Moreover, our model is not restricted to graph learning. A particularly exciting direction for future work is to apply GaAN to natural language processing tasks like machine translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of a three-head gated attention aggregator with two center nodes in a mini-batch. |N 1 | = 3 and |N 2 | = 2 respectively. Different colors indicate different attention heads. Gates in darker color stands for larger values. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of different graph aggregators. The aggregators are drawn for only one aggregation step. The nodes in red are center nodes and the nodes in blue are neighboring nodes. The bold black lines between the center node and neighbor nodes indicate that a learned pairwise relationship is used for calculating the relative importance. The oval in dash line around the neighbors means the interaction among neighbors is utilized when determining the weights. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Performance of different models with a varying number of output dimensions on PPI.(b) Visualization of 8 gate values of 5 example nodes on Reddit. Each row represents a learned gate vector for one node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Ablation analysis on PPI and Reddit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Effect of the merge operation. Both methods sample a maximum of 15 neighborhoods without replacement for three recursive steps on the Reddit dataset. We start from 512 seed nodes. The total number of nodes after the lth sampling step is denoted as |B |. The sampling process is repeated for ten times and the mean is reported.</figDesc><table><row><cell>Strategy/Sample Step |B 0 |</cell><cell>|B 1 |</cell><cell>|B 2 |</cell><cell>|B 3 |</cell></row><row><cell cols="4">Sample without merge 512 7.8K 124.4K 1.9M</cell></row><row><cell cols="2">Sample and merge 512 7.5K</cell><cell cols="2">70.7K 0.2M</cell></row><row><cell cols="4">complexity is O(|E|), where |V| and |E| are the total</cell></row><row><cell cols="4">number of nodes and edges. Instead, similar to Graph-</cell></row><row><cell cols="4">SAGE, we only sample a subset of the neighborhoods</cell></row><row><cell cols="4">for each node. In our implementation, at the th sam-</cell></row><row><cell>pling step, we sample min(|N</cell><cell></cell><cell></cell><cell></cell></row></table><note>i |, S ) neighbors without replacement for the node i, where S is a hyperparameter that controls the maximum number of sampled neighbors at the th step. Moreover, to improve over GraphSAGE and further reduce memory cost, we merge repeated nodes that are sampled from different seeds' neighbor- hoods within each mini-batch. This greatly reduces the size of B s as shown in Table 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Datasets for inductive node classification. 'multi' stands for multilabel classification and 'single' otherwise.</figDesc><table><row><cell cols="3">Data #Nodes #Edges #Fea</cell><cell>#Classes</cell></row><row><cell>PPI</cell><cell>56.9K 806.2K</cell><cell cols="2">50 121(multi)</cell></row><row><cell cols="2">Reddit 233.0K 114.6M</cell><cell cols="2">602 41(single)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the test F1 score on the Reddit and PPI datasets with different sampling neighborhood sizes and attention head number K. S 1 and S 2 are the maximum number of sampled neighborhoods in the 1st and 2nd sampling steps. 'all' means to sample all the neighborhoods.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Reddit</cell><cell></cell><cell></cell><cell>PPI</cell><cell></cell></row><row><cell>Models</cell><cell>#Param</cell><cell>S 1 , S 2 25,10</cell><cell>S 1 , S 2 50,20</cell><cell>S 1 , S 2 100,40</cell><cell>S 1 , S 2 200,80</cell><cell>#Param</cell><cell>S 1 , S 2 all, all</cell></row><row><cell>2-Layer FNN</cell><cell cols="5">1.71M 73.58?0.09 73.58?0.09 73.58?0.09 73.58?0.09</cell><cell cols="2">1.23M 54.07?0.06</cell></row><row><cell>Avg. pooling</cell><cell cols="5">866K 95.78?0.07 96.11?0.07 96.28?0.05 96.35?0.02</cell><cell cols="2">274K 96.85?0.19</cell></row><row><cell>Max pooling</cell><cell cols="5">866K 95.62?0.03 96.06?0.09 96.18?0.11 96.33?0.04</cell><cell cols="2">274K 98.39?0.05</cell></row><row><cell>Pairwise+sigmoid</cell><cell cols="5">965K 95.86?0.08 96.19?0.04 96.33?0.05 96.38?0.08</cell><cell cols="2">349K 98.39?0.05</cell></row><row><cell>Pairwise+tanh</cell><cell cols="5">965K 95.80?0.03 96.11?0.05 96.26?0.03 96.36?0.04</cell><cell cols="2">349K 98.32?0.18</cell></row><row><cell>Attention-only-K1</cell><cell cols="5">562K 96.15?0.06 96.40?0.05 96.48?0.02 96.54?0.07</cell><cell cols="2">168K 96.31?0.08</cell></row><row><cell>Attention-only-K2</cell><cell cols="5">571K 96.19?0.07 96.40?0.04 96.52?0.02 96.57?0.02</cell><cell cols="2">178K 97.36?0.08</cell></row><row><cell>Attention-only-K4</cell><cell cols="5">587K 96.11?0.06 96.40?0.02 96.49?0.03 96.56?0.02</cell><cell cols="2">196K 98.09?0.07</cell></row><row><cell>Attention-only-K8</cell><cell cols="5">620K 96.10?0.03 96.38?0.01 96.50?0.04 96.53?0.02</cell><cell cols="2">233K 98.46?0.09</cell></row><row><cell>GaAN-K1</cell><cell cols="5">620K 96.29?0.05 96.50?0.08 96.67?0.04 96.73?0.05</cell><cell cols="2">201K 96.95?0.09</cell></row><row><cell>GaAN-K2</cell><cell cols="5">629K 96.33?0.02 96.59?0.02 96.71?0.05 96.82?0.05</cell><cell cols="2">211K 97.92?0.05</cell></row><row><cell>GaAN-K4</cell><cell cols="5">645K 96.36?0.03 96.60?0.03 96.73?0.04 96.83?0.03</cell><cell cols="2">230K 98.42?0.02</cell></row><row><cell>GaAN-K8</cell><cell cols="5">678K 96.31?0.13 96.60?0.02 96.75?0.03 96.79?0.08</cell><cell cols="2">267K 98.71?0.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of different models for traffic speed forecasting on the METR-LA dataset. Models marked with ' ?' treat sensor map as a directed graph while other models convert it into an undirected graph. Scores under "? min" are the scores at the ? 5 th predicted frame. The last three columns contain the average scores of the 15 min, 30 min, and 60 min forecasting horizons.</figDesc><table><row><cell>Models / T</cell><cell>15 min MAE RMSE MAPE</cell><cell>30 min MAE RMSE MAPE</cell><cell>60 min MAE RMSE MAPE</cell><cell>Average MAE RMSE MAPE</cell></row><row><cell>FC-LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The Dataset used for traffic speed forecasting.</figDesc><table><row><cell cols="4">Data #Nodes #Edges #Timestamps</cell></row><row><cell>METR-LA</cell><cell>207</cell><cell>1,515</cell><cell>34,272</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The performance reported in the paper is relatively low because the author has not trained their model into convergence. Also, it is not fair to compare it with the other scores because it uses the sampling strategy while the others have not.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Endto-end continuous speech recognition using attentionbased recurrent NN: First results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, Workshop on Deep Learning and Representation Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning and Representation Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="6533" to="6542" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="129" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Im-ageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07659</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yeung</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for precipitation nowcasting: A benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5622" to="5632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Mootha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gillette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Pomeroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Lander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="15545" to="15550" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
