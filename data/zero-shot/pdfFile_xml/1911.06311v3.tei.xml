<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sato: Contextual Semantic Type Detection in Tables ? agatay Demiralp</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
							<email>dzhang@cs.umass.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
							<email>jinfeng@megagon.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelon</forename><surname>Hulsebos</surname></persName>
							<email>mmhulsebos@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megagon</forename><surname>Labs</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagatay@megagon</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
							<email>wangchiew@megagon.ai</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UMASS</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Megagon Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Megagon Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">The HEINEKEN Company</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Megagon Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sato: Contextual Semantic Type Detection in Tables ? agatay Demiralp</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting the semantic types of data columns in relational tables is important for various data preparation and information retrieval tasks such as data cleaning, schema matching, data discovery, and semantic search. However, existing detection approaches either perform poorly with dirty data, support only a limited number of semantic types, fail to incorporate the table context of columns or rely on large sample sizes for training data. We introduce Sato, a hybrid machine learning model to automatically detect the semantic types of columns in tables, exploiting the signals from the context as well as the column values. Sato combines a deep learning model trained on a large-scale table corpus with topic modeling and structured prediction to achieve supportweighted and macro average F1 scores of 0.925 and 0.735, respectively, exceeding the state-of-the-art performance by a significant margin. We extensively analyze the overall and per-type performance of Sato, discussing how individual modeling components, as well as feature categories, contribute to its performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Many data preparation and information retrieval tasks including data cleaning, integration, discovery and search rely on the ability to accurately detect data column types. Automated data cleaning uses transformation and validation rules that depend on data types <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>. Schema matching for data integration leverages data types to find correspondences between data columns across tables <ref type="bibr" target="#b37">[38]</ref>. Similarly, data discovery benefits from detecting the types of data columns in order to return semantically relevant results for user queries <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Recognizing the semantics of table values helps aggregate information from multiple tabular data sources. Search engines also rely on the detection of semantically relevant column names to extend support to tables <ref type="bibr" target="#b47">[48]</ref>.</p><p>We can consider two categories of types for table columns: atomic and semantic. Atomic types such as boolean, integer, and string provide basic, low-level type information about a column. On the other hand, semantic types such as location, birthDate, and name, convey finer-grained, richer information about column values. Detecting semantic types can be a powerful tool, and in many cases may be essential for enhancing the effectiveness of data preparation and analysis systems. In fact, commercial systems such as Google Data * Work done during internship at Megagon Labs Studio <ref type="bibr" target="#b16">[17]</ref>, Microsoft Power BI <ref type="bibr" target="#b31">[32]</ref>, Tableau <ref type="bibr" target="#b43">[44]</ref>, and Trifacta <ref type="bibr" target="#b45">[46]</ref> attempt to detect semantic types, typically using a combination of regular expression matching and dictionary lookup. While reliable for detecting atomic types and simple, well-structured semantic types such as credit card numbers or e-mail addresses, these rule-based approaches are not robust enough to process dirty or missing data, support only a limited variety of types, and fall short for types without strict validations. However, many tables found in legacy enterprise databases and on the Web have column names that are either unhelpful (cryptic, abbreviated, malformed, etc.) or missing altogether.</p><p>In response, recent work <ref type="bibr" target="#b21">[22]</ref> introduced Sherlock, a deep learning model for semantic type detection trained on a massive table corpora <ref type="bibr" target="#b20">[21]</ref>. Sherlock formulates semantic type detection as a multi-class classification problem where classes correspond to semantic types. It leverages more than 600K real-world table columns for learning with a multiinput feed forward deep neural network, providing state-ofthe-art results.</p><p>While Sherlock represents a significant leap in applying deep learning to semantic typing, it suffers from two problems. First, it under-performs for types that do not have a sufficiently large number of samples in the training data. Although this is a known issue for deep learning models, it nevertheless restricts Sherlock's application to underrepresented types, which form a long tail of data types appearing in tables at large. Second, Sherlock uses only the values of a column to predict its type, without considering the column's context in the table. Predicting the semantic type of a column based solely on the column values, however, is an under-determined problem in many cases.</p><p>Consider the example in <ref type="figure" target="#fig_0">Fig. 1</ref>: for a column that contains 'Florence,' 'Warsaw,' 'London,' and 'Braunschweig' as values, location, city, or birthPlace could all be reasonable semantic types for the column. It can be hard to resolve such ambiguities using only column values because the semantic types also depend on the context of the table. Continuing with the example, it is highly likely that the column's type would be birthPlace if it came from <ref type="table" target="#tab_1">Table A since the table  contains</ref> biographical information about influential personalities. However, the same column in <ref type="table" target="#tab_1">Table B</ref> would be more likely to have the type city, as the table's other columns present information about European cities.</p><p>In this paper, we introduce Sato (SemAntic Type detection with table cOntext), a hybrid machine learning model that incorporates table contexts to predict the semantic types    <ref type="table" target="#tab_1">(Table A and Table B</ref>) from the VizNet corpora. The last column of <ref type="table" target="#tab_1">Table A</ref> and the first column of <ref type="table" target="#tab_1">Table B</ref> have identical values: 'Florence,' 'Warsaw,' 'London,' and 'Braunschweig.' However powerful, a prediction model based solely on column values (i.e., single-column prediction) cannot resolve the ambiguity to infer the correct semantic types, birthplace and city. Sato incorporates signals from table context and perform a multicolumn type prediction to help effectively resolve ambiguities like these and improve the accuracy of semantic type predictions.</p><p>of table columns. Sato combines topic modeling <ref type="bibr" target="#b3">[4]</ref> and structured learning <ref type="bibr" target="#b24">[25]</ref> together with single-column type prediction based on the Sherlock model. Similar to earlier work <ref type="bibr" target="#b21">[22]</ref>, we consider 78 common semantic types and use the WebTables dataset from the VizNet corpus <ref type="bibr" target="#b20">[21]</ref> to train our model. We summarize our main contributions below:</p><p>1. Sato significantly outperforms the state-of-the-art in semantic type prediction, increasing the macro and support-weighted F1 scores by as much as 14.4% and 5.3%, respectively. Through a comparative analysis of per-type predictions, we also show that Sato's performance gains are primarily due to improved predictions for underrepresented semantic types in the long tail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM FORMULATION</head><p>Our goal is to predict semantic types for table columns using their values, without considering the header information. We formulate it as multi-class classification, each class corresponding to a predefined semantic type.</p><p>We consider the training data as a set of tables. Let c1, c2, . . . , cm be the columns of a given table and t1, t2, . . . , tm be the true semantic types of these columns, where ti ? T , the set of labels for possible semantic types considered (e.g., city, country, population). Similarly, let ? be a feature extractor function that takes a single column ci and returns an n-dimensional feature vector ?i. One approach to semantic typing is to learn a mapping f single : ? n ? T from values of single columns to semantic types. We refer to this model as single-column prediction. The Sherlock <ref type="bibr" target="#b21">[22]</ref> model falls into this category.</p><p>In Sato, in order to make the best use of table contexts and resolve semantic ambiguity with single-column predictions, we formulate the problem as multi-column prediction. A multi-column prediction model learns a mapping f mult : ? n?m ? T m from the entire table (a sequence of columns) to a sequence of semantic types. This formulation allows us to incorporate table context into semantic type prediction in two ways.</p><p>First, we use features generated from the entire table as table context. For example, the column values 'Italy,' 'Poland,' ... and '380,948,' '1,777,972,' ... are also used to predict the semantic type of the first column in <ref type="table" target="#tab_1">Table  B</ref> (in <ref type="figure" target="#fig_0">Fig. 1</ref>.) Second, we can jointly predict the semantic types of columns from the same <ref type="table" target="#tab_1">table. Again, for Table B</ref>, with the joint prediction the predicted types country and population of neighboring columns would help to make a more accurate prediction for the first column. <ref type="table" target="#tab_1">Table context</ref> As demonstrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the contextual information of a column can be used to resolve ambiguities and improve the semantic type prediction for the column. To this end, we identify two basic types of context that collectively characterize the context of a table column: global context and local context. We define the global context for a column to be the set of all the cell values in the table. In this sense, all the columns in a given table have the same global context. We show in Section 3.2 how the global context can be used to compute a global descriptor effectively capturing the intent of a table. We define the local context of a column as the set of independently predicted semantic types of the neighboring columns in the same table. Local context can be used to resolve semantic type ambiguities when combined with single-column predictions. The scope of such a local neighborhood is flexible and can be adjusted based on the desired trade-off between model performance and model complexity. In this work, we restrict the local neighborhood to immediately adjacent columns. We demonstrate in Section 3.3 how local context can be effectively used to improve the semantic type detection accuracy through structured predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODEL</head><p>Next, we will show how Sato effectively captures contextual signals from both global and local sources using a hybrid machine learning model. It has two modeling components: (1) A topic-aware prediction component that estimates the intent (a global descriptor) of a table using topic modeling and extends the single-column prediction model with an additional topic subnetwork. (2) A structured output prediction model that combines the topic-aware predictions for all m columns and performs multi-column joint semantic type prediction. <ref type="figure">Fig. 2</ref> illustrates the high-level architecture of Sato. We next discuss each Sato component and its implementation in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structured Prediction Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-column Model</head><p>Column Feature Extractor  <ref type="figure">Figure 2</ref>: In Sato, the topic-aware module extends singlecolumn models with additional topic subnetworks, incorporating a context modeling table intent into the model. The structure prediction module then combines the topic-aware results for all m columns, providing the final semantic type prediction for the columns in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single-column prediction model</head><p>As shown in <ref type="figure">Fig. 2</ref>, Sato's topic-aware module is built on top of a single-column prediction model that uses a deep neural network. We first provide a brief background on deep learning and a description of the single-column model. <ref type="bibr" target="#b26">[27]</ref> is a form of representation learning that uses neural networks with multiple layers. Through simple but non-linear transformations of the input representation at each layer, deep learning models can learn representations of the data at varying levels of abstractions that are useful for the problem at hand (e.g., classification, regression). Deep learning combined with the availability of massive table corpora <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref> presents opportunities to learn from tables in the wild <ref type="bibr" target="#b18">[19]</ref>. It also presents opportunities to improve existing approaches to semantic type detection as well as other research problems related to data preparation and information retrieval. Although prior research has used shallow neural networks for related tasks (e.g., <ref type="bibr" target="#b27">[28]</ref>), it is only more recently that Hulsebos et al. <ref type="bibr" target="#b21">[22]</ref> developed Sherlock, a large-scale deep learning model for semantic typing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning Deep learning</head><p>Deep learning for single-column prediction Sato builds on single-column prediction by using column-wise features and employs an architecture which allows any single-column prediction model to be used. In our current work, we choose Sherlock as our single-column prediction model due to its recently demonstrated performance.</p><p>The column-wise features used in Sato include character embeddings (Char), word embeddings (Word), paragraph embeddings (Para), as well as column statistics (e.g., mean, std) (Stat.)</p><p>A multi-layer subnetwork is applied to the column-wise features to compress high-dimensional vectors into compact dense vectors, with the exception of the Stat feature set, which consists of only 27 features. The output of the three subnetworks is concatenated to the statistical features, forming the input to the primary network. After the concatenation of these features, in the primary network two fullyconnected layers (ReLU activation) with BatchNorm and Dropout layers are applied before the output layer. The final output layer, which includes a softmax function, generates confidence values (i.e., probabilities) for the 78 semantic types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic-aware prediction model</head><p>The first component of Sato is a topic-aware prediction module. This module first creates a vector representation for the global context of a given table by computing a topic vector from the values of the entire table. The topic-aware prediction module then feeds this topic vector as input to the column-wise prediction model. The column-wise prediction model extends the neural network model above with an additional subnetwork in order to take topic vectors as input. We next discuss how taking the global context of a table into account in semantic type prediction can help resolve ambiguities.  <ref type="figure" target="#fig_0">Fig. 1</ref>, <ref type="table" target="#tab_1">Table A</ref> intends to provide biographical information about influential personalities in history and Table B talks about geographical information about cities in Europe. However, as with column semantics, a clear and well-structured description of intent is not always available in real-world tables. Therefore we need to estimate table intent without relying on any header or meta information.</p><p>Sato estimates a table's intent by mapping its values onto a low-dimensional space. Each of these dimensions corre-    Topic-aware Model <ref type="table" target="#tab_1">Table  Values</ref> Pre-trained LDA model <ref type="figure">Figure 3</ref>: Sato's topic-aware modeling is based on the premise that every table is created with an intent in mind and that the semantic types of the columns in a table are expressions of that intent with thematic coherence. In other words, (a) the intent of a table determines the semantic types of the columns in the table, which in turn generate the column values, acting as latent variables. (b) Sato estimates the intent of a given table with a topic vector obtained from a pre-trained LDA model and combines it with the local evidence from per-column values using a deep neural network. sponds to a "topic," describing one aspect of a possible table intent. The final estimation is a distribution over the latent topic dimensions generated using topic modeling approaches. Next, we provide a brief background on topic models and explain how Sato extracts topic vectors from tables and feed them to topic-aware models.</p><formula xml:id="formula_0">1 3 2 (b)</formula><p>Topic models Finding the topical composition of textual data is useful for many tasks, such as document summarization or featurization. Topic models <ref type="bibr" target="#b3">[4]</ref> aim to automatically discover thematic topics in text corpora and discrete data collections in an unsupervised manner. Latent Dirichlet allocation (LDA) <ref type="bibr" target="#b5">[6]</ref> is a simple yet powerful generative probabilistic topic model, widely used for quantifying thematic structures in text. LDA represents documents as random mixtures of latent topics and each latent topic as a distribution over words. The main advantage of probabilistic topic models such as LDA over clustering algorithms is that probabilistic topic models can represent a data point (e.g., document) as a mixture of topics. Although LDA was originally applied to text corpora, since then many variants have been developed to discover thematic structures in nontextual data as well (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b50">51]</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table intent estimator</head><p>We use an LDA model to estimate a table's intent as a topic-vector, treating values of each table as a "document." As illustrated in <ref type="figure">Fig. 3b</ref>, we implement the table intent estimator as a pre-trained LDA model. It takes table values as input and outputs a fixed-length vector named "table topic vector" over the topic dimensions. For Sato, we pre-train an LDA model with 400 topic dimensions on public tables that have had their headers and captions removed.</p><p>The topics are generated during training according to the data's semantic structure, so they do not have pre-defined meanings. However, by looking at the representative semantic types associated with each topic, we found some examples with good interpretations. For example, topic # 192 is closely associated with the semantic types "origin, nationality, country, continent, and sex" and thus possibly captures aspects about personal information, while topic # 264 cor-responds to "code, description, create, company, symbol" and can be interpreted as a business-related topic. Detailed topic analysis can be found in Section 5.5.</p><p>Learning and prediction <ref type="figure">Fig. 3b</ref> shows how topic-aware models take the values in a table topic vector as additional features for both learning and prediction. We augment the single-column neural network model with an additional subnetwork to take topic vectors as input and then append its output before feeding into the primary network. In this way, the topic-aware model will learn not only relationships between the input column and its type but also how the column type correlates to the table-level contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Structured prediction model</head><p>We have described how Sato captures the global context of a column by computing a topic vector for the entire table and passing it to the single-column model as additional input. Incorporating only the global context into the model may not be sufficient, however, as the topic-aware model does not directly take the relationships among the semantic types of neighboring columns (i.e., local context) into account. Therefore, we incorporate local context using a structured output prediction model which comprises the second component of Sato.</p><p>Through preliminary analysis, we confirm that certain pairs of semantic types co-occur in tables much more frequently than others. For example, in a WebTables sample, the most frequent pair city and state co-occurs 4 times more often than the tenth most frequent pair name and type (detailed co-occurrence statistics available in Section 4.1). Such inter-column relationships show the value of "local" contextual information from surrounding columns in addition to the "global" table topic. Sato models the relationships between columns through pairwise dependencies in a graphical model and performs table-wise prediction using structured learning techniques. Although the notion of local context is not limited to immediate neighbors, Sato only models pairwise relations between adjacent columns because of its simplicity, efficiency, and empirical accuracy. We leave the study of the broader local context, which can be mod-  <ref type="figure">Figure 4</ref>: (a) Sato uses a linear-chain CRF to model the dependencies between columns types given their values. (b) For each column, Sato plugs in the column-wise prediction scores for each type as the unary potentials of the corresponding node in the CRF model. Then Sato learns the pairwise potential through backpropagation updates using stochastic gradient descent, maximizing the posterior probability P (t|c). Although we choose to use predictions from topic-aware models in the current implementation, the Sato architecture is flexible to support unary potentials from arbitrary column-wise models. eled using high-order graphical models (further discussed in Section 6), as future work. Structured output learning In addition to semantic type detection, many other prediction problems such as named entity extraction, language parsing, and image segmentation have spatial or semantic structures that are inherent to them. Such structures mean that predictions of neighboring instances correlate to one another. Structured learning algorithms <ref type="bibr" target="#b2">[3]</ref>, including probabilistic graphical models <ref type="bibr" target="#b23">[24]</ref> and recurrent neural networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref>, model dependencies among the values of structurally linked variables such as neighboring pixels or words to perform joint predictions.</p><p>A conditional random field (CRF) <ref type="bibr" target="#b24">[25]</ref> is a discriminative undirected probabilistic graphical model and a popular techniques for structured learning with successful applications in labeling, parsing and segmentation problems across domains. Similar to Markov random fields (MRFs) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>, exact inference for general CRFs is intractable but there are special structure such as linear-chains that allow exact inference. There are also several efficient approximate inference algorithms based on message passing, linear-programming relaxation, and graph cut optimization for CRFs with general graphs <ref type="bibr" target="#b24">[25]</ref>.</p><p>Modeling column dependencies Sato uses a linear-chain CRF to explicitly encode the inter-column relationship while still considering features for each column. We encode the output of a column-wise prediction model (i.e., predicted semantic types of the columns) and the combinations of semantic types of columns in the same table as CRF parameters. As shown in <ref type="figure">Fig. 4a</ref>, in the CRF model, each variable ti represents the type of a column with corresponding column values ci as the observed evidence. Variables representing the types of adjacent columns are linked with an edge. Given a sequence of columns c in a table, the goal is to find the best sequence of semantic types t, which provides the largest conditional probability P (t|c).</p><p>The conditional probability can be written as a normalized product of a set of real-valued functions. Following the convention, we refer to these functions in log scale as "potential functions." Unary potential ?UNI(ti, ci) captures the likelihood of predicting type ti based on the content of the corresponding column ci. Pairwise potential ?PAIR(ti, tj) represents the "coupling degree" between types ti and tj.</p><p>We use a linear-chain CRF, where the conditional distribution is defined by the unary prediction potentials and pairwise potentials between adjacent columns:</p><formula xml:id="formula_1">P (t|c) = 1 Z(c) exp m i=1 ?UNI(ti, ci) + m i=1 m j=i+1 ?PAIR(ti, tj) , where Z(c) = t exp m i=1 ?UNI(ti, ci) + m i=1 m j=i+1 ?PAIR(ti, tj)</formula><p>is an input-dependent normalization function.</p><p>Unary potential functions We use unary potentials to model the probability of a semantic type given the column content. In other words, the unary potential of a semantic type for a given column can be considered the probability of that semantic type based on the values of the column. The architecture of Sato supports using estimates of any valid column-wise prediction model as unary potentials. In the current work, we obtain the unary potentials of the semantic types for a given column from the output of our topic-aware prediction model, which uses both table-level topic vector and column features as input. Using the examples of <ref type="figure" target="#fig_0">Fig. 1</ref>, we expect that the highlighted column, which contains 'Florence, 'Warsaw,' 'London, and 'Braunschweig, would have high unary potential scores for location-related semantic types such as location, city, and birthplace.</p><p>In other words, the unary potentials calculate column-wise prediction scores, which are used to select semantic type candidates for each column.</p><p>Pairwise potential functions Pairwise potentials capture the relationship between the semantic types of two columns in the same table. These relationships can be parameterized with a |T | ? |T | matrix P , where T is the set of all possible types and Pij (= ?PAIR(ti, tj)) is a weight parameter for the "coupling degree" of semantic types ti and tj in adjacent columns. Such a coupling degree can be approximated by the co-occurrence frequency. We expect the pairwise weight of two semantic types to be proportional to their frequency of co-occurrence in adjacent columns. Pairwise potential weights in our CRF model are trainable parameters, updated by gradient descent. Through the training step, we expect that Sato updates the CRF parameters so that frequently co-occurred pairs like (city, country) and (occupation, birthplace) have higher pairwise potential scores. Thus, the trained model can resolve the disambiguate issue (shown in <ref type="figure" target="#fig_0">Fig. 1</ref>) by using pairwise potentials and achieves context-aware predictions.</p><p>Learning and prediction We use the following objective function to train a Sato model. The objective function is the log-likelihood of semantic types of columns in the same table:</p><formula xml:id="formula_2">log P (t|c) = m i=1 ?UNI(ti, ci) + m i=1 m j=i+1 ?PAIR(ti, tj) ? log Z(c).</formula><p>Here, the normalization term Z sums over all possible semantic type combinations. To efficiently calculate Z, we can use the forward-backward algorithm <ref type="bibr" target="#b36">[37]</ref>, which uses dynamic programming to cache intermediate values while moving from the first to the last columns. After the training phase, as shown in <ref type="figure">Fig. 4b</ref>, Sato performs holistic type prediction with learned pairwise potential and unary potential provided by topic-aware prediction. To obtain prediction results, we conduct maximum a posteriori (MAP) inference of semantic types:</p><formula xml:id="formula_3">t = argmax t log P (t|c) = argmax t ?UNI + ?PAIR .</formula><p>Z(c) does not affect argmax since it is a constant with respect to t. Then we use the Viterbi algorithm <ref type="bibr" target="#b48">[49]</ref> to calculate and store partial combinations with the maximum score at each step of the column sequence traversal, avoiding redundant computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION</head><p>We compare Sato and its two basic variants obtained by ablation with the state-of-the-art Sherlock <ref type="bibr" target="#b21">[22]</ref> implemented as the Base method. We omit comparisons with matchingbased algorithms, decision-tree-based semantic typing since they are outperformed by Sherlock as demonstrated in <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the effectiveness of the proposed models on the WebTables corpus from VizNet <ref type="bibr" target="#b20">[21]</ref> and restrict ourselves to the relational web tables with valid headers that appear in the 78 semantic types. These types resulted from a selection process <ref type="bibr" target="#b21">[22]</ref> and originate from the T2Dv2 Gold Standard 1 , which describes 237 DBpedia properties frequently occurring in the WebTables corpus. To avoid filtering out columns with slight variation in capitalization and representation, we convert all column headers to a "canonical form" before matching. The canonicalization process starts with trimming content in parentheses. We then convert strings to lower case, capitalize words except for the first (if there are more than one word) and concatenate the results into a single string. For example, strings 'YEAR,' 'Year' and 'year (first occurrence)' will all have canonical form 'year,' and 'birth place (country)' will be converted to 'birthPlace. <ref type="bibr">'</ref> Since we formulate semantic typing as a multi-column type detection problem, we extract 80K tables, instead of columns, from a subset of the WebTables corpus as our dataset D. The column headers in their canonical forms act as the groundtruth labels for semantic types. To help evaluate the importance of incorporating table semantics, we also create a filtered version D mult with 33K tables. We filter out singleton tables (those containing only one column) since they lack context as defined in this paper. We then conduct 5-fold cross-validation where we use 80% of the tables for training and a held-out set (20%) for evaluation in each iteration. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the count of each semantic type in the dataset D. The distribution is clearly unbalanced with a long tail. Single-column models tend to perform poorly on the lesscommon types that comprise the long-tail. By effectively incorporating context, Sato significantly improves prediction accuracy for those types.</p><p>To better understand relationships between the semantic types of columns in the same table, we conduct a preliminary analysis on the co-occurrence patterns of types. <ref type="figure" target="#fig_5">Fig. 6</ref>, shown in log-scale for readability, reports the frequencies of selected pairs of semantic types occurring in the same table.</p><p>Most frequently co-occurring pairs include (city, state), (age, weight), (age, name), (code, description).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature extraction</head><p>We use the public Sherlock feature extractors 2 to extract the four groups of base features, Char, Word, Para and Stat, generating a feature vector with 1587 dimensions for each column in a table. Those features have been proven effective for semantic type detection and provide good coverage of the granularity spectrum, ranging from characterlevel distribution features to global statistics. In addition, the Word and Para features take advantage of powerful pre-trained word and paragraph embeddings which enable a better understanding of natural language contents.</p><p>To make a fair comparison, these base features were used by both baseline methods and proposed methods in the experiments. To generate table topics as introduced in Section 3.2, we train an LDA model that captures the mapping from table values to the latent topic dimensions. Since LDA is an unsupervised model, we only need the vocabulary (i.e., set of all cell values) of the tables without any headers or semantic annotation. We convert numerical values into strings and then concatenate all values in the table sequentially to form a "document" for each </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model implementation</head><p>We implement the multi-input neural network introduced in <ref type="bibr" target="#b21">[22]</ref> using PyTorch <ref type="bibr" target="#b32">[33]</ref> as the Base single-column model. Throughout the experiments discussed here, we train the Base neural network model for 100 epochs using the Adam optimizer with a learning rate of 1e ? 4 and a weight decay rate of 1e ? 4.   For topic-aware prediction in Sato, the table topic features go through a separate subnetwork with an architecture identical to the subnetworks of the Base feature groups. Before going into the primary network, the outputs of all four subnetworks are concatenated with Stat to form a single vector. We train Sato's CRF layer with a batch size of 10 tables, using the Adam optimizer with a learning rate of 1e ? 2 for 15 epochs. We initialize the pairwise potential parameters of the CRF model with the column co-occurrence matrix calculated from a held-out set of the WebTables corpus. We set the CRF unary potentials for columns to be their normalized topic-aware prediction score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation metrics</head><p>We measure the prediction performance on each target semantic type by calculating F1 = 2 ? precision?recall precision+recall . Since the semantic type distribution is not uniform, we report two types of average performances using the support-weighted F1 and macro average F1. The support-weighted F1 score is the average of per-type F1 values weighted by support (sample size in the test set for the respective type) and reflects the overall performance. The macro average F1 score is the unweighted average of the per-type F1 scores, treating all types equally, and is therefore more sensitive to types with small sample sizes compared to support-weighted F1. <ref type="table" target="#tab_1">Table 1</ref> reports improvements of the Sato variants over the Base method on both the dataset D mult , which includes only tables with more than one column, and the complete dataset D. We implemented Base using features and neural network structure of the Sherlock <ref type="bibr" target="#b21">[22]</ref> model. On multicolumn tables, Sato improves the macro average F1 score by 0.093 (14.4%) and the support-weighted F1 score by 0.046 (5.3%) compared to the single-column Base. When evaluated on all tables we still see a 0.064 (9.3%) improvement on macro average F1 score and 0.035 (4.0%) improvement on support-weighted F1, although these scores are diluted by the inclusion of tables without valid table context. The results confirm that Sato can effectively improve the accuracy of semantic type prediction by incorporating contextual information embedded in table semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>We also evaluate the variants of Sato with single components: SatonoStruct only performed topic-aware prediction using table values and SatonoTopic conducted structured prediction using Base output as unary potential without considering table topic features. As shown in <ref type="table" target="#tab_1">Table 1</ref>, both SatonoStruct and SatonoTopic provide improvements over the Base model but are outperformed by the combined effort in Sato. The results indicate that the structured prediction model and the topic-aware prediction model make use of different pieces of table context information for semantic type detection.</p><p>We note that there are always larger improvements on macro average F1 scores than support-weighted F1 scores, suggesting that a significant amount of Sato's improvements come from boosting accuracy for the less represented types. To better understand the influence of techniques used in Sato, we next perform a per-type evaluation for both Sato components on multi-column tables. <ref type="figure" target="#fig_7">Fig. 7</ref> shows the per-type comparison of F1 scores between models with and without the topic-aware prediction component. More specifically, <ref type="figure" target="#fig_7">Fig. 7a</ref>    against Base. Including information in table values improved 59 out of 78 semantic types for SatonoTopic with 9 types getting equal and 10 types getting worse performances. Similarly, SatonoStruct improves the performance for 64 types and decreases it for 11 types. The prediction performance stays unchanged for 3 types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Topic-aware prediction</head><p>We also see significant improvements in the previously "hard" semantic types with small support size. The types with the highest accuracy increases, affiliate, director, person, ranking, and sales, all come from the fifteen least represented types as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. This shows incorporating table values effectively alleviates the problem of lacking training data for the rare types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Structured prediction</head><p>To evaluate the contribution of structured prediction, we compare Sato with its variant without structured prediction, SatonoStruct <ref type="figure" target="#fig_9">(Fig. 8a)</ref>. Similarly, we compare the performance of SatonoTopic (structured prediction directly on Base output) with that of Base <ref type="figure" target="#fig_9">(Fig. 8b)</ref>. Base is improved on 50 types and SatonoStruct is improved on 59 types with structured prediction. For a subset of rare types (e.g., depth, sales, affiliate,) the prediction accuracy is dramatically improved. While for others (e.g., person, director, ranking,) there is no noticeable improvement as with topic-aware prediction. This shows structured prediction is less effective in boosting the accuracy of rare types compared to topic-aware prediction. However, at the same time, both the number of types that get worse accuracy (4 and 5 respectively) and the drop in F1 scores for those types are smaller with structured prediction as compared to topic-aware prediction. Enforcing table-level context can be too aggressive sometimes, leading to worse performance for certain types. Through modeling relationships between inferred types of surrounding columns, the structured prediction module in Sato "salvages" some of these overly aggressive predictions. We conduct qualitative analysis in Section 5.7 to further look into this effect.</p><p>In conclusion, multi-column predictions from the structured prediction model, with or without topic modeling,   <ref type="figure">Figure 9</ref>: Importance scores for the feature categories used in our models obtained by measuring the drop in both aggregated F1 values from permutation experiments. Topic features are the most important feature category with respect to the macro average F1 score in the full Sato model, providing additional evidence for the contribution of topic modeling in predicting underrepresented semantic types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Efficiency</head><p>We show that the Sato model successfully improves prediction accuracy by introducing the topic-aware features and the CRF layer. However, the additional components may cause additional time cost. To evaluate the efficiency of Sato, we repeated the training and prediction procedures for 5 times and measured the training and prediction time of Base and Sato on the multi-column dataset D mult . The training data contains 26K tables and the test data contains 6.4K tables. For further investigation on the cost of the topic-aware features and the CRF layer, we separately measured the time for training the main model, and the time for training the CRF layer. We use the same hyperparameters used in the experiment (described in 4.3) for both of the models for a fair comparison. The experiment was conducted on a single machine with 2.1GHz CPUs (64 cores) and 512GB RAM. <ref type="table" target="#tab_1">Table 2</ref> summarizes the average training and prediction time for those models.</p><p>From the results, we confirm that adding the topic-aware features and the CRF layer increases approximately 81 s and 367 s for training time, respectively. We would like to emphasize that we do not need to retrain a model unless we obtain a significant amount of additional training data. Thus, we consider that the difference is not critical. On average, Sato takes +1.4 s than Base to generate predictions for all 6.4K tables in the test set of D mult , which is 0.2 ms per table. We believe the overhead will be mostly unnoticeable in practice, and the average prediction time per table (0.8 ms) is can support the interactive use of Sato.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Feature importance</head><p>To better understand the influence of the different feature groups, we perform permutation importance <ref type="bibr" target="#b0">[1]</ref> analysis on Base and Sato variants. For each fitted model and a specific feature group, we take the input tables and perform shuffling by only swapping features in the specified feature  group with randomly selected tables. Such feature mismatch will cause less accurate predictions and a worse overall performance. Shuffling crucial features will break the strong relationships between input and output, leading to a significant drop in accuracy. We took the average of the normalized drop in F1 scores over five random trials as the feature importance measurement. <ref type="figure">Fig. 9</ref> shows that for both the Base model and SatonoTopic, the Word and Char feature groups are the most important feature groups. This matches the conclusions in <ref type="bibr" target="#b21">[22]</ref>. When considering the global context, the additional Topic feature group has comparable or greater importance than Word and Char. The effect is more obvious with respect to the macro average F1 metric, confirming the help of table values information, especially on less-represented types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Topic interpretation</head><p>We conduct qualitative analysis on the LDA model to investigate how the model captures semantics from each table and provides contextual information to Sato. To obtain the topic distribution of each semantic type, we calculate the average topic distribution based on the topic distributions ?i of the i-th table that contains the semantic type. For each topic, we chose top-k semantic types as representative semantic types by the probability of the topic.</p><p>We find that some topics had "flat" distributions where most semantic types have almost the same probabilities. Since these topics are not very useful for classifying semantic types, we compute a saliency score for each topic and sort the topics by their saliency. Our saliency score averages the probabilities of the top-k semantic types for each topic. <ref type="table" target="#tab_1">Table 3</ref> shows the top-5 salient topics and the representative semantic types. Following the standard approach in topic model analysis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>, we manually devise an interpretation for each topic. For example, topic dimension #192 and #99 are activated by personal information in table values, whereas #264 is closely related to business tables. These examples demonstrate that semantic space learned using LDA could capture intent information from tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Column embeddings (Col2Vec)</head><p>To verify how the table intent features help the Sato model capture the table semantics, we analyze and compare the embedding vectors from the final layer of the Sato model and the baseline Sherlock model as column embeddings. We can consider these embeddings as column embeddings since the final layer combines input signals to compose semantic representations. For comparison, we used the final layer of the single-column prediction model of Sato, before the CRF layer. Therefore, we assume that the <ref type="table" target="#tab_1">Table Intent</ref> features account for the difference in the embeddings.</p><p>Following prior examples (e.g., <ref type="bibr" target="#b51">[52]</ref>), we analyze column embeddings of the test columns used in the experiments.  <ref type="table" target="#tab_1">Table 3</ref>: Examples of the topics learned by the LDA model, semantic types associated with each topic that are obtained by using a saliency metric, and our interpretation for each topic.</p><p>(a) (b) <ref type="figure" target="#fig_0">Figure 10</ref>: Two-dimensional visualizations of column embeddings by (a) SatonoStruct, and (b) Sherlock. Colors denote semantic types. Gray-colored regions are manually added to emphasize the areas of "ambiguity"in the column embeddings. SatonoStruct appears to separate similar semantic types better.</p><p>We use t-SNE <ref type="bibr" target="#b46">[47]</ref> to reduce the dimensionality of the embedding vectors to two and then visualize them using a twodimensional scatterplot. To embed vectors of the two methods in a common space, we fit a single t-SNE model for all data points, and then visualize major semantic types that are related to organizations (affiliate, teamName, family, and manufacturer) to investigate how the Sato model with the <ref type="table" target="#tab_1">Table Intent</ref> features can distinguish columns of those ambiguous semantic types. <ref type="figure" target="#fig_0">Fig. 10</ref> shows the visualization of embedding vectors of Sato and Sherlock. With Sherlock, the column embeddings of each semantic type partially form a cluster, but some clusters are overlapped compared to the column embeddings by Sato. In <ref type="figure" target="#fig_0">Fig. 10 (a)</ref>, we observe a clearer separation between the organization-related semantic types with little perturbation. The results qualitatively confirm that topicaware prediction helps Sato distinguish semantically similar semantic types by capturing the table context of an input table. Note that these column embeddings are from the test set, and any label information from these columns was not used to obtain the column embeddings. Thus, we can also confirm that Sato appropriately generalizes and learns column embeddings for these semantic types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Qualitative analysis</head><p>To better understand how structured prediction further helped Sato with the existence of topic-aware predictions, we conducted qualitative analysis by identifying examples where table-wise prediction "salvages" bad predictions in the column-wise (i.e., Base and SatonoStruct) predictions.   <ref type="table" target="#tab_1">Table 4</ref>: Examples of the mispredictions that are corrected by performing a structured prediction using the linear-chain CRF. <ref type="table" target="#tab_1">Table 4a</ref> shows a selected set of example tables from the test sets where the incorrect predictions from the Base model are corrected by applying structured prediction using our trained CRF layer. For example, with table #4575, the columns company and sales were incorrectly predicted as name and duration by the single-column Base model. By modeling inter-column dependencies, SatonoTopic correctly predicts the types company and sales, which tend to cooccur more with surrounding columns symbol and isbn for tables about books and magazines. <ref type="table" target="#tab_1">Table 4b</ref> shows examples where SatonoStruct made incorrect predictions using table values and was subsequently corrected by the use of structured prediction (i.e., Sato). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>Using learned representations Sato's single column prediction module based on Sherlock incorporates four categories of features that characterize different aspects of column values, amassing more than 1.5K feature values per column. However, the availability of large-scale table corpora presents a unique opportunity to develop pre-trained representation models and eschew manual feature extraction. To test the viability of using representation models, we fine-tuned the BERT model <ref type="bibr" target="#b11">[12]</ref>, a state-of-the-art model for language representation, for our semantic type detection task. Models based on fine-tuning BERT have recently improved prior art on several NLP benchmarks without manual featurization <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. We trained the BERT model using the default BERT parameters, achieving a supportweighted F1 score of 0.866, which is slightly better than 0.852 achieved by the Sherlock model. This result is promising because a "featurization-free" method with default parameters is able to achieve a prediction accuracy comparable to that of Sherlock. However, our multi-column prediction still outperforms the BERT model by a large margin, indicating the importance of incorporating table context into column type prediction. A promising avenue of future research is to combine our multi-column model with BERTlike pre-trained learned representation models.</p><p>Exploiting type hierarchy through ontology In this paper, we consider semantic types without hierarchy. However, it is possible to form natural parent-child relationships between many types. For instance, country and city are types (subclasses) of location and club and company are types of organization. Factoring hierarchical type relations into prediction (e.g., <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45]</ref>) requires an ontology codifying the type hierarchy and, crucially, additional annotation over training dataset, which can be infeasible to manually carry out for large training datasets such as the one used here. Nevertheless, modeling and predicting hierarchical semantic types can provide richer information for downstream tasks. It can also further improve the prediction accuracy by leveraging the additional structure afforded by hierarchical relations, especially for the types that have small numbers of training samples.</p><p>High-order CRFs Several studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref> developed high-order CRF models that implement potential functions that take n (n &gt; 2) predictions into account. However, the computational complexity of exact inference steps for training and prediction becomes exponentially expensive: O(L K ), where L is the input sequence length (i.e., # of columns) and K is the number of states (i.e., # of semantic types.) The computational cost is significantly expensive compared to the original linear-chain CRFs O(KL 2 ). As Sato with the linear-chain CRF model significantly improved the performance for the semantic type detection task, we decided not to use the degree of the order for efficiency.</p><p>Additionally, we believe that high-order dependency between predictions is not always necessary if we incorporate contextual features into the model. <ref type="bibr" target="#b35">[36]</ref> shows that contextual features that take into account surrounding information are more useful than a high-order CRF architecture for named entity recognition tasks. Since table topic features provide table-wise contextual information, we consider the original CRF model with pairwise potential functions as the right choice for improving the model accuracy efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>Regular expression and dictionary lookup Semantic type detection enhances the functionality of commercial data preparation and analysis systems such as Microsoft Power BI <ref type="bibr" target="#b31">[32]</ref>, Trifacta <ref type="bibr" target="#b45">[46]</ref>, and Google Data Studio <ref type="bibr" target="#b16">[17]</ref>. These commercial tools typically rely on manually defined rulebased approaches such as regular expression patterns dictionary lookups to detect semantic types. For instance, Trifacta detects around 10 types and Power BI only supports time-related semantic types. Open source libraries such as messytables <ref type="bibr" target="#b13">[14]</ref>, and csvkit <ref type="bibr" target="#b17">[18]</ref> similarly use heuristics to detect a limited set of types.</p><p>Ontology-based Prior work, with roots in the semantic web and schema matching literature, provide alternative approaches to semantic type detection. One body of work leverages existing data on the web, such as WebTables <ref type="bibr" target="#b7">[8]</ref>, and ontologies (or, knowledge bases) such as DBPedia <ref type="bibr" target="#b1">[2]</ref>, Wikitology <ref type="bibr" target="#b42">[43]</ref>, and Freebase <ref type="bibr" target="#b6">[7]</ref>. Venetis et al. <ref type="bibr" target="#b47">[48]</ref> construct a database of value-type mappings, then assign types using a maximum likelihood estimator based on column values. Syed et al. <ref type="bibr" target="#b42">[43]</ref> use column headers and values to build a Wikitology query mapping columns to types.</p><p>Statistical similarity Several earlier approaches rely on statistical similarity or other measures of data similarity to match columns with types. Ramnandan et al. <ref type="bibr" target="#b39">[40]</ref> first separate numerical and textual column types, then compare column values to those with labels from a dataset using the Kolmogorov-Smirnov (K-S) test and Term Frequency-Inverse Document Frequency (TF-IDF,) respectively. Pham et al. <ref type="bibr" target="#b33">[34]</ref> use additional features and tests, including the Mann-Whitney test for numerical data and Jaccard similarity for textual data, to train logistic regression and random forest models.</p><p>Synthesized Puranik <ref type="bibr" target="#b34">[35]</ref> proposes combining the predictions of "experts," including regular expressions, dictionaries, and machine learning models. More recently, Yan and He <ref type="bibr" target="#b49">[50]</ref> introduced a system that, given a search keyword and a set of positive examples, synthesizes type detection logic from open source GitHub repositories. This system provides a novel approach to leveraging domain-specific heuristics for parsing, validating, and transforming semantic types.</p><p>Learned Another line of prior work employs machine learning, including probabilistic graphical models. Goel et al. <ref type="bibr" target="#b15">[16]</ref> split each cell (field) value in a table into tokens and attempted to predict the field and token labels using CRF models with different graph structures capturing dependencies among tokens and fields. For instance, a cell value 'Mountain View, CA' is split into a sequence of tokens 'Mountain', 'View',',', 'CA'. Then a multi-layer CRF model is used to assign labels cityName, cityName, symbol, and state for those tokens along with the cell label place. This approach requires curating cell-and token-level annotations for training, which is impractical for large-scale table corpora. Furthermore, it has limited robustness over missing, dirty, and heterogeneous data, as well as semantic data types with highly variable formatting. Sato avoids the need for fine-grained token-level annotations and only uses automatically annotated column level semantic types, relying on the power of a deep neural network and word embeddings to capture cell-level information.</p><p>Limaye et al. <ref type="bibr" target="#b28">[29]</ref> use a Markov random field (MRF) model to annotate values with entities, columns with types, and column pairs with relationships. This approach assumes the existence of a catalog specifying entities, types, and relations between them and relies on good matches between entity lemma and cell text to make accurate predictions of both cell and column types. However, in practice, an accurate catalog can be expensive or impossible to obtain for large corpora or new domains and many tables have missing or noisy (incomprehensible, malformed, etc.) headers. Takeoka et al. <ref type="bibr" target="#b44">[45]</ref> extend Limaye et al. <ref type="bibr" target="#b28">[29]</ref>'s work with multi-label classifiers to support additional types, including numerical data types, and improve its predictive performance. However, this approach also relies on training data (183 tables) collected through human annotation and its application to massive table corpora can get extremely expensive.</p><p>Similar to earlier approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45]</ref> discussed above, Sato also uses a probabilistic graphical model for structured output prediction. However, in contrast to this earlier work, Sato employs a CRF model to combine the topic-aware predictions of a large-scale deep learning model, leveraging a large number of real-world tables for training. These tables are automatically annotated without resorting to human labeling, which makes Sato easier to extend and scale than prior work using probabilistic graphical models.</p><p>Although prior research used shallow neural networks for related tasks (e.g., <ref type="bibr" target="#b27">[28]</ref>), Sherlock <ref type="bibr" target="#b21">[22]</ref> is the first deep learning model directly applied to semantic type detection for table columns. Trained on a large number of columns, Sherlock uses a multi-input neural network to make type prediction based on features of column values. Sato builds on Sherlock and addresses its two related drawbacks; the low prediction accuracy for underrepresented types and the lack of consideration for table context in prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>Automated semantic typing is becoming more important than ever due to a rapid increase in the demand for better data preparation tools. The semantics of a table column (or any other data source for that matter) are embodied by its context as well as its raw data values. Here, we introduce Sato to automatically detect the semantic types of table columns, leveraging the signals from the table context of columns as well as the data values of columns. Sato combines the power of large-scale deep learning together with structured prediction and topic modeling to achieve a prediction performance that significantly exceeds the state-ofthe-art. Through ablation and permutation experiments, we evaluate Sato extensively and show how individual modeling choices as well as feature types contribute to the performance. To facilitate future applications and extended research, we are publicly releasing our trained model and source code for training along with an interactive web application demonstrating Sato's use at https://github.com/ megagonlabs/sato.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">ACKNOWLEDGMENTS</head><p>We thank Jonathan Engel for suggesting the name Sato and his proofreading help. We also thank Kevin Hu for his help in making the Sherlock source code accessible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two actual tables with unknown column types</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>s s ta te c a te g o ry w e ig h t c o d e c lu b a rt is t re s u lt p o s it io n c o u n tr y n o te s c la s s c n a re a s e rv ic e te a m N a m e o rd e r is b n fi le S iz e g ra d e s p u b li s h e r p la y s o ri g in e le v a ti o n a ff il ia ti o n c o m p o n e n t o w n e r g e n re m a n u fa c tu re r b ra n d fa m il y c re d it d e p th c la s s if ic a ti o n c o ll e c ti o n s p e c ie s c o m m a n d n a ti o n a li ty c u rr e n c y ra n g e a ff il ia te b ir th D a te ra n k in g c a p a c it y b ir th P la c e p e rs o n c re a to r o p e ra to r re li g io n e d u c a ti o n re q u ir e m e n t d ir e c to r s a le s c o n ti n e n t o rg a n is a ti o n Counts of the 78 semantic types in the dataset D form a long-tailed distribution. Sato improves the prediction accuracy for the types with fewer samples (those in the long-tail) by effectively incorporating table context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Co-occurrence frequencies in log scale for a selected set of types. Certain pairs like (city, state) or (age, weight) appear in the same table more frequently than others. There are non-zero diagonal values as tables can have multiple columns of the same semantic type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>P la c e e le v a ti o n n o te s o ri g in c re d it o w n e r b ra n d p ro d u c t re g io n n a ti o n a li ty ra n g e p la y s s p e c ie s o rd e r c la s s if ic a ti o n c re a to r a re a m a n u fa c tu re r c a p a c it y c o m m a n d s e rv ic e a ff il ia te d ir e c to r p e rs o n ra n k in g b ir th D a te c o n ti n e n t g ra d e s c o ll e c ti o n o p e ra to r p u b li s h e r c u rr e n c y fa m il y d e p th e d u c a ti o n in d u s tr y re q u ir e m e n t a ff il ia ti o n s e x la n g u a g e d u ra ti o n c o m p o n e n t g e n re is bp ro d u c t o p e ra to r jo c k e y p u b li s h e r e le v a ti o n c la s s c o m p o n e n t c re d it ra n g e la n g u a g e is b n fi le S iz e o w n e r re g io n n o te s p la y s c la s s if ic a ti o n c re a to r b ra n d fa m il y te a m N a m e s p e c ie s n a ti o n a li ty a re a re q u ir e m e n t m a n u fa c tu re r c o m m a n d s e rv ic e c a p a c it y d e p th o rd e r a ff il ia te d ir e c to r p e rs o n ra n k in g s a le s g ra d e s in d u s tr y c u rr e n c y c o n ti n e n t e d u c a ti o n a ff il ia ti o n c o d e s e x s ta tu s c lu b a d d re s s s y m b o</head><label></label><figDesc>SatonoStruct vs. Base</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>F1 scores for each type obtained with (blue) and without (orange) topic-aware prediction. (a) compares Sato and SatonoTopic (Sato without the topic-aware module), (b) compares SatonoStruct (Base with topic) and Base, showing improvements on the majority of types. The effect is significant for many underrepresented types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>te re s u lt c o d e d e s c ri p ti o n c lu b s ta tu s e le v a ti o n s e x ra n k n a m e p o s it io n a ff il ia ti o n c it y c a te g o ry s y m b o l c o m p a n y te a m N a m e a d d re s s te a m c re d it a lb u m c o n ti n e n t a rt is t c o u n tr y re g io n fi le S iz e p la y s n o te s o rd e r s a le s c o u n ty ty p e p ro d u c t o w n e r c re a to r o ri g in c o m p o n e n t p u b li s h e r c la s s la n g u a g e b ra n d re q u ir e m e n t d e p th a ff il ia te b ir th D a te d ir e c to r g ra d e s b ir th P la c e jo c k e y o p e ra to r d a y ra n g e d u ra ti o n g e n re n a ti o n a li ty m a n u fa c tu re r s p e c ie s a re a c a p a c it y c la s s if ic a ti o n c u rr e n c y is b n fa m il y s e rv ic e p e rs o n ra n k in g e d u c a ti o n c o ll e c ti o n in d u s tr y y e a r lo c a ti o n c o m m a nk e y p u b li s h e r e le v a ti o n c la s s c o m p o n e n t c re d it la n g u a g e is b n fi le S iz e o w n e r re g io n n o te s p la y s b ra n d fa m il y te a m N a m e s p e c ie s n a ti o n a li ty a re a re q u ir e m e n t m a n u fa c tu re r c o m m a n d c a p a c it y d e p th o rd e r s a le s c o n ti n e n t e d u c a ti o n g ra d e s in d u s tr y a ff il ia ti o n s e x c u rr e n c y ra n g e c la s s if ic a ti o n c re a to r s e rv ic e a ff il ia te d ir e c to r p e rs o n ra n k in g a d d re s s re s u lt o ri g in p ro d u c</head><label></label><figDesc>ti o n g e n re b ir th P la c e ty p e o p e ra to r jo c SatonoTopic vs. Base</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>F1 scores for each type obtained with (blue) and without (orange) structured prediction (a) compares Sato and SatonoStruct (Sato without the structured prediction module), (b) compares SatonoTopic (Base with structured prediction) and Base, showing improvements on the majority of types. Although the improvements on long-tail types are less significant compared to the topic-aware model inFig. 7, fewer types get worse predictions (shown in the right panels). Structured prediction can correct mispredictions by directly modeling column relationships. outperforms the column-wise models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table A :</head><label>A</label><figDesc>Influential people in historyTable B: Cities in Europe</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table Intent</head><label>Intent</label><figDesc></figDesc><table><row><cell></cell><cell>Sato</cell></row><row><cell></cell><cell></cell><cell>Multi-column</cell></row><row><cell></cell><cell></cell><cell>Model</cell></row><row><cell>Topic-aware Model</cell><cell>?</cell></row><row><cell cols="2">Topic Subnetwork</cell><cell>Column-wise Model</cell></row><row><cell></cell><cell></cell><cell>Feature</cell></row><row><cell cols="2">Estimator</cell><cell>Extraction</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table semantics Tables are collections of related data en- tities organized in rows. To incorporate table semantics in our model, we build on an intuition by Venetis et al. [48] that a user constructing a table has a particular intent or schema in mind. We extend this intuition and argue that semantic types of the columns in a table can be considered a meaning- ful expression (or utterance) of the user intent. Each</head><label>semantics</label><figDesc>column of the table partially fulfills the intent by describing one attribute of the entities. As illustrated inFig. 3a, the intent of a table is a latent component determining the semantic types of the columns in the table, which in turn generates the column values. We refer to the set of all column values ina table as table values.</figDesc><table><row><cell>Thus, being able to accurately infer the table intent can</cell></row><row><cell>help to improve the prediction of column semantics. Table</cell></row><row><cell>captions or titles usually capture table intent. For exam-</cell></row><row><cell>ple, in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>table .</head><label>.</label><figDesc>Using the gensim [41] library, we train an LDA model with 400 topics on a separate dataset of 10K tables. With the pre-trained LDA, we can extract topic vectors for tables using values from the entire table as input. Every table has a single topic vector, shared across columns.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 1 :</head><label>1</label><figDesc>compares the full Sato against Sato without table values (i.e., SatonoTopic,) and Fig. 7b compares SatonoStruct (only topic-aware model) Multi-column tables D mult ?0.015 0.879 ?0.002 0.692 ?0.007 0.867 ?0.003 Sato 0.735 ?0.022 (14.4%?) 0.925 ?0.003 (5.3%?) 0.756 ?0.011 (9.3%?) 0.902 ?0.002 (4.0%?) SatonoStruct 0.713 ?0.025 (11.0%?) 0.909 ?0.002 (3.5%?) 0.746 ?0.011 (7.8%?) 0.891 ?0.003 (2.8%?) SatonoTopic 0.681 ?0.016 (6.6%?) 0.907 ?0.002 (3.2%?) 0.711 ?0.006 (2.9%?) 0.884 ?0.002 (2.0%?) Performance comparison of the methods across the datasets D mult (multi-column only) and D (the full dataset) Numbers are the average values over a 5-fold cross validation. ? denotes 95% CI. () shows the relative improvements in percentage over Base. We conducted statistical tests using paired t-test with Bonferroni correction for multiple comparisons. Sato, SatonoStruct, SatonoTopic perform significantly better than Base (p &lt; .005 in all metrics.) Sato performs significantly better than SatonoStruct (p &lt; .005 in all metrics) and SatonoTopic (p &lt; .005 on D mult and n.s. on D.)</figDesc><table><row><cell>All tables D</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>? 0.04 Sato 678.5 ? 15.1 366.9 ? 66.<ref type="bibr" target="#b7">8</ref> 5.2 ? 0.06</figDesc><table><row><cell cols="2">Training time [s]</cell><cell>Prediction time [s]</cell></row><row><cell>Features</cell><cell>Structured</cell><cell></cell></row><row><cell>Base 596.9 ? 9.2</cell><cell>N/A</cell><cell>3.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 2 :</head><label>2</label><figDesc>Average training and prediction time over 5 trials D mult . ? denotes 95% CI. Training time for the columnwise features (Features) and the CRF layer (Structured) is reported separately.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table ID</head><label>ID</label><figDesc></figDesc><table><row><cell></cell><cell>True Columns</cell><cell cols="2">Base (w/o structured prediction) SatonoTopic (w/ structured prediction)</cell></row><row><cell>6299</cell><cell>code, name, city</cell><cell>symbol, team, city</cell><cell>code, name, city</cell></row><row><cell>898</cell><cell>company, location</cell><cell>name, city</cell><cell>company, location</cell></row><row><cell>4575</cell><cell cols="2">symbol, company, isbn, sales symbol, name, isbn, duration</cell><cell>symbol, company, isbn, sales</cell></row><row><cell>5712</cell><cell>type, description</cell><cell>weight, name</cell><cell>type, description</cell></row><row><cell>3865</cell><cell>year, teamName, age</cell><cell>year, city, weight</cell><cell>year, teamName, age</cell></row><row><cell></cell><cell></cell><cell>(a) Corrected tables from Base predictions</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table ID</head><label>ID</label><figDesc></figDesc><table><row><cell></cell><cell>True Columns</cell><cell cols="2">SatonoStruct (w/o structured prediction) Sato (w/ structured prediction)</cell></row><row><cell>410</cell><cell>brand, weight</cell><cell>artist, code</cell><cell>brand, weight</cell></row><row><cell>5655</cell><cell>code, name, city</cell><cell>club, name, name</cell><cell>code, name, city</cell></row><row><cell>4369</cell><cell>day, location, notes</cell><cell>name, location, location</cell><cell>name, location, notes</cell></row><row><cell>30</cell><cell cols="2">language, name, origin language, name, description</cell><cell>language, name, origin</cell></row><row><cell>4531</cell><cell>rank, name, city</cell><cell>rank, location, location</cell><cell>rank, location, city</cell></row><row><cell></cell><cell></cell><cell>(b) Corrected tables from SatonoStruct predictions</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table #4369</head><label>#4369</label><figDesc>and table #4531 are examples where location-related vocabulary in tables made a large impact. It produced overly aggressive predictions with multiple location columns, whereas Sato with the additional structured inference step successfully corrected one of the columns. Furthermore, considering surrounding types, structured prediction effectively improves performance for numerical columns like duration/sales from table #4575, age/weight from table #3865, code/weight from table #410.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://webdatacommons.org/webtables/ goldstandardV2.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/mitmedialab/sherlock-project</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Permutation importance: a corrected feature importance measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tolo?i</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lengauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1340" to="1347" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Predicting structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bak?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling annotated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Webtables: Exploring the power of tables on the web. VLDB</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="538" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aurum: A data discovery system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Castro</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Abedjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Koko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="page" from="1001" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Seeping semantics: Linking datasets using word embeddings for data discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Castro</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Qahtan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional random field with high-order dependencies for sequence labeling and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="981" to="1009" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Foundation</surname></persName>
		</author>
		<title level="m">Messytables ? pypi</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Markov random field image models and their applications to computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Graffigne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international congress of mathematicians</title>
		<meeting>the international congress of mathematicians</meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploiting structure within data for accurate labeling using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Google Data Studio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">C. Groskopf and contributors. csvkit</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8" to="12" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="80" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Viznet: Towards a large-scale visualization learning and benchmarking repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gaikwad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Demiralp</surname></persName>
		</author>
		<editor>CHI. ACM</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sherlock: A deep learning approach to semantic data type detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1500" to="1508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wrangler: Interactive visual specification of data transformation scripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paepcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning the structure of variable-order CRFs: a finite-state perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP &apos;17</title>
		<meeting>EMNLP &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="433" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic integration in heterogeneous databases using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clifton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Annotating and searching web tables using entities, types and relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Limaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>VLDB</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1338" to="1347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fine-tune BERT for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10318</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Power BI -Interactive Data Visualization BI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic labeling: a domain-independent approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szekely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="446" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A specialist approach for classification of column data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>Puranik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-08" />
			<pubPlace>Baltimore County</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparse higher order conditional random fields for improved sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML &apos;09</title>
		<meeting>ICML &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">849856</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey of approaches to automatic schema matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDBJ</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="334" to="350" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Potter&apos;s wheel: An interactive data cleaning system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="381" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Assigning semantic labels to data sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ramnandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szekely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="403" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>?eh??ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC Workshop</title>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Exploiting a web of semantic data for interpreting tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mulwad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<editor>WebSci</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Tableau</forename><surname>Tableau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desktop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Meimei: An efficient probabilistic approach for semantically annotating tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oyamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okadome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trifacta</surname></persName>
		</author>
		<title level="m">Data Wrangling Tools &amp; Software</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Visualizing data using t-SNE. JMLR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Venetis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pa?ca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Recovering semantics of tables on the web. VLDB</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="528" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Synthesizing type-detection logic for rich semantic data types using open-source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discovering regions of different functions in a city using human mobility and pois</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="186" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
