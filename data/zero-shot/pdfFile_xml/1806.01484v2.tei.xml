<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Human Pose Estimation with 2D Marginal Heatmaps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
							<email>anibali@students.latrobe.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">La Trobe University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">La Trobe University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">La Trobe University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">La Trobe University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Human Pose Estimation with 2D Marginal Heatmaps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatically determining three-dimensional human pose from monocular RGB image data is a challenging problem. The two-dimensional nature of the input results in intrinsic ambiguities which make inferring depth particularly difficult. Recently, researchers have demonstrated that the flexible statistical modelling capabilities of deep neural networks are sufficient to make such inferences with reasonable accuracy. However, many of these models use coordinate output techniques which are memory-intensive, not differentiable, and/or do not spatially generalise well. We propose improvements to 3D coordinate prediction which avoid the aforementioned undesirable traits by predicting 2D marginal heatmaps under an augmented soft-argmax scheme. Our resulting model, MargiPose, produces visually coherent heatmaps whilst maintaining differentiability. We are also able to achieve state-of-the-art accuracy on publicly available 3D human pose estimation data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Capturing the three-dimensional locations of a person's skeletal joints has varied applications in areas such as animation <ref type="bibr" target="#b49">[22]</ref>, video game control <ref type="bibr" target="#b50">[23]</ref>, and physical rehabilitation <ref type="bibr" target="#b29">[2]</ref>. Current standard practice is to use specialised equipment to acquire the pose of human subjects, such as wearable motion sensors, depth cameras (e.g. Microsoft Kinect <ref type="bibr" target="#b53">[25]</ref>), or marker-based motion capture systems. Specialised equipment can be expensive and restrictive, requiring a specific set up and calibration to be effective. Recently, effort has gone into constructing models which infer joint locations using only monocular RGB images taken with a standard camera. Such models can dramatically improve the accessibility and usability of pose estimation technology.</p><p>Inferring the three-dimensional pose of a human subject from a monocular image is an inherently under-constrained problem, with the primary source of ambiguity being a lack of explicit depth information in the image. However, humans are able to manually recreate the three-dimensional pose depicted in a photograph by drawing upon a wealth of prior knowledge and "intuition" about visual depth cues, permissible joint rotations, and likely limb lengths <ref type="bibr" target="#b42">[15]</ref>. Engineering an algorithm by hand to emulate this behaviour explicitly has proven much more difficult.</p><p>In contrast to hand-crafted algorithms, deep learning approaches are able to implicitly learn rich statistical relationships between input and output data. This makes it possible to address the under-constrained nature of 3D pose estimation by exploiting patterns contained within training examples, partially encoding the elusive "prior knowledge" possessed by humans about likely poses into model weights. Systems based on deep convolutional neural networks (CNNs) currently achieve state-of-the-art performance for both 2D and 3D pose estimation, as is evidenced by results for popular benchmark datasets such as MPII Human Pose <ref type="bibr" target="#b28">[1]</ref> and Human3.6M <ref type="bibr" target="#b36">[9]</ref>.</p><p>Most existing CNN-based models for 3D pose estimation use either fully connected output layers or volumetric heatmaps to form joint location predictions. Fully connected layers lack the inherent spatial equivariance required for good generalisation, and volumetric heatmaps can be memory-intensive to produce.</p><p>We propose MargiPose <ref type="figure" target="#fig_0">(Figure 1</ref>), a highly effective CNN-based approach to 3D pose estimation. In contrast to existing solutions, our model uses 2D marginal heatmaps (in xy, xz, and zy space) to predict joint locations. The term "marginal heatmap" is an allusion to the marginalisation of the trivariate probability mass function represented by a volumetric heatmap. MargiPose's architecture is specifically designed to produce marginal heatmaps from monocular 2D RGB input, accounting for changes in coordinate space where appropriate. Since it does not produce memory-intensive volumetric heatmaps, the memory requirement of our model is comparable to that of 2D pose estimation models. Numerical coordinate values are calculated from the heatmaps using soft-argmax <ref type="bibr" target="#b30">[3,</ref><ref type="bibr" target="#b40">13,</ref><ref type="bibr" target="#b61">33]</ref> with a regularisation term which improves the coherence and interpretability of learnt heatmap representations. Through experimental evaluation we find that MargiPose achieves state-of-the-art 3D pose estimation results on the MPI-INF-3DHP dataset and is highly competitive on the Human3.6M dataset.</p><p>Our contributions. Firstly, we extend the heatmap-based output strategies commonly found in 2D pose estimation to the task of 3D pose estimation by predicting three twodimensional marginal heatmaps per joint. This is more memory efficient than the existing heatmap-based 3D pose estimation approaches which represent heatmaps with volumetric activations.</p><p>Secondly, we show that adding a regularisation term to minimise divergence from ideal Gaussian heatmaps leads to improved accuracy when using soft-argmax 3D joint location prediction. Furthermore, the resultant heatmaps are much more visually coherent and readily interpretable than those predicted by a model trained without regularisation.</p><p>Thirdly, we implement a novel CNN model architecture for monocular 3D human pose estimation 1 and perform evaluation to demonstrate state-of-the-art results on public datasets. We introduce a technique called axis permutation which manipulates activations in order to account for the discrepancy between input and output spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. 2D human pose estimation</head><p>DeepPose <ref type="bibr" target="#b59">[31]</ref> is one of the earliest CNN-based models for human pose estimation. The model architecture is mostly convolutional, with fully-connected output layers used to predict the joint coordinates directly as numerical values. DeepPose features a cascaded design, where predicted coordinates are repeatedly refined to produce more accurate results. The basic concept of cascading has carried forward into more recent 2D pose estimation architectures, such as Stacked Hourglass <ref type="bibr" target="#b46">[19]</ref>. The Stacked Hourglass architecture makes extensive use of residual connections <ref type="bibr" target="#b35">[8]</ref>, and is segmented into multiple stages which permit intermediate supervision to guide training. The high accuracy of Stacked Hourglass-as demonstrated by practical results on the MPII Human Pose dataset <ref type="bibr" target="#b28">[1]</ref>-has inspired subsequent work to build upon the architecture directly <ref type="bibr" target="#b31">[4]</ref><ref type="bibr" target="#b32">[5]</ref><ref type="bibr" target="#b33">[6]</ref><ref type="bibr" target="#b60">32]</ref>.</p><p>The overwhelming majority of 2D pose estimation models developed after DeepPose (including Stacked Hourglass) have ceased using fully-connected output, instead favouring the more accurate heatmap matching approach <ref type="bibr" target="#b58">[30]</ref>. Heatmap matching works by training a model to output spatial maps which indicate joint locations with high-valued pixels. During training, loss is calculated using the mean squared error between the output heatmap and an ideal target, 2D spherical Guassian mean-centred on the ground truth joint location. During inference, numerical coordinates are calculated from heatmaps through the use of a non-differentiable argmax operation. In some implementations, the pixels neighbouring the highest-valued pixel are also considered in order to make minor sub-pixel adjustments for higher precision results <ref type="bibr" target="#b46">[19]</ref>.</p><p>Recently, soft-argmax <ref type="bibr" target="#b30">[3,</ref><ref type="bibr" target="#b61">33]</ref> has emerged as a differentiable alternative to argmax for heatmap-based human pose estimation models <ref type="bibr" target="#b40">[13]</ref>. Since it is possible to backpropagate through the soft-argmax operation, a distance-based loss is applied directly to the predicted coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Monocular 3D human pose estimation</head><p>In 2D human pose estimation, coordinates predicted by the model are in the same xy coordinate space as the input, making it straightforward to construct a simple fully convolutional network which maps RGB inputs to xy heatmaps. However, for 3D pose estimation the output coordinates exist in a space with one more dimension than the input (xyz-space); an xy-space heatmap does not encode enough information to recover the depth of a pose joint.</p><p>Broadly speaking, researchers have reacted to this by either resorting to fully-connected output layers, or devising new ways of generating and utilising heatmaps in the context of 3D coordinate prediction. Fully-connected output. There are many 3D pose estimation systems which use fully-connected output layers <ref type="bibr" target="#b39">[12,</ref><ref type="bibr" target="#b43">16,</ref><ref type="bibr" target="#b57">29]</ref>. Although conceptually simple, the dense connections of fully connected layers undermine spatial equivariance in convolutional neural networks, which can hinder generalisation. This is exemplified by the inferior performance of such approaches in the context of 2D pose estimation. It is therefore desirable to explore techniques beyond fully-connected output for 3D pose estimation. Volumetric heatmaps. One way of extending the heatmap notion to 3D is by designing a model which produces volumetric heatmaps. A disadvantage of this approach is that the addition of another dimension to the spatial activations increases the memory requirements of the system considerably. Pavlakos et al. <ref type="bibr" target="#b47">[20]</ref> partially mitigate the im-pact on memory consumption by gradually building up the depth resolution of the activations throughout the network in a coarse-to-fine fashion. Luvizon et al. <ref type="bibr" target="#b41">[14]</ref> use the softargmax operation to calculate coordinates from volumetric heatmaps. In contrast to argmax, soft-argmax permits the use of low resolution heatmaps, which in turn lowers overall memory consumption for the model. Location maps. Mehta et al. <ref type="bibr" target="#b45">[18]</ref> introduce the concept of "location maps", which are spatial representations of location where each pixel contains an estimate of a particular coordinate's value. For the z-coordinate, this is conceptually similar to a depth map. Since the location maps are 2D, they can be produced by models that are less memory-intensive than for volumetric heatmaps. Unfortunately, evaluation on the Human3.6M dataset revealed that such an approach is not competitive with the accuracy of existing techniques, including volumetric heatmaps.</p><p>We therefore propose marginal heatmaps as an alternative output strategy for 3D coordinate prediction. Marginal heatmaps are two-dimensional, and hence do not require memory-intensive volumetric activations. Furthermore, we find that marginal heatmaps can be used to generate predictions on benchmark datasets that are highly competitive with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Marginal heatmaps with soft-argmax</head><p>In this section we will derive marginal heatmaps for use in 3D pose estimation from the volumetric heatmaps used by existing work <ref type="bibr" target="#b41">[14,</ref><ref type="bibr" target="#b47">20]</ref>. Volumetric heatmaps. Let X, Y , and Z be random variables corresponding to the x-, y-, and z-coordinates of the predicted location for a particular human pose joint in 3D space. Under the soft-argmax prediction strategy for pose estimation <ref type="bibr" target="#b40">[13]</ref>, the estimated location of the joint, ?, is taken to be the expectation of the random variables. That is,</p><formula xml:id="formula_0">? x = E [X], ? y = E [Y ], and ? z = E [Z].</formula><p>If we constrain X, Y , and Z to only cover the values of discrete locations within a cuboid, we can represent the trivariate probability mass function P (X = x, Y = y, Z = z) with a volumetric heatmap. The volumetric heatmap,?, is a depth?height?width tensor where each element represents the probability that the joint is located at the corresponding spatial location. All elements of the heatmap must be non-negative and sum to one in order to define a valid probability mass function. Equation 1 describes how the estimated location may be computed from? under the soft-argmax scheme. The coordinate indicator tensors X , Y, and Z consist of elements referencing their own x, y, and z coordinates respectively (that is, X i,j,k = k, Y i,j,k = j, and Z i,j,k = i). We denote the scalar product of vectorised tensors by ?, ? .    . It is a logical extension of the graphical representation for 2D soft-argmax first presented in <ref type="bibr" target="#b41">[14]</ref>. Marginal heatmaps. Volumetric heatmap-based approaches are functional, but three-dimensional model outputs typically imply high memory usage for neural networks. This drawback is identified and partially mitigated within existing work by reducing the resolution of the volumetric heatmaps in some way <ref type="bibr" target="#b41">[14,</ref><ref type="bibr" target="#b47">20]</ref>. However, it is possible to circumvent the issue entirely by using marginal heatmaps to avoid the explicit representation of volumetric heatmaps altogether.</p><formula xml:id="formula_1">? x = ? , X , ? y = ? , Y , ? z = ? , Z<label>(1)</label></formula><p>Consider the bivariate marginal probability mass functions P (X = x, Y = y), P (Y = y, Z = z), and P (X = x, Z = z), and their corresponding two-dimensional heatmap representations? (xy) ,? (zy) , and? <ref type="bibr">(xz)</ref> . Like?, these marginal heatmaps are sufficient for calculating the expected values of joint coordinates. Conceptually, the volumetric and marginal heatmaps are related as follows: More intuitively, the marginal heatmaps may be considered as "views" of?, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><formula xml:id="formula_2">H (xy) = i? i,:,: H (zy) = ( k? :,:,k ) T H (xz) = j? :,j,:<label>(2)</label></formula><formula xml:id="formula_3">E [X] E [Y ] E [Z] ? (xy) , X 1,:,: ? (xy) , Y :,:,1 ? (zy)T , Z :,:,1 ? (xz) , X :,1,: ? (zy)T , Y 1,:,: ? (xz) , Z :,1,:</formula><p>We can use ? x = ? (xy) , X 1,:,: , or equivalently ? x = ? (xz) , X :,1,: , to calculate a soft-argmax estimate for the</p><p>x-coordinate of the joint location. The yand z-coordinates may be estimated similarly with appropriate heatmaps, as shown in <ref type="table" target="#tab_0">Table 1</ref>. Using this formulation we only require a model which predicts three 2D heatmaps per joint, obviating the need to predict? directly. Such a model will typically require less memory than a volumetric equivalent. This is a notable difference from existing work using marginal distributions, which still produce three-dimensional heatmaps as an intermediate step <ref type="bibr" target="#b41">[14,</ref><ref type="bibr" target="#b48">21]</ref>.</p><p>Since the model is free to predict the marginal heatmaps independently, they will generally not be consistent with one another. That is to say, there is no guarantee that the rows in <ref type="table" target="#tab_0">Table 1</ref> will resolve to the same values. There are therefore two ways to calculate each of E [X], E [Y ], and E [Z], depending on which marginal heatmap is used. We address this by making coordinate predictions according to <ref type="bibr">Equation 3</ref>. Due to the xy heatmap having the same orientation as the input image, the xand y-coordinates are predicted using? (xy) alone. However, the z-coordinate is taken to be the average expectation obtained from the other two heatmaps to better estimate depth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model architecture</head><p>It is necessary to have a model capable of predicting marginal heatmaps in order to use the prediction strategy outlined in Section 3. Since pose estimation data is inherently spatial, convolutional layers are a natural foundation for the model.</p><p>The calculation performed by each convolutional layer is spatially local. That is, for any given output pixel, the value of that pixel is calculated using input pixels that are within a fixed spatial neighbourhood. This is appropriate when both the input and output images exist in the same coordinate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correspondence with input Heatmap Horizontal</head><p>Vertical</p><formula xml:id="formula_5">H (xy) Yes Ye? H (xz)</formula><p>Yes N? H <ref type="bibr">(zy)</ref> No Yes space and there is a correlation between the locations of input and output features. For example, in 2D pose estimation the output heatmaps and input RGB image both exist in xy coordinate space, and the ground truth target spherical Gaussians align with the joints in the input image. However, we require our model to not only output an xy heatmap,? (xy) , but also heatmaps that have one axis in the z-direction,? (zy) and? (xz) . This poses a challenge for convolution-based computation. Consider the case of predicting a heatmap in the zy-plane,? (zy) , from an input image in the xy-plane. In general, a location in the z-direction does not correspond to a location in the x-direction. This means that there may be quite some distance between visual evidence in the input image and the desired prediction location in the output image. Such an arrangement is generally not ideal for convolutional neural networks.</p><p>For 3D pose estimation, the spatial discrepancy is never along both axes at once ( <ref type="table" target="#tab_1">Table 2</ref> shows axis correspondences for each of the three heatmaps). It is therefore desirable to preserve spatial locality of computation along the appropriate axes. Axis permutation. By transposing the intermediate activations it is possible to permute the axis undergoing spatially-local calculations with the axis undergoing densely connected calculations. Therefore the model can be built using convolutional layers without depending on spatial correspondence between mismatched axes. This allows the model to aggregate depth cues into feature maps, which will then become pixel values along the z-axis. <ref type="figure" target="#fig_7">Figure 5</ref> illustrates the axis permutation operation for? <ref type="bibr">(zy)</ref> . Note that the permutation operation is simply a fixed manipulation of the activations, and does not add any parameters to the model. Overall model architecture. <ref type="figure" target="#fig_8">Figure 6</ref> illustrates the arrangement of residual blocks we used to produce heatmaps from image features. Residual blocks are constructed as per ResNet using "option C" shortcut connections <ref type="bibr" target="#b35">[8]</ref>. For the network paths predicting? (zy) and? (xz) , the axis permutation operation is applied mid-way through the stage.</p><p>The complete model is assembled according to <ref type="figure">Figure 4</ref>. Features are extracted from 256 ? 256 pixel input images using a truncated Inception v4 model <ref type="bibr" target="#b56">[28]</ref>. Multiple heatmap prediction stages are stacked together after the feature extractor to increase the capacity of the model. "Adapter"  <ref type="figure">Figure 4</ref>: The complete high-level model architecture. The internal structure of each stage is detailed in <ref type="figure" target="#fig_8">Figure 6</ref>. "Feature CNN" is a truncated Inception v4 model <ref type="bibr" target="#b56">[28]</ref>. Loss is computed at each stage output.   1 ? 1 convolution layers are placed in between the stages to combine the previous heatmap predictions into feature maps, which are added with the previous stage's input to form a large skip connection. This stacking technique is inspired by the Stacked Hourglass architecture for 2D pose estimation <ref type="bibr" target="#b46">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Joint location loss</head><p>The typical heatmap matching with argmax approach used by most current 2D pose estimation networks has proven to be effective for producing highly accurate predictions <ref type="bibr" target="#b46">[19,</ref><ref type="bibr" target="#b58">30]</ref>. The loss function used in this arrangement is simply the mean squared error between? (the predicted heatmap), andH, a synthetic heatmap drawn using a Gaussian centred on the ground truth joint location.</p><formula xml:id="formula_6">L HM = ? ?H 2 2 (4)</formula><p>Such a loss function provides a strong training signal to the network as a result of the pixel-wise output gradients. However, this approach to coordinate prediction has one major issue-the actual location of the joint is estimated using the non-differentiable argmax operation. This final calculation means that the resulting model is not entirely differentiable, and hence cannot be built upon and trained using end-to-end backpropagation. Furthermore, argmax causes the precision of the coordinates to be limited by the resolution of the heatmap. This severely impedes models which produce heatmaps at a low spatial resolution.</p><p>Soft-argmax <ref type="bibr" target="#b40">[13]</ref> is a differentiable alternative to argmax which does not have its precision tied to the resolution of heatmaps. In contrast to the heatmap matching approach, loss is applied to the coordinates directly and backpropagated through the soft-argmax calculation. For example, the 2 loss between the predicted joint location (?) and the ground truth joint location (?) may be used, as shown in <ref type="figure" target="#fig_7">Equation 5</ref>.</p><formula xml:id="formula_7">L 2 = ? ?? 2 where ? = E[?]<label>(5)</label></formula><p>There are many possible heatmaps which will minimise L 2 equally well. Factors such as the "spread" or "shape" of the heatmap do not affect the loss, provided that the expected value remains constant. Initially this might seem beneficial, as the model is free to represent heatmaps in whichever way facilitates more accurate predictions. However, in practice the lack of pixel-wise supervision results in weaker gradients which do not support training particularly well and can hinder test-set performance. Furthermore the 2 loss alone does not result in visually coherent heatmaps, as is shown in <ref type="figure">Figure 7a</ref>.  <ref type="figure">Figure 7</ref>: Right wrist joint xz heatmaps generated by models trained using (a) 2 loss only, and (b) 2 loss with Jensen-Shannon divergence regularisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Regularised soft-argmax</head><p>We propose to combine the strengths of heatmap matching and soft-argmax by introducing a regularisation term to the soft-argmax loss function which explicitly guides the form of predicted heatmaps. Such guidance dramatically improves the visual coherence of predicted heatmaps, as is shown in <ref type="figure">Figure 7b</ref>. As we will later demonstrate in our experimental results, introducing such a regularisation term also significantly improves prediction accuracy.</p><p>Let us once again return to the probabilistic interpretation of heatmaps. If we want to encourage heatmaps to mimic the shape of a specific probability distribution, we can minimise the the Jensen-Shannon divergence (JSD) <ref type="bibr" target="#b38">[11]</ref> from that particular distribution. As long as the mean of the target distribution matches the ground truth joint location, minimising such a divergence will not compete with minimising L 2 . Therefore the two objectives remain complementary and training is stable.</p><p>In accordance with existing work in 2D pose estimation, we use spherical Gaussians as targets for predicted heatmaps. <ref type="figure" target="#fig_8">Equation 6</ref> shows the complete hybrid per-joint loss function, where ? 2 is the variance of the target Gaussian. We find that setting ? = I (1 pixel) works well for 32 ? 32 pixel heatmaps.</p><formula xml:id="formula_8">L 3D = ? ?? 2 + JSD ? (xy) N (? xy , ? 2 ) + JSD ? (zy) N (? zy , ? 2 ) + JSD ? (xz) N (? xz , ? 2 )<label>(6)</label></formula><p>For examples which contain only 2D joint annotations, loss is applied to? (xy) only. That is, L 2D = ? xy ?? xy 2 + JSD ? (xy) N (? xy , ? 2 ) (7) In practice, the additional divergence-based loss term helps to make the heatmaps more coherent and interpretable. For example, <ref type="figure">Figure 7b</ref> clearly shows that the heatmap has a greater spread along the z-axis, which expresses that in this particular instance the z coordinate is less certain than the x coordinate. In contrast, soft-argmax without regularisation <ref type="figure">(Figure 7a</ref>) makes it much more difficult to interpret the output of the model. <ref type="figure" target="#fig_10">Figure 8</ref> depicts all three marginal heatmaps generated for a right ankle joint prediction by MargiPose after training with JSD regularisation. The effect of the regularisation is clear-each heatmap strongly resembles a Gaussian centred on the joint location. Applying the loss. Since MargiPose has multiple prediction stages, loss is calculated after each stage. Such intermediate supervision supplies the model with gradients closer to the input, providing more guidance for training early layers. The model can be trained on 3D-and 2D-annotated data simultaneously by switching between L 3D and L 2D on a per-example basis, then aggregating to form a batch loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>Our experimental evaluation is focussed on two publicly available datasets for 3D pose estimation: Human3.6M and MPI-INF-3DHP. We also make use of MPII Human Pose data to augment training. <ref type="bibr" target="#b28">[1]</ref> is a popular 2D pose estimation dataset comprised of still frames from YouTube videos. Each image contains at least one human subject that has been manually annotated with a 16-joint skeleton in 2D. Human3.6M <ref type="bibr" target="#b36">[9]</ref> consists of footage recorded in a lab environment. Each subject's joints were labelled in 3D using an automated, marker-based system. Human3.6M is very popular as a 3D pose benchmark, but has some important limitations. Firstly, models trained on this data can exploit the wearable markers as visual cues, and as such it is difficult to evaluate how well such a model would generalise to markerless situations. Secondly, Human3.6M is not representative of real-world scene variety due to each image containing the same background environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPII Human Pose</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP [17]</head><p>is a recent 3D pose dataset which overcomes some of the limitations of Human3.6M. All training video is taken in a lab environment against a greenscreen, but the test set contains a mixture of indoor and outdoor footage. Hence models must generalise beyond the green-screen lab environment in order to achieve high accuracy on the test set. Unlike Human3.6M, subjects in this dataset do not wear visible markers.</p><p>Annotations from all three datasets were unified using a canonical skeleton of 17 joints in order to better facilitate model training. The joints included in the canonical skeleton are head (top and front), neck, shoulders, elbows, wrists, hips, knees, ankles, pelvis, and spine (middle).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Data augmentation</head><p>Training examples were dynamically augmented using random adjustments to scale, position, rotation, colour, and horizontal flipping. Additionally, compositing was used on images from the MPI-INF-3DHP dataset to vary the appearance of clothing and backgrounds <ref type="bibr" target="#b44">[17,</ref><ref type="bibr" target="#b45">18]</ref>.</p><p>For each experimental configuration we report metrics using unaugmented test set examples. We additionally report results using the ten-crop test data augmentation scheme of Luvizon et al. <ref type="bibr" target="#b41">[14]</ref>. Such results are marked explicitly as "multi-crop".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Training</head><p>Separate models were trained for the Human3. Stochastic gradient descent with a momentum value of 0.9 was used to optimise the model parameters. The learning rate was varied according to the 1-cycle learning rate schedule <ref type="bibr" target="#b54">[26]</ref>, with LR max = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Evaluation protocols</head><p>For the Human3.6M dataset we adopt the popular evaluation protocol of using subjects S1, S5, S6, S7, and S8 for training, and evaluating on every 64 th frame for subjects S9 and S11 <ref type="bibr" target="#b41">[14,</ref><ref type="bibr" target="#b55">27]</ref>. The ground truth root joint depth is used to recover the depth of the predicted skeleton, which is necessary to break the ambiguity between depth and scale. The predicted and ground truth skeletons are then translated so that the root joints align with the origin before acquiring comparison metrics.</p><p>For the MPI-INF-3DHP dataset, we report results using universally-scaled skeletons (fixed scale of 920 mm knee-neck). Since the scale is known, the ground truth root joint depth is not used to find the absolute depth of the predicted skeleton. As with the Human3.6M dataset, skeletons are translated to the origin before comparison. We use the same subset of 14 joints as Mehta et al. <ref type="bibr" target="#b45">[18]</ref> for our evaluation (i.e. pelvis, spine, and front of head are excluded). The initial release of the MPI-INF-3DHP dataset used by related works <ref type="bibr" target="#b34">[7,</ref><ref type="bibr" target="#b37">10,</ref><ref type="bibr" target="#b45">18]</ref> had systematic errors in pose labels for test set subjects TS3 and TS4. We evaluate on the original, erroneous labels in order to compare with results reported by existing works, but also provide results on the corrected test set as a reference point for future models.</p><p>The main metrics considered are PCK (percentage of correct keypoints), MPJPE (mean per joint position error), and AUC (area under curve). PCK measures the percentage of predicted joint locations which are within 150mm of the ground truth. MPJPE measures the mean 2 distance between predicted and ground truth joint locations in millimetres. AUC measures the average PCK over a range of thresholds (0-150mm).</p><p>Some of our results are marked as using Procrustes alignment. In these instances the predicted pose skeletons are compared with the ground truth up to a similarity transform, which is useful for disentangling the local configuration of the pose from global positioning within the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Ablative study</head><p>An ablative study was conducted to investigate how much each component of the   <ref type="bibr" target="#b37">[10]</ref> 88.0 58.1 Rogez et al. <ref type="bibr" target="#b52">[24]</ref> 87.7 71.6 Mehta et al. <ref type="bibr" target="#b45">[18]</ref> 80.5 -Pavlakos et al. <ref type="bibr" target="#b47">[20]</ref> 71.9 51.9 Martinez et al. <ref type="bibr" target="#b43">[16]</ref> 62.9 47.7 Sun et al. <ref type="bibr" target="#b55">[27]</ref> 59.   <ref type="table" target="#tab_4">Table 3</ref>.</p><p>Enabling regularisation for soft-argmax had a very noticeable positive impact on accuracy, improving PCK by 1.8 percentage points and MPJPE by 6 mm. This finding provides compelling empirical evidence for the benefits of combining soft-argmax with a pixel-wise heatmap loss. Increasing the model capacity by raising the number of heatmap prediction stages from one to four resulted in an additional performance increase of similar magnitude.</p><p>Axis permutation was shown to be of only minor benefit within the context of our model. This is likely due to the effective receptive field of the network being large enough for spatially disparate locations to be bridged by convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Benchmark dataset results</head><p>We compare the results of our 3D pose estimation model to the results reported by a range of existing work on the Hu-man3.6M dataset. For these experiments our models were trained for 4.8 million iterations. <ref type="table" target="#tab_6">Table 4</ref> shows that Margi-Pose achieves the best Procrustes-aligned MPJPE of all systems which report this metric. This indicates that accurately inferring the local configuration of pose joints is a strength of our model. Without Procrustes alignment, MargiPose is still highly competitive with the state-of-the-art.</p><p>Evaluation   provide a better indication of model generalisation to realworld scenarios. On the MPI-INF-3DHP dataset (with the original, uncorrected test set labels), our model exhibits much higher accuracy than existing approaches ( <ref type="table" target="#tab_8">Table 5</ref>). In particular, MargiPose achieves a full 9.4 percentage points greater PCK than the next best model. We also report our results using the updated MPI-INF-3DHP test set labels in <ref type="table" target="#tab_9">Table 6</ref>. These results are intended to provide a baseline for future work to compare against. Qualitative results. In order to evaluate the ability of MargiPose to generalise to challenging "in-the-wild" images, we generated predictions for MPII test set examples using the model trained for MPI-INF-3DHP prediction. Since the MPII dataset does not include 3D annotations, we could not train a model on MPII data alone. <ref type="figure" target="#fig_11">Figure 9</ref> exhibits sample predictions generated by MargiPose. Despite all of the 3D-annotated training data originating from a laboratory environment, our model is able to generalise to a wide range of different situations and poses. Predictions are poorest for images containing high levels of distortion (e.g. motion blur), extreme occlusion, or other people nearby the subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>2D marginal heatmaps are a memory-efficient alternative to volumetric heatmaps when building models for 3D pose estimation. By extending the soft-argmax loss function with a regularisation term which guides the shape of heatmaps, such models can be trained effectively to produce accurate joint predictions. An interesting direction for future work would be to make more effective use of examples with 2Donly annotations, thus improving the ability of MargiPose to learn from varied environments and poses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>High-level system overview for MargiPose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Graphical description of the soft-argmax calculation for a volumetric heatmap. Darker colours indicate higher tensor element values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Multiple two-dimensional "views" of a spherical Gaussian from outside the volumetric heatmap can fully describe the location of a joint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2</head><label>2</label><figDesc>illustrates the 3D soft-argmax calculation described by Equation 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 ? 2 ?</head><label>22</label><figDesc>? x = ? (xy) , X 1,:,: ? y = ? (xy) , Y 1,:,: ? z = 1 (zy)T , Z :,:,1 + 1 (xz) , Z :,1,:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Using axis permutation on activations to transition from xy to zy space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>The internal structure of a heatmap prediction stage. Residual blocks are labelled with the number of output channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Right ankle heatmaps predicted by MargiPose for a test set example. The dashed crosshairs indicate the calculated expectation of the joint location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Good (left) and poor (right) 3D pose predictions generated by our model on MPII dataset images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>6M and MPI-INF-3DHP datasets in order to compare against existing work. Each training batch consists of 32 examples, with 16 samples drawn from the 3D dataset and 16 from the 2D MPII dataset. The visual diversity in the 2D MPII dataset examples aids generalisation at inference time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The expected value of each coordinate can be calculated from either of two marginal heatmaps.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Image axes correspondences between different output heatmap types and the input image in xy-space.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablative study results evaluated on the MPI-INF-3DHP test set with Procrustes alignment.</figDesc><table><row><cell>Method</cell><cell>MPJPE PA MPJPE</cell></row><row><cell>Kanazawa et al.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results on the Human3.6M dataset. Metrics are shown with and without Procrustes alignment (PA).</figDesc><table><row><cell>utes to pose estimation accuracy. For these experiments</cell></row><row><cell>our models were trained for 3.2 million iterations. Starting</cell></row><row><cell>with a simple model containing a single heatmap prediction</cell></row><row><cell>stage, additional components were enabled in a cumulative</cell></row><row><cell>fashion. Evaluation was performed on the MPI-INF-3DHP</cell></row><row><cell>test set with corrected labels and Procrustes alignment en-</cell></row><row><cell>abled. Our results are shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results on the MPI-INF-3DHP dataset (uncorrected labels) without Procrustes alignment.</figDesc><table><row><cell>Method</cell><cell cols="3">PCK MPJPE AUC</cell></row><row><cell>MargiPose</cell><cell>87.6</cell><cell>87.6</cell><cell>48.8</cell></row><row><cell>MargiPose (multi-crop)</cell><cell>88.3</cell><cell>85.2</cell><cell>49.6</cell></row><row><cell>MargiPose (PA)</cell><cell>94.8</cell><cell>61.6</cell><cell>61.4</cell></row><row><cell cols="2">MargiPose (PA, multi-crop) 95.1</cell><cell>60.1</cell><cell>62.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results on the MPI-INF-3DHP dataset (corrected labels). Metrics are shown with and without multi-crop evaluation and Procrustes alignment (PA).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">PyTorch code available at https://github.com/anibali/margipose</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Softmax Res. block</title>
		<imprint>
			<biblScope unit="page">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Block</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A kinectbased system for physical rehabilitation: A pilot study for young adults with motor disabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Research in developmental disabilities</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradient descent optimization of smoothed information retrieval metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">formation retrieval</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial PoseNet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV. IEEE</title>
		<meeting>ICCV. IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<title level="m">Structure-aware and temporally coherent 3D human pose estimation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Divergence measures based on the Shannon entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Inf. Theory</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recurrent 3D pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">2D/3D pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pictorial human spaces: A computational study on the human perception of 3D articulated poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV. IEEE</title>
		<meeting>ICCV. IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3DV</title>
		<meeting>3DV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">VNect: Real-time 3D human pose estimation with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Motion capture assisted animation: Texturing and synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Interactive motion mapping for real-time character control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobalt</surname></persName>
		</author>
		<editor>CGF. Wiley</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV. IEEE</title>
		<meeting>ICCV. IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to fuse 2D and 3D image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV. IEEE</title>
		<meeting>ICCV. IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV. IEEE</title>
		<meeting>ICCV. IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
