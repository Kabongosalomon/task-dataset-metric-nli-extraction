<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Competitive Pathway Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
							<email>yschen@cs.nctu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Competitive Pathway Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1-15</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNN</term>
					<term>object recognition</term>
					<term>competitive mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the design of deep neural architectures, recent studies have demonstrated the benefits of grouping subnetworks into a larger network. For examples, the Inception architecture integrates multi-scale subnetworks and the residual network can be regarded that a residual unit combines a residual subnetwork with an identity shortcut. In this work, we embrace this observation and propose the Competitive Pathway Network (CoPaNet). The CoPaNet comprises a stack of competitive pathway units and each unit contains multiple parallel residual-type subnetworks followed by a max operation for feature competition. This mechanism enhances the model capability by learning a variety of features in subnetworks. The proposed strategy explicitly shows that the features propagate through pathways in various routing patterns, which is referred to as pathway encoding of category information. Moreover, the cross-block shortcut can be added to the CoPaNet to encourage feature reuse. We evaluated the proposed CoPaNet on four object recognition benchmarks: CIFAR-10, CIFAR-100, SVHN, and ImageNet. CoPaNet obtained the state-of-the-art or comparable results using similar amounts of parameters. The code of CoPaNet is available at: https://github.com/JiaRenChang/CoPaNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks (CNNs) have been shown to be highly effective in image classification with large datasets, such as CIFAR-10/100 <ref type="bibr" target="#b9">(Krizhevsky and Hinton, 2009</ref>), SVHN <ref type="bibr" target="#b13">(Netzer et al., 2011)</ref>, and ImageNet <ref type="bibr" target="#b2">(Deng et al., 2009)</ref>. Improvements in computer hardware and network architectures have made it possible to train deeper and more complex networks.</p><p>Network grouping is an efficient technique to improve the accuracy in model learning. The Inception architecture <ref type="bibr" target="#b18">(Szegedy et al., 2015)</ref> was proposed to aggregate abundant features via multi-scale subnetworks. In addition, dueling architecture <ref type="bibr" target="#b21">(Wang et al., 2015)</ref> in deep reinforcement learning can explicitly exploit subnetworks to represent state value and action advantages. Recently, the Residual Networks (ResNets) <ref type="bibr" target="#b4">(He et al., 2015a</ref><ref type="bibr" target="#b6">(He et al., , 2016</ref> can be regarded that a residual unit includes an identity shortcut and a residual subnetwork. This approach can alleviate the vanishing gradient problem by bypassing the gradients without attenuation and thus can increase the network depth up to more than 100 layers. As suggested in <ref type="bibr" target="#b0">(Abdi and Nahavandi, 2016;</ref><ref type="bibr" target="#b8">Huang et al., 2016b;</ref><ref type="bibr" target="#b20">Veit et al., 2016)</ref>, ResNets gains its superior performance by implicitly averaging many subnetworks.</p><p>The redundancy problem of ResNets has been raised in <ref type="bibr" target="#b8">(Huang et al., 2016b;</ref><ref type="bibr" target="#b23">Zagoruyko and Komodakis, 2016)</ref>. Some studies primarily aimed at the improvement of the propaga-Linear "bird" "ship" tion in ResNet, thereby reducing the redundancy problem. Stochastic Depth <ref type="bibr" target="#b8">(Huang et al., 2016b)</ref> tackled this problem by randomly disabling residual units during training. Wide Residual Networks <ref type="bibr" target="#b23">(Zagoruyko and Komodakis, 2016)</ref> addressed this problem by decreasing the depth and increasing the width of residual units for faster training. Both of these network architectures are attempts to shorten the network and thereby improve information back-propagation during training. Without shortening network, a recent work <ref type="bibr" target="#b6">(He et al., 2016)</ref> analyzed various usages of rectified linear unit (ReLU) and batch normalization <ref type="bibr">(BN)</ref> in ResNets for direct propagation, and proposed methods for identity mapping in residual units to improve training in very deep ResNets. Some studies encouraged the direct feature reuse by replacing the element-wise addition in ResNets with concatenation. FractalNet <ref type="bibr" target="#b10">(Larsson et al., 2016)</ref> repeatedly combines many subnetworks in a fractal expansion rule to obtain large nominal network depth. DenseNet <ref type="bibr" target="#b7">(Huang et al., 2016a)</ref> is similar to FractalNet with the difference that DenseNet connects each layer to all of its preceding layers. These approaches exhibit a behavior of mimicking deep supervision, which is important to the learning of discriminative features.</p><p>Some studies aimed at the improvement of the residual units by representing the residual function with many tiny subnetworks. Inception-ResNet <ref type="bibr" target="#b19">(Szegedy et al., 2016)</ref> presented Inception-type residual units. PolyNet  replaces the original residual units with polynomial combination of Inception units for enhancing the structural diversity. Multi-residual networks <ref type="bibr" target="#b0">(Abdi and Nahavandi, 2016)</ref> and ResNeXt <ref type="bibr" target="#b22">(Xie et al., 2016)</ref> both aggregate residual transformations from many tiny subnetworks.</p><p>The idea behind the use of subnetworks is to simplify network for efficient training. By explicitly factoring the network into a series of operations, features can be learned independently. In this work, we embrace this observation and propose a novel deep architecture referred to as Competitive Pathway Network (CoPaNet). Because the underlying mapping function can be decomposed into the maximum of multiple simpler functions and the residual learning <ref type="bibr" target="#b4">(He et al., 2015a</ref>) is a good strategy for approximating the mapping functions, the proposed competitive pathway (CoPa) unit was designed to comprise multiple parallel residual-type subnetworks followed by a max operation for feature competition. Furthermore, identity cross-block shortcuts can be added to the CoPaNet to enhance feature reuse. These strategies offer several advantages: 1. Feature redundancy can be reduced by dropping unimportant features through competition. 2. The competitive mechanism facilitates the network to modularize itself into multiple parameter-sharing subnetworks for parameter efficiency <ref type="bibr" target="#b15">(Srivastava et al., 2013)</ref>. 3. CoPaNet uses residual-type subnetworks and therefore inherits the advantage of ResNet for training very deep network. 4. With competitive mechanism and residual-type subnetworks, the CoPaNet explicitly exhibits the property of pathway encoding, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Because the residual-type subnetwork can preserve feature identity such that the winning path can be traced back within the entire network. That is, the routing pattern of propagating features encodes category information. 5. The cross-block shortcuts encourage coarse feature reuse and implicit deep supervision.</p><p>CoPaNet was evaluated using several benchmark datasets such as CIFAR-10, CIFAR-100, SVHN, and ImageNet. Our resulting models performed equally to or better than the state-of-the-art methods on the above-mentioned benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Residual Networks (ResNets)</head><p>ResNets <ref type="bibr" target="#b4">(He et al., 2015a)</ref> are motivated by the counterintuitive observation that the performance of neural networks actually gets worse when developed to a very great depth. This problem can be attributed to the fact that the gradient vanishes when information back-propagates through many layers. <ref type="bibr" target="#b4">He et al. (2015a)</ref> proposed skipping some of the layers in convolutional networks through the implementation of shortcut connections, in the formulation of an architecture referred to as residual units. The original residual unit performs the following computation:</p><formula xml:id="formula_0">x l+1 = ReLU (id(x l ) + f l (x l )) ,</formula><p>where x l denotes the input feature of the l-th residual unit, id(x l ) performs identity mapping, and f l represents layers of the convolutional transformation of the l-th residual unit. <ref type="bibr" target="#b6">He et al. (2016)</ref> further suggested to replace ReLU with another identity mapping, allowing the information to be propagated directly. Thus, they proposed a pre-activation residual unit with the following form:</p><formula xml:id="formula_1">x l+1 = id(x l ) + f l (x l ) .</formula><p>Furthermore, the positions of BN and ReLU are changed to allow the gradients to be backpropagated without any transformation. Their experimental results demonstrated the high efficiency of pre-activation residual units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Competitive Nonlinear functions</head><p>Maxout Networks <ref type="bibr" target="#b3">(Goodfellow et al., 2013)</ref> were recently introduced to facilitate optimization and model averaging via Dropout. The authors of this work proposed a competitive nonlinearity referred to as maxout, which was constructed by obtaining the maximum across several maxout hidden pieces. Maxout Networks can be regarded as universal approximators and can provide better gradient back-propagation than other activation functions. Without down-sampling the features, Local Winner-Take-All (LWTA) <ref type="bibr" target="#b15">(Srivastava et al., 2013)</ref> was inspired by the characteristics of biological neural circuits. Each LWTA block contains several hidden neurons and produces an output vector determined by local competition between hidden neurons activations. Only the winning neuron retains its activation, whereas other hidden neurons are forced to shut off their activation. In empirical experiments, both network architectures have been shown to have advantages over ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Competitive Pathway Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Competitive pathway unit</head><p>CoPaNet is an attempt to separate model into subnetworks through competition. In the following, we refer to residual-type subnetworks as pathways. In a CoPa unit, multiple pathways are compiled in parallel and features are selected by using a max operation. A CoPa unit includes output x l+1 with K pathways, which can be formulated as follows:</p><formula xml:id="formula_2">x l+1 = max k?[1,...,K] z k l , where z k l = id(x l ) + h k l (x l ),</formula><p>x l is the input feature, and h k l (x l ) represents layers of transformations on the k-th pathway at the l-th CoPa unit. <ref type="figure" target="#fig_1">Figure 2a</ref> illustrates the CoPa unit (featuring two pathways) used in this paper.</p><p>Competitive pathways appear complex; however, the proposed CoPaNet is easy to train. Notice that residual learning <ref type="bibr" target="#b4">(He et al., 2015a)</ref> is based on the hypothesis that underlying mapping function H(x) is very hard to fit. Nevertheless, the mapping function can be decomposed into two simpler functions: <ref type="bibr" target="#b4">He et al. (2015a)</ref> claimed that the residual function F (x) is easier to approximate. Motivated by the idea of streamlining the process of approximating the underlying mapping function, we first decompose the underlying mapping function into the maximum of two simpler functions, that is, H(x) = max{f (x), g(x)}. We then use residual learning <ref type="bibr" target="#b4">(He et al., 2015a</ref>) and let f (x) = x + h 1 (x) and g(x) = x + h 2 (x). The desired mapping function becomes H(x) = max{x + h 1 (x), x + h 2 (x)}. This illustrates the need for two parallel networks (one each for h 1 (x) and h 2 (x)), each of which comprises several stacked layers in order to approximate discrete residual functions. Because f (x) and g(x) are simpler, it would be easier to approximate h 1 (x) and h 2 (x) than the original residual learning <ref type="bibr" target="#b4">(He et al., 2015a)</ref>. Our CoPa unit is different from maxout unit <ref type="bibr" target="#b3">(Goodfellow et al., 2013)</ref>. The original maxout unit is constructed to obtain the maximum across several elementary neurons. Our method replaces the elementary neurons with generic functions, which are modeled by ResNets.</p><formula xml:id="formula_3">H(x) = x + F (x).</formula><p>Further, the property of pathway encoding reveals in this architecture. We consider a 2-pathway (denote as h 1 l , h 2 l ) CoPaNet with three stacked CoPa units, as show in <ref type="figure" target="#fig_0">Figure 1</ref>. We denote that the output of the first CoPa unit is y</p><formula xml:id="formula_4">1 = x + h 1 1 (x) (if h 1 1 wins) where x is the input feature.</formula><p>The output of the second CoPa unit can be written as y 2 = y 1 + h 1 2 (y 1 ) (if h 1 2 wins). The output of the third CoPa unit can be written as y 3 = y 2 + h 2 3 (y 2 ) (if h 2 3 wins). The final output actually can be expressed as y 3 = x + h 1 1 (x) + h 1 2 (y 1 ) + h 2 3 (y 2 ). This indicates that the final output is contributed by three winning subnetworks h 1 1 , h 1 2 , h 2 3 with reference to x. Thus, the routing pattern can be revealed by propagating x through the entire network.</p><p>Within a biological context, competitive mechanisms play an important role in attention <ref type="bibr" target="#b11">(Lee et al., 1999)</ref>. Researchers formulated a biological computational model in which attention activates a winner-take-all competition among neurons tuned to different visual patterns. In this model, attention alters the thresholds used to detect orientations and spatial frequencies. This suggested that winner-take-all competition can be used to explain many of the basic perceptual consequences of attention <ref type="bibr" target="#b11">(Lee et al., 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CoPaNet Architecture</head><p>CoPaNets can be simply constructed by stacking CoPa units. Let the opponent factor k denote the number of pathway in a CoPa unit and the widening factor m multiplies the number of features in convolutional layers. That is, the baseline CoPa unit corresponds to k = 2, m = 1; whereas ResNet corresponds to k = 1, m = 1. <ref type="figure" target="#fig_1">Figure 2b</ref> shows the architecture for CIFAR and SVHN as well as <ref type="table" target="#tab_0">Table 1</ref> detailed the deployment. The residual shortcut in the proposed network performs identity mapping and the projection shortcut is used only to match dimensions (using 1?1 convolutions) as ResNet <ref type="bibr" target="#b4">(He et al., 2015a</ref><ref type="bibr" target="#b6">(He et al., , 2016</ref>. For each pathway, we adopted a "bottleneck" residual- type unit comprising three convolutional layers (1?1, 3?3, 1?1). Alternatively, we could select a "basic" residual-type unit comprising two convolutional layers (3?3, 3?3). In practice, a "bottleneck" residual-type unit is deeper than a "basic" one, providing higher dimensional features. In the proposed CoPaNet, we placed BN and ReLU after all but the last convolutional layer in every pathway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-block Shortcut</head><p>The cross-block shortcuts were motivated by DenseNet <ref type="bibr" target="#b7">(Huang et al., 2016a)</ref> which reused features from all previous layers with matching feature map sizes. In contrast to DenseNet <ref type="bibr" target="#b7">(Huang et al., 2016a)</ref>, we propose a novel feature reuse strategy: to reuse the features from previous CoPa block (stacked by many CoPa units). This is accomplished by adding identity shortcuts after pooling layers and concatenate with the output of the next block. We refer to our model with the cross-block shortcuts as CoPaNet-R, as shown in <ref type="figure" target="#fig_1">Figure 2c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We have tested the proposed CoPaNets and CoPaNets-R on several datasets, and compared the results with those of the state-of-the-art network architectures, especially ResNets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training</head><p>We constructed a CoPaNet-164, with a set number of pathways (k = 2), and network width (m = 1, 2, 4), detailed in <ref type="table" target="#tab_0">Table 1</ref>. Furthermore, we constructed a CoPaNet-R-164, with a set number of pathways (k = 2), and network width (m = 2, 3). The networks were trained from scratch by using Stochastic Gradient Descent with 300 and 20 epochs for CIFAR and SVHN datasets, respectively. The learning rate for CIFAR began at 0.1, divided by 10 at 0.6 and 0.8 fractions of the total number of training epochs. The learning rate for SVHN began at 0.1, divided by 10 at 0.5 and 0.75 fractions of the total number of training epochs. A batch size of 128 was used for all tests, except for m = 4 when we used a batch size of 64.</p><p>On ImageNet, we trained from scratch for 100 epochs. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we constructed several CoPaNets with 2 pathways for ImageNet. The learning rate began at 0.1 and was divided by 10 after every 30 epochs. The model was implemented using Torch7 from the Github repository fb.resnet.torch (https://github.com/facebook/fb.resnet. torch). Other settings were set exactly the same as those used for ResNet. We adopted a weight decay of 0.0001 and momentum of 0.9 as in <ref type="bibr" target="#b4">(He et al., 2015a)</ref>. Weights were initialized in accordance with the methods outlined by <ref type="bibr" target="#b5">He et al. (2015b)</ref>. We also applied Dropout <ref type="bibr" target="#b14">(Srivastava et al., 2014a)</ref> after the average poolings except the last pooling, and it was deterministically multiplied by (1 -Dropout-rate) at test time. The Dropout rate was set to 0.2 for CIFAR and SVHN as well as 0.1 for ImageNet. The test error was evaluated using the model obtained from the final epoch at the end of training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CIFAR-10</head><p>The CIFAR-10 dataset consists of natural color images, 32?32 pixels in size, from 10 classes, and with 50,000 training and 10,000 test images. Color normalization was performed as data preprocessing. To enable a comparison with previous works, the dataset was augmented by translation as well as random flipping on the fly throughout training. As shown in <ref type="table" target="#tab_1">Table 2</ref>, we obtained test error of 4.50%, 4.10%, and 3.74% when using network width of m = 1, 2, and 4, respectively. We then compared CoPaNet-164 (1.75 M, m = 1) to pre-activation ResNet-1001 (10.2 M), for which <ref type="bibr" target="#b6">He et al. (2016)</ref> reported test error of 4.62% (we obtained 4.87% in our training procedure). <ref type="figure" target="#fig_2">Figure 3a</ref> presents a comparison of training and testing curves. Furthermore, Our best result on CIFAR-10 was obtained by CoPaNet-R. We obtained 3.38% test error with only 15.7 M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">CIFAR-100</head><p>The CIFAR-100 dataset is the same size and format as CIFAR-10; however, it contains 100 classes. Thus, the number of images in each class is only one tenth that of CIFAR-10. Color normalization was performed as data preprocessing. We also performed data augmentation (translation and horizontal flipping) on the CIFAR-100 dataset.</p><p>As shown in <ref type="table" target="#tab_1">Table 2</ref>, we obtained the test error of 22.86%, 20.48%, and 18.67% for network width of m = 1, 2, and 4 with Dropout, respectively. CoPaNet-164 (1.75 M, m = 1) was compared to pre-activation ResNet-164 (1.7 M) for which <ref type="bibr" target="#b6">He et al. (2016)</ref> reported test error of 24.33%. This puts the proposed network on par with pre-activation ResNet-1001 (10.2 M) which achieved test error of 22.71%. However, CoPaNet-R showed few benefits on CIFAR-100, and it obtained same level of accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Competitive Pathway Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">SVHN</head><p>The SVHN dataset consists of color images of house numbers (32?32 pixels) collected from Google Street View. This includes 73,257 digits in the training set, 26,032 digits in the test set, and 531,131 in an extra set. We used the entire training set and extra set for training. We did not perform any data augmentation or preprocessing except for dividing the image intensity by 255. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the CoPaNet-164 (1.75 M, width m = 1) with test error of 1.86%. CoPaNet-R-164 (width m = 3) achieved the state-of-the-art results (1.58%) with only 15.7 M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">ImageNet</head><p>The ImageNet 2012 dataset consists of 1000 classes of images with 1.28 millions for training, 50,000 for validation, and 100,000 for testing. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we constructed twopathway CoPaNet with various depths for ImageNet. However, we reduce the number of feature maps to approximately 70% in order to retain a similar number of parameters. For a fair comparison, all results were achieved when the crop size was 224?224. Our results of single crop top-1 validation error showed better performance than ResNet, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. These results reveal that CoPaNets perform on par with the state-of-the-art ResNets, while requiring fewer parameters. CoPaNets performed worse than DenseNet with similar amounts of parameters. The major reason could be that DenseNets were much deeper than CoPaNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Parameter Efficiency</head><p>The competitive mechanism modularizes the network into multiple parameter-sharing subnetworks and thus can improve parameter efficiency <ref type="bibr" target="#b15">(Srivastava et al., 2013)</ref>. We trained multiple small networks with various depths on CIFAR-10+. As shown in <ref type="figure" target="#fig_2">Figure 3b</ref>, both Test error (%) <ref type="figure">Figure 5</ref>: The influence of the number of pathways on performance in experiments based on CIFAR-10+. More pathways tends to lower test errors at the expense of more parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of pathways</head><p>CoPaNet and its variant outperformed pre-activation ResNet. The CoPaNet-R achieved better performance than CoPaNet. When achieving the same level of accuracy, furthermore, CoPaNet requires around a half of the parameters of pre-activation ResNet. <ref type="figure">Figure 5</ref> demonstrates that CoPaNet has the capacity to exploit many pathways. We trained several CoPaNets-56 (width m = 1) for use on CIFAR-10+ using various numbers of pathways with the Dropout rate set to 0.2. As shown in <ref type="figure">Figure 5</ref>, CoPaNet gains its benefit by increasing the number of pathways to handle complex dataset. More pathways tend to lower test errors at the expense of more parameters. Nonetheless, we adopted two pathways in our experiments to restrict the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Number of Pathways</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Pathway Encoding</head><p>One paper <ref type="bibr" target="#b17">(Srivastava et al., 2014b)</ref> argued that ReLU network can also encode on subnetwork activation pattern, such as maxout and LWTA networks. <ref type="bibr" target="#b17">Srivastava et al. (2014b)</ref> discussed about the activation pattern of many filters in the same layer. In contrast to <ref type="bibr" target="#b17">Srivastava et al. (2014b)</ref>, we demonstrated the routing pattern that one feature map propagate through many stacked pathways (subnetworks). We suppose that the routing patterns are similar within the same semantics and are different between distinct semantics, which is termed as pathway encoding. As shown in <ref type="figure">Figure 6</ref>, we calculated the preference of routing patterns in a trained 2-pathway CoPaNet-164 (width m = 1). The preference of pathway was statistically estimated from the CIFAR-10 test set and can reveal the characteristics of the category. We illustrates the routing patterns in the last block (comprising 18 CoPa units) which contained high-level features. Each sub-figure showns the routing pattern of one feature map (4 representative feature maps were manually selected from the total of 180), and the color denoted the preference of pathways. As shown in <ref type="figure">Figure 6a</ref>, a selected routing pattern can be regarded as encoding the non-living or living groups and the routing patterns are similar in the same group. <ref type="figure">Figure 6b</ref> illustrates that the routing pattern may be encoding the flying concept such that the routing patterns of airplanes are similar to those of birds. Notice that although airplanes belong to non-living group, there exists a special pattern resembling those of animals, including the bird, as shown in <ref type="figure">Figure 6c</ref>. Furthermore, <ref type="figure">Figure 6d</ref> illustrates the diversity of routing patterns for different categories. The similarity and diversity support our hypothesis that CoPaNet is able to use pathway encoding to well represent the object images of different groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Coarse Feature Reuse</head><p>The CoPaNet-R architecture adds identity cross-block shortcuts to encourage feature reuse. This facilitates that the last classification layer can reuse coarse features from all previous blocks. Thus those shortcuts provide additional supervision because classifiers are attached to every CoPa blocks. We trained a CoPaNet-R-164 (width m = 2) on CIFAR-10+ and it achieved 3.55% test error, as shown in <ref type="table" target="#tab_1">Table 2</ref>. <ref type="figure">Figure 7</ref> shows the L 1 -norm of weights of the last classification layer. In this figure, we can observe that the last classification layer uses features from early blocks. The concentration towards the final block suggests that high-level features dominate in classification.</p><p>However, CoPaNet-R did not outperform CoPaNet on CIFAR-100 and ImageNet. This may be due to the relatively few training samples for each class (500 samples per class in CIFAR-100 as well as around 1000 samples per class in ImageNet). We conducted an experiment to demonstrate this effect. We used a small CIFAR-10 dataset (1000 training samples per class) to train CoPaNet-164 and CoPaNet-R-164, both with width m = 2, and achieved test errors of 12.58% and 12.53%, respectively. There is no significant difference in this case. With full training set (5000 training samples per class), CoPaNet-R has significant improvement compared to CoPaNet, as shown in <ref type="table" target="#tab_1">Table 2</ref>. The coarse feature reuse may be effective only when the amount of training samples is large enough for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper proposes a novel convolutional neural network architecture, the CoPaNet. It introduces a nice property that input features transmit through various routing patterns for different category information, called pathway encoding. Empirical results demonstrate that the category information plays a role in selecting pathways. We showed that CoPaNet inherits the advantages of ResNet which can scale up to hundreds of layers. In our experiments, CoPaNet yielded improvements in accuracy as the number of parameters increased. Moreover, CoPaNet requires fewer parameters to achieve the same level of accuracy as the state-of-the-art ResNet. We further proposed a novel feature reuse strategy, CoPaNet-R: adding cross-block shortcuts in order to encourage the reuse of output from all previous blocks. According to our experiments, CoPaNet-R can learn accurate models by exploiting the reuse of coarse features.</p><p>Our study showed that network partitioning, feature competition, and feature reuse can lead to performance improvements. CoPaNet and its variant obtained the state-of-the-art Routing patterns revealing category information <ref type="figure">Figure 6</ref>: Routing patterns showing the preference of pathway selection in a trained 2pathway CoPaNet-164 for the CIFAR-10 test dataset. Red color denotes a preference for the left pathway, blue color for the right pathway, and white color for no preference. The vertical axis denotes the l-th CoPa units, where l indicates the depth. The category information can be represented by the routing pattern, which is referred to as pathway encoding in the proposed work. Each sub-figure denotes the routing pattern that one feature map propagates through its preferred route in the network. Routing patterns between (a) non-living vs. living, (b) non-flying vs. flying, (c) non-animal vs. animal, and (d) different categories are illustrated. Notice that the airplane category shows the routing pattern of "bird" in the "animal" group.</p><p>Deep Competitive Pathway Networks 2 4 6 block1 block2 block3 <ref type="figure">Figure 7</ref>: The color-encoded L 1 -norm of the weights of the last classification layer. Notice that the last classification layer concatenates outputs from all of the three CoPa blocks through cross-block shortcuts.</p><p>or competitive results on several image recognition datasets. Other studies showed that competitive networks have other beneficial properties such as mitigation of catastrophic forgetting <ref type="bibr" target="#b15">(Srivastava et al., 2013)</ref>. In the future, we will try to adopt the trained CoPaNet to perform other tasks, such as object detection and segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The concept of pathway encoding in the proposed architecture. The category information is encoded on the route (red arrows) through which features propagate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed CoPa unit and network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Training loss (dashed line) and test error (solid line) curves of the preactivation ResNet-1001 (10.2M), CoPaNet-164 (1.75M), and CoPaNet-R-164 (1.75M). (b) Comparison of the parameter efficiency between pre-activation ResNets, CoPaNet, and CoPaNet-R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The comparison of top-1 validation error (single model and single crop with size 224?224) across various number of parameters among ResNet, DenseNet, and CoPaNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Network architectures for CIFAR/SVHN (left) and ImageNet (right). Parameters of competitive pathway units are presented in braces (see also Figures 2b and c).Construction parameters for internal pathways are shown in brackets. The number of pathway is determined by the factor k and the network width is determined by the factor m. The numbers in CoPaNet-26/50/101/164 denote the depths of neural network. For the sake of clarity, the final classification layer has been omitted.</figDesc><table><row><cell cols="3">CIFAR/SVHN</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Output size</cell><cell cols="2">CoPaNet-164</cell><cell>Output size</cell><cell cols="2">CoPaNet-26</cell><cell cols="2">CoPaNet-50</cell><cell cols="2">CoPaNet-101</cell><cell cols="2">CoPaNet-R-101</cell></row><row><cell>32 ? 32</cell><cell cols="2">3 ? 3,16</cell><cell>112 ? 112</cell><cell cols="2">7 ? 7,64, st. 2</cell><cell cols="2">7 ? 7,64, st. 2</cell><cell cols="2">7 ? 7,64, st. 2</cell><cell cols="2">7 ? 7,64, st. 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>56 ? 56</cell><cell cols="2">3 ? 3 max-pool, st. 2</cell><cell cols="2">3 ? 3 max-pool, st. 2</cell><cell cols="2">3 ? 3 max-pool, st. 2</cell><cell cols="2">3 ? 3 max-pool, st. 2</cell></row><row><cell>32 ? 32</cell><cell>1 ? 1, 12 ? 1 ? 1,48 ? 3 ? 3,12 ?</cell><cell>? ? 18</cell><cell>56 ? 56</cell><cell>1 ? 1, 45 3 ? 3,45 1 ? 1,180</cell><cell>? 2 ? 2</cell><cell>1 ? 1, 45 3 ? 3,45 1 ? 1,180</cell><cell>? 2 ? 3</cell><cell>1 ? 1, 45 3 ? 3,45 1 ? 1,180</cell><cell>? 2 ? 3</cell><cell>1 ? 1, 42 3 ? 3,42 1 ? 1,168</cell><cell>? 2 ? 3</cell></row><row><cell>16 ? 16</cell><cell cols="2">2 ? 2 avg-pool, st. 2</cell><cell>28 ? 28</cell><cell cols="2">2 ? 2 avg-pool, st. 2 1 ? 1, 90</cell><cell cols="2">2 ? 2 avg-pool, st. 2 1 ? 1, 90</cell><cell cols="2">2 ? 2 avg-pool, st. 2 1 ? 1, 90</cell><cell cols="2">2 ? 2 avg-pool, st. 2 1 ? 1, 84</cell></row><row><cell>16 ? 16</cell><cell>1 ? 1, 24 ? 1 ? 1,96 ? 3 ? 3,24 ?</cell><cell>? ? 18</cell><cell>28 ? 28 14 ? 14</cell><cell cols="2">3 ? 3,90 1 ? 1,360 2 ? 2 avg-pool, st. 2 ? 2 ? 2</cell><cell cols="2">3 ? 3,90 1 ? 1,360 2 ? 2 avg-pool, st. 2 ? 2 ? 4</cell><cell cols="2">3 ? 3,90 1 ? 1,360 2 ? 2 avg-pool, st. 2 ? 2 ? 4</cell><cell cols="2">3 ? 3,84 1 ? 1,336 2 ? 2 avg-pool, st. 2 ? 2 ? 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 ? 1, 180</cell><cell></cell><cell>1 ? 1, 180</cell><cell></cell><cell>1 ? 1, 180</cell><cell></cell><cell>1 ? 1, 168</cell><cell></cell></row><row><cell>8 ? 8</cell><cell cols="2">2 ? 2 avg-pool, st. 2</cell><cell>14 ? 14</cell><cell>3 ? 3,180 1 ? 1,720</cell><cell>? 2 ? 2</cell><cell>3 ? 3,180 1 ? 1,720</cell><cell>? 2 ? 6</cell><cell>3 ? 3,180 1 ? 1,720</cell><cell>? 2 ? 23</cell><cell>3 ? 3,168 1 ? 1,672</cell><cell>? 2 ? 23</cell></row><row><cell></cell><cell>1 ? 1, 45 ?</cell><cell></cell><cell>7 ? 7</cell><cell cols="2">2 ? 2 avg-pool, st. 2</cell><cell cols="2">2 ? 2 avg-pool, st. 2</cell><cell cols="2">2 ? 2 avg-pool, st. 2</cell><cell cols="2">2 ? 2 avg-pool, st. 2</cell></row><row><cell>8 ? 8</cell><cell>1 ? 1,180 ? 3 ? 3,45 ?</cell><cell>? ? 18</cell><cell>7 ? 7</cell><cell>1 ? 1, 360 3 ? 3,360</cell><cell>? 2 ? 2</cell><cell>1 ? 1, 360 3 ? 3,360</cell><cell>? 2 ? 3</cell><cell>1 ? 1, 360 3 ? 3,360</cell><cell>? 2 ? 3</cell><cell>1 ? 1, 336 3 ? 3,336</cell><cell>? 2 ? 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 ? 1,1440</cell><cell></cell><cell>1 ? 1,1440</cell><cell></cell><cell>1 ? 1,1440</cell><cell></cell><cell>1 ? 1,1344</cell><cell></cell></row><row><cell>1 ? 1</cell><cell cols="2">8 ? 8 avg-pool</cell><cell>1 ? 1</cell><cell cols="2">7 ? 7 avg-pool</cell><cell cols="2">7 ? 7 avg-pool</cell><cell cols="2">7 ? 7 avg-pool</cell><cell cols="2">7 ? 7 avg-pool</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of test error on CIFAR and SVHN. The value of k denotes the number of hidden pieces or pathways used in a given competition. The symbol "+" indicates data augmentation (translation and horizontal flipping ).</figDesc><table><row><cell cols="2">Method Maxout Network (k=2) (Goodfellow et al., 2013) Network In Network (Lin et al., 2014) Maxout Network In Network (k=5) Chang and Chen (2015)</cell><cell cols="6">Dropout Depth Params C10+ C100+ SVHN ? --9.38 38.57 2.47 ? -0.98 M 8.81 35.68 2.35 ? -1.6 M 6.75 28.86 1.81</cell></row><row><cell cols="2">Highway Network (Srivastava et al., 2015)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>7.60</cell><cell>32.34</cell><cell>-</cell></row><row><cell cols="2">ResNet (He et al., 2015a)</cell><cell></cell><cell>110</cell><cell>1.7 M</cell><cell>6.43</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Stochastic Depth Huang et al. (2016b)</cell><cell></cell><cell>110</cell><cell>1.7 M</cell><cell>5.23</cell><cell>24.58</cell><cell>1.75</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1202</cell><cell>19.4 M</cell><cell>4.91</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">pre-activation ResNet (He et al., 2016)</cell><cell></cell><cell>164</cell><cell>1.7 M</cell><cell>5.46</cell><cell>24.33</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1001</cell><cell>10.2 M</cell><cell>4.62</cell><cell>22.71</cell><cell>-</cell></row><row><cell cols="2">Wide ResNet (width=8) (Zagoruyko and Komodakis, 2016) (width=10)</cell><cell>?</cell><cell>16 28</cell><cell>11.0 M 36.5 M</cell><cell>4.27 3.89</cell><cell>20.43 18.85</cell><cell>--</cell></row><row><cell>DenseNet</cell><cell>(growth rate=24) (Huang et al., 2016a)</cell><cell></cell><cell>100</cell><cell>27.2 M</cell><cell>3.74</cell><cell>19.25</cell><cell>1.59</cell></row><row><cell cols="2">DenseNet-BC (growth rate=40) CoPaNet (k=2, width=1) CoPaNet (k=2, width=2) CoPaNet (k=2, width=4) CoPaNet-R (k=2, width=2) CoPaNet-R (k=2, width=3)</cell><cell>? ? ? ? ?</cell><cell>190 164 164 164 164 164</cell><cell cols="2">25.6 M 1.75 M 6.98 M 27.9 M 7.00 M 15.7 M 3.38 3.46 4.50 4.10 3.74 3.55</cell><cell>17.18 22.86 20.48 18.67 20.29 18.90</cell><cell>-1.86 1.83 1.73 1.72 1.58</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the Taiwan Ministry of Science and Technology (Grants MOST-106-2221-E-009-164-MY2 and MOST-105-2218-E-009-033).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Nahavandi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05672</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Multi-residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02583</idno>
		<title level="m">Batch-normalized maxout network in network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<title level="m">Maxout networks. ICML (3)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09382</idno>
		<title level="m">Deep networks with stochastic depth</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fractalnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention activates winnertake-all competition among visual filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="375" to="381" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<ptr target="http://arxiv.org/abs/1312.4400" />
		<title level="m">Network in network. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Compete to compute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohrob</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Kazerounian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Understanding locally competitive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.1165</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06581</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Deep Competitive Pathway Networks</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<title level="m">Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Polynet: A pursuit of structural diversity in very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05725</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
