<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Virtual Event</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date>October 20-24, 2021. October 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunda</forename><surname>Pei</surname></persName>
							<email>peishunda@tju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<email>hzfu@ieee.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wan</surname></persName>
							<email>lwan@tju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
							<email>wfeng@ieee.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunda</forename><surname>Pei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University Tianjin</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University Tianjin</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Inception Institute of Artificial Intelligence Abu Dhabi</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Sun Yat-sen University Guangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Tianjin University Tianjin</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Tianjin University Tianjin</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Virtual Event</title>
					</analytic>
					<monogr>
						<title level="m">China Proceedings of the 29th ACM International Conference on Multimedia (MM &apos;21)</title>
						<meeting> <address><addrLine>China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event</publisher>
							<date type="published">October 20-24, 2021. October 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475331</idno>
					<note>* Joint first authors. ? Corresponding author. ACM Reference Format: 2021. From Synthetic to Real: Image Dehazing Collaborating with Unlabeled Real Data. In ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Computer vision tasks</term>
					<term>Com- puter vision</term>
					<term>Scene understanding</term>
					<term>Artificial intelligence</term>
					<term>Com- puter vision problems</term>
					<term>KEYWORDS Single image dehazing, feature disentangling, unlabeled real data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image dehazing is a challenging task, for which the domain shift between synthetic training data and real-world testing images usually leads to degradation of existing methods. To address this issue, we propose a novel image dehazing framework collaborating with unlabeled real data. First, we develop a disentangled image dehazing network (DID-Net), which disentangles the feature representations into three component maps, i.e. the latent haze-free image, the transmission map, and the global atmospheric light estimate, respecting the physical model of a haze process. Our DID-Net predicts the three component maps by progressively integrating features across scales, and refines each map by passing an independent refinement network. Then a disentangled-consistency mean-teacher network (DMT-Net) is employed to collaborate unlabeled real data for boosting single image dehazing. Specifically, we encourage the coarse predictions and refinements of each disentangled component to be consistent between the student and teacher networks by using a consistency loss on unlabeled real data. We make comparison with 13 state-of-the-art dehazing methods on a new collected dataset (Haze4K) and two widely-used dehazing datasets (i.e., SOTS and HazeRD), as well as on real-world hazy images. Experimental results demonstrate that our method has obvious quantitative and qualitative improvements over the existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Hazy images usually suffer from content distorting and accuracy degrading for subsequent visual analysis. To improve the overall scene visibility, many image dehazing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> have been proposed to recover the latent haze-free image from the single hazy input. The image degradation caused by the haze could be formulated by a physical model <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>:</p><formula xml:id="formula_0">= ? + ? (1 ? ),<label>(1)</label></formula><p>where is the observed hazy image, is the underlying haze-free image to be recovered, is the transmission map, which represents the distance-dependent factor affecting the fraction of light that reaches the camera sensor, and is the global atmospheric light, indicating the ambient light intensity. Early dehazing methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b36">37]</ref> employed hand-crafted priors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b49">50]</ref> based on the statistics of clean images to estimate the transmission map , and then use the physical model to recover the haze-free results. Recently, a lot of methods based on convolutional neural networks (CNNs) are proposed to learn the transmission map from labeled datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>, or directly build the mapping from input hazy images to haze-free counterparts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Although achieving superior image dehazing performances over methods based on hand-crafted priors, existing CNN-based methods suffer from several limitations. First, these methods usually utilize synthesized hazy images to train networks in a supervised learning manner, and thus suffer from degraded performance in real-world hazy photos due to the domain shift between synthetic training images and real-world testing photos. Second, according to the physical model of Eq. (1), an input hazy image is a combination of a transmission map, a global atmospheric map, and an underlying haze-free image, showing that CNN features learned from input hazy image include several factors of the physical model. Unfortunately, many existing methods employed such CNN features to predict only one factor (e.g., transmission map or haze-free result), hindering image dehazing performance.</p><p>To address these problems, this work develops a disentangled image dehazing framework to leverage disentangled feature learning and unlabeled real data for boosting image dehazing performance. Specifically, we first propose a disentangled image dehazing network (DID-Net) to disentangle features at each scale into three feature components, which are transmission-distilled features for a transmission map estimation, latent-distilled features for a latent haze-free image estimation, and light-distilled features for a global atmospheric light estimation. After that, we progressively integrate transmission features, latent image features, and light features at adjacent scales to predict a transmission map, a haze-free image, and a global atmospheric map. On the other hand, for integrating synthesized and real-world hazy images, we first assign DID-Net into a mean-teacher framework, and then compute a disentangled supervised loss on labeled synthesized data and a consistency loss on unlabeled real-world data to constrain the coarse predictions and refinements of the network. By doing so, our approach achieves a superior dehazing performance over state-of-the-art methods. The contributions of this work are:</p><p>? We present an image dehazing framework to leverage disentangled feature representations and unlabeled real-world hazy images for boosting single image dehazing. ? We devise a disentangled image dehazing network (DID-Net) to predict a transmission map, a latent haze-free image, and an atmospheric light map via a coarse-to-fine strategy. ? A disentangled-consistency mean-teacher network (DMT-Net) is employed to collaborate the labeled synthetic data and unlabeled real data with disentangled consistency losses. We compare our network against 13 state-of-the-art dehazing methods on a new collected dataset, a widely-used dehazing benchmark datasets and various real-world hazy images. The experimental results demonstrate that our network outperforms state-ofthe-art dehazing methods. Our code, trained models, and results at https:// github.com/ liuye123321/ DMT-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Traditional Dehazing Methods</head><p>Traditional dehazing methods utilized image priors (e.g., dark channel prior <ref type="bibr" target="#b13">[14]</ref>, color-line priors <ref type="bibr" target="#b10">[11]</ref>, and haze-line priors <ref type="bibr" target="#b1">[2]</ref>) from hazy and latent clean images to compute a transmission map for haze removal. Please refer to Zhang et al. <ref type="bibr" target="#b42">[43]</ref> for a comprehensive review. By assuming that a linear relationship exists in the minimum channel between the hazy image and the haze-free image, a single image dehazing method is proposed based on linear transformation <ref type="bibr" target="#b38">[39]</ref>. Note that these hand-crafted priors from human observations do not always hold in diverse real-world hazy photos. Hence, they tend to suffer from undesirable color distortions <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning based Dehazing Methods</head><p>Early works formulated CNNs to estimate a transmission map for recovering the clean image via the physical model in Eq. (1). Ren et al. <ref type="bibr" target="#b29">[30]</ref> employed a coarse-scale network to predict a holistic transmission map and then used a fine-scale network for a transmission map refinement. Cai et al. <ref type="bibr" target="#b3">[4]</ref> computed a transmission map by developing a DehazeNet equipped with BReLU based feature extraction layers. However, inaccurate transmission map estimation hinders haze removal quality of these methods.</p><p>Later, CNN-based methods directly learned the latent clean image from a single hazy image in an end-to-end manner. Yang et al. <ref type="bibr" target="#b41">[42]</ref> predicted a clean image by integrating the physical model and image prior into a CNN. Song et al. <ref type="bibr" target="#b35">[36]</ref> presented a novel ranking convolutional neural network for single image dehazing. Li et al. <ref type="bibr" target="#b22">[23]</ref> embedded VGG-features <ref type="bibr" target="#b34">[35]</ref> and an 1 -regularized gradient prior into conditional generative adversarial network (cGAN) <ref type="bibr" target="#b14">[15]</ref> for a clean image estimation. Ren et al. <ref type="bibr" target="#b30">[31]</ref> learned confidence maps via an encoder-decoder network from three derived inputs and fused confidence maps for generating a final dehazed result. Zheng et al. <ref type="bibr" target="#b45">[46]</ref> devised a multi-guide bilateral learning for reaching a real-time dehazing of 4K images. However, the disjoint optimization on these deep models failed to capture the inherent relations among the transmission map, the atmospheric light, and the dehazed result, thereby degrading the overall dehazing performance.</p><p>To alleviate this issue, Zhang et al. <ref type="bibr" target="#b42">[43]</ref> employed two networks to estimate the transmission map and the atmospheric light separately, and computed the haze-free image according to the physical haze model (see Eq. (1)), which are all integrated into an end-toend dehazing network (DCPDN). Deng et al. <ref type="bibr" target="#b7">[8]</ref> attentively fused multiple mathematical haze separation models for image dehazing. Qu et al. <ref type="bibr" target="#b28">[29]</ref> presented a dehazing GAN with a multi-resolution generator module, the enhancer module, and a multi-scale discriminator module. Liu et al. <ref type="bibr" target="#b24">[25]</ref> devised a CNN with a pre-processing module, an attention-based multi-scale backbone module and a post-processing module. Deng et al. <ref type="bibr" target="#b6">[7]</ref> stacked haze-aware representation distillation (HARD) modules with normalization layers into a GAN to attentively fuse global atmospheric brightness and local spatial structures. Dong et al. <ref type="bibr" target="#b8">[9]</ref> explicitly utilized the physical model into a encoder-decoder network. Although improving the overall scene visibility, these methods are trained on synthesized images in a supervised learning manner, suffering from limited capability to generalize well to real-world hazy images.</p><p>Shao et al. <ref type="bibr" target="#b32">[33]</ref> formulated a domain adaptation network with two image translation modules between synthesized and real hazy images and two image dehazing modules to alleviate the domain shift problem. However, the image dehazing modules of Shao et al. <ref type="bibr" target="#b32">[33]</ref> learned CNN features from input hazy image to predict only one factor (i.e., the latent haze-free image), thereby hindering the dehazing performance. Although Li et al. <ref type="bibr" target="#b16">[17]</ref> also leveraged  <ref type="figure">Figure 1</ref>: The schematic illustration of the developed DMT-Net. We first develop a disentangled image dehazing network (DID-Net; see <ref type="figure" target="#fig_1">Figure 2</ref>) for detecting haze-free maps, transmission maps, and atmospheric maps via a coarse-to-fine strategy, and build reconstructed haze maps. After that, we compute a supervised loss for labeled data and a consistency loss for unlabeled data and add them together to train our image dehazing netowrk. EMA: exponential moving average. the layer separation mechanism based on the physical model, this work mainly addressed the image dehazing in an unsupervised and zero-shot manner. To alleviate this issue, we develop a disentangled image dehazing network to learn disentangled feature presentations and leverage unlabeled data for improving dehazing performance. <ref type="figure">Figure 1</ref> shows the architecture of our DMT-Net, which learns disentangled representations and leverages unlabeled real-world hazy images for image dehazing. Specifically, we first develop a disentangled image dehazing network (DID-Net) to disentangle features at each scale into three components for jointly computing a dehazed map, a transmission map, and an atmospheric map via a coarse-to-fine mechanism (see predictions at Stage I and Stage II in <ref type="figure">Figure 1</ref>). Moreover, DID-Net reconstructs hazy images from the estimated maps of the three components, and then computes two reconstruction losses between the input hazy image and two reconstructed ones to make them similar. Then, our DID-Net is assigned to both the student network and the teacher network. During the training, the labeled data is fed into the student network, and a supervised loss is computed by adding prediction losses of three component maps and the reconstruction losses. Then, for unlabeled data, we produce one auxiliary image from the input hazy image and feed it into the student network and the teacher network, respectively. A disentangled consistency loss is computed on the two groups of three component maps and a reconstructed hazy map. In the testing procedure, we only utilize the student network to generate the dehazing result of an input image.</p><formula xml:id="formula_1">! " " # $ % " # $ % # $ % " &amp; # &amp; $ ! % &amp; # &amp; $ ! % ! " &amp; # &amp; $ ! % ! "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Disentangled Image Dehazing Network</head><p>According to the physical model of a haze process (see Eq. (1)), CNN features learned from an input hazy image encode information of three haze components, i.e. the haze-free map, the transmission map, and the atmospheric map. In this work, we propose a disentangled image dehazing network (DID-Net) by harnessing a disentangled feature learning strategy <ref type="bibr" target="#b25">[26]</ref> to separate each feature into three components: a latent-distilled feature for predicting the haze-free map , a transmission-distilled feature for predicting the transmission , and a light-distilled feature for predicting the atmospheric-light map , respectively; see <ref type="figure" target="#fig_1">Figure 2</ref>. By doing so, the proposed DID-Net is capable of simultaneously extracting features for all the three components, and hence providing comprehensive information for dehazing. Note that, before our work, two recently published papers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43]</ref> also propose to jointly estimate the three components, but they utilize different networks for component estimation and totally rely on synthesized data, resulting in performance degradation on real-world photos. <ref type="figure" target="#fig_1">Figure 2</ref> shows the schematic illustration of the proposed DID-Net. Specifically, given an input hazy image , we first extract a set of feature maps with different spatial resolutions, and these feature maps are denoted as EF (1 ? ? 5). We decompose each into latent-distilled disentangled features DFJ , transmission-distilled disentangled features DFT , and light-distilled disentangled features DFA . After that, we devise a coarse-to-fine mechanism to estimate , , and based on these disentangled features. To do so, we first integrate disentangled features at different scales to generate coarse predictions for the three components (denoted as , and ). Then, a hazy image can be figured out as follows:</p><formula xml:id="formula_2">( ) = ( ) ? ( ) + ( ) ? (1 ? ( ))<label>(2)</label></formula><p>where is the -th pixel. We then feed the three coarse predictions, , and , into three independent U-Net based residual blocks to generate three corresponding refined predictions. The three refined predictions are denoted as?,?, and?, respectively, based on which another hazy image?is computed from?,?, and?: Given an input hazy image , we first extract a set of feature maps with different spatial resolutions, and then disentangle these features into three components: dehazing-distilled features for estimating a haze-free image ( ), transmission-distilled features for estimating a transmission map ( ), and light-distilled features for estimating an atmospheric light map ( ). We then devise a coarse-to-fine strategy to predict , , and . The coarse predictions ( , , and ) are obtained via iteratively merging disentangled features, while refinement predictions (?,?, and?) are produced by feeding coarse results into U-Net structures. Moreover, we reconstruct two hazy images ( and?) from the physical haze model with coarse/refined predictions, and compute reconstruction losses against the input hazy image .</p><formula xml:id="formula_3">( ) =?( ) ??( ) +?( ) ? (1 ??( )) . (3) Stage ? Stage ? Supervision EF1 EF2 EF3 EF4 EF5 DFJ5 DFT5 DFA5 DFJ4 DFJ3 DFJ2 DFJ1 DFT4 DFT3 DFT2 DFT1 DFA4 DFA3 DFA2 DFA1</formula><p>Once reconstructing two hazy images and?from the coarse and refined predictions, our DID-Net computes a reconstruction loss (L ) between the input image and and?. The L is defined as:</p><formula xml:id="formula_4">L = | ? | 1 + | ??| 1 ,<label>(4)</label></formula><p>where | ? | 1 is the 1 loss function.</p><p>How to generate coarse predictions. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, our DID-Net devises three independent branches to progressively aggregate disentangled features from deep layers to shallow layers for generating coarse predictions , , and . Here, we take the branch for generating the haze-free image prediction as an example to describe the workflow. This branch aggregates DFJ (1 ? ? 5) for predicting the haze-free prediction, where the key operation is to merge features at adjacent layers. When fusing two adjacent features (DFJ and DFJ ?1 , 2 ? ? 5), we up-sample the lowresolution feature DFJ to the same spatial resolution with the highresolution feature DFJ ?1 , enhance upsampled features by feeding them into a series of residual channel attention blocks (RCABs) <ref type="bibr" target="#b44">[45]</ref>, and apply a 1?1 convolutional layer on the concatenation of the RCAB-enhanced feature and the high-resolution feature DFJ ?1 to output a merged feature map, which is denoted as ?1 . The ?1 can be computed by:</p><formula xml:id="formula_5">?1 = M (DFJ , DFJ ?1 ) = ( (? RCAB (DFJ ), DFJ ?1 )) ,<label>(5)</label></formula><p>where M (?) denotes the operation of merging two features; the is a 1?1 convolutional layer; the ? RCAB is the refinement block consisting of a number of RCABs. Then, similarly, we merge ?1 with features DFJ at the next CNN layer until reaching features DFJ (with the largest spatial resolution) at the first CNN layer, and finally pass the resultant features to a 1?1 convolutional layer for predicting :</p><formula xml:id="formula_6">= (M (DFJ 1 , 2 )) , 2 = M (DFJ 2 , 3 ) , 3 = M (DFJ 3 , 4 ) , 4 = M (DFJ 4 , DFJ 5 ) ,<label>(6)</label></formula><p>where M is the feature operation of Eq. <ref type="bibr" target="#b4">(5)</ref>. We conduct the similar operations to incrementaly aggregate transmission-distilled features and light-distilled features. Note that the numbers of RCABs in merging features are different. In our experiments, we empirically use 20 RCABs to merge adjacent features for computing and , and 2 RCABs to merge features for computing , since is a global parameter and simpler than the other two components.</p><p>How to generate fine predictions. We further pass these coarse predictions, , , and , to a U-Net residual block to produce their refined results. For example, given the coarse haze-free map prediction , we pass it to a U-Net <ref type="bibr" target="#b31">[32]</ref> with 5 convolutional layers, to produce an intermediate image U ( ), which is then added with to obtain a refinement (denoted as?) of ,</p><formula xml:id="formula_7">= + U ( ) .<label>(7)</label></formula><p>Similarly, we compute a refinement?of , and a refinement? f as follows:?= + U ( ) ,</p><formula xml:id="formula_8">= + U ( ) ,<label>(8)</label></formula><p>where U ( ) and U ( ) are the U-Net structure on and . Note that U ( ), U ( ), and U ( ) have the same encoder-decoder structures, but do not share network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supervised Loss on Labeled data</head><p>Note that a synthesized hazy image (labeled image) is usually generated by passing a given clean image, a given transmission image, and a given atmospheric image to the physically-based model introduced by Eq. (1), which can be naturally taken as the ground truths. Based on the ground truths, we first compute a disentangled multitask supervised loss (denoted as L ( )) for a labeled hazy image ( ) by adding the supervised losses of the clean image prediction (L ), transmission image prediction (L ), and atmospheric image prediction (L ), i.e.</p><formula xml:id="formula_9">L ( ) = L + 1 L + 2 L ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_10">L = | ? | 1 + | ??| 1 , L = | ? | 1 + | ??| 1 , L = | ? | 1 + | ??| 1 .<label>(10)</label></formula><p>Here, , and represent the ground truths of the clean image, the transmission image and the atmospheric image, respectively. We empirically set the weights 1 =0.3 and 2 =0.1 in the network training. By adding L with the reconstruction loss of Eq. (4), we compute the supervised loss of labeled data as follows:</p><formula xml:id="formula_11">L ( ) = L ( ) + 3 L ,<label>(11)</label></formula><p>where 3 =0.1 in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Consistency Loss on Unlabeled Data</head><p>For the unlabeled real-world data, we pass it into the student network to obtain eight results, which are two clean images (denoted as and?), two transmission images ( and?), and two atmospheric images ( and?), and two reconstructed hazy images ( and?). Meanwhile, by first adding a Gaussian noise into the real-world hazy image and feeding the noisy image into the teacher network, we can generate another two clean images ( and?), two transmission images ( and?), two atmospheric images ( and?), and two reconstructed hazy images ( and?). We then enforce the predictions of eight prediction results from the student network and the teacher network to be consistent, resulting in a disentangled multi-task consistency loss (L ). Mathematically, L for an unlabeled image (denoted as ) is</p><formula xml:id="formula_12">L ( ) = L + 4 L + 5 L + 6 L ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_13">L = | ? | 1 + |???| 1 , L = | ? | 1 + |???| 1 , L = | ? | 1 + |???| 1 , L = | ? | 1 + |???| 1 ,<label>(13)</label></formula><p>L , L , L , and L denote the consistency loss of the clean image estimation, the transmission image estimation, the atmospheric image estimation, and the reconstructed hazy image, respectively. For simplicity, we set 4 = 1 = 0.3, 5 = 2 = 0.1, and 6 = 3 =0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Our Network</head><p>As a semi-supervised framework, our method fuses labeled synthesized images and unlabeled real-world images for training. The total loss of our network is</p><formula xml:id="formula_14">L = ?? ?L L ( ) + ?? ?U L ( ) ,<label>(14)</label></formula><p>where L and U denote the labeled dataset and the unlabeled dataset. L ( ) represents the supervised loss (see Eq. <ref type="formula" target="#formula_0">(11)</ref>) for a labeled hazy image x of L. L ( ) is the consistency loss (see Eq. <ref type="formula" target="#formula_0">(12)</ref>) for a unlabeled hazy image of U. We follow <ref type="bibr" target="#b4">[5]</ref> to apply a time dependent Gaussian warming up function to compute the weight :</p><formula xml:id="formula_15">( ) = (?5(1? / ) 2 )</formula><p>, where denotes the current training iteration and is the maximum training iteration. In our experiments, we empirically set =1. We minimize L to train the student network, and the parameters of the teacher network are updated via the exponential moving average (EMA) strategy with a EMA deay of 0.99; please refer to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b37">38]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Our unlabeled data</head><p>Note that <ref type="bibr" target="#b32">[33]</ref> provides an unlabeled dataset with 1, 000 real-world hazy images to train a domain adaption network for haze removal. To conduct fair comparisons, we use the same real-world dataset of <ref type="bibr" target="#b32">[33]</ref> as the unlabeled data of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training and Testing Strategies</head><p>Training parameters. To accelerate the training procedure and reduce the overfitting risk, we initialize the parameters of DID-Net (student network) by ResNeXt <ref type="bibr" target="#b40">[41]</ref>, which has been well-trained for the image classification task on the ImageNet. Other parameters in the DID-Net are initialized as random values. We implement our framework in PyTorch and utilize ADAM optimizer to train the network. The learning rate is adjusted by a poly strategy <ref type="bibr" target="#b23">[24]</ref> with the initial learning rate of 0.0001 and the power of 0.9. We randomly crop all the labeled and unlabeled images to 240?240 for the training on two GTX 2080Ti GPU, and augment the training set by random horizontal flipping. We use the mini-batch size of 16, which means the usage of 8 labeled images and 8 unlabeled data images in each training epoch.</p><p>Inference. In the testing stage, we feed the input image into the student network and utilize the predicted dehazed map of the student network as the final output of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>We compare our dehazing network against 13 state-of-the-art image dehazing methods, including DCP <ref type="bibr" target="#b13">[14]</ref>, NLD <ref type="bibr" target="#b1">[2]</ref>, MSCNN <ref type="bibr" target="#b29">[30]</ref>, De-hazeNet <ref type="bibr" target="#b3">[4]</ref>, AOD-Net <ref type="bibr" target="#b17">[18]</ref>, GFN <ref type="bibr" target="#b30">[31]</ref>, DCPDN <ref type="bibr" target="#b42">[43]</ref>, EPDN <ref type="bibr" target="#b28">[29]</ref>, GDN <ref type="bibr" target="#b24">[25]</ref>, DM 2 F-Net <ref type="bibr" target="#b7">[8]</ref>, FFA <ref type="bibr" target="#b27">[28]</ref>, MSBDN <ref type="bibr" target="#b12">[13]</ref> and DA <ref type="bibr" target="#b32">[33]</ref>. Among the compared methods, DCP and NLD focused on handcrafted features for haze removal, while others are based on convolutional neural networks (CNNs). We retrain the original (released) implementations of these methods or directly report their results on the public datasets. Furthermore, we employ two widely-used metrics for quantitative comparisons, and they are peak signal to noise ratio (PSNR) <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> and structural similarity index (SSIM) <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49]</ref>.   Datasets. We first test each image dehazing method on a public benchmark dataset, i.e., SOTS <ref type="bibr" target="#b32">[33]</ref>, which consists of 1,000 testing images. We follow existing works <ref type="bibr" target="#b32">[33]</ref> to set the associate training set with 6,000 synthesized images, which consists of 3,000 from the indoor training set (ITS), and 3,000 from the outdoor training set (OTS) of the RESIDE dataset <ref type="bibr" target="#b18">[19]</ref>. Second, HazeRD <ref type="bibr" target="#b43">[44]</ref> containing 15 outdoor images with more realistic haze is introduced for testing. Apart from SOTS and HazeRD, we also create a synthesized dataset (denoted as Haze4K) with 4,000 hazy images, in which each hazy image has the associate ground truths of a latent clean image, a transmission map, and an atmospheric light map. To be specific, we collected 1,000 clean images by randomly selecting 500 indoor images from NYU-Depth <ref type="bibr" target="#b33">[34]</ref> and 500 outdoor images from OTS <ref type="bibr" target="#b18">[19]</ref>. Among them, 250 images are randomly selected from both indoor image set (125 images) and outdoor image set (125 images), to form the test set, and the remaining 750 images are used for the training set. After that, for each clean image, we followed <ref type="bibr" target="#b42">[43]</ref> to randomly sample four parameter settings, i.e. atmospheric light conditions ? [0.5, 1] and scattering coefficients ? [0.5, 2], to generate transmission maps and atmospheric light maps, which are then employed to obtain the corresponding hazy images via the physic model in Eq. (1). Hence, Haze4K has 4,000 hazy images with 3,000 training images and 1,000 testing images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on Synthetic Images</head><p>We retrain released models of these compared methods on the training set of our Haze4K dataset to obtain their results, while we follow the training setting of DA <ref type="bibr" target="#b32">[33]</ref> to produce our results on SOTS and HazeRD for fair comparisons. <ref type="table" target="#tab_1">Table 1</ref> reports PSNR and SSIM scores of different dehazing methods. In general, CNN-based methods have larger PNSR and SSIM values than hand-crafted-prior based methods (DCP &amp; NLD). Among all the compared methods, FFA-Net has the largest PSNR and SSIM scores (i.e., 26.97 and 0.95) on Haze4K, and the largest SSIM score (0.80) on HazeRD, while DA has the largest PSNR and SSIM values (i.e., 27.76 and 0.93) on SOTS, and the largest SSIM value (18.07) on HazeRD. Also note that our Haze4K dataset contains more challenging dehazing photos than SOTA, and existing dehazing methods suffer from a degraded PSNR and SSIM performance. DID-Net, as our sub-network with only labeled data, already outperforms most existing CNN-based methods in terms of PSNR and SSIM metrics, which proves the effectiveness of our disentangled feature learning for haze removal. Furthermore, our method consistently has the largest PSNR and SSIM scores on Haze4K, SOTS, and HazeRD, demonstrating that our semi-supervised dehazing network can better recover the underlying clean images for these hazy images.   color distortion especially in the sky regions for the three images. In contrast, the dehazed results of our network in <ref type="figure" target="#fig_2">Figure 3</ref> (h) is closest to the latent ground truth images (see <ref type="figure" target="#fig_2">Figure 3</ref> (i)). To summarize, our dehazed results (DMT-Net) tend to produce higher visual quality and less color distortions, which are also verified by the largest PSNR and SSIM values shown in <ref type="figure" target="#fig_2">Figure 3</ref>. <ref type="figure" target="#fig_4">Figure 4</ref> and <ref type="figure" target="#fig_5">Figure 5</ref> visually compare the dehazed maps on realworld hazy photos from the RESIDE dataset <ref type="bibr" target="#b18">[19]</ref>. DA suffers from color distortions in almost all the five photos. This is particularly evident in the first and third images of <ref type="figure" target="#fig_4">Figure 4</ref>. GDN tends to darken several areas; see the first image (the lane area) of <ref type="figure" target="#fig_5">Figure 5</ref>. AOD-Net, DM 2 F-Net, FFA, and MSBDN remove few fog, and there is still a large amount of fog in the generated images; see blown-up views of <ref type="figure" target="#fig_5">Figure 5</ref>. Our method can more effectively remove haze while producing realistic colors than these compared state-of-theart methods.  means that "basic" is equal to progressively merge 5 to 1 on labeled data for predicting a haze-free map . The second baseline (denoted as "basic+StageI") adds the feature disentangling operations into "basic", demonstrating that three branches to fuse disentangled features are employed. Lastly, we construct the third baseline (denoted as "basic+two-stages") by adding U-Net refinement blocks to coarse predictions, which equals to train DID-Net (see <ref type="figure" target="#fig_1">Fig. 2</ref>) on labeled data for haze removal. Quantitative comparison. <ref type="table" target="#tab_2">Table 2</ref> summarizes PSNR and SSIM results of our method (DMT-Net) and the constructed three baselines. From the results, we find that "basic+StageI" has larger PSNR and SSIM scores than "basic", which indicates disentangling CNN features from the input hazy can produce a more accurate dehazed result. Similarly, "basic+two-stages" has a superior PSNR and SSIM performance than "basic+StageI", showing that three refinement blocks further improve the dehazing performance. Lastly, our DMT-Net outperforms "basic+two-stages" in terms of PSNR and SSIM metrics. It further demonstrates that the unlabeled data helps our method to obtain better performance. Visual comparison. As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, "basic+StageI" tends to produce a relatively clean texture than "basic", but both of them tend to modify colors of the building regions. This phenomenon is improved by "basic+two-stages", and our method can further generates better textures and visual quality. In conclusion, our network can effectively remove haze and simultaneously maintain the latent color distributions inside building regions, which is also proved by superior PSRN/SSIM scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Real-world Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">More Discussions</head><p>Hyper-parameter study. As presented in Eq. 9 and Eq. 12, our supervised loss on labeled data and unsupervised loss on unlabeled data contain six hyper-parameters to weight different loss functions, and we set them as: 4 = 1 , 5 = 2 , and 6 = 3 . <ref type="table" target="#tab_3">Table 3</ref> shows the quantitative results of our network and other its modifications. We can see that different settings of these hyper-parameters have a certain impact on the dehazed results, and overall they all achieve good results.</p><p>Model complexity analysis. The model complexity and inference time of our method are 51.79M/0.127s, worse than the lightweight model AOD-Net (1761/0.004s). We take the task of reducing the model complexity and inference time as one of our future work. Results of the teacher model. The dehazed PSNR/SSIM of the teacher network are 28.34/0.96, only slightly worse than the student network. Following all research works based on the mean-teacher framework, we also utilized the student network to do the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This work presents a disentangled-consistency mean teacher network (DMT-Net) for boosting single-image dehazing by leveraging feature disentangled learning and unlabeled real-world images. Our key idea is to first disentangle features from input hazy photos for simultaneously predicting clean images, transmission maps, and atmospheric images, for which we develop a disentangled image dehazing network (DID-Net) following a coarse-to-fine strategy. Then we assign DID-Net as the student and teacher networks to impose disentangled consistency loss for leveraging additional unlabeled data. Experimental results on synthesized datasets and real-world photos demonstrate the effectiveness of our network, which clearly outperforms the state-of-the-art image dehazing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The schematic illustration of our DID-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visual comparisons on dehazed results of various methods on synthetic hazy photos. Please zoom in for a better illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>visually compares the dehazed results. In the first and third images, DehazeNet produces an obvious color distortion in the ground regions of the dehazed results. Although obtaining a better dehazing performance than DehazeNet, the CNN-based methods (e.g., GDN, DM 2 F-Net, FFA, and MSBDN) tend to darken several areas in their results; see Figures 3 (c)-(f). DA may produce</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparisons on dehazed results produced by our network (h) and SOTA methods (b)-(g) on real-world hazy photos (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visual comparisons on dehazed results produced by our network (h) and SOTA methods (b)-(g) on more real-world hazy photos (a). Please see blown-up views for better visual comparisons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Visual comparisons of dehazed results produced by our method and three baselines (ablation study). Please zoom in for a better illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparisons between our network and compared methods on three synthetic dehazing datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Haze4K</cell><cell cols="2">SOTS [31]</cell><cell cols="2">HazeRD [44]</cell></row><row><cell>method</cell><cell cols="7">Year PSNR SSIM PSNR SSIM PSNR SSIM</cell></row><row><cell>Our DMT-Net</cell><cell>-</cell><cell cols="6">28.53 0.96 29.42 0.97 18.55 0.85</cell></row><row><cell>Our DID-Net</cell><cell>-</cell><cell>27.81</cell><cell>0.95</cell><cell>28.30</cell><cell>0.95</cell><cell>18.07</cell><cell>0.84</cell></row><row><cell>DA [33]</cell><cell cols="2">2020 24.03</cell><cell>0.90</cell><cell>27.76</cell><cell>0.93</cell><cell>18.07</cell><cell>0.63</cell></row><row><cell>FFA-Net [28]</cell><cell cols="2">2020 26.97</cell><cell>0.95</cell><cell>26.88</cell><cell>0.95</cell><cell>17.56</cell><cell>0.80</cell></row><row><cell>MSBDN [13]</cell><cell cols="2">2020 22.99</cell><cell>0.85</cell><cell>24.15</cell><cell>0.86</cell><cell>16.87</cell><cell>0.75</cell></row><row><cell cols="3">DM 2 F-Net [8] 2019 24.61</cell><cell>0.92</cell><cell>23.87</cell><cell>0.91</cell><cell>15.88</cell><cell>0.74</cell></row><row><cell>GDN [25]</cell><cell cols="2">2019 23.29</cell><cell>0.93</cell><cell>26.05</cell><cell>0.95</cell><cell>15.92</cell><cell>0.77</cell></row><row><cell>EPDN [29]</cell><cell cols="2">2019 21.08</cell><cell>0.86</cell><cell>23.82</cell><cell>0.89</cell><cell>17.37</cell><cell>0.56</cell></row><row><cell>DCPDN [43]</cell><cell cols="2">2018 23.86</cell><cell>0.91</cell><cell>19.39</cell><cell>0.65</cell><cell>16.12</cell><cell>0.34</cell></row><row><cell>GFN [31]</cell><cell>2018</cell><cell>-</cell><cell>-</cell><cell>22.30</cell><cell>0.88</cell><cell>13.98</cell><cell>0.37</cell></row><row><cell cols="3">AOD-Net [18] 2017 17.15</cell><cell>0.83</cell><cell>19.06</cell><cell>0.85</cell><cell>15.63</cell><cell>0.45</cell></row><row><cell cols="3">DehazeNet [4] 2016 19.12</cell><cell>0.84</cell><cell>21.14</cell><cell>0.85</cell><cell>15.54</cell><cell>0.41</cell></row><row><cell>MSCNN [30]</cell><cell cols="2">2016 14.01</cell><cell>0.51</cell><cell>17.57</cell><cell>0.81</cell><cell>15.57</cell><cell>0.42</cell></row><row><cell>NLD [2]</cell><cell cols="2">2016 15.27</cell><cell>0.67</cell><cell>17.27</cell><cell>0.75</cell><cell>16.16</cell><cell>0.58</cell></row><row><cell>DCP [14]</cell><cell cols="2">2011 14.01</cell><cell>0.76</cell><cell>15.49</cell><cell>0.64</cell><cell>14.01</cell><cell>0.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average PSNR and SSIM values in ablation study.</figDesc><table><row><cell></cell><cell cols="2">Haze4K</cell><cell cols="2">SOTS [31]</cell></row><row><cell>method</cell><cell cols="4">PSNR SSIM PSNR SSIM</cell></row><row><cell>basic</cell><cell>25.39</cell><cell>0.93</cell><cell>27.09</cell><cell>0.94</cell></row><row><cell>basic+StageI</cell><cell>26.85</cell><cell>0.94</cell><cell>27.82</cell><cell>0.95</cell></row><row><cell cols="2">basic+two-stages 27.81</cell><cell>0.95</cell><cell>28.30</cell><cell>0.95</cell></row><row><cell cols="5">DMT-Net (ours) 28.53 0.96 29.42 0.97</cell></row><row><cell>4.3 Ablation Study</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Baseline network setting. We perform ablation study experi-</cell></row><row><cell cols="5">ments to evaluate the effectiveness of major components of our</cell></row><row><cell cols="5">network. Here, we construct three baseline networks, and list their</cell></row><row><cell cols="5">PSRN and SSIM results on Haze4K and SOTS [20, 31]. The first</cell></row><row><cell cols="5">baseline (denoted as "basic") is constructed by removing the two</cell></row><row><cell cols="5">branches of predicting transmission maps and atmospheric maps,</cell></row><row><cell cols="5">removing prediction refinement, and removing unlabeled data. It</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Average PSNR and SSIM values of our network on Haze4K under different hyper-parameter settings in Eq. 9 and Eq. 12.</figDesc><table><row><cell></cell><cell>1 ( 4 )</cell><cell>2 ( 5 )</cell><cell cols="3">3 ( 6 ) PSNR SSIM</cell></row><row><cell>1</cell><cell>0.3</cell><cell>0.7</cell><cell>0.1</cell><cell>28.70</cell><cell>0.97</cell></row><row><cell>2</cell><cell>0.3</cell><cell>0.7</cell><cell>0.7</cell><cell>28.40</cell><cell>0.96</cell></row><row><cell>3</cell><cell>0.7</cell><cell>0.7</cell><cell>0.1</cell><cell>28.84</cell><cell>0.97</cell></row><row><cell>4</cell><cell>0.7</cell><cell>0.1</cell><cell>0.7</cell><cell>28.07</cell><cell>0.96</cell></row><row><cell>Ours</cell><cell>0.7</cell><cell>0.1</cell><cell>0.7</cell><cell>28.53</cell><cell>0.96</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single image dehazing by multi-scale fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Codruta Orniana Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ancuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="3271" to="3282" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Air-light estimation using haze-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">De-hazeNet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunmei</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Multi-task Mean Teacher for Semi-supervised Shadow Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5610" to="5619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorela</forename><surname>Ila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05624</idno>
		<title level="m">Semantic singleimage dehazing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">HardGAN: A Haze-Aware Representation Distillation GAN for Single Image Dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qili</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziling</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chi</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="722" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep multi-model fusion for single-image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2453" to="2462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Physics-Based Feature Dehazing Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="188" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOGSIG</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOGSIG</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the duality between Retinex and image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Alvarez-Gila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Vazquez-Corral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalm?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8212" to="8221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-Scale Boosted Dehazing Network with Dense Feature Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Jinshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hu Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xinyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ming-Hsuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2154" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Zero-shot image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbiao</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><forename type="middle">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8457" to="8466" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">AOD-Net: An all-in-one network for dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Benchmarking single-image dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="492" to="505" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Benchmarking Single-Image Dehazing and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="492" to="505" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PDR-Net: Perception-Inspired Single Image Dehazing Network With Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="704" to="716" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Cascaded Convolutional Neural Network for Single Image Dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="24877" to="24887" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single Image Dehazing via Conditional Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runde</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8202" to="8211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ParseNet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Griddehazenet: Attention-based multi-scale network for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongrui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7313" to="7322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring disentangled feature representation beyond face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2080" to="2089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vision in bad weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="820" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FFA-Net: Feature Fusion Attention Network for Single Image Dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Xu Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11908" to="11915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enhanced pix2pix dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8160" to="8168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gated fusion network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Changxin Gao, and Nong Sang. 2020. Domain Adaptation for Image Dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerenhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2805" to="2814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Single Image Dehazing Using Ranking Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Automatic recovery of the atmospheric light in hazy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Sulami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Glatzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Werman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast Image Dehazing Method Based on Linear Transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1142" to="1155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Proximal Dehaze-Net: A Prior Learning-Based Deep Network for Single Image Dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="729" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">HAZERD: an outdoor scene dataset and benchmark for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3205" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="294" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ultra-High-Definition Image Dehazing via Multi-Guided Bilateral Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenglong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyi</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Jing Qin, and Pheng-Ann Heng. 2021. Learning gated non-local residual for single-image rain streak removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemiao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2147" to="2159" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A non-local low-rank framework for ultrasound speckle reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="493" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Joint bi-layer optimization for single-image rain streak removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2526" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A Fast Single Image Haze Removal Algorithm Using Color Attenuation Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
