<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Vision Transformers by Revisiting High-frequency Components</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawang</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of ECE at Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Data Platform</orgName>
								<orgName type="laboratory">Lab 5 Peng Cheng Laboratory</orgName>
								<address>
									<addrLine>Tencent 4 Sea AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Data Platform</orgName>
								<orgName type="laboratory">Lab 5 Peng Cheng Laboratory</orgName>
								<address>
									<addrLine>Tencent 4 Sea AI</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Vision Transformers by Revisiting High-frequency Components</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The transformer models have shown promising effectiveness in dealing with various vision tasks. However, compared with training Convolutional Neural Network (CNN) models, training Vision Transformer (ViT) models is more difficult and relies on the large-scale training set. To explain this observation we make a hypothesis that ViT models are less effective in capturing the high-frequency components of images than CNN models, and verify it by a frequency analysis. Inspired by this finding, we first investigate the effects of existing techniques for improving ViT models from a new frequency perspective, and find that the success of some techniques (e.g., RandAugment) can be attributed to the better usage of the high-frequency components. Then, to compensate for this insufficient ability of ViT models, we propose HAT, which directly augments high-frequency components of images via adversarial training. We show that HAT can consistently boost the performance of various ViT models (e.g., +1.2% for ViT-B, +0.5% for Swin-B), and especially enhance the advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the superiority can also be maintained on out-of-distribution data and transferred to downstream tasks. The code is available at: https://github.com/jiawangbai/HAT.</p><p>Recently, transformer models have shown high effectiveness in various vision tasks and attracted growing attention. The pioneering work is Vision Transformer (ViT) <ref type="bibr" target="#b22">[23]</ref>, which is a full-transformer architecture directly inherited from natural language processing [63] but taking raw image patches as input. After that, many ViT variants <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b68">69]</ref> have been proposed and achieved competitive performance with Convolutional Neural Network (CNN) models. Though promising in vision tasks, ViT models suffer training difficulty and require significantly more training samples [23] compared with CNN models.</p><p>Corresponding authors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head> <ref type="bibr" target="#b53">[54]</ref> <p>following <ref type="bibr" target="#b61">[62]</ref>. The top-1 accuracy of ViT-B, ResNet-101, ResNet-50, and ViT-B (+KD) on the ImageNet validation set is 82.0%, 79.8%, , 81.6%, and 83.6%, respectively.</p><p>One reason for this difficulty may be that ViT models can not effectively exploit the local structures as they split an image to a sequence of patches and model their dependencies with the self-attention mechanism <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b49">50]</ref>. In contrast, CNN models can effectively extract local features within the receptive fields <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref> with convolution operation. From some previous studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b60">61]</ref>, the local structures (e.g., edges and lines) are more related to the high-frequency components of the images. We then naturally make such a hypothesis: ViT models are less effective in capturing the high-frequency components of images than CNN models.</p><p>To verify our hypothesis, we use the discrete Fourier transform (DFT) to decompose the original images into the low-and high-frequency components and evaluate the model performance on them respectively <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b65">66]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows a comparison between ViT-B <ref type="bibr" target="#b22">[23]</ref> and ResNet-50 <ref type="bibr" target="#b30">[31]</ref>, where a larger filter size for the low-and high-pass filtering means more low-and high-frequency components, respectively. In our experiments, ViT-B has a higher top-1 accuracy on the original ImageNet validation set (82.0% vs. <ref type="bibr" target="#b78">79</ref>.8%) and a larger model size (86.6M vs. 25.6M). We can see that ViT-B performs better than ResNet-50 on the low-frequency components <ref type="figure" target="#fig_0">(Figure 1 (a)</ref>), but worse than ResNet-50 on the high-frequency components <ref type="figure" target="#fig_0">(Figure 1 (b)</ref>), which supports our hypothesis.</p><p>Motivated by the above observation, we study existing techniques for ViT models from a frequency perspective, including knowledge distillation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b61">62]</ref>, architecture design <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b13">14]</ref>, and data augmentations <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b49">50]</ref>, and provide some useful insights which are beneficial to improving performance of ViT models. Through extensive experiments, we find that i ) knowledge distillation is helpful to a ViT model using a CNN teacher in capturing high-frequency components of the images; ii ) compared to the original ViT <ref type="bibr" target="#b22">[23]</ref>, some advanced architectures utilizing convolutional-like operation <ref type="bibr" target="#b23">[24]</ref> or multi-scale feature maps <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b13">14]</ref> can more effectively exploit high-frequency components of the images; iii ) RandAugment <ref type="bibr" target="#b14">[15]</ref> is more helpful for catching high-frequency components of the images than CutMix <ref type="bibr" target="#b84">[85]</ref> and Mixup <ref type="bibr" target="#b87">[88]</ref>.</p><p>Furthermore, we propose to compensate for the insufficient capacity of ViT models in capturing the high-frequency components of the images by directly augmenting the high-frequency components via adversarial training <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b86">87]</ref>. Specifically, we craft adversarial examples by altering clean images with high-frequency perturbations, and jointly train ViT models over clean images and adversarial examples. Our results indicate that this training strategy improves the performance of the ViT model by compensating for its ability to capture the highfrequency components of the images. Moreover, since adversarial perturbations can naturally influence the high-frequency components in our case, we directly use adversarial perturbations without high-frequency limitation, resulting in a simple but effective method, named HAT, standing for improving ViT models on the high-frequency components via adversarial training. Note that HAT does not bring extra complexity during the inference stage or alter the model architecture.</p><p>Our main contributions are summarized as follows:</p><p>-Based on our frequency analysis, we validate that compared to CNN models, ViT models are less effective in capturing the high-frequency components of images, which may lead to the difficulty of training ViT models. -We analyze the effects of existing techniques for improving the performance of ViT models from a frequency perspective. -We propose HAT, which improves the performance of ViT models by influencing the high-frequency components of images directly. -Our results on ImageNet classification and out-of-distribution data demonstrate the superiority of HAT. We also find that pre-trained models with HAT are beneficial to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Transformer Models in Vision Tasks. Transformer models <ref type="bibr" target="#b62">[63]</ref> entirely rely on the self-attention mechanism to build long-distance dependencies, which have achieved great success in almost all natural language processing tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b4">5]</ref>. Vision Transformer (ViT) <ref type="bibr" target="#b22">[23]</ref> is one of the earlier attempts to introduce transformer models into vision tasks, which applies a pure transformer architecture on non-overlapping image patches for image classification and has achieved state-ofthe-art accuracy. Since ViT models excel at capturing spatial information, they have also been extended to more challenging tasks, including object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b15">16]</ref>, segmentation <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b57">58]</ref>, image enhancement <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b79">80]</ref>, and video processing <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b69">70]</ref>. Besides, many efforts have been devoted to designing new ViT architectures <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b68">69]</ref>. For example, Liu et al. <ref type="bibr" target="#b43">[44]</ref> presented a hierarchical architecture with shifted window based attention that can efficiently extract multi-scale features; Yuan et al. <ref type="bibr" target="#b83">[84]</ref> introduced outlook attention to efficiently encode finer-level features and contexts into tokens.</p><p>Training Strategies for ViT Models. It is shown that training ViT models is more challenging than training CNN models, and requires large-scale datasets (e.g., ImageNet-22K <ref type="bibr" target="#b18">[19]</ref> and JFT-300M <ref type="bibr" target="#b58">[59]</ref>) to perform pre-training <ref type="bibr" target="#b22">[23]</ref>. To enable ViT to be effective on the smaller ImageNet-1K dataset <ref type="bibr" target="#b18">[19]</ref>, many training strategies have been explored. In <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b56">57]</ref>, applying strong data augmentation and model regularization makes a quick solution to this problem. Among them, CutMix <ref type="bibr" target="#b84">[85]</ref>, Mixup <ref type="bibr" target="#b87">[88]</ref>, and RandAugment <ref type="bibr" target="#b14">[15]</ref> are proven to be particularly helpful <ref type="bibr" target="#b61">[62]</ref>. Besides, some customized augmentations for training ViT models are presented <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b67">68]</ref>. Utilizing a trained CNN teacher, knowledge distillation (KD) <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b39">40]</ref> can significantly boost the performance of ViT models. There are also some works solving this problem by using a better optimization strategy, such as promoting patch diversification <ref type="bibr" target="#b26">[27]</ref> and sharpness-aware minimizer <ref type="bibr" target="#b11">[12]</ref>. Unlike these works, we focus on directly compensating for the ability of ViT models in capturing the high-frequency components for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Revisiting ViT Models from a Frequency Perspective</head><p>To investigate ViT models from a frequency perspective, we use the discrete Fourier transform (DFT) to evaluate the model performance on certain frequency components of test samples <ref type="bibr" target="#b81">[82]</ref>. Let x ? R H?W (omitting the dimension of image channels) and y ? R C represent an image in the spatial domain and its label vector, where C is the number of classes. We transform x to the frequency spectrum by the DFT F : R H?W ? C H?W and transform signals of the image from frequency back to the spatial domain by the inverse DFT F ?1 : C H?W ? R H?W . In this work, the low-frequency components are shifted to the center of the frequency spectrum. For a mask m ? {0, 1} H?W , the low-pass filtering M S l and high-pass filtering M S h with the filter size S are formally defined as:</p><formula xml:id="formula_0">M S l (x) = F ?1 (m?F(x)), where m i,j = 1, if min(|i? H 2 |, |j ? W 2 |) ? S 2 0, otherwise ,<label>(1)</label></formula><formula xml:id="formula_1">M S h (x) = F ?1 (m?F(x)), where m i,j = 0, if min(|i? H 2 |, |j ? W 2 |) ? min(H,W )?S 2 1, otherwise ,<label>(2)</label></formula><p>where ? is element-wise multiplication and m i,j denotes the value of m at position (i, j). For images containing multiple color channels, the filtering operates  on each channel independently. To make a comprehensive analysis, we evaluate various ViT architectures and training strategies with different filter sizes based on the ImageNet validation set. We provide the visualized examples in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Comparison of ViT and CNN Models. Firstly, we compare the performance of ViT-B with ResNet-50 and ResNet-101, which are trained with the same data augmentations. The plots in <ref type="figure" target="#fig_0">Figure 1</ref>(a) show that ViT-B surpasses ResNet on the low-frequency components of images. However, although ViT-B achieves a higher accuracy (82.0%) than ResNet-50 (79.8%) and ResNet-101 (81.6%) on the original ImageNet validation set, its performance is lower than CNN models on the high-frequency components of images, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). This observation indicates that ViT models can capture the global contexts effectively, but fails to well leverage local details compared to CNN models. It may be because cascading self-attention blocks in ViT models is equivalent to repeatedly applying a low-pass filter, corresponding to the theoretical justification in <ref type="bibr" target="#b66">[67]</ref>, while CNN models utilizing convolution operations behave like a series of highpass filters <ref type="bibr" target="#b49">[50]</ref> to catch more high-frequency components <ref type="bibr" target="#b65">[66]</ref>. We further study the distillation method introduced in [62], i.e., transferring the learned knowledge in a ViT model using a CNN teacher. We use a RegNetY-16GF model <ref type="bibr" target="#b53">[54]</ref> as a teacher with the hard-label distillation, and adopt all settings in <ref type="bibr" target="#b61">[62]</ref>. The results in <ref type="figure" target="#fig_0">Figure 1</ref> show that the improvement of KD (from 82.0% to 83.6%) is primarily attributed to the stronger ability to exploit the highfrequency components of images. It also confirms that there is a gap between the abilities of ViT and CNN models in capturing the high-frequency components. Various ViT Architectures. Recently, various ViT architectures are proposed and show excellent results <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b13">14]</ref>. We compare ViT-B with three advanced architectures, including ConViT-B <ref type="bibr" target="#b23">[24]</ref>, Twins-SVT-L <ref type="bibr" target="#b43">[44]</ref>, and Swin-B <ref type="bibr" target="#b13">[14]</ref>, with a similar model size, and present a reason for the success of these architectures from the frequency perspective. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, all architectures perform similarly on the low-frequency components, while three advanced architectures achieve higher accuracy than ViT-B on the high-frequency components. Our results also provide evidence for the effects of the proposed components of these <ref type="bibr" target="#b19">20</ref> 60 100 140 180 220</p><p>Filter Size   <ref type="bibr" target="#b14">[15]</ref> (+2.1%). We make a comparison between the effects of these three augmentations from the frequency perspective. The results are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We can see that the ranking of these three augmentations w.r.t. improvements they bring is CutMix &gt; Mixup &gt; RandAugment on the low-frequency components. However, on the high-frequency components, the case is opposite: RandAugment &gt; Mixup &gt; CutMix. Our observation reveals that CutMix can help ViT models leverage the global context information of an image by removing a random region and replacing it with a patch from another image. Moreover, it also indicates that the transformations used in RandAugment can force the trained model to pay more attention to high-frequency information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Proposed Method</head><p>In this section, we firstly describe the proposed HAT, and then demonstrate its effects on ViT models via a case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adversarial Training with High-frequency Perturbations</head><p>As demonstrated by the analysis in Section 3, the ability of ViT models to capture the high-frequency components is limited, and compensating for this limitation is a key to boosting their performance. Therefore, different from previous data augmentation methods, we propose to directly augment the high-frequency components during the training stage. We alter the high-frequency components of training images by adding adversarial perturbations and training ViT models on these altered images. It corresponds to adversarial training <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b86">87]</ref> with high-frequency perturbations and is stated formally below. Given a ViT model f with the weights ?, f ? (x) denotes its softmax output of the input sample x. Inspired by the min-max formulation of adversarial training <ref type="bibr" target="#b46">[47]</ref>, the objective function of adversarial training with high-frequency perturbations is as follows:</p><formula xml:id="formula_2">E (x,y)?D L ?, x,y + max ||?||??? ?L ?, x+M S h (?),y +?L kl ?, x+M S h (?), x ,<label>(3)</label></formula><p>where ? denotes the maximum perturbation strength. and KL(?) calculate the cross-entropy and the Kullback-Leibler divergence, respectively. ? and ? are two hyperparameters. We use the high-pass filtering M S h with a given filter size to limit the perturbations in the high-frequency domain. Our experiments in Section 4.2 demonstrate that optimizing Eq. (3) can compensate for the ability of the ViT model to capture the high-frequency components of images and thus improve its performance.</p><formula xml:id="formula_3">L(?, x, y) = CE(f ? (x), y) and L kl (?, x 1 , x 2 ) = 1 2 [KL(f ? (x 1 ), f ? (x 2 )) + KL(f ? (x 2 ), f ? (x 1 ))], where CE(?)</formula><p>Then, we notice that adversarial perturbations are naturally imposed on the high-frequency components in our case <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45]</ref>. It is validated in <ref type="figure" target="#fig_4">Figure 4</ref> that compared to natural images, adversarial perturbations show higher concentrations in the high-frequency domain. Therefore, we directly use full-frequency adversarial perturbations in our HAT with the below objective:</p><formula xml:id="formula_4">E (x,y)?D L ?, x, y + max ||?||??? ?L ?, x+?, y +?L kl ?, x+?, x .<label>(4)</label></formula><p>The inner maximization in Eq. (D.1) can be solved by project gradient descent (PGD) for K steps <ref type="bibr" target="#b46">[47]</ref>. Different from the standard PGD, following <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b24">25]</ref>, we accumulate the gradients of the model weights in each PGD step, and update the parameters at once with the accumulated gradients. In this way, the perturbations in each PGD step can be used for training. This procedure is detailed in Algorithm 1 in Appendix C. Besides, to address the mismatched distribution between clean images and adversarial examples <ref type="bibr" target="#b74">[75]</ref>, we perform adversarial training in some initial epochs (200 epochs in our setting) and train normally in the rest epochs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Case Study using ViT-B</head><p>To illustrate how the proposed method influences the ViT models, we conduct a case study using ViT-B on ImageNet. For training ViT-B, we adopt the hyperparameters in <ref type="bibr" target="#b27">[28]</ref> in all cases. Without considering our adversarial training, these hyper-parameters result in a strong baseline with a 82.0% top-1 accuracy. For our method, we set ? = 2/255, K = 3, and ? = 1/255. The parameters ? and ? are fixed at 3 and 0.01, respectively. We compare three types of adversarial perturbations: low-frequency perturbations with the filter size 10, high-frequency perturbations with the filter size 10, and full-frequency perturbations.</p><p>The results on the ImageNet validation set are shown in <ref type="table" target="#tab_1">Table 1</ref>. We can see that adversarial training with the high-frequency perturbations brings 1.0% gains over the baseline. <ref type="figure" target="#fig_5">Figure 5</ref> reveals the reason for that: the ability of ViT-B to capture the high-frequency components of images is stronger than the baseline, which exactly confirms our expectation. In contrast, there is no improvement for the case of using low-frequency perturbations. For adversarial training with the full-frequency perturbations, without the high-filter operation and setting the filter size, it is more simple, but can also improve the performance of ViT-B. This case study illustrates that using the full-frequency perturbations is a reasonable choice for our HAT, which will be further verified in our below experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We evaluate the proposed method on ImageNet <ref type="bibr" target="#b18">[19]</ref>. Our code is implemented based on PyTorch <ref type="bibr" target="#b50">[51]</ref> and the timm library <ref type="bibr" target="#b71">[72]</ref>. We conduct experiments on various model architectures: three variants of ViT <ref type="bibr" target="#b22">[23]</ref> (ViT-T, ViT-S, and ViT-B with 16?16 input patch size) following <ref type="bibr" target="#b61">[62]</ref>, three variants of Swin Transformer <ref type="bibr" target="#b43">[44]</ref> (Swin-T, Swin-S, and Swin-B), and two variants of VOLO <ref type="bibr" target="#b83">[84]</ref> (VOLO-D1 and VOLO-D5). "T", "S", and "B" denote tiny, small, and base model sizes, respectively. Following the standard training schedule, we train all models on the ImageNet-1K training set for 300 epochs with strong data augmentation (e.g., CutMix <ref type="bibr" target="#b84">[85]</ref>, Mixup <ref type="bibr" target="#b87">[88]</ref>, and RandAugment <ref type="bibr" target="#b14">[15]</ref>) and model regularization (e.g., stochastic depth <ref type="bibr" target="#b37">[38]</ref> and weight decay <ref type="bibr" target="#b45">[46]</ref>). Specifically, we use the hyperparameters in <ref type="bibr" target="#b27">[28]</ref> for training ViT-B, and train ViT-T and ViT-S with the same hyper-parameters except for throwing away EMA, resulting in strong baselines. For training variants of Swin Transformer and VOLO, we follow the training setup of the original paper <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b83">84]</ref> (including token labeling <ref type="bibr" target="#b39">[40]</ref> for VOLO). The default image resolution for these models is 224?224. We also finetune variants of VOLO on larger image resolutions (384?384, 448?448, and 512?512). For the proposed HAT, in all cases, the PGD learning rate ? is 1/255, and the parameters ? and ? are set to 3 and 0.01, respectively. We set the maximum perturbation strength ? as 2/255 and the number of PGD steps K as 3 by default. For VOLO-D5, the largest model in our experiments, we set K = 2 with ? = 1/255 to reduce the training time. For all ViT models, we adopt the training strategy in Algorithm 1 in the first 200 epochs and perform normal training in the rest 100 epochs. In our HAT, each PGD step requires one forward and backward pass. Accordingly, for the whole training, HAT leads to about 1.7? and 2.3? computation cost for K = 1 and K = 2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on ImageNet Classification</head><p>Results of Various ViT Architectures. We present the results of variants of ViT, Swin Transformer, and VOLO trained without and with our HAT in <ref type="table" target="#tab_2">Table 2</ref>. "Top-1", "Real Top-1", and "V2 Top-1" refer to the top-1 accuracy evaluated on the ImageNet <ref type="bibr" target="#b18">[19]</ref>, ImageNet-Real <ref type="bibr" target="#b2">[3]</ref>, and ImageNet-V2 <ref type="bibr" target="#b55">[56]</ref> validation set, respectively, where ImageNet-Real is built by relabeling the validation set of the original ImageNet for correcting labeling errors and ImageNet-V2 is a newly collected version of the ImageNet validation set. Note that these models in <ref type="table" target="#tab_2">Table 2</ref> are with different architectures and sizes, and the baselines are all carefully tuned with various data augmentation and model regularization techniques. As can be seen, HAT can steadily improve the performance of all models. To be specific, we boost top-1 accuracy on the ImageNet validation set by 1.1%, 0.8%, and 1.2% for ViT-T, ViT-S, and ViT-B, respectively. Even for Swin Transformer and VOLO, more advanced architectures, we can still consistently improve the performance of their variants. The performance gains of our method are preserved through finetuning at higher resolutions. In particular, when the image resolution is 512?512, VOLO-D5 with our HAT reaches a top-1 accuracy of 87.3% on the ImageNet validation set. Comparison to Other Methods with ViT-B. We compare our proposed HAT with other state-of-the-art training strategies in <ref type="table" target="#tab_3">Table 3</ref>. We conduct these experiments using ViT-B. Compared to DeiT <ref type="bibr" target="#b61">[62]</ref> and TransMix <ref type="bibr" target="#b10">[11]</ref>, which utilize data augmentations to empower ViT models, we achieve a significantly higher top-1 accuracy. Most closely related to our work is pyramid adversarial training (PyramidAT) <ref type="bibr" target="#b35">[36]</ref>, which leverages structured adversarial perturbations. However, due to a bigger number of PGD steps, its training cost is twice as high as ours, but resulting in a lower top-1 accuracy than our HAT. Besides the supervised training methods, we also compare with the methods that pre-train on the ImageNet-1K training set in a self-supervised manner and then perform supervised finetuning, including DINO <ref type="bibr" target="#b8">[9]</ref>, MoCo v3 <ref type="bibr" target="#b12">[13]</ref>, BEiT <ref type="bibr" target="#b1">[2]</ref>, and MAE <ref type="bibr" target="#b27">[28]</ref>. Our HAT shows very competitive performance with them. Furthermore, combining the proposed HAT with knowledge distillation <ref type="bibr" target="#b61">[62]</ref>, we obtain the performance of 84.3%, which is the highest top-1 accuracy among these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on Out-of-distribution Data</head><p>We evaluate the proposed HAT on five out-of-distribution datasets: ImageNet-A which contains 7,500 examples that are harder and may cause mistakes across various models <ref type="bibr" target="#b33">[34]</ref>; ImageNet-C <ref type="bibr" target="#b32">[33]</ref> which applies a set of common visual corruptions to the ImageNet validation set; ImageNet-Sketch <ref type="bibr" target="#b64">[65]</ref> which contains  <ref type="bibr" target="#b25">[26]</ref> which is a stylized version of ImageNet created by applying AdaIN style transfer <ref type="bibr" target="#b38">[39]</ref> to ImageNet images. We report the top-1 accuracy on all datasets, except ImageNet-C where we report the normalized mean Corruption Error (mCE) (lower is better) following the original paper <ref type="bibr" target="#b32">[33]</ref>.</p><p>The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. Note that all models are trained on the ImageNet-1K training set and tested on these five out-of-distribution datasets. As can be seen, our method can bring performance gains for all architectures, demonstrating that HAT can enhance the robustness of ViT models to out-ofdistribution data. Accordingly, HAT breaks the trade-off between in-distribution and out-of-distribution generalization <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b86">87]</ref>, or in other words, it can achieve better performance in these two cases simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Transfer Learning to Downstream Tasks</head><p>ImageNet pre-training is widely used in various vision tasks <ref type="bibr" target="#b28">[29]</ref>. For the downstream tasks, the backbones can be initialized by the model weights pre-trained on ImageNet. In this section, we demonstrate that the advantages of HAT can be transferred to the downstream tasks, including object detection, instance segmentation, and semantic segmentation. More implementation details can be found in Appendix D. Object Detection and Instance Segmentation. We take three variants of Swin Transformer trained without and with our HAT as pre-trained models to evaluate the performance in object detection and instance segmentation. The experiments are conducted on COCO 2017 <ref type="bibr" target="#b40">[41]</ref> with the Cascade Mask R-CNN object detection framework <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref>. We present the results in <ref type="table" target="#tab_5">Table 5</ref>. As can be seen, HAT helps three variants of Swin Transformer achieve higher detection performance. These results show that the superiority of HAT can be transferred   to downstream tasks. Moreover, we would like to emphasize that HAT does not introduce extra parameters or computation cost in inference. Semantic Segmentation. We also use Swin Transformer to evaluate the performance in semantic segmentation. We report results on the widely-used segmentation benchmark ADE20K <ref type="bibr" target="#b88">[89]</ref> with the UperNet <ref type="bibr" target="#b73">[74]</ref> segmentation framework. The results are shown in <ref type="table" target="#tab_6">Table 6</ref>. We can see that HAT brings significant gains for these three variants. Especially for Swin-T and Swin-S, the improvements of HAT are more than 1.0% mIOU. These results further show the benefits of our proposed HAT to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Studies</head><p>Ablation on Maximum Perturbation Strength. We ablate the effects of the maximum perturbation strength ?, where a larger ? indicates stronger adversarial examples for the adversarial training. We test HAT with ? ? {1/255, 2/255, 3/255, 4/255, 5/255}. Correspondingly, we set the number of PGD steps as K ? {2, 3, 4, 5, 6}. The PGD learning rate ? is fixed at 1/255. The results are shown in <ref type="table" target="#tab_7">Table 7</ref>. We can see that HAT can achieve superior performance when     <ref type="figure" target="#fig_6">Figure 6</ref>. It is shown that HAT can improve the performance of ViT-B under all sizes of the training set. Especially for the small-scale training set, HAT brings more gains, e.g., +7.1 for the smallest training set. It may be because the insufficient ability of ViT models to capture high-frequency components is amplified on the small-scale dataset and the effectiveness of HAT on compensating for this ability is more significant in this case. In short, our results illustrate that HAT enables ViT models to handle the small-scale training set better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+HAT Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Discussions</head><p>Fourier Heat Maps of ViT Models. We investigate the effects of HAT on the model sensitivity to low-and high-frequency corruptions. We adopt the Fourier heat map <ref type="bibr" target="#b81">[82]</ref>, which visualizes the error rates of a model tested on perturbed images with additive Fourier basis noise. We fix ? 2 -norm of the additive noise as 15.7 and average the error rates over the entire ImageNet validation set. Following <ref type="bibr" target="#b81">[82]</ref>, we present the 63?63 square centered at the lowest frequency in the Fourier domain. The Fourier heat maps of ViT-B trained without and with HAT are shown in <ref type="figure" target="#fig_7">Figure 7</ref>. As we can see, the baseline model is highly sensitive to additive noise in the high-frequency. In contrast, the model trained with HAT is more robust to the noise, especially in the high-frequency.  Longer Normal Training. To further verify the effectiveness of HAT, we increase the number of epochs of normal training to match the computational cost of HAT. The results are presented in <ref type="table" target="#tab_10">Table 8</ref>. We can see that 500 epochs are enough for the normal training to converge, and HAT surpasses normal training under 1.7? and 2.3? cost.</p><p>Beyond ViT Models. We explore the performance of HAT on the CNN and MLP models. We conduct experiments on ResNet-50 <ref type="bibr" target="#b30">[31]</ref> and ViP-Small-7 <ref type="bibr" target="#b36">[37]</ref>. We train ResNet-50 for 800 epochs following the setup in <ref type="bibr" target="#b72">[73]</ref> and ViP-Small-7 for 300 epochs following the setup in <ref type="bibr" target="#b36">[37]</ref>. For our HAT, we perform adversarial training in the first 600 epochs for ResNet-50 and in the first 200 epochs for ViP-Small-7, and keep other settings unchanged. The results in <ref type="table" target="#tab_11">Table 9</ref> show that HAT brings improvements of 0.4% and 0.6% for ResNet-50 and ViP-Small-7, respectively. Therefore, the proposed training strategy is promising to be extended to other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper, we study ViT models from a frequency perspective. We find that compared to CNN models, ViT models can not well exploit the high-frequency components of images. We also present a new frequency analysis of existing techniques for improving the performance of ViT models. To compensate for this insufficient ability of ViT models, we propose HAT, a simple but effective training strategy based on adversarial training. Extensive experiments verify its effectiveness on diverse benchmarks. Despite achieving higher performance, HAT has an increased training time compared to the normal training. Therefore, a future study is to improve the efficiency of the proposed HAT. Also, the insights provided in this paper further prompt us to explore other techniques to compensate for the ability of ViT models to capture the high-frequency components of images. where L dist ?, x, y, y t = 1 2 CE(f ? (x), y) + 1 2 CE(f ? (x), y t ) is the distillation loss and y t is the hard decision of the teacher model. We also keep the optimization and all hyper-parameters unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Algorithm Pipeline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Object Detection and Instance Segmentation</head><p>We take three variants of Swin Transformer trained without and with our HAT as pre-trained models to evaluate the performance in object detection and instance segmentation. The experiments are conducted on COCO 2017 <ref type="bibr" target="#b40">[41]</ref>, which contains 118K training, 5K validation, and 20K test-dev images. We use the Cascade Mask R-CNN object detection framework <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> with multi-scale training (resizing the input such that the shorter side is between 480 and 800 while the longer side is at most 1,333), AdamW optimizer (initial learning rate of 0.0001, <ref type="bibr" target="#b19">20</ref> 60 100 140 180 220</p><p>Filter Size weight decay of 0.05, and batch size of 16), and 3x schedule (36 epochs). Our implementation is based on Swin Transformer and more details can be found in <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Semantic Segmentation</head><p>We also use Swin Transformer to evaluate the performance in semantic segmentation. We report results on the widely-used segmentation benchmark ADE20K <ref type="bibr" target="#b88">[89]</ref>, where ADE20K contains 25K images in total, including 20K images for training, 2K images for validation, and 3K images for test. And the UperNet <ref type="bibr" target="#b73">[74]</ref> is selected as the segmentation framework. In training, we follow the setup of the original paper <ref type="bibr" target="#b43">[44]</ref>. Specifically, we utilize the AdamW optimizer with an initial learning rate of 6 ? 10 ?5 and a weight decay of 0.01, and we set the linear learning schedule with a minimum learning rate of 5 ? 10 ?6 . Models are trained on 8 GPUs with 2 images per GPU for 160K iterations. In inference, we perform multi-scale test with interpolation rates of [0.75, 1.0, 1.25, 1.5, 1.75].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Frequency Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Large-scale Pre-training</head><p>We analyze supervised (i.e., ImageNet-22K) and self-supervised (i.e., SimMIM <ref type="bibr" target="#b76">[77]</ref>) pre-training in <ref type="figure" target="#fig_8">Figure 8</ref>. It shows that both are helpful for exploiting highfrequency components, especially for SimMIM. Besides, we directly use HAT in fine-tuning under the SimMIM pre-training setting. Specifically, we perform adversarial training in the first 80 epochs for ViT-b and normal training in the rest 20 epochs, and keep other settings unchanged. Training with HAT results in an 83.9% top-1 accuracy, which is better than an 83.8% top-1 accuracy reported in its paper and an 83.6% top-1 accuracy in our reproduction without HAT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Knowledge Distillation</head><p>Here we also analyze the knowledge distillation with CNN (i.e., RegNetY-16GF) and ViT (i.e., Swin-B) teacher in <ref type="figure" target="#fig_9">Figure 9</ref>. It indicates that, compared to the ViT teacher, the CNN teacher is more helpful for ViT-B to capture the highfrequency components.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Comparison of ViT-B, ResNet-50, ResNet-101, and ViT-B(+KD) on lowand high-pass filtered validation set with different filter sizes. ViT-B is the base ViT model taking as input a sequence of 16?16 patches. KD denotes knowledge distillation, where the teacher model is a RegNetY-16GF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Comparison of ViT-B, ConViT-B, Twins-SVT-L, and Swin-B on low-and high-pass filtered validation set with different filter sizes. The top-1 accuracy of ViT-B, ConViT-B, Twins-SVT-L, and Swin-B on the ImageNet validation set is 82.0%, 82.3%, 83.7%, and 83.5%, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison of vanilla training and three data augmentations on low-and high-pass filtered validation set with different filter sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Heat maps of Fourier spectrum for natural images and adversarial perturbations. They are obtained by averaging over a batch of data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of the baseline and adversarial training (AT) with three types of perturbations on low-and high-pass filtered validation set with different filter sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Performance of ViT-B trained without and with the proposed HAT on various sizes of training sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fourier heat maps of ViT-B trained without and with HAT. The Fourier heat map reflects the sensitivity of a model to high-and low-frequency corruptions. Error rates are averaged over the entire ImageNet validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Comparison of vanilla training and two pre-training methods on low-and high-pass filtered validation set with different filter sizes. The top-1 accuracy of ImageNet-1K Vanilla Training, ImageNet-22K Pre-training, and ImageNet-1K SimMIM Pre-training on the ImageNet validation set is 76.7%, 84.5%, and 83.8%, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Comparison of ViT-B and ViT-B (+KD) with different teacher models on low-and high-pass filtered validation set with different filter sizes. The top-1 accuracy of ViT-B, ViT-B (+KD, Teacher: Swin-B), and ViT-B (+KD, Teacher: RegNetY-16GF) on the ImageNet validation set is 82.0%, 82.8%, and 83.6%, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the baseline and adversarial training (AT) with three types of perturbations, where the case of using the full-frequency perturbations corresponds to the proposed HAT.</figDesc><table><row><cell></cell><cell>20 40 60 80 Accuracy (%)</cell><cell>Low-pass Filtering</cell><cell cols="2">20 40 60 80 Accuracy (%)</cell><cell>High-pass Filtering +AT (Full-freq. Pert.) +AT (High-freq. Pert.) +AT (Low-freq. Pert.) Baseline</cell></row><row><cell></cell><cell cols="3">20 60 100 140 180 220 Filter Size</cell><cell>170 180 190 200 210 220 Filter Size</cell></row><row><cell>Training Strategy</cell><cell>Top-1 ACC (%)</cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>82.0</cell><cell></cell><cell></cell></row><row><cell>+AT (Low-freq. Pert.)</cell><cell>81.9</cell><cell></cell><cell></cell></row><row><cell>+AT (High-freq. Pert.)</cell><cell>83.0</cell><cell></cell><cell></cell></row><row><cell>+AT (Full-freq. Pert.)</cell><cell>83.2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of various ViT architectures trained without and with the proposed HAT on the ImageNet, ImageNet-Real, and ImageNet-V2 validation set.</figDesc><table><row><cell>Model</cell><cell cols="6">Params FLOPs Test Size Top-1 Real Top-1 V2 Top-1</cell></row><row><cell>ViT-T +HAT</cell><cell>5.7M</cell><cell>1.6G</cell><cell>224</cell><cell>72.2 73.3</cell><cell>80.0 81.1</cell><cell>60.1 61.0</cell></row><row><cell>ViT-S +HAT</cell><cell>22.1M</cell><cell>4.7G</cell><cell>224</cell><cell>80.1 80.9</cell><cell>85.7 86.6</cell><cell>68.2 70.0</cell></row><row><cell>ViT-B +HAT</cell><cell cols="2">86.6M 17.6G</cell><cell>224</cell><cell>82.0 83.2</cell><cell>87.1 87.9</cell><cell>71.0 72.6</cell></row><row><cell>Swin-T +HAT</cell><cell>28.3M</cell><cell>4.5G</cell><cell>224</cell><cell>81.2 82.0</cell><cell>86.8 87.3</cell><cell>70.5 71.5</cell></row><row><cell>Swin-S +HAT</cell><cell>49.6M</cell><cell>8.7G</cell><cell>224</cell><cell>83.0 83.3</cell><cell>87.8 87.7</cell><cell>72.4 72.8</cell></row><row><cell>Swin-B +HAT</cell><cell cols="2">87.8M 15.4G</cell><cell>224</cell><cell>83.5 84.0</cell><cell>87.9 88.2</cell><cell>72.9 73.8</cell></row><row><cell cols="2">VOLO-D1 26.6M +HAT</cell><cell>6.8G</cell><cell>224</cell><cell>84.2 84.5</cell><cell>89.0 89.2</cell><cell>74.0 74.9</cell></row><row><cell cols="3">VOLO-D1 26.6M 22.8G +HAT</cell><cell>384</cell><cell>85.2 85.5</cell><cell>89.6 89.8</cell><cell>75.6 76.6</cell></row><row><cell cols="3">VOLO-D5 295.5M 69.0G +HAT</cell><cell>224</cell><cell>86.1 86.3</cell><cell>89.9 90.2</cell><cell>76.3 76.8</cell></row><row><cell cols="3">VOLO-D5 295.5M 304G +HAT</cell><cell>448</cell><cell>87.0 87.2</cell><cell>90.6 90.6</cell><cell>77.8 78.6</cell></row><row><cell cols="3">VOLO-D5 295.5M 412G +HAT</cell><cell>512</cell><cell>87.1 87.3</cell><cell>90.6 90.7</cell><cell>78.0 78.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of HAT with other training strategies.</figDesc><table><row><cell>All re-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of various ViT architectures trained without and with the proposed HAT on five out-of distribution datasets. Note that for mean Corruption Error (mCE), lower is better. The test resolution of all below models is 224?224.</figDesc><table><row><cell>Model</cell><cell cols="2">ImageNet-A Top-1 +HAT Top-1</cell><cell>ImageNet-C mCE? +HAT mCE?</cell><cell>Sketch Top-1 +HAT Top-1</cell><cell>Rendition Top-1 +HAT Top-1</cell><cell>Stylized +HAT Top-1 Top-1</cell></row><row><cell>ViT-T</cell><cell>7.7</cell><cell>7.3</cell><cell>70.0 66.8</cell><cell>19.8 22.9</cell><cell>31.9 35.8</cell><cell>9.5 12.5</cell></row><row><cell>ViT-S</cell><cell cols="2">18.5 23.1</cell><cell>53.3 49.7</cell><cell>29.3 32.3</cell><cell>41.6 45.1</cell><cell>15.8 18.1</cell></row><row><cell>ViT-B</cell><cell cols="2">25.3 30.6</cell><cell>46.4 42.2</cell><cell>36.1 38.5</cell><cell>49.6 51.3</cell><cell>21.8 24.7</cell></row><row><cell>Swin-T</cell><cell cols="2">22.1 25.7</cell><cell>58.0 53.9</cell><cell>28.5 31.0</cell><cell>41.4 43.8</cell><cell>13.1 13.8</cell></row><row><cell>Swin-S</cell><cell cols="2">32.7 34.6</cell><cell>51.8 48.6</cell><cell>32.7 33.9</cell><cell>45.2 46.5</cell><cell>14.2 15.1</cell></row><row><cell>Swin-B</cell><cell cols="2">35.8 40.0</cell><cell>51.7 46.9</cell><cell>32.2 36.4</cell><cell>45.8 49.0</cell><cell>15.7 16.4</cell></row><row><cell cols="3">VOLO-D1 39.0 42.9</cell><cell>46.8 43.7</cell><cell>38.5 39.5</cell><cell>50.3 51.9</cell><cell>19.2 21.2</cell></row><row><cell cols="3">VOLO-D5 50.9 54.5</cell><cell>41.8 38.4</cell><cell>44.3 45.7</cell><cell>57.9 59.7</cell><cell>24.6 25.9</cell></row></table><note>sketch-like images and matches the ImageNet validation set in categories and scale; ImageNet-Rendition [32], a 30,000 image test set containing various ren- ditions (e.g., paintings, embroidery); Stylized ImageNet</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Object detection and instance segmentation performance of the models pre-trained without and with HAT on COCO val 2017. We adopt the Cascade Mask R-CNN object detection framework. AP box and AP mask are box average precision and mask average precision, respectively.Backbone Params FLOPs AP box AP box 50 AP box 75 AP mask AP mask 50</figDesc><table><row><cell>AP mask 75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Semantic segmentation performance of the models pre-trained without and with HAT on the ADE20K validation set. We adopt the UperNet segmentation framework. MS denotes testing with variable input size.</figDesc><table><row><cell cols="5">Backbone Params FLOPs mIoU mIoU(MS) mAcc</cell></row><row><cell>Swin-T +HAT</cell><cell>60M 945G</cell><cell>44.5 45.6</cell><cell>46.1 46.7</cell><cell>55.6 57.4</cell></row><row><cell>Swin-S +HAT</cell><cell>81M 1038G</cell><cell>47.6 48.1</cell><cell>49.5 49.7</cell><cell>58.8 59.5</cell></row><row><cell>Swin-B +HAT</cell><cell>121G 1088G</cell><cell>48.1 48.9</cell><cell>49.7 50.3</cell><cell>59.1 60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">: Performance of</cell></row><row><cell cols="3">ViT-B trained with the</cell></row><row><cell cols="3">proposed HAT under dif-</cell></row><row><cell cols="3">ferent maximum pertur-</cell></row><row><cell cols="2">bation strength ?.</cell><cell></cell></row><row><cell>? Top-1</cell><cell>Real Top-1</cell><cell>V2 Top-1</cell></row><row><cell cols="3">1/255 83.1 87.8 72.4</cell></row><row><cell cols="3">2/255 83.2 87.9 72.6</cell></row><row><cell cols="3">3/255 83.1 88.0 72.5</cell></row><row><cell cols="3">4/255 83.1 87.9 72.5</cell></row><row><cell cols="3">5/255 83.0 87.9 72.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>This ablation study and other experiments in this paper verify that ? = 2/255 and K = 3 are reasonable choices in most cases. Scaling the Training Set Size. HAT In this part, we investigate the effects of with various training set sizes. Specifically, we experiment on training sets with different sizes by randomly sampling 1/8, 1/4, and 1/2 images from the original ImageNet-1K training set, resulting in training sets with 16K, 32K, and 64K samples. Then, we train ViT-B on these datasets without and with the proposed HAT and evaluate on the original ImageNet validation set. The comparison is presented in</figDesc><table><row><cell>? is set as 2/255 or 3/255. It demonstrates that medium strength adversarial</cell></row><row><cell>examples are more helpful for training ViT models than weaker or stronger ad-</cell></row><row><cell>versarial examples.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Comparison of HAT and longer</figDesc><table><row><cell cols="4">normal training with ViT-B under vari-</cell></row><row><cell cols="2">ous computational costs.</cell><cell></cell></row><row><cell>Cost</cell><cell cols="3">Normal Training Setting Top-1 Setting Top-1 +HAT</cell></row><row><cell cols="2">1? 300 epochs 82.0</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">1.7? 500 epochs 82.5</cell><cell cols="2">K = 2 83.1</cell></row><row><cell cols="2">2.3? 690 epochs 82.4</cell><cell cols="2">K = 3 83.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Performance of CNN (ResNet-50) and MLP (ViP-Small/7) models trained without and with the proposed HAT.</figDesc><table><row><cell>Model</cell><cell cols="3">Top-1 Real Top-1 V2 Top-1</cell></row><row><cell>ResNet-50</cell><cell>79.8</cell><cell>86.9</cell><cell>70.7</cell></row><row><cell>+HAT</cell><cell>80.2</cell><cell>87.2</cell><cell>71.3</cell></row><row><cell cols="2">ViP-Small/7 81.6</cell><cell>85.5</cell><cell>67.8</cell></row><row><cell>+HAT</cell><cell>82.2</cell><cell>86.1</cell><cell>69.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Algorithm 1</head><label>1</label><figDesc>Training a ViT model with HAT for one epoch Input: Training set X = {(x, y)}, model weights ?, learning rate ? , maximum perturbation strength ?, number of PGD steps K, PGD step size ? 1: for minibatch B ? X do Combining HAT with Knowledge DistillationWe combine HAT with knowledge distillation by optimizing the loss function, which is obtained by replacing the cross-entropy loss in Eq. (4) with the distillation loss used in DeiT<ref type="bibr" target="#b61">[62]</ref>, as follows:E (x,y)?D L dist ?, x, y, y t + max</figDesc><table><row><cell>2:</cell><cell>?0 ? 0</cell><cell></cell></row><row><cell>3:</cell><cell>for t = 1...K do</cell><cell></cell></row><row><cell>4:</cell><cell cols="2">? Compute gradients of model weights and perturbations simultaneously</cell></row><row><cell>5:</cell><cell>if t = 1 then</cell><cell></cell></row><row><cell>6:</cell><cell>? ? , ? ? ? ?L ?, x + ?0, y</cell><cell></cell></row><row><cell>7:</cell><cell>else</cell><cell></cell></row><row><cell>8:</cell><cell cols="2">? ? , ? ? ? ? 1 K?1 [?L(?, x+?t?1, y)+?L kl (?, x+?t?1, x)]</cell></row><row><cell>9:</cell><cell>end if</cell><cell></cell></row><row><cell>10:</cell><cell>gt ? gt?1+? ?</cell><cell>? Accumulate gradients of model weights</cell></row><row><cell>11:</cell><cell>?t ? clip(?t?1+ ? ? sign(? ? ), ??, ?)</cell><cell>? Update and clip perturbations</cell></row><row><cell>12:</cell><cell>end for</cell><cell></cell></row><row><cell>13:</cell><cell>? ? ? ? ? ? gK</cell><cell>? Update model weights</cell></row><row><cell cols="2">14: end for</cell><cell></cell></row><row><cell cols="2">D Implementation Details</cell><cell></cell></row><row><cell>D.1</cell><cell></cell><cell></cell></row></table><note>||?||??? ?L dist ?, x+?, y, y t +?L kl ?, x+?, x ,</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">l (A k v)||2 = 0 for any vector v ? R n , where A k denotes the product of A with itself k times.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A More Related Works</head><p>Deep neural networks (DNNs) have been showing state-of-the-art performances in various vision tasks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b80">81]</ref>, however, previous works mainly investigated DNNs in the spatial domain, and also it is still an open problem to understand their mechanisms. Recently, many researchers take the Fourier transformation as a tool to improve DNNs' performance or analyze their behaviors <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b66">67]</ref>. One recent work <ref type="bibr" target="#b65">[66]</ref> investigates the CNN models from a frequency perspective. It reveals that CNN can exploit the high-frequency image components that are not perceivable to human, and demonstrates that it helps the generalization behaviors of CNN models. The method in <ref type="bibr" target="#b77">[78]</ref> pre-processes the inputs in the frequency domain to better preserve image information and achieve improved accuracy in various vision tasks. Inspired by properties of the Fourier transformation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52]</ref>, Xu et al. <ref type="bibr" target="#b78">[79]</ref> proposed a novel Fourier-based data augmentation strategy called amplitude mix to help domain generalization. Also, there are some works studying ViT models from the frequency domain. Park et al. demonstrated that <ref type="bibr" target="#b49">[50]</ref> multi-head self-attentions in ViT models are low-pass filters, but convolutional layers are high-pass filters, and proposed a novel architecture to combine these two operations. Theoretically, Wang et al. explained that <ref type="bibr" target="#b66">[67]</ref> cascading self-attention blocks in ViT models is equivalent to repeatedly applying a low-pass filter. Besides, most recent works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b75">76]</ref> solved the self-supervised visual pre-training through the lens of the frequency domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Theoretical Analysis</head><p>Based on definitions of the low-pass filtering M S l and high-pass filtering M S h in Section 3, following <ref type="bibr" target="#b66">[67]</ref>, we present Theorem 1 for the attention map A ? R n?n produced by the softmax function in ViT models. It is proven based on the property of the matrix A and the details can be found in <ref type="bibr" target="#b66">[67]</ref>. Theorem 1 reveals that the self-attention mechanism diminishes the high-frequency information with depth increasing, which further confirms our hypothesis. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alphafold at casp13</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="4862" to="4865" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Approximating cnns with bag-of-local-features models works surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Application of fourier analysis to the visibility of gratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Robson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">551</biblScope>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09833</idno>
		<title level="m">Transmix: Attend to mix for vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">When vision transformers outperform resnets without pre-training or strong data augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shield: Fast, practical defense and vaccination for deep learning using jpeg compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kounavis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Valois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>De Valois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of psychology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="309" to="341" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mutual component convolutional neural networks for heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3102" to="3114" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL-HLT</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12753</idno>
		<title level="m">Vision transformers with patch diversification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15121</idno>
		<title level="m">Pyramid adversarial training improves vit performance</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vision permutator: A permutable mlp-like architecture for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">All tokens matter: Token labeling for training better vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The devil is in the frequency: Geminated gestalt autoencoder for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08227</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Feature distillation: Dnn-oriented jpeg compression against adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Phase in speech and pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kopec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pohlig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<publisher>ICASSP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The importance of phase in signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="529" to="541" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">How do vision transformers work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Pytorch: an imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A demonstration of the visual importance and flexibility of spatial-frequency amplitude and phase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Piotrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="346" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End2end occluded face recognition by masking corrupted features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Understanding and mitigating the tradeoff between robustness and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<title level="m">Do imagenet classifiers generalize to imagenet? In: ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10270</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Rethinking transformer-based set prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The lifting scheme: A construction of second generation wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sweldens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on mathematical analysis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="546" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">High-frequency component helps explain the generalization of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Crossformer: A versatile vision transformer hinging on cross-scale attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">A discriminative deep feature learning approach for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4414861" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<title level="m">Resnet strikes back: An improved training procedure in timm</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Masked frequency modeling for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07706</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Learning in the frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">A fourier-based framework for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Larnet: Lie algebra residual network for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11738" to="11750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">A fourier perspective on model robustness in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gontijo Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Volo: Vision outlooker for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>El Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Freelb: Enhanced adversarial training for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
