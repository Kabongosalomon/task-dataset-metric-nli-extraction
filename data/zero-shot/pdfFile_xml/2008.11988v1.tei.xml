<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cloze Test Helps: Effective Video Anomaly Detection via Learning to Complete Video Events Video anomaly detection, video event completion ACM Reference Format</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Wang</surname></persName>
							<email>wangsiqi10c@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Cai</surname></persName>
							<email>zpcai@nudt.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En</forename><surname>Zhu</surname></persName>
							<email>enzhu@nudt.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanfu</forename><surname>Xu</surname></persName>
							<email>xuchuanfu@nudt.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Yin</surname></persName>
							<email>jpyin@dgut.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
							<email>kloft@cs.uni-kl.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanfu</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Dongguan University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<settlement>Kaiserslautern</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cloze Test Helps: Effective Video Anomaly Detection via Learning to Complete Video Events Video anomaly detection, video event completion ACM Reference Format</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">20</biblScope>
							<date type="published">October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413973</idno>
					<note>ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00 KEYWORDS and Marius Kloft. 2020. Cloze Test Helps: Effective Video Anomaly De-tection via Learning to Complete Video Events. In Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20), October 12-16, 2020, Seattle, WA, USA. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS CONCEPTS ? Computing methodologies ? Scene anomaly detection; Un- supervised learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a vital topic in media content interpretation, video anomaly detection (VAD) has made fruitful progress via deep neural network (DNN). However, existing methods usually follow a reconstruction or frame prediction routine. They suffer from two gaps: (1) They cannot localize video activities in a both precise and comprehensive manner.</p><p>(2) They lack sufficient abilities to utilize high-level semantics and temporal context information. Inspired by frequently-used cloze test in language study, we propose a brand-new VAD solution named Video Event Completion (VEC) to bridge gaps above: First, we propose a novel pipeline to achieve both precise and comprehensive enclosure of video activities. Appearance and motion are exploited as mutually complimentary cues to localize regions of interest (RoIs). A normalized spatio-temporal cube (STC) is built from each RoI as a video event, which lays the foundation of VEC and serves as a basic processing unit. Second, we encourage DNN to capture high-level semantics by solving a visual cloze test. To build such a visual cloze test, a certain patch of STC is erased to yield an incomplete event (IE). The DNN learns to restore the original video event from the IE by inferring the missing patch. Third, to incorporate richer motion dynamics, another DNN is trained to infer erased patches' optical flow. Finally, two ensemble strategies using different types of IE and modalities are proposed to boost VAD performance, so as to fully exploit the temporal context and modality information for VAD. VEC can consistently outperform state-ofthe-art methods by a notable margin (typically 1.5%-5% AUROC) on commonly-used VAD benchmarks. Our codes and results can be verified at github.com/ yuguangnudt/ VEC_VAD. * Authors contributed equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Videos play a key role in multimedia. Video anomaly detection (VAD), which performs anomaly detection by interpreting the video content automatically, has been appealing to both academia and industry, since it is valuable to various safety-critical scenarios like municipal and traffic management. Formally, VAD refers to detecting video activities that divert significantly from the observed normal routine. Despite many efforts, VAD remains challenging for two features of anomaly <ref type="bibr" target="#b4">[5]</ref>: (1) Scarcity. As anomalies are usually rare, collecting real anomalies for training is often hard or even impossible.</p><p>(2) Ambiguity. The anomaly does not possess fixed semantics and may refer to different activities based on different context, so it can be highly variable and unpredictable. Such features render modeling anomalies directly unrealistic. Thus, VAD usually adopts an one-class classification setup <ref type="bibr" target="#b14">[15]</ref>. This setup collects training videos with only normal activities, which are much more accessible than anomalies, to build a normality model. Activities that do not conform to this model are then viewed as anomalies. As all training data are normal, discriminative supervised learning is usually not applicable. Instead, the unsupervised/self-supervised learning has been the commonly-used scheme in VAD. Following such a scheme, existing VAD solutions fall into two categories: <ref type="bibr" target="#b0">(1)</ref> Classic VAD, which requires domain knowledge to design handcrafted descriptors to depict high-level features (e.g. trajectory, speed) or low-level features (e.g. gradient, texture) of video activities.  <ref type="figure">Figure 1</ref>: Typical solutions for DNN based VAD. Left: Reconstruction based methods train DNN to reconstruct inputs, e.g. video frames/patches/cubes. Middle: Frame prediction based methods take several previous frames as inputs of DNN to predict current frame. Right: VEC first encloses video events with spatio-temporal cubes (STCs) based on a novel pipeline that synthesizes appearance and motion cues. Then, erasing the patch at STC's different positions produces different types of incomplete events (IEs), which serves as different "visual cloze tests". Each DNN is trained to solve a visual cloze test, i.e. learning to complete the missing patch of a certain type of IE. Note that cubes for reconstruction differs from STCs in VEC, as they are yielded by a relatively coarse strategy (e.g. sliding windows) and cannot enclose video events both precisely and comprehensively.</p><p>like one-class support vector machine (OCSVM) to spot anomalies. Feature engineering of classic VAD can be labor-intensive and suboptimal <ref type="bibr" target="#b39">[40]</ref>, and designed descriptors are often hard to transfer among different scenes. (2) Deep neural network (DNN) based VAD, which is inspired by DNN's success in traditional vision tasks <ref type="bibr" target="#b18">[19]</ref>. Due to DNNs' strong capabilities in feature learning and activity modeling <ref type="bibr" target="#b10">[11]</ref>, DNN based VAD achieves superior performance and enjoys surging popularity when compared with classic VAD.</p><p>Despite the fruitful progress, DNN based VAD still suffers from two gaps. Gap #1: Existing DNN based VAD methods cannot localize video activities in a both precise and comprehensive manner. A standard practice in VAD is to use a sliding window with motion filtering <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref>, but such localization is obviously imprecise. Recent DNN based VAD methods like <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref> simply ignore this issue by learning on the whole frame, but this suffers from scale variations incurred by image depth and foreground-background imbalance problem <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b47">48]</ref>. Few studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> improve localization precision by a pre-trained object detector, but it causes a "closed world" problem-The detector only recognize objects in training data and tends to omit video novelties, which leads to non-comprehensive localization results. Such a gap degrades later video activitiy modeling; Gap #2: Existing DNN based methods lack sufficient abilities to exploit high-level semantics and temporal context in video activities. As illustrated by <ref type="figure">Fig. 1</ref>, two paradigms (reconstruction and frame prediction) dominate DNN based VAD in the literature: Reconstruction based methods learn to reconstruct inputs and detect poorly reconstructed data as anomalies. However, in this case DNNs tend to memorize low-level details rather than learning high-level semantics <ref type="bibr" target="#b17">[18]</ref>, and they even reconstruct anomalies well due to overly strong modeling power <ref type="bibr" target="#b9">[10]</ref>; Frame prediction based methods learn to predict a normal video frame from previous video frames, and detect poorly predicted frames as anomalies. Prediction makes it hard to simply memorize details for reducing training loss, but it scores anomalies by the prediction error of a single frame, which overlooks the temporal context. Thus, neither reconstruction nor frame prediction provides a perfect solution. Unlike recent research that focuses on exploring better network architectures to improve reconstruction or frame prediction, we are inspired by cloze test in language study and mitigate gaps above by proposing Video Event Completion (VEC) as a new DNN based VAD solution (see <ref type="figure">Fig. 1</ref>). Our contributions are summarized below:</p><p>? VEC for the first time combines both appearance and motion cues to localize video activities and extract video events. It overcomes the "closed world" problem and enables both precise and comprehensive video activity enclosure, and it lays a firm foundation for video event modeling in VEC. ? VEC for the first time designs visual cloze tests as a new learning paradigm, which trains DNNs to complete the erased patches of incomplete video events, to substitute frequentlyused reconstruction or frame prediction based methods. ? VEC also learns to complete the erased patches' optical flow, so as to integrate richer information of motion dynamics. ? VEC utilizes two ensemble strategies to fuse detection results yielded by different types of incomplete events and data modalities, which can further boost VAD performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Classic VAD. Classic VAD usually consists of two stages: Feature extraction by hand-crafted descriptors and anomaly detection by classic machine learning methods. As to feature extraction, early VAD methods usually adopt tracking <ref type="bibr" target="#b16">[17]</ref> to extract high-level features like motion trajectory <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b43">44]</ref> and destination <ref type="bibr" target="#b2">[3]</ref>. However, they are hardly applicable to crowded scenes <ref type="bibr" target="#b25">[26]</ref>. To this end, lowlevel features are extensively studied for VAD, such as dynamic texture <ref type="bibr" target="#b25">[26]</ref>, histogram of optical flow <ref type="bibr" target="#b6">[7]</ref>, spatio-temporal gradients <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>, 3D SIFT <ref type="bibr" target="#b5">[6]</ref>, etc. Afterwards, various classic machine learning methods are explored to perform anomaly detection, such as probabilistic models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref>, sparse coding and its variants <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b44">45]</ref>, one-class classifier <ref type="bibr" target="#b42">[43]</ref>, sociology inspired models <ref type="bibr" target="#b26">[27]</ref>. However, feature extraction is the major bottleneck for classic VAD: Manual feature engineering is complicated and labor-intensive, while the designed descriptors often suffer from limited discriminative power and poor transferability among different scenes.</p><p>DNN Based VAD. DNN based VAD differs from classic VAD by learning features automatically from raw inputs with DNNs. The learned features are fed into a classic model or embedded into DNNs for end-to-end VAD. With only normal videos for training, existing DNN based VAD basically falls into a reconstruction or frame prediction routine. They are reviewed respectively below: (1) Reconstruction based methods learn to reconstruct inputs from normal training videos, and assume that a large reconstruction error signifies the anomaly. Autoencoder (AE) and its variants are the most popular DNNs to perform reconstruction. For example, <ref type="bibr" target="#b38">[39]</ref> pioneers DNN based VAD by introducing stacked denoising AE (SDAE) and propose its improvement <ref type="bibr" target="#b39">[40]</ref>; <ref type="bibr" target="#b10">[11]</ref> adopts convolutional AE (CAE) that are more suitable for modeling videos, while recent works explore numerous CAE variants such as Winner-takeall CAE (WTA-CAE) <ref type="bibr" target="#b36">[37]</ref> and Long Short Term Memory based CAE (ConvLSTM-AE) <ref type="bibr" target="#b23">[24]</ref>; <ref type="bibr" target="#b40">[41]</ref> integrates variational AE into a twostream recurrent framework (R-VAE) to realize VAD; <ref type="bibr" target="#b0">[1]</ref> equips AE with a parametric density estimator (PDE-AE) for anomaly detection; <ref type="bibr" target="#b9">[10]</ref> propose a memory-augmented AE (Mem-AE) to make AE's reconstruction error more discriminative. In addition to AE, other types of DNNs are also used for the reconstruction purpose, e.g. sparse coding based recurrent neural network (SRNN) <ref type="bibr" target="#b24">[25]</ref> and generative adversarial network (GAN) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>. Cross-modality reconstruction is also shown to produce good VAD performance by learning the appearance-motion correspondence (AM-CORR) <ref type="bibr" target="#b28">[29]</ref>.</p><p>(2) Frame prediction based methods learn to predict current frames by several previous frames, while a poor prediction is viewed as abnormal. <ref type="bibr" target="#b20">[21]</ref> for the first time formulates frame prediction as an independent VAD method, and imposes appearance and motion constraints for prediction quality. <ref type="bibr" target="#b22">[23]</ref> improves frame prediction by using a convolutional variational RNN (Conv-VRNN). However, prediction by a per-frame basis leads to a bias to background <ref type="bibr" target="#b19">[20]</ref>, and <ref type="bibr" target="#b47">[48]</ref> proposes attention mechanism to ease the issue. Another natural idea is to combine prediction with reconstruction as a hybrid VAD solution: <ref type="bibr" target="#b45">[46]</ref> design a spatio-temporal CAE (ST-CAE), in which an encoder is followed by two decoders for reconstruction and prediction purpose respectively; <ref type="bibr" target="#b27">[28]</ref> reconstructs and predicts human skeletons by a message-passing encoder-decoder RNN (MPED-RNN); <ref type="bibr" target="#b41">[42]</ref> integrates reconstruction into prediction by a predictive coding network based framework (AnoPCN); <ref type="bibr" target="#b35">[36]</ref> conducts prediction and reconstruction in a sequential manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIDEO EVENT COMPLETION (VEC) 3.1 Video Event Extraction</head><p>In this paper, video event extraction aims to enclose video activities by numerous normalized spatio-temporal cubes (STCs). A STC is then viewed as a video event, which serves as the basic processing unit in VEC. A both precise and comprehensive localization of video activities is the key to video event extraction. Ideally, precise localization expects the subject of a video activity is intactly extracted with minimal irrelevant background, while comprehensive localization requires all subjects associated with video activities are extracted. As explained by Gap #1 (Sec. 1) and the intuitive comparison in <ref type="figure" target="#fig_1">Fig. 2</ref>, existing DNN based VAD methods fail to realize precise and comprehensive localization simultaneously, which undermines the quality of video activity modeling and VAD performance. Hence, we exploit appearance and motion as mutually complementary cues to achieve both precise and comprehensive localization ( <ref type="figure" target="#fig_1">Fig. 2 (d)</ref>). This new pipeline is detailed as follows: Motivation. As video activities are behaviors conducted by certain subjects in videos, we consider both appearance cues from those subjects and motion cues from their behaviors to localize regions of interest (RoIs) that enclose those video activities. To utilize appearance cues, a natural solution is modern object detection <ref type="bibr" target="#b3">[4]</ref>. With pre-training on large-scale public datasets like Microsoft COCO, pre-trained detectors can precisely localize RoIs with frequentlyseen objects. Therefore, we use a pre-trained object detector to realize precise localization of most video activities by detecting their associated subjects, e.g. humans. However, pre-trained detectors only detect objects in the "closed world" formed by known object classes in the training dataset. This leads to a fatal problem: Anomalies, which are often novel classes outside the "closed world", will be omitted. To this end, motion cues like temporal gradients are proposed as complimentary information to accomplish more comprehensive RoI localization. More importantly, we argue that appearance and motion cues should not be isolated: RoIs already localized by appearance should be filtered when exploiting motion cues, which reduces computation and makes motion based RoI localization more precise (see <ref type="figure" target="#fig_1">Fig. 2 (d)</ref>). As illustrated by the overview in <ref type="figure">Fig. 3</ref> and Algorithm 1, we elaborate each component of the new video event extraction pipeline below.</p><p>Appearance Based RoI Extraction. Given a video frame I and a pre-trained object detector model M, our goal is to obtain a RoI set B a based on appearance cues from subjects of video activities, where B a ? R 4 and each entry of B a refers to a RoI enclosed by a bounding box. The bounding box is denoted by the coordinates of its top-left and bottom-right vertex, which is a 4-dimensional vector. As shown by the green module in <ref type="figure">Fig. 3</ref>, we first feed I into M, and obtain a preliminary RoI set B ap with confidence scores above the threshold T s (class labels are discarded). Then, we introduce two efficient heuristic rules to filter unreasonable RoIs: (1) RoI area threshold T a that filters out overly small RoIs. (2) Overlapping ratio T o that removes RoIs that are nested or significantly overlapped with larger RoIs in B ap . In this way, we ensure that extracted RoIs can precisely enclose subjects of most everyday video activities.</p><p>Motion Based RoI Extraction. To enclose those activities outside the "closed world", motion based RoI extraction aims to yield a complementary bounding box set B m based on motion cues. We leverage the temporal gradients of frames as motion cues and complementary information. As shown by the red module of <ref type="figure">Fig. 3</ref> ... threshold T ? , so as to yield a binary map that indicates regions with intense motion. Instead of using this map directly, we propose to subtract appearance based RoIs B a from the map, which benefits motion based RoI extraction in two ways: First, the subtraction of appearance based RoIs enables us to better localize objects that are not detected by appearance cues, otherwise the gradient map of multiple objects may be overlapped and jointly produce large and imprecise RoIs (see <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>). Second, the subtraction reduces the computation. Finally, we propose to perform contour detection to yield the contour and its corresponding bounding box b m , while simple heuristics (RoI area threshold T a and maximum aspect-ratio threshold T ar ) are used to obtain final RoI set B m . Based on two complementary RoI sets, the final RoI set B = B a ? B m . The whole RoI extraction process is formally presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(D-1) Frames Current Frame</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatio-Temporal Cube Construction Motion Based RoI Extraction</head><p>Spatio-temporal Cube Construction. Finally, we use each RoI in B to build a spatio-temporal cube (STC) as the video event, which represents the fundamental unit to enclose video activities. As shown by yellow module in <ref type="figure">Fig. 3</ref>, we not only extract the patch p 1 in the RoI from current frame, but also extract corresponding patches p 2 , ? ? ? , p D by this RoI from previous (D ? 1) frames. In this way, we incorporate the temporal context into the extracted video event. To normalize video activities with different scales, we resize patches from the same RoI into H ? W new patches p ? 1 , ? ? ? , p ? D , which are then stacked into a H ? W ? D STC: C = [p ? 1 ; ? ? ? ; p ? D ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Cloze Tests</head><p>As explained by Gap #2 in Sec. 1, previous methods typically rely on a reconstruction or frame prediction paradigm. They cannot fully exploit high-level semantics and temporal context information.</p><p>As a remedy, we propose to build visual cloze tests as a new learning paradigm, so as to model normal video activities represented by STCs above. We present it in terms of the following aspects: Motivation. We are inspired by cloze test, an extensively-used exercise in language learning and instruction. It requires completing a text with its certain words erased. Cloze test aims to test students' ability to understand the vocabulary semantics and language context <ref type="bibr" target="#b37">[38]</ref>. Recently, learning to solve cloze tests is also shown to be a fairly effective pretraining method in natural language processing (NLP), which enables DNN to capture richer high-level semantics from text <ref type="bibr" target="#b7">[8]</ref>. This naturally inspires us to compare the patch sequence of a STC to the word sequence in classic cloze test. Similarly, we can erase a certain patch p ? i in video event (STC) to build a visual cloze test, which is solved by completing the resultant incomplete event (IE) with the DNN's inferredp ? i . Such a learning paradigm benefits DNN in two aspects: (1) In order to complete such a visual cloze test, DNN is encouraged to capture high-level semantics in STC. For example, suppose a video event contains a walking person. DNN must attend to those key moving parts (e.g. the forwarding leg and swinging arm) in the patch to achieve a good completion. This makes visual cloze test more meaningful than frequently-used reconstruction, which tends to memorize every low-level detail of inputs to minimize training loss.</p><p>(2) Since any patch in a STC can be erased to generate an IE, we can build multiple cloze tests by erasing the patch at different temporal positions. This enables us to fully exploit the temporal context by enumerating all possible IEs for cloze tests. By contrast, prediction based VAD methods only consider prediction errors of a single frame to detect anomalies, which involves poor temporal context information in video activities. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, we detail VEC's components below. Appearance Completion. Given j-th video event represented by the STC C j = [p ? j,1 ; ? ? ? ; p ? j, D ], we first erase the i-th patch p ? j,i of C j to build an IE C (i) j = [p ? j,1 ; ? ? ? p ? j,i?1 ; p ? j,i+1 ; ? ? ? p ? j, D ] as a cloze test, i ? {1, ? ? ? D} (blue module in <ref type="figure" target="#fig_3">Fig. 4</ref>). All IEs built by erasing the i-th patch of the STC are collected as the type-i IE set</p><formula xml:id="formula_0">C (i) = {C (i) 1 , ? ? ? C (i) N },</formula><p>where N is the number of extracted video events (STCs). Afterwards, as shown by red module in <ref type="figure" target="#fig_3">Fig. 4</ref>, a typei IE C (i) j in C (i) and its corresponding erased patch p ? j,i are used as the input and learning target respectively to train a generative DNN f </p><formula xml:id="formula_1">L (i) a = 1 N N j=1 ?p ? j,i ? p ? j,i ? p p<label>(1)</label></formula><p>where ? ? ? p denotes p-norm. In our experiments, we found that choosing p = 2 already works well. To further improve the fidelity of generated patch, other values of p or techniques like adversarial training can also be explored to design L (i) a . For inference, any error measure S (i) a (p ? j,i , p ? j,i ) can be used to yield the appearance anomaly score of patch p ? j,i , such as mean square error (MSE) or Peak Signal to Noise Ratio (PSNR) <ref type="bibr" target="#b20">[21]</ref>. Empirically, choosing S (i)</p><formula xml:id="formula_2">a (p ? j,i , p ? j,i )</formula><p>to be simple MSE has been effective enough to score anomalies, and poorly completed STCs with higher MSE are more likely to be anomalies. Since appearance completion is actually a challenging learning task for DNN, an independent DNN f (i) a is trained to handle one IE type C (i) . Otherwise, using one DNN for all IE types will degrade the performance of appearance completion.</p><p>Motion Completion. Motion is another type of valuable prior in videos. Optical flow, which estimates the pixel-wise motion velocity and direction between two consecutive frames, is a popular low-level motion representation in videos. We feed two consecutive frames into a pre-trained FlowNet model <ref type="bibr" target="#b12">[13]</ref>, and a forward pass can yield the optical flow efficiently. For each STC C j , we extract optical flow patches o j,1 , ? ? ? o j, D that correspond to video patches p j,1 , ? ? ? p j, D , and also resize them into H ? W patches o ? j,1 , ? ? ? o ? j, D . Motion completion requires a DNN f </p><formula xml:id="formula_3">L (i) m = 1 N N j=1 ?? ? j,i ? o ? j,i ? p p<label>(2)</label></formula><p>Likewise, we also adopt p = 2 for L (i) m and simple MSE to compute the motion anomaly score S</p><formula xml:id="formula_4">(i) m (? ? j,i , o ? j,i ) during inference.</formula><p>With motion completion, we encourage DNN to infer the motion statistics from the temporal context provided by IEs, which enables VEC to consider richer motion dynamics. The process of both appearance and motion completion for a type-i IE are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>Ensemble Strategies. Ensemble is a powerful tool that combines multiple models into a stronger one <ref type="bibr" target="#b8">[9]</ref>. During inference, we propose two ensemble strategies to improve the VEC performance:</p><p>(1) IE type ensemble. Erasing a different patch in STC produces a different IE, which contains a different patch combination as temporal context. To fully exploit all possible temporal context for VAD, we compute the final appearance anomaly score by an ensemble of scores, which are yielded by multiple DNNs for different IE types:</p><formula xml:id="formula_5">S a (C j ) = 1 D D i=1 S (i) a (p ? j,i , p ? j,i )<label>(3)</label></formula><p>IE type ensemble is also applicable to the final motion score S m (C j ):</p><formula xml:id="formula_6">S m (C j ) = 1 D D i=1 S (i) m (? ? j,i , o ? j,i )<label>(4)</label></formula><p>(2) Modality ensemble. Since two different modalities, raw pixels and optical flow, are considered to perform completion in VEC, we must fuse their results to yield the overall anomaly score. For simplicity, we use a weighted sum of S a (C j ) and S m (C j ) to compute the overall anomaly score S(C j ) for a video event C j :</p><formula xml:id="formula_7">S(C j ) = w a ? S a (C j ) ?S a ? a + w m ? S m (C j ) ?S m ? m<label>(5)</label></formula><p>whereS a , ? a ,S m , ? m denote the means and standard deviations of appearance and motion scores for all normal events in training, which are used to normalize appearance and motion scores into the same scale. In addition to this straightforward weighting strategy, other more sophisticated strategies like late fusion <ref type="bibr" target="#b39">[40]</ref> are also applicable to achieve better modality ensemble performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION 4.1 Experimental Settings</head><p>Evaluation is performed on three most commonly-used benchmark datasets for VAD: UCSDped2 <ref type="bibr" target="#b25">[26]</ref>, Avenue <ref type="bibr" target="#b21">[22]</ref> and ShanghaiTech <ref type="bibr" target="#b24">[25]</ref>. For video event extraction, cascade R-CNN <ref type="bibr" target="#b3">[4]</ref> pre-trained on COCO dataset is used as object detector as it achieves a good trade-off between performance and speed. Other parameters are set as follows for UCSDped2, Avenue and ShanghaiTech respectively: Confidence score threshold T s : (0.5, 0.25, 0.5); RoI area threshold T a : (10 ? 10, 40 ? 40, 8 ? 8); Overlapping ratio T o : (0.6, 0.6, 0.65); Gradient binarization threshold T ? : <ref type="bibr" target="#b17">(18,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15)</ref>; Maximum aspect-ratio threshold T ar = 10. For cube construction, we set H = W = 32 and D = 5. As to VEC, we adopt U-Net <ref type="bibr" target="#b32">[33]</ref> as the basic network architecture of generative DNNs (see <ref type="figure" target="#fig_7">Fig. 5</ref>), which are optimized by the default Adam optimizer in PyTorch <ref type="bibr" target="#b29">[30]</ref>. Considering the dataset scale, DNNs are trained by 5, 20, 30 epochs with a batch size 128 on UCSDped2, Avenue and ShanghaiTech respectively. For anomaly scoring, we set (w a , w m ) to be (0.5, 1), (1, 1) and (1, 0.5) for UCSDped2, Avenue and ShanghaiTech respectively. For quantitative evaluation, we adopt the most frequently-used metric: Area Under the Receiver Operating Characteristic curves (AUROC) that are computed with frame-level detection criteria <ref type="bibr" target="#b25">[26]</ref>. Frame-level Equal Error Rate (EER) <ref type="bibr" target="#b25">[26]</ref> is also reported in the supplementary material. We run experiments on a PC with 64 GiB RAM, Nvidia Titan Xp GPUs and a 3.6GHz Intel i7-9700k CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-Art methods</head><p>Within our best knowledge, we extensively compare VEC's performance with 18 state-of-the-art DNN based VAD methods reviewed in Sec. 2. Note that we exclude <ref type="bibr" target="#b13">[14]</ref> from comparison as it actually uses a different evaluation metric from commonly-used frame-level AUROC, which leads to an unfair comparison. As discussed in Sec. 2, existing methods can be categorized into reconstruction based, frame prediction based and hybrid methods in <ref type="table">Table 1</ref>. As to VEC, we design two configurations for IE type ensemble: (1) VEC-A: IE type ensemble is applied to appearance completion, while it is  not applied to motion completion (i.e. only type-D IEs are used to train the DNN for motion completion). (2) VEC-AM: IE type ensemble is applied to both appearance completion and motion completion. Besides, modality ensemble is applied to both VEC-A and VEC-AM. The results are reported in <ref type="table">Table 1</ref>, and we visualize the yielded frame-level ROC curves in <ref type="figure" target="#fig_8">Fig. 6</ref>. We draw the following observations: First, both VEC-A and VEC-AM consistently outperform existing state-of-the-art DNN based VAD methods on three benchmarks. In particular, we note that VEC achieves notable performance gain on recent challenging benchmarks (Avenue and ShanghaiTech) with constantly ? 3% and ? 1% AUROC improvement respectively against all state-of-the-art methods. Meanwhile, we note that VEC-A even achieves over 90% frame-level AUROC on Avenue, which is the best performance ever achieved on Avenue dataset to our knowledge. In terms of the comparison between VEC-A and VEC-AM, two configurations yield fairly close performance, despite of slight differences on different datasets. As a consequence, in addition to those thoroughly-studied reconstruction or prediction based methods, the proposed VEC provides a highly promising alternative for DNN based VAD with state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Detailed Analysis</head><p>Ablation Studies. To show the role of the proposed video event extraction and ensemble strategies, we perform corresponding ablation studies and display the results in <ref type="table">Table 2</ref>: (1) As to video event extraction, we compare four practices for localizing video activities: Frame (FR, i.e. no localization at all), multi-scale sliding windows with motion filtering (SDW), appearance based RoI extraction only (APR) and the proposed appearance and motion based RoI extraction (APR+MT). Note that we did not report SDW's results on ShanghaiTech, since it produces excessive STCs that are beyond the limit of our hardware, which is actually an important downside of SDW. There are several observations: First, with the same ensemble strategies, the proposed APR+MT constantly outperforms other methods by a sensible margin. Specifically, APR+MT has an obvious advantage (2.7%-4.6% AUROC gain) over FR and SDW, which are commonly-used strategies of recent VAD methods. Interestingly, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>UCSDped2 Avenue ShanghaiTech Reconstruction Based Methods CAE <ref type="bibr" target="#b10">[11]</ref> 85.0% 80.0% 60.9% SDAE + OCSVM <ref type="bibr" target="#b39">[40]</ref> 90.8% --SRNN <ref type="bibr" target="#b24">[25]</ref> 92.2% 81.7% 68.0% GAN <ref type="bibr" target="#b31">[32]</ref> 93.5% --ConvLSTM-AE <ref type="bibr" target="#b23">[24]</ref> 88.1% 77.0% -WTA-CAE + OCSVM <ref type="bibr" target="#b36">[37]</ref> 96.6% 82.1% -R-VAE <ref type="bibr" target="#b40">[41]</ref> 92.4% 79.6% -PDE-AE <ref type="bibr" target="#b0">[1]</ref> 95.4% -72.5% Mem-AE <ref type="bibr" target="#b9">[10]</ref> 94.1% 83.3% 71.2% AM-CORR <ref type="bibr" target="#b28">[29]</ref> 96.2% 86.9% -AnomalyNet <ref type="bibr" target="#b46">[47]</ref> 94  <ref type="table">Table 1</ref>: AUROC comparison between the proposed VEC and state-of-the-art VAD methods.</p><p>note that SDW performs worse than FR on UCSDped2 and Avenue. This indicates that an imprecise localization of video activities even degrades VAD performance, and it also justifies the importance of a precise localization. Meanwhile, when compared with APR, the proposed APR+MT brings evident improvement by 1.8%, 2.5% and 1.2% AUROC gain on UCSDped2, Avenue and ShanghaiTech respectively. Such observations demonstrate that a more comprehensive localization of video activities will contribute to VAD performance.</p><p>(2) As to ensemble strategies, we compare three cases: Not using IE type ensemble (for both appearance and motion completion), not using modality ensemble (w m = 0), and both IE type and modality ensemble are used (VEC-AM). We yield the following observations: First, IE type ensemble contributes to VAD performance by 1.3%, 2.1% and 0.4% AUROC on UCSDped2, Avenue and ShanghaiTech respectively, which justifies the importance to fully exploit temporal context. Second, modality ensemble enables a remarkable 8% AUROC gain on UCSDped2. This is because UCSDped2 contains low-resolution gray-scale frames, and motion clues are more important for detecting anomalies. For Avenue and ShanghaiTech with high-resolution colored frames, modality ensemble also enables over 1% AUROC improvement, although using the modality of raw pixel only already leads to satisfactory performance. Visualization. To show how visual cloze tests in VEC helps discriminating anomalies in a more intuitive way, we visualize generated patches and optical flow of representative normal/abnormal video events in <ref type="figure" target="#fig_9">Fig. 7</ref>. Heat maps are used for a better visualization of the pixel-wise completion errors. By <ref type="figure" target="#fig_9">Fig. 7</ref>, it is worth noting several phenomena: First of all, VEC can effectively complete normal events and their optical flow. For normal events, minor completion errors are observed to be distributed around foreground contour in a relatively uniform manner, and their optical flow can also be Dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Event Extraction</head><p>Ensemble AUROC FR SDW APR APR+MT IE Type Modality UCSDped2 " " " 94.6% " " " 93.3% " " " 95.5% " " 96.0% " " 89.6%</p><p>" " " 97.3%</p><p>Avenue " " " 86.8% " " " 85.2% " " " 87.1% " " 87.5% " "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>88.2%</head><p>" " " 89.6%</p><p>ShanghaiTech " " " 70.2% " " " -" " " 73.6% " " 74.4% " "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>73.5%</head><p>" " " 74.8% <ref type="table">Table 2</ref>: Ablation Studies for VEC.</p><p>soundly recovered. By contrast, abnormal events produces prominent completion errors in terms of both raw pixel and optical flow completion. Next, it is noted that the distribution of anomalies' completion errors is highly non-uniform. As shown by heat maps, large completion errors are often observed at those regions that have clear high-level semantics, e.g. the bicycle that the man was riding with (UCSDped2), falling paper with its shape and position wrongly inferred (Avenue), the backpack that was thrown (Shang-haiTech). By contrast, other regions are endowed with relatively smaller errors. Such observations imply that VEC indeed attends to those parts with high-level semantics in abnormal events. Other Remarks. (1) VEC adopts a similar U-Net architecture to previous works, but it achieves significantly better performance, which exactly verifies visual cloze tests' effectiveness as a new learning paradigm. Thus, better network architecture can be explored, while techniques like adversarial training and attention mechanism are also applicable. In fact, VEC with only type-D IEs can be viewed as predicting the last patch of STCs (as shown in <ref type="table">Table 2</ref>, it is also better than frame prediction <ref type="bibr" target="#b20">[21]</ref>). Besides, when visual cloze tests in VEC are replaced by plain reconstruction, experiments report a 3% to 7% AUROC loss on benchmarks, which demonstrates that our video event extraction and visual cloze tests are both indispensable.</p><p>(2) Details of VEC's computation cost and parameter sensitivity are also discussed in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose VEC as a new solution to DNN based VAD. VEC first extracts STCs by exploiting both appearance and motion cues, which enables both precise and comprehensive video event extraction. Subsequently, motivated by the widely-used cloze test, VEC learns to solve visual cloze tests, i.e. training DNNs to infer deliberately erased patches from incomplete video events/STCs, so as to learn better high-level semantics. Motion modality is also involved by using DNNs to infer the erased patches' optical flow. Two ensemble strategies are further adopted to fully exploit temporal context and motion dynamics, so as to enhance VAD performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Extracted features are fed into classic anomaly detection methods arXiv:2008.11988v1 [cs.CV] 27 Aug 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of localization strategies: Sliding window (a) or motion only (b) produces imprecise localization, while appearance only (c) yields non-comprehensive localization results. The proposed (d) pipeline achieves more precise and comprehensive localization simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Algorithm 1 4 :</head><label>314</label><figDesc>Pipeline of video event extraction: (1) Appearance based RoI extraction (green): Appearance based RoIs are extracted with a pre-trained object detector and filtered based on efficient heuristic rules. (2) Motion based RoI extraction (red): First, temporal gradients are binarized by magnitude into a binary map. Then, highlighted pixels in appearance based RoIs are subtracted from the binary map. Finally, contour detection and simple heuristics are applied to the binary map for final motion based RoIs. (3) Spatio-temporal cube (STC) extraction (yellow): For each RoI, corresponding patches from current frame and (D ? 1) previous frames are extracted. D patches are then resized and stacked into a STC, which represents a video event. Appearance and Motion based RoI Extraction Input: Frame I and its gradient map G, pre-trained object detector M, threshold T s ,T a ,T o ,T ? ,T ar Output: RoIs represented by a bounding box set B 1: B ap ? ObjDet(I, M,T s ) # Detecting activity subjects 2: B a = {} # Heuristic filtering 3: for b ap ? B ap do if Area(b ap ) &gt; T a and Overlap(b ap , B ap ) &lt; T o then 5: B a = B a ? {b ap } 6: end if 7: end for 8: G b ? GradBin(G,T ? ) # Gradient binarization 9: G b ? RoISub(G b , B a ) # Subtract appear. based RoIs 10: C ? ContourDet(G b ) # Contour detection 11: B m = {} 12: for c ? C do 13: b m = Boundin?Box(c) # Get contour bounding box 14: if Area(b m ) &gt; T a and 1 T ar &lt; AspectRatio(b m ) &lt; T ar then 15: B m = B m ? {b m } 16: end if 17: end for 18: B = B a ? B m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visual cloze tests with a type-i incomplete event (IE): (1) Erase the i-th patch (blue): The i-th patch of a STC is erased to build a type-i IE, while the erased patch is used as the learning target of appearance completion. (2) Appearance completion (green): To complete the IE, a DNN takes the IE as input and learns to generate the erased patch. (3) Motion completion (red): A DNN takes the IE as input and learns to generate the optical flow patch that corresponds to the erased patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>aj</head><label></label><figDesc>(e.g. autoencoder, generative adversarial networks, U-Net, etc.), which aims to generate a patchp ? j,i = f into the original event C j . To train f (i) a , here we can simply minimize the pixel-wise appearance loss L (i) a for type-i IE set C (i) :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>m</head><label></label><figDesc>to infer the optical flow patch o ? j,i of the erased patch p ? j,i by the type-i IE C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>DNN architecture used in our experiments. Appearance completion network f (i) a and motion completion network f (i) m share the same U-Net architecture, except that f (i) a has 3 output channels (images) while f (i) m has 2 (optical flow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of frame-level ROC curves on different VAD benchmark datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of erased patches and their optical flow (Target), completed patches (Output) by VEC and completion errors (Error). Brighter color indicates larger errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, we first binarize the absolute values of temporal gradients by a</figDesc><table><row><cell>Appearance Based</cell><cell></cell></row><row><cell>RoI Extraction</cell><cell></cell></row><row><cell></cell><cell>Patches Resized</cell></row><row><cell></cell><cell>and Sacked</cell></row><row><cell>Appearance Based</cell><cell></cell></row><row><cell>RoI Subtraction</cell><cell>Spatio-Temporal Cube</cell></row><row><cell></cell><cell>(Video Event)</cell></row><row><cell>Appearance and Motion</cell><cell></cell></row><row><cell>RoI Combination</cell><cell></cell></row><row><cell>Gradient Binarization</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video parsing for abnormality detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borislav</forename><surname>Anti?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2415" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning object motion patterns for anomaly detection and improved object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Basharat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Gritai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization using hierarchical feature representation and Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yie-Tarng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsien</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2909" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on multiple classifier systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3619" to="3627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object-centric auto-encoders and dummy anomalies for abnormal event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">One-class classification: taxonomy of study and review of techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shehroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael G</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="345" to="374" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ko</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1446" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-online Multi-people Tracking by Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Boesen Lindbo Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><forename type="middle">Kaae</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring Background-bias for Anomaly Detection in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1490" to="1499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Future Frame Prediction Using Convolutional VRNN for Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Seyed Shahabeddin Nabavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Remembering history with convolutional lstm for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Viral Bhalodia, and Nuno Vasconcelos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
	<note>Anomaly detection in crowded scenes</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning Regularity in Skeleton Trajectories for Anomaly Detection in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romero</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11996" to="12004" />
		</imprint>
	</monogr>
	<note>Moussa Mansour, and Svetha Venkatesh</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Anomaly Detection in Video Sequence with Appearance-Motion Correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Trong-Nguyen Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meunier</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1273" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Trajectorybased anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gian</forename><forename type="middle">Luca</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1544" to="1554" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep-anomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Integrating prediction and reconstruction for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="123" to="130" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Anomaly detection using a convolutional winner-take-all autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference 2017. British Machine Vision Association</title>
		<meeting>the British Machine Vision Conference 2017. British Machine Vision Association</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Cloze test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Cloze_test" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>8.1-8.8</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Detecting anomalous events in videos by learning deep representations of appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection from Videos using a Two-stream Recurrent Variational Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">AnoPCN: Video Anomaly Detection via Deep Predictive Coding Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1805" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sensor-based abnormal human-activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey Junfeng</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1082" to="1090" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning semantic scene models by object classification and trajectory clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1940" to="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatio-temporal autoencoder for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">AnomalyNet: An anomaly detection network for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick Siow Mong</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention-Driven Loss for Anomaly Detection in Video Surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
