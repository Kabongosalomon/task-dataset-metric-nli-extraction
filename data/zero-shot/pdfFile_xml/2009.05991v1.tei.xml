<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GIKT: A Graph-based Interaction Model for Knowledge Tracing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
							<email>yanruqu2@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoming</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<email>wnzhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GIKT: A Graph-based Interaction Model for Knowledge Tracing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Knowledge Tracing ? Graph Neural Network ? Information Interaction</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rapid development in online education, knowledge tracing (KT) has become a fundamental problem which traces students' knowledge status and predicts their performance on new questions. Questions are often numerous in online education systems, and are always associated with much fewer skills. However, the previous literature fails to involve question information together with high-order question-skill correlations, which is mostly limited by data sparsity and multi-skill problems. From the model perspective, previous models can hardly capture the long-term dependency of student exercise history, and cannot model the interactions between student-questions, and student-skills in a consistent way. In this paper, we propose a Graph-based Interaction model for Knowledge Tracing (GIKT) to tackle the above probems. More specifically, GIKT utilizes graph convolutional network (GCN) to substantially incorporate question-skill correlations via embedding propagation. Besides, considering that relevant questions are usually scattered throughout the exercise history, and that question and skill are just different instantiations of knowledge, GIKT generalizes the degree of students' master of the question to the interactions between the student's current state, the student's history related exercises, the target question, and related skills. Experiments on three datasets demonstrate that GIKT achieves the new state-of-the-art performance, with at least 1% absolute AUC improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In online learning platforms such as MOOCs or intelligent tutoring systems, knowledge tracing (KT) <ref type="bibr" target="#b5">[6]</ref> is an essential task, which aims at tracing the knowledge state of students. At a colloquial level, KT solves the problem of predicting whether the students can answer the new question correctly according to their arXiv:2009.05991v1 [cs.AI] 13 Sep 2020 previous learning history. The KT task has been widely studied and various methods have been proposed to handle it.</p><p>Existing KT methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b1">2]</ref> commonly build predictive models based on the skills that the target questions correspond to rather than the questions themselves. In the KT task, there exists several skills and lots of questions where one skill is related to many questions and one question may correspond to more than one skill, which can be represented by a relation graph such as the example shown in <ref type="figure" target="#fig_1">Figure 1</ref>. Due to the assumption that skill mastery can reflect whether the students are able to answer the related questions correctly to some extent, it is a feasible alternative to make predictions based on the skills just like previous KT works.  Although these pure skill-based KT methods have achieved empirical success, the characteristics of questions are neglected, which may lead to degraded performance. For instance, in <ref type="figure" target="#fig_1">Figure 1</ref>, even though the two questions q 2 and q 3 share the same skills, their different difficulties may result in different probabilities of being answered correctly. To this end, several previous works <ref type="bibr" target="#b13">[14]</ref> utilize the question characteristics as a supplement to the skill inputs. However, as the number of questions is usually large while many students only attempt on a small subset of questions, most questions are only answered by a few students, leading to the data sparsity problem <ref type="bibr" target="#b27">[28]</ref>. Besides, for those questions sharing part of common skills (e.g. q 1 and q 4 ), simply augmenting the question characteristics loses latent inter-question and inter-skill information. Based on these considerations, it is important to exploit high-order information between the questions and skills.</p><p>In this paper, we first investigate how to effectively extract the high-order relation information contained in the question-skill relation graph. Motivated by the great power of Graph Neural Networks (GNNs) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10]</ref> to extract graph representations by aggregating information from neighbors, we leverage a graph convolutional network (GCN) to learn embeddings for questions and skills from high-order relations. Once the question and skill embeddings are aggregated, we can directly feed question embeddings together with corresponding answer embeddings as the input of KT models.</p><p>In addition to the input features, another key issue in KT is the model framework. Recent advances in deep learning simulate a fruitful line of deep KT works, which leverage deep neural networks to sequentially capture the changes of students' knowledge state. Two representive deep KT models are Deep Knowledge Tracing (DKT) <ref type="bibr" target="#b20">[21]</ref> and Dynamic Key-Value Memory Networks (DKVMN) <ref type="bibr" target="#b34">[35]</ref> which leverage Recurrent Neural Networks (RNN) <ref type="bibr" target="#b30">[31]</ref> and Memory-Augmented Neural Networks (MANN) respectively to solve KT. However, they are notoriously unable to capture long-term dependencies in a question sequence <ref type="bibr" target="#b0">[1]</ref>. To handle this problem, Sequential Key-Value Memory Networks (SKVMN) <ref type="bibr" target="#b0">[1]</ref> proposes a hop-LSTM architecture that aggregates hidden states of similar exercises into a new state and Exercise-Enhanced Recurrent Neural Network with Attention mechanism (EERNNA) <ref type="bibr" target="#b24">[25]</ref> uses the attention mechanism to perform weighted sum aggregation for all history states.</p><p>Instead of aggregating related history information into a new state for prediction directly, we take a step further towards improving long-term dependency capture and better modeling student's mastery degree. Inspired by SKVMN and EERNNA, we introduce a recap module to select several the most related hidden exercises according to the attention weight with the intention of noise reduction. Considering the mastery of the new question and its related skills, we generalize the interaction module and interact the relevant exercises and the current hidden states with the aggregated question embeddings and skill embeddings. The generalized interaction module can better model student's mastery degree of question and skills. Besides, an attention mechanism is applied on each interaction to make final predictions, which automatically weights the prediction utility of all the interactions.</p><p>To sum up, in this paper, we propose an end-to-end deep framework, namely Graph-based Interaction for Knowledge Tracing (GIKT), for knowledge tracing. Our main contributions are summarized as follows: 1) By leveraging a graph convolutional network to aggregate question embeddings and skill embeddings, GIKT is capable to exploit high-order question-skill relations, which mitigates the data sparsity problem and the multi-skill issue. 2) By introducing a recap module followed by an interaction module, our model can better model the student's mastery degree of the new question and its related skills in a consistent way. 3) Empirically we conduct extensive experiments on three benchmark datasets and the results demonstrate that our GIKT outperforms the state-ofthe-art baselines substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Tracing</head><p>Existing knowledge tracing methods can be roughly categorized into two groups: traditional machine learning methods and deep learning methods. In this paper, we mainly focus on the deep KT methods.</p><p>Traditional machine learning KT methods mainly involve two types: Bayesian Knowledge Tracing (BKT) <ref type="bibr" target="#b5">[6]</ref> and factor analysis models. BKT is a hidden Markov model which regards each skill as a binary variable and uses bayes rule to update state. Several works extends the vanilla BKT model to incorporate more information into it such as slip and guess probabilty <ref type="bibr" target="#b1">[2]</ref>, skill difficulty <ref type="bibr" target="#b18">[19]</ref> and student individualization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref>. On the other hand, factor analysis models focus on learning general paramaters from historical data to make predictions. Among the factor analysis models, Item Response Theory (IRT) <ref type="bibr" target="#b7">[8]</ref> models parameters for student ability and question difficulty, Performance Factors Analysis (PFA) <ref type="bibr" target="#b19">[20]</ref> takes into account the number of positive and negative responses for skills and Knowledge Tracing Machines <ref type="bibr" target="#b26">[27]</ref> leverages Factorization Machines <ref type="bibr" target="#b23">[24]</ref> to encode side information of questions and users into the parameter model.</p><p>Recently, due to the great capacity and effective representation learning, deep neural networks have been leveraged in the KT literature. Deep Knowledge Tracing (DKT) <ref type="bibr" target="#b20">[21]</ref> is the first deep KT method, which uses recurrent neural network (RNN) to trace the knowledge state of the student. Dynamic Key-Value Memory Networks (DKVMN) <ref type="bibr" target="#b34">[35]</ref> can discover the underlying concepts of each skill and trace states for each concept. Based on these two models, several methods have been proposed by considering more information, such as the forgetting behavior of students <ref type="bibr" target="#b15">[16]</ref>, multi-skill information and prerequisite skill relation graph labeled by experts <ref type="bibr" target="#b3">[4]</ref> or student individualization <ref type="bibr" target="#b14">[15]</ref>. GKT <ref type="bibr" target="#b16">[17]</ref> builds a skill relation graph and learns their relation explicitly. However, these methods only use skills as the input, which causes information loss.</p><p>Some deep KT methods take question characteristices into account for predictions. Dynamic Student Classification on Memory Networks (DSCMN) <ref type="bibr" target="#b13">[14]</ref> utilizes question difficulty to help distinguish the questions related to the same skills. Exercise-Enhanced Recurrent Neural Network with Attention mechanism (EERNNA) <ref type="bibr" target="#b24">[25]</ref> encodes question embeddings using the content of questions so that the question embeddings can contain the characteristic information of questions, however in reality it is difficult to collect the content of questions. Due to the data sparsity problem, DHKT <ref type="bibr" target="#b28">[29]</ref> augments DKT by using the relations between questions and skills to get question representations, which, however, fails to capture the inter-question and inter-skill relations. In this paper we use GCN to extract the high-order information contained in the question-skill graph. To handle the long term dependency issue, Sequential Key-Value Memory Networks (SKVMN) <ref type="bibr" target="#b0">[1]</ref> uses a modified LSTM with hops to enhance the capacity of capturing long-term dependencies in an exercise sequence. And EERNNA <ref type="bibr" target="#b24">[25]</ref> assumes that current student knowledge state is a weighted sum aggregation of all historical student states based on correlations between current question and historical quesitons. Our method differs from these two works in the way that they aggregate related hidden states into a new state for prediction, while we first select the most useful history exercises to reduce the effects of the noisy in the current state, and then we perform pairwise interaction for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>In recent years, graph data is widely used in deep learning models. However, the traditional neural network suffers from the complex non-Euclidean structure of graph. Inspired by CNNs, some works use the convolutional method for the graph-structure data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7]</ref>. Graph convolutional networks (GCNs) <ref type="bibr" target="#b12">[13]</ref> is proposed for semi-supervised graph classification, which updates node representations based on itself and its neighbors. In this way, updated node representations contain attributes of neighbor nodes and information of high-order neighbors if multiple graph-convolutional layers are used. Due to the great success of GCNs, some variants are further proposed for graph data <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>With the development of Graph Neural Networks (GNNs), many applications based on GNNs appear in various domains, such as natural language processing (NLP) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref>, computer vision (CV) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref> and recommendation systems <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23]</ref>. As GNNs help to capture high-order information, we use GCN in our GIKT model to extract relations between skills and questions into their representations. To the best of our knowledge, our method GIKT is the first work to model question-skill relations via graph neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminarilies</head><p>Knowledge Tracing. In the knowledge tracing task, students sequentially answer a series of questions that the online learning platforms provide. After the students answer each question, a feedback of whether the answer is correct will be issued. Here we denote an exercise as x i = (q i , a i ), where q i is the question ID and a i ? {0, 1} represents whether the student answered q i correctly. Given an exercise sequence X X X = {x 1 , x 2 , ..., x t?1 } and the new question q t , the goal of KT is to predict the probability of the student correctly answering it p(a t = 1|X X X, q t ).</p><p>Question-Skill Relation Graph. Each question q i corresponds to one or more skills {s 1 , ..., s ni }, and one skill s j is usually related to many questions q 1 , ..., q nj , where n i and n j are the number of skills related to question q i and the number of questions related to skill s j respectively. Here we denote the relations as a question-skill relation bipartite graph G, which is defined as {(q, r qs , s)|q ? Q, s ? S}, where Q and S correpsond to the question and skill sets respectively. And r qs = 1 if the question q is related to the skill s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Proposed Method GIKT</head><p>In this section, we will introduce our method in detail, and the overall framework is shown in <ref type="figure">Figure 2</ref>. We first leverage GCN to learn question and skill representations aggregated on the question-skill relation graph, and a recurrent layer is used to model the sequential change of knowledge state. To capture long term dependency and exploit useful information comprehensively, we then design a recap module followed by an interaction module for the final prediction.</p><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Interaction</head><formula xml:id="formula_0">... ... ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolutional Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN RNN</head><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN</head><p>... <ref type="figure">Fig. 2</ref>. An illustration of GIKT at time step t, where qt is the new question. First we use GCN to aggregate question and skill embeddings. Then a recurrent neural network is used to model the sequential knowledge state ht. In recap module we select the most related hidden exercises of qt, which corresponds to soft selection and hard selection implementation. The information interaction module performs pairwise interaction between the students current state, the selected students history exercises, the target question and related skills for the prediction pt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN Recap</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Embedding Layer</head><p>Our GIKT method uses embeddings to represent questions, skills and answers. Three embedding matrices E s ? R |S|?d , E q ? R |Q|?d , E a ? R 2?d are denoted for look-up operation where d stands for the embedding size. Each row in E s or E q corresponds to a skill or a question. The two rows in E a represent incorrect and correct answers respectively. For i-th row vector in matrices, we use s i , q i and a i to represent them respectively. In our framework, we do not pretrain these embeddings and they are trained by optimizing the final objective in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Embedding Propagation</head><p>From training perspective, sparsity in question data raises a big challenge to learn informative question representations, especially for those with quite limited training examples. From the inference perspective, whether a student can answer a new question correctly depends on the mastery of its related skills and the question characteristic. When he/she has solved similar questions before, he/she is more likely answer the new question correctly. In this model, we incorporate question-skill relation graph G to solve sparsity, as well as to utilize prior correlations to obtain better question representations.</p><p>Considering the question-skill relation graph is bipartite, the 1st hop neighbors of a question should be its corresponding skills, and the 2nd hop neighbors should be other questions sharing same skills. To extract the high-order information, we leverage graph convolutional network (GCN) <ref type="bibr" target="#b12">[13]</ref> to encode relevant skills and questions into question embeddings and skill embeddings.</p><p>Graph convolutional network stacks several graph convolution layers to encode high-order neighbor information, and in each layer the node representations can be updated by embeddings of itself and neighbor nodes. Denote the representation of node i in the graph as x i (x i can represent skill embedding s i or question embedding q i ) and the set of its neighbor nodes as N i , then the formula of l-th GCN layer can be expressed as:</p><formula xml:id="formula_1">x l i = ?( 1 |N i | j?Ni?{i} w l x l?1 j + b l ),<label>(1)</label></formula><p>where w l and b l are the aggregate weight and bias to be learned in l-th GCN layer, ? is the non-linear transformation such as ReLU.</p><p>After embedding propagation by GCN, we get the aggregated embedding of questions and skills. We use q and s to represent the question and skill representation after embedding propagation. For easy implementation and better parallelization, we sample a fixed number of question neighbors (i.e., n q ) and skill neighbors (i.e., n s ) for each batch. And during inference, we run each example multiple times (sampling different neighbors) and average the model outputs to obtain stable prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Student State Evolution</head><p>For each history time t, we concatenate the question and answer embeddings and project to d-dimension through a non-linear transformation as exercise representations:</p><formula xml:id="formula_2">e t = ReLU(W 1 ([ q t , a t ]) + b 1 ),<label>(2)</label></formula><p>where we use [, ] to denote vector concatenation. There may exist dependency between different exercises, thus we need to model the whole exericise process to capture the student state changes and to learn the potential relation between exercises. To model the sequential behavior of a student doing exercise, we use LSTM <ref type="bibr" target="#b10">[11]</ref> to learn student states from input exercise representations:</p><formula xml:id="formula_3">i t = ?(W i [e t , h t?1 , c t?1 ] + b i ),<label>(3)</label></formula><formula xml:id="formula_4">f t = ?(W f [e t , h t?1 , c t?1 ] + b f ),<label>(4)</label></formula><formula xml:id="formula_5">o t = ?(W o [e t , h t?1 , c t?1 ] + b o ), (5) c t = f t c t?1 + i t tanh (W c [e t , h t?1 ] + b c ) ,<label>(6)</label></formula><formula xml:id="formula_6">h t = o t tanh (c t ),<label>(7)</label></formula><p>where h t , c t , i t , f t , o t represents hidden state, cell state, input gate, forget gate, output gate respectively. It is worth mentioning that this layer is important for capturing coarse-grained dependency like potential relations between skills, so we just learn a hidden state h t ? R d as the current student state, which contains coarse-grained mastery state of skills.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">History Recap Module</head><p>In a student's exercise history, questions of relevant skills are very likely scattered in the long history. From another point, consecutive exercises may not follow a coherent topic. These phenomena raise challenges for LSTM sequence modeling in traditional KT methods: (i) As is well recognized, LSTM can hardly capture long-term dependencies in very long sequences, which means the current student state h t may "forget" history exercises related to the new target question q t . (ii) The current student state h t considers more about recent exercises, which may contain noisy information for the new target question q t . When a student is answering a new question, he/she may quickly recall similar questions he/she has done before to help him/her to understand the new question. Inspired from this behavior, we propose to select relevant history exercises(question-answer pair) 3 {e i |i ? [1, . . . , t ? 1]} to better represent a student's ability on a specific question q t , called history recap module. We develop two methods to find relevant history exercises. The first one is hard selection, i.e., we only consider the exercises sharing same skills with the new question:</p><formula xml:id="formula_7">I e = {e i |N qi = N qt , i ? [1, .., t ? 1]} ,<label>(8)</label></formula><p>Another method is soft selection, i.e., we learn the relevance between target question and history states through an attention network, and choose top-k states with highest attention scores:</p><formula xml:id="formula_8">I e = {e i |R i,t ? k, V i,t ? v, i ? [1, .., t ? 1]} ,<label>(9)</label></formula><p>where R i,t is the ranking of attention function f (q i , q t ) like cosine similarity, V i,t is the attention value and v is the lower similarity bound to filter less relevant exercises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Generalized Interaction Module</head><p>Previous KT methods predict a student's performace mainly according to the interaction between student state h t and question representation q t , i.e., h t , q t . We generalize the interaction in the following aspects: (i) we use h t , q t to represent the student's mastery degree of question q t , h t , s j to represent the student's mastery degree of the corresponding skill s j ? N qt , (ii) we generalize the interaction on current student state to history exercises, which reflect the relevant history mastery i.e., e i , q t and e i , s j , e i ? I e , which is equivalent to let the student to answer the target question in history timesteps.</p><p>Then we consider all above interactions for prediction, and define the generalized interaction module. In order to encourage relevant interactions and reduce noise, we use an attention network to learn bi-attention weights for all interaction terms, and compute the weighted sum as the prediction:</p><formula xml:id="formula_9">? i,j = Softmax i,j (W W W T [f f f i , f f f j ] + b)<label>(10)</label></formula><p>p</p><formula xml:id="formula_10">t = f f f i?Ie?{ht} f f f j ? Nq t ?{ qt} ? i,j g(f f f i , f f f j ) (11)</formula><p>where p t is the predicted probability of answering the new question correctly, N qt represents the aggregated neighbor skill embeddings of q t and we use inner product to implement function g. Similar to the selection of neighbors in relation graph, we set a fixed number of I e and N qt by sampling from these two sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Optimization</head><p>To optimize our model, we update the parameters in our model using gradient descent by minimizing the cross entropy loss between the predicted probability of answering correctly and the true label of the student's answer:</p><formula xml:id="formula_11">L = ? t (a t log p t + (1 ? a t ) log (1 ? p t )).<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we conduct several experiments to investigate the performance of our model. We first evaluate the prediction error by comparing our model with other baselines on three public datasets. Then we make ablation studies on the GCN and the interaction module of GIKT to show their effectiveness in Section 5.5. Finally, we evaluate the design decisions of the recap module to investigate which design performs better in Section 5.6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>To evaluate our model, the experiments are conducted on three widely-used datasets in KT and the detailed statistics are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>-ASSIST09 4 was collected during the school year 2009-2010 from ASSISTments online education platform 5 . We conduct our experiments on "skillbuilder" dataset. Following the previous work <ref type="bibr" target="#b31">[32]</ref>, we remove the duplicated records and scaffolding problems from the original dataset. This dataset has 3852 students with 123 skills, 17,737 questions and 282,619 exercises. -ASSIST12 6 was collected from the same platform as ASSIST09 during the school year 2012-2013. In this dataset, each question is only related to one skill, but one skill still corresponds to several questions. After the same data processing as ASSIST09, it has 2,709,436 exercises with 27,485 students, 265 skills and 53,065 questions. -EdNet 7 was collected by <ref type="bibr" target="#b4">[5]</ref>. As the whole dataset is too large, we randomly select 5000 students with 189 skills, 12,161 questions and 676,974 exercises.</p><p>Note that for each dataset we only use the sequences of which the length is longer than 3 in the experiments as the too short sequences are meaningless. For each dataset, we split 80% of all the sequences as the training set, 20% as the test set. To evaluate the results on these datasets, we use the area under the curve (AUC) as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>In order to evaluate the effeciveness of our proposed model, we use the following models as our baselines:</p><p>-BKT <ref type="bibr" target="#b5">[6]</ref> uses Bayesian inference for prediction, which models the knowledge state of the skill as a binary variable. -KTM <ref type="bibr" target="#b26">[27]</ref> is the latest factor analysis model that uses Factorization Machine to interact each feature for prediction. Although KTM can use many types of feature, for fairness we only use question ID, skill ID and answer as its side information in comparison. -DKT <ref type="bibr" target="#b20">[21]</ref> is the first method that uses deep learning to model knowldge tracing task. It uses recurrent neural network to model knowldge state of students. -DKVMN <ref type="bibr" target="#b34">[35]</ref> uses memory network to store knowledge state of different concepts respectively instead of using a single hidden state.</p><p>-DKT-Q is a variant of DKT that we change the input of DKT from skills to questions so that the DKT model directly uses question information for prediction. -DKT-QS is a variant of DKT that we change the input of DKT to the concatenation of questions and skills so that the DKT model uses question and skill information simultaneously for prediction. -GAKT is a variant of the model Exercise-Enhanced Recurrent Neural Network with Attention mechanism (EERNNA) <ref type="bibr" target="#b24">[25]</ref> as EERNNA utilizes question text descriptions but we can't acquire this information from public datasets. Thus we utilize our input question embeddings aggregated by GCN as input of EERNNA and follow its framework design for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>We implement all the compaired methods with TensorFlow. The code for our method is available online 8 . The embedding size of skills, questions and answers are fixed to 100, all embedding matrices are randomly initialized and updated in the training process. In the implementation of LSTM, a stacked LSTM with two hidden layers is used, where the sizes of the memory cells are set to 200 and 100 respectively. In embedding propagation module, we set the maximal aggregate layer number l = 3. We also use dropout with the keep probability of 0.8 to avoid overfitting. All trainable parameters are optimized by Adam algorithm <ref type="bibr" target="#b11">[12]</ref> with learning rate of 0.001 and the mini-batch size is set to 32. Other hyperparameters are chosen by grid search, including the number of question neighbors in GCN, skill neighbors in GCN, related exercises and skills related to the new question. <ref type="table">Table 2</ref> reports the AUC results of all the compared methods. From the results we observe that our GIKT model achieves the highest performance over three datasets, which verifies the effectiveness of our model. To be specific, our proposed model GIKT achieves at least 1% higher results than other baselines. Among the baseline models, traditional machine learning models like BKT and KTM perform worse than deep learning models, which shows the effectiveness of deep learning methods. DKVMN performs slightly worse than DKT on average as building states for each concept may lose the relation information between concepts. Besides, GAKT performs worse than our model, which indicates that exploiting high-order skill-question relations through selecting the most related exercises and performing interaction makes a difference. On the other hand, we find that directly using questions as input may achieve superior performance than using skills. For the question-level model DKT-Q, it has comparable or better performance than DKT over ASSIST12 and EdNet datasets. However, DKT-Q performs worse than DKT in ASSIST09 dataset. The <ref type="table">Table 2</ref>. The AUC results over three datasets. Among these models, BKT, DKT and DKVMN predict for skills, other models predict for questions. Note that "*" indicates that the statistically significant improvements over the best baseline, with p-value smaller than 10 ?5 in two-sided t-test. reason may be that the average number of attempts per question in ASSIST09 dataset is significantly less than other two datasets as observed in <ref type="table" target="#tab_0">Table 1</ref>, which illustrates DKT-Q suffers from data sparsity problem. Besides, the AUC results of the model DKT-QS are higher than DKT-Q and DKT, except on ASSIST12 as it is a single-skill dataset, which indicates that considering question and skill information together improves overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Overall Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Studies</head><p>To get deep insights on the effect of each module in GIKT, we design several ablation studies to further investigate on our model. We first study the influence of the number of aggregate layers, and then we design some variants of the interaction module to investigate their effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Embedding Propagation Layer</head><p>We change the number of the aggregate layers in GCN ranging from 0 to 3 to show the effect of the high-order question-skill relations and the results are shown in <ref type="table" target="#tab_2">Table 3</ref>. Specially, when the number of the layer is 0, it means the question embeddings and skill embeddings used in our model are indexed from embedding matrices directly. From <ref type="table" target="#tab_2">Table 3</ref> we find that, when the number of aggregate layer from zero to one, the performance of GIKT changes slightly, as we have already used 1-order relation in recap module and interaction module. However, GIKT achieves better performance when the number of aggregate layers increases, which validates the effectiveness of GCN. The results also imply that exploiting high-order relations contained in the question-skill graph is necessary for adequate results as the performance of adopting more layers is better than using less layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Interaction Module</head><p>To verify the impact of interaction module in GIKT, we conduct ablation studies on four variants of our model. The details of the four settings are listed as below and the performance of them is shown in <ref type="table" target="#tab_3">Table 4</ref>. the attention mechanism after interaction, which treats each interaction pair as equally important and average the prediction scores directly in interaction part for prediction. From <ref type="table" target="#tab_3">Table 4</ref> we have the following findings: Our GIKT model considering all interaction aspects achieve best performance, which shows the effectiveness of the interaction module. Meanwhile, from the results of GIKT-RH we can find that relevant history states can help better model the student's ability on the new question. Besides, the performance of GIKT-RS is slightly worse than GIKT, which implies that model mastery degree of question and skills simultaneously can further help prediction. Note that as ASSIST12 is a single-skill dataset, use skill information in interaction module is redundant after selecting history exercises sharing the same skill, thus we set the number of sampled related skill as 0. Comparing the results of GIKT-RA with GIKT, the worse performance confirms the effectiveness of attention in interaction module, which distinguishes different interaction terms for better prediction results. By calculating different aspects of interaction and weighted sum for the prediction, information from different level can be fully interacted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Recap Module Design Evaluation</head><p>To evaluate the detailed design of the recap module in GIKT, we conduct experiments of several variants. The details of the settings are listed as below and the performance of them is shown in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>-GIKT-HE (Hard select history Exercises) For GIKT-HE, we select the related exercises sharing the same skills.  From <ref type="table" target="#tab_4">Table 5</ref> we find that selecting history exercises performs better than selecting hidden states. This result implies that the hidden state contain irrelevant information for the next question as it learns a general mastery for a student. Instead, selecting exercises directly can reduce noise to help prediction. The performances of hard selection and soft selection distinguish on different datasets. Using attention mechanism can achieve better selection coverage while the hard selection variant can select exercises via explicit constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a framework to employ high-order question-skill relation graphs into question and skill representations for knowledge tracing. Besides, to model the student's mastery for the question and related skills, we design a recap module to select relevant history states to represent student's ability. Then we extend a generalized interaction module to represent the student's mastery degree of the new question and related skills in a consistent way. To distinguish relevant interactions, we use an attention mechanism for the prediction. The experimental results show that our model achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Addendum Version</head><p>After the deadline of submitting the camera-ready version to the proceedings, we found our realization of "soft selection" mentioned in Section 4.4 is somewhat unreasonable, thus we adopt another suitable realization for this strategy, which causes some differences of the results with the proceeding version. This version is the newest version and we report the revised experiment results in this version with the code. As the conference proceedings have been prepared already, PC chairs suggested us to upload an addendum version by ourself. Please refer to this version.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>A simple example of question-skill relation graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>-</head><label></label><figDesc>GIKT-RHS (Remove History related exercises and Skills related to the new question) For GIKT-RHS, we just use the current state of the student and the new question to perform interaction for prediction. -GIKT-RH (Remove History related exercises) For GIKT-RH, we only use the current state of the student to model mastery of the new question and related skills. -GIKT-RE (Remove Skills related to the new question) For GIKT-RS, we do not model the mastery of skills related to the new question. -GIKT-RA (Remove Attention in interaction module) GIKT-RA removes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-GIKT-SE (Soft select history Exercises) For GIKT-SE, we select history exercises according to the attention weight. -GIKT-HS (Hard select hidden States) For GIKT-HS, we select the related hidden states of the exercises sharing the same skills. -GIKT-SS (Soft select hidden States) For GIKT-SS, we select hidden states according to the attention weight. The reported results of GIKT in previous sections are taken by the performance of GIKT-SS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset statistics</figDesc><table><row><cell></cell><cell cols="3">ASSIST09 ASSIST12 EdNet</cell></row><row><cell>#students</cell><cell>3,852</cell><cell>27,485</cell><cell>5000</cell></row><row><cell>#questions</cell><cell>17,737</cell><cell cols="2">53,065 12,161</cell></row><row><cell>#skills</cell><cell>123</cell><cell>265</cell><cell>189</cell></row><row><cell>#exercises</cell><cell cols="3">282,619 2,709,436 676,974</cell></row><row><cell>questions per skill</cell><cell>173</cell><cell>200</cell><cell>147</cell></row><row><cell>skills per question</cell><cell>1.197</cell><cell>1.000</cell><cell>2.280</cell></row><row><cell>attempts per question</cell><cell>16</cell><cell>51</cell><cell>56</cell></row><row><cell>attempts per skill</cell><cell>2,743</cell><cell>10,224</cell><cell>8,420</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Effect of the number of aggregate layers.</figDesc><table><row><cell cols="3">Layers ASSIST09 ASSIST12 EdNet</cell></row><row><cell>0</cell><cell>0.7843</cell><cell>0.7738 0.7438</cell></row><row><cell>1</cell><cell>0.7844</cell><cell>0.7710 0.7432</cell></row><row><cell>2</cell><cell>0.7894</cell><cell>0.7736 0.7466</cell></row><row><cell>3</cell><cell>0.7896</cell><cell>0.7754 0.7523</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Effect of Information Module</figDesc><table><row><cell cols="3">Model ASSIST09 ASSIST12 EdNet</cell></row><row><cell cols="2">GIKT-RHS 0.7814</cell><cell>0.7672 0.7420</cell></row><row><cell cols="2">GIKT-RH 0.7808</cell><cell>0.7703 0.7463</cell></row><row><cell>GIKT-RS</cell><cell>0.7864</cell><cell>0.7754 0.7428</cell></row><row><cell cols="2">GIKT-RA 0.7856</cell><cell>0.7711 0.7500</cell></row><row><cell>GIKT</cell><cell>0.7896</cell><cell>0.7754 0.7523</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Results of Different Recap Module Design</figDesc><table><row><cell cols="2">Model ASSIST09 ASSIST12 EdNet</cell></row><row><cell>GIKT-HE 0.7896</cell><cell>0.7753 0.7481</cell></row><row><cell>GIKT-SE 0.7870</cell><cell>0.7686 0.7523</cell></row><row><cell>GIKT-HS 0.7788</cell><cell>0.7672 0.7364</cell></row><row><cell>GIKT-SS 0.7743</cell><cell>0.7683 0.7417</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We try other implementations like using history states instead of history exercises , and the results show using history exercises results in a better performance as history states contain other irrelevant information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://sites.google.com/site/assistmentsdata/home/ assistment-2009-2010-data/skill-builder-data-2009-2010 5 https://new.assistments.org/ 6 https://sites.google.com/site/assistmentsdata/home/ 2012-13-school-data-with-affect 7 https://github.com/riiid/ednet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/Rimoku/GIKT</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge tracing with sequential key-value memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 42nd International ACM SIGIR Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">More accurate student modeling through contextual estimation of slip and guess probabilities in bayesian knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aleven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent tutoring systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="406" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09835</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prerequisite-driven deep knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03072</idno>
		<title level="m">Ednet: A large-scale hierarchical dataset in education</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge tracing: Modeling the acquisition of procedural knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User modeling and user-adapted interaction</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="253" to="278" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memory: A contribution to experimental psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ebbinghaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of neurosciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic student classiffication on memory networks for knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Minn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Desmarais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="163" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep knowledge tracing and dynamic student classification for knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Minn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Desmarais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Vie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1182" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Augmenting knowledge tracing by considering forgetting behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohkuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3101" to="3107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph-based knowledge tracing: Modeling student proficiency using graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/WIC/ACM International Conference on Web Intelligence</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="156" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling individualization in a bayesian networks implementation of knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">A</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on User Modeling, Adaptation, and Personalization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="255" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kt-idem: Introducing item difficulty to the knowledge tracing model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">A</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on user modeling, adaptation, and personalization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Performance factors analysis-a new alternative to knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Pavlik</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Koedinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online Submission</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An end-to-end neighborhood-based interaction model for knowledge-enhanced recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data</title>
		<meeting>the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exercise-enhanced sequential modeling for student performance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge tracing machines: Factorization machines for knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Vie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="750" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep hierarchical knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://drive.google.com/file/d/1wlW6zAi-l4ZAw8rBA_mXZ5tHgg6xKL00" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Educational Data Mining, EDM 2019</title>
		<meeting>the 12th International Conference on Educational Data Mining, EDM 2019<address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep hierarchical knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://drive.google.com/file/d/1wlW6zAi-l4ZAw8rBA_mXZ5tHgg6xKL00" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Educational Data Mining, EDM 2019</title>
		<meeting>the 12th International Conference on Educational Data Mining, EDM 2019<address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kgat: Knowledge graph attention network for recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with deep knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Van Inwegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Educational Data Mining Society</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Individualized bayesian knowledge tracing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Yudelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Koedinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence in education</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic key-value memory networks for knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on World Wide Web</title>
		<meeting>the 26th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
