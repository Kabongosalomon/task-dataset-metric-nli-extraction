<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Copy-and-Paste Networks for Deep Video Inpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seoung</forename><forename type="middle">Wug</forename><surname>Oh</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Yonsei University DaeYeun Won Hyundai MNSOFT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon</forename><forename type="middle">Joo</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Copy-and-Paste Networks for Deep Video Inpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel deep learning based algorithm for video inpainting. Video inpainting is a process of completing corrupted or missing regions in videos. Video inpainting has additional challenges compared to image inpainting due to the extra temporal information as well as the need for maintaining the temporal coherency. We propose a novel DNN-based framework called the Copy-and-Paste Networks for video inpainting that takes advantage of additional information in other frames of the video. The network is trained to copy corresponding contents in reference frames and paste them to fill the holes in the target frame. Our network also includes an alignment network that computes affine matrices between frames for the alignment, enabling the network to take information from more distant frames for robustness. Our method produces visually pleasing and temporally coherent results while running faster than the state-of-the-art optimization-based method. In addition, we extend our framework for enhancing over/under exposed frames in videos. Using this enhancement technique, we were able to significantly improve the lane detection accuracy on road videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inpainting is a task of completing an image that has empty pixels by filling the empty regions with visually plausible pixels. Inpainting is very useful in image editing process, and is usually utilized to generate more satisfying images by removing unwanted objects in images. There is a large body of literature on image inpainting and significant progress has been made recently by employing deep learning for image inpainting. Impressive inpainting results are reported by applying evolving deep generative models <ref type="bibr" target="#b6">[7]</ref>, synthesizing visually pleasing images even for complex scenes.</p><p>In this paper, we focus on the video inpainting problem. Videos with additional temporal information makes the already difficult problem even more challenging. In addition to filling the holes for every frame, the algorithm has to ensure that the completed frames are temporally con-  <ref type="figure">Figure 1</ref>: (a) We propose a DNN framework for video inpainting. Our Copy-and-Paste network learns to find corresponding pixels in other frames to fill in the holes in the given frame. (b) Another application of our framework for restoring an over-saturated image. sistent. Due to these challenges, we have only seen one work that tackles the problem using deep neural networks (DNN) <ref type="bibr" target="#b12">[13]</ref>, compared to the image inpainting problem where many deep learning based algorithms have been introduced.</p><p>While video inpainting is more challenging compared to image inpainting, it inherently includes more cues for the problem as valid pixels for missing regions in a frame may exist in other frames. Therefore, we propose a novel DNN based framework called the Copy-and-Paste Networks for video inpainting that takes advantage of additional information in other frames in the video. As the name suggests, the network is trained to copy the necessary pixels from other frames and paste those pixels on the holes in the current frame <ref type="figure">(Fig. 1)</ref>.</p><p>The key components of our DNN system are the alignment and the context matching. To find corresponding pixels in other frames for the holes in the given frame, the frames need to be registered first. We propose a selfsupervised alignment networks, which estimates affine matrices between frames. While DNNs for computing the affine matrix or homography exist <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref>, our alignment method is able to deal with holes in images when computing the affine matrices. After the alignment, the novel context matching algorithm is used to compute the similarity between the target frame and the reference frames. The network learns which pixels are valuable for copying through the context matching, and those pixels are used to paste and complete an image. By progressively updating the reference frames with the inpainted results at each step, the algorithm can produce videos with temporal consistency.</p><p>Our results are comparable to the state-of-the-art method <ref type="bibr" target="#b8">[9]</ref>, and outperform other deep learning based approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>. Moreover, we can easily extend our method for restoring saturated/under-exposed images as shown in ( <ref type="figure">Fig. 1(b)</ref>). By enhancing the saturated/underexposed images, we were able to significantly increase the lane detection accuracy.</p><p>In summary, the major contribution of our paper is as follows:</p><p>? We propose a self-supervised deep alignment networks that can compute affine matrices between images that contain large holes.</p><p>? We propose a novel context-matching algorithm to combine reference frame features based on similarity between images.</p><p>? Our method produces visually pleasing completed videos, running much faster than the state-of-the-art method. Additionally, we extend our framework for enhancing over/under exposed frames in videos that can help to improve other vision tasks such as the lane detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Image Inpainting</head><p>In traditional image inpainting methods, an image is filled by referencing pixels outside the hole in the image or in the external image database. As one of the most representative inpainting methods, PatchMatch <ref type="bibr" target="#b0">[1]</ref> reconstructs the missing region by searching the patches outside the hole based on the approximate nearest neighbor algorithm. With this type of approach, however, it is difficult to inpaint images with complicated scenes, or when the images do not contain sufficient information for filling the holes.</p><p>Since deep image inpainting has been introduced in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, many deep generative models for image inpainting have been proposed recently, showing impressive restoration results on complex scenes. Yu et al. <ref type="bibr" target="#b23">[24]</ref> proposed the contextual attention module between the completed structure of the hole area and the patches outside the hole. Liu et al. <ref type="bibr" target="#b14">[15]</ref> and Yu et al. <ref type="bibr" target="#b22">[23]</ref> applied the partial convolution and the gated convolution to compensate the weakness of the vanilla convolution for image inpainting. In particular, Liu et al. <ref type="bibr" target="#b14">[15]</ref> corrected the blurred results based on the perceptual and the style loss without the adversarial loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video Inpainting</head><p>Video inpainting has additional challenges of restoring the holes in every frame and maintaining the temporal consistency between reconstructed frames. Meanwhile, unlike in image inpainting, one can utilize redundant information between frames of video in video inpainting. However, directly exploiting the redundant information in videos is difficult due to image variation from the movements of the camera and the objects. To compensate for the movements, Granados et al. <ref type="bibr" target="#b7">[8]</ref> proposed to align the frames based on the homographies. They also applied the optical flow between completed frames to maintain the temporal consistency.</p><p>In <ref type="bibr" target="#b15">[16]</ref>, Newson et al. proposed 3D PatchMatch to maintain the temporal consistency in addition to using the affine transformation to compensate the motion. While the spatiotemporal patches improve the short-term temporal consistency, the long-term consistency of complicated scenes remained as a limitation. To solve this limitation, Huang et al. <ref type="bibr" target="#b8">[9]</ref> proposed the optical flow optimization in spatial patches to complete images while preserving the temporal consistency. This method shows the state-of-the-art performance up until now. All the methods explained above are based on a heavy optimization, and therefore suffers in the computational time, limiting their practical use.</p><p>Wang et al. <ref type="bibr" target="#b21">[22]</ref> proposed the first deep learning based video inpainting by using 3D encoder-decoder networks. However, this work does not cover the object removal task in general videos, and was only applied to a few specific domains. Kim et al. <ref type="bibr" target="#b12">[13]</ref> proposed 3D-2D encoderdecoder networks to complete the missing contents efficiently. The temporal consistency is maintained through a recurrent feedback and a memory layer with the flow and the warping loss. The temporal window for the referencing is small in their method, and therefore it is difficult to use valid pixels in distant frames, resulting in a limited performance for scenes with large objects or slowly moving objects.</p><p>Our copy-and-paste network overcome the issues in <ref type="bibr" target="#b12">[13]</ref> by aligning the frames with affine matrices computed by our alignment network instead of using the optical flow. With the novel context matching algorithm, our method can extract valid pixels in distant frames, resulting in more ac- curate reconstruction for general scenes. The performance of our method is comparable to the state-of-the-art method in <ref type="bibr" target="#b8">[9]</ref> while being more practical with faster runtime due to the feed forward nature of DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Copy-and-Paste Network Algorithm</head><p>The overview of our framework is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The system takes a video (X) annotated with the missing pixels (M ) in each frame and outputs (? ) the completed video. The video is processed frame-by-frame in the temporal order. We call the frame to be filled as the target frame and the other frames as the reference frames. For each target frame, our network completes the missing region by copying-andpasting contents from the reference frames.</p><p>To complete a target frame, each reference frame is first aligned to the target frame through the alignment network. Then in the copy network, pixels to be copied from the aligned reference frames are determined by the context matching module. Finally, the outputs from the copy networks are decoded to produce inpainted target frame in the paste network. The input video in the memory is updated with the completed frame, which will subsequently be used as a reference frame, providing more information for the following frames.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Alignment Network</head><p>In video inpainting, a large temporal window is essential as valuable information is more likely to be in distant frames. With an optical flow based alignment as used in <ref type="bibr" target="#b12">[13]</ref>, the temporal range of information is too small to extract useful information. As illustrated in <ref type="figure">Fig. 3</ref>, a reference frame temporally close to the target frame lacks information to fill the hole as there are too much overlap between the holes in the images. Moreover, computing optical flows between images with holes is more difficult as the holes themselves become occlusion factors. Therefore, our alignment network estimates the affine matrices to align the reference frames with the target frame.</p><p>The alignment network consists of shared alignment encoders and alignment regressors. Details on the network architectures are provided in the supplementary materials. To train the alignment network, we minimize the selfsupervised loss, which is the L1 distance between the target frame (X t ) and the aligned reference frame (X r?t ). To exclude the hole regions, this pixel-wise loss is only measured with pixels that are valid in both images as follows:</p><formula xml:id="formula_0">L align = r ||V (X t ? X r?t )|| 1 ,<label>(1)</label></formula><p>where V = V t V r?t is the visibility map, is the element-wise product, t is the target frame index, and r is the reference frame index 1 . The visibility map is computed <ref type="bibr" target="#b0">1</ref> The symbol r ? t indicates aligning a reference frame r to a target frame t. V r?t indicates the visibility map of the reference aligned to the target Global similarity <ref type="bibr" target="#b14">15</ref> 19 <ref type="bibr">18 16</ref> Visiblity maps from the given masks, where 0 indicates hole pixels and 1 represents non-hole pixels. Note that the alignment network is jointly trained with other networks in an end-to-end manner, not independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Copy-and-Paste Network</head><p>After the frame alignment, the aligned frames are mapped into the feature space through the shared encoders. The context matching module computes the importance of each pixel in the reference frames in completing the holes as well as a mask (C mask ) indicating the visibility of each pixel throughout the video. Finally, the decoder takes the output of the context matching module in addition to the target frame feature to restore values for the missing pixels.</p><p>Encoder Encoder networks extract the features from the target and the aligned reference frames. The input to the encoder is a concatenation of an RGB image and the corresponding binary mask. The details on the architecture will be described in the supplementary materials.</p><p>Context matching module Together with the encoder, the context matching module constitutes the copy network. The context matching module is illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. First, global similarities (? r,t ) between the aligned reference frames and the target frame in the feature space is computed as follows:</p><formula xml:id="formula_1">? r,t = 1 (x,y) V (x, y) ? (x,y)</formula><p>V (x, y)?F t (x, y)?F r?t (x, y).</p><p>(2) The above equation is basically computing the cosine similarity between the two feature maps, excluding the hole pixels.</p><p>Then, a saliency map C r match for each reference frame is computed as follows:</p><formula xml:id="formula_2">S r,t = ? r,t ? V r?t ,<label>(3)</label></formula><formula xml:id="formula_3">C r match (x, y) = ? ? ? exp(S r,t (x,y)) r exp(S r,t (x,y)) if V r?t (x, y) = 1 0</formula><p>otherwise.</p><p>(4) <ref type="figure" target="#fig_4">Fig. 5</ref> simplifies the steps for computing the saliency map in 1-D. Each pixel value in the saliency map C r match holds the weight that specific pixels have on filling the hole in the target. The reference features are aggregated through a weighted sum with the C r match , producing the features to be used for the decoder (C out ).</p><formula xml:id="formula_4">C out (x, y) = r F r?t (x, y) ? C r match (x, y).<label>(5)</label></formula><p>The hole masks for the reference frames are also aggregated in a similar fashion, resulting in C mask . C mask indicates pixels that is never visible throughout the reference frame.</p><p>The process of the aggregation is expressed as:</p><formula xml:id="formula_5">C mask (x, y) = 1 ? ( r C r match (x, y)).<label>(6)</label></formula><p>Decoder The decoder network completes the target frame given target features, aggregated reference features, and mask C mask . The inputs are concatenated before being fed into the decoder. Decoder is basically our paste network that learns to fill the missing region by using the aggregated reference features and the visibility of those features. The pixels marked on C mask are pixels that are never visible in all reference frame because those pixels always fall into holes. Therefore, the decoder has to be able to synthesize contents for those pixels as well. We add dilated convolution blocks to grow the receptive field and design the decoder network deeper than the other networks, in order to enhance the completion results for the unseen area by looking at other pixels within the image itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Consistency</head><p>Each frame in the video is sequentially completed by the network, one by one. The completed frame at each iteration replaces its reference, providing more information for the following frames as the holes are now filled with contents. This iterative reference update procedure not only improves the quality of the restored images, but also enhances the temporal consistency. This is analyzed later in the ablation study. To further ensure the temporal consistency, we actually run the feed-forward network twice -completing the video from the first to the last frame, and also in the reverse order. Then the final results are computed as follows:</p><formula xml:id="formula_6">Y t final =? t forward ? t N +? t reverse ? (N ? t) N .<label>(7)</label></formula><p>4. Training</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Loss functions</head><p>All the networks are trained jointly in an end-to-end manner. First, we compute the loss between the completed target frame and the ground truth. The losses for the hole region and the non-hole region are separately calculated. Furthermore, the hole region can be divided into areas depending on whether the pixel value can be copied from reference frames or not. Therefore, we also apply the losses in the hole region separately.</p><formula xml:id="formula_7">L hole(visible) = N t M t C mask ||? t ? Y t || 1 , L hole(invisible) = N t M t (1. ? C mask ) ||? t ? Y t || 1 , L non-hole = N t (1 ? M t ) ||? t ? Y t || 1 .</formula><p>(8) C mask is properly resized to fit the size of the target frame.</p><p>To further improve the visual quality of the results, we also apply perceptual, style, and total variation loss.</p><formula xml:id="formula_8">L perceptual = 1 P ? P p ||? p (? comp ) ? ? p (Y )|| 1 , L style = 1 P ? P p ||G ? p (? comp ) ? G ? p (Y )|| 1 ,<label>(9)</label></formula><p>where? comp is combination of the decoder output? t in the hole region and the input X t outside the hole, ? is the output of the pooling layer in pretrained VGG-16 <ref type="bibr" target="#b20">[21]</ref> on ImageNet <ref type="bibr" target="#b3">[4]</ref>, p is the pooling index, G is the gram matrix multiplication <ref type="bibr" target="#b11">[12]</ref>.</p><p>The total-loss function is as follows: L = 2 ? L align + 10 ? L hole(visible) + 20 ? L hole(invisible)</p><formula xml:id="formula_9">+ 6 ? L non-hole + 0.01 ? L perceptual + 24 ? L style + 0.1 ? L tv ,<label>(10)</label></formula><p>where L tv is the total variation loss for smoothing the checkerboard effect <ref type="bibr" target="#b11">[12]</ref>. The weight for each loss is empirically determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>Our goal is to complete holes in video sequences. Inputs are image sequences with holes and binary masks indicating the hole regions. However, no public video dataset for video inpainting exist. Therefore, we synthesized a dataset for video inpainting using background images and segmentation masks. We synthesize videos by compositing background image sequences with object masks <ref type="figure" target="#fig_5">(Fig. 6</ref>). To build background image sequences, we use the Places (amount of 1.8M images) <ref type="bibr" target="#b24">[25]</ref> single image datasets. To synthesize a sequence of images from a single image, we applied random crops and successive random transformations (shear, scale, translation, rotation) on the image. Additionally, we crawled the Youtube video clips and divided them according to the scene (7.3K scenes). Frames are randomly sampled from video clips to form a image sequence. The source of the background image sequence is randomly selected in an equal chance.</p><p>To simulate masks for holes, we use object masks from MIT Saliency Benchmark(amount of 11K masks) <ref type="bibr" target="#b1">[2]</ref> and Pascal VOC 2012(amount of 14.3K masks) <ref type="bibr" target="#b5">[6]</ref>. A mask is randomly resized to be smaller than the size of the background frames. And the mask is randomly transformed to be a mask sequence by simulating the moving objects. A training sample is made by compositing a background image sequence and a mask sequence made above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Details</head><p>Our model runs on hardware with the Intel(R) Core(TM) i7-7800X CPU(3.50GHz) CPU and NVIDIA TITAN XP GPUs. We train with the randomly selected five 256 ? 256 frames from the synthesized video sequences as inputs. To train the network, we set the batch size as 40. We use the Adam Optimizer <ref type="bibr" target="#b13">[14]</ref> with learning rates 10 ?4 and reduce the running rate factor of 10 every 1 million iterations. The training process takes about 7 days using three NVIDIA TI-TAN XP GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To evaluate our algorithm, we provide both quantitative and qualitative analysis, as well as a user study. We conducted the experiments using the videos, which were scaled in half (424 ? 240). Our code will be available online. We also show an application of our work in restoring under/over-exposed images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>PSNR SSIM Huang et al. <ref type="bibr" target="#b8">[9]</ref> 28.14 0.859 Ours 28.37 0.851 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Results</head><p>We first conducted quantitative evaluation by measuring the quality of video restoration. For this experiment, we randomly selected 25 video sequences in DAVIS dataset <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, which consists of pairs of video and object segmentation mask sequences. To simulate image restoration, we synthesized videos by putting imaginary object masks from DAVIS <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> on the videos. The video without the object masks are used as the ground truth. <ref type="table" target="#tab_1">Table 1</ref> compares the PSNR and the SSIM measures between our method and <ref type="bibr" target="#b8">[9]</ref>. Both methods show good performance with similar measures. Note that VINet <ref type="bibr" target="#b12">[13]</ref> is excluded in this experiment because the official code has not been published yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">User Study and Qualitative Analysis</head><p>We further conducted experiments on dynamic object removal in videos with 30 videos from DAVIS dataset <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. We compared our methods with the state-of-the-art video inpainting models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. Results of the previous methods were gathered by using the official code released by the authors <ref type="bibr" target="#b8">[9]</ref> and by requesting the results from the authors <ref type="bibr" target="#b12">[13]</ref>.</p><p>The user study result performed the Amazon Mechanical Turk (AMT) is shown in <ref type="figure" target="#fig_8">Fig. 8</ref> and <ref type="table" target="#tab_2">Table 2</ref> . The workers were asked to rank the video completion results and we also allowed them to give ties. All tests were evaluated by 40 participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average ranking Huang et al. <ref type="bibr" target="#b8">[9]</ref> 1.74 VINet <ref type="bibr" target="#b12">[13]</ref> 2.08 Ours 1.77 The user study shows that our method is highly competitive to the optimization based method <ref type="bibr" target="#b8">[9]</ref>, while VINet <ref type="bibr" target="#b12">[13]</ref> is not on par with the other two methods. While the method in <ref type="bibr" target="#b8">[9]</ref> was slightly more favored, it requires average completion time of 952 seconds per video, whereas our method only takes 27.14 seconds.</p><p>Qualitative comparisons of the object removal results are shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. These comparisons show similar results as the user study. Our results are comparable to the state-    of-the-art method in <ref type="bibr" target="#b8">[9]</ref>, while showing much better results compared to the other deep learning based approach in <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Applications</head><p>We extend our method for restoring under/over-exposed image sequences. The restoration process is similar to video inpainting problem in that it fills areas with missing information. This problem often happens to image sequences taken by a camera attached to a vehicle due to rapid exposure changes (e.g. tunnel entry and exit).</p><p>As shown in <ref type="figure">Fig. 9</ref>, both the texture and the color are improved. To validate the effectiveness of our restoration process, we ran a lane detection algorithm on road images before and after the enhancement. We collected 469 frames videos 2 that contains rapid exposure changes due to tunnels and the internal color histogram-based lane detection <ref type="bibr" target="#b1">2</ref> The dataset were taken by using Mobile Mapping System Camera of Hyundai MnSOFT, Inc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lane detection input</head><p>Lane detection accuracy Over/under-exposed image 46.69% Restored input by our model 83.00% <ref type="table">Table 3</ref>: The lane detection accuracy results.</p><p>method was used. As shown in <ref type="figure">Fig. 9</ref> and <ref type="table">Table 3</ref>, lane detection results are significantly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Study</head><p>Masked softmax We conducted an ablation study to verify that masked softmax contributes to the performance improvements. We train our model using normal softmax under the same conditions. As shown in the <ref type="figure" target="#fig_10">Fig. 10</ref>, using masked softmax results are sharper than using the normal one.</p><p>Target frame</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lane detection Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference frames</head><p>Target frame Output</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lane detection Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference frames</head><p>Restoration of under-exposed images Restoration of over-saturated images <ref type="figure">Figure 9</ref>: Application of our method for the restoration of under/over-exposed images.  Reference update To produce temporally coherent outputs, we update the past reference frames with the inpainted version. To visualize the effect of this updating protocol, we compare the temporal profile <ref type="bibr" target="#b2">[3]</ref> of resulting videos in <ref type="figure" target="#fig_12">Fig. 11</ref>. As shown in <ref type="figure" target="#fig_12">Fig. 11</ref>, the update procedure contributes in enhancing the temporal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we presented a novel DNN framework for video inpainting. The proposed method inpaints the missing information by copy-and-pasting contents from the reference frames. The reference information is dynamically updated by the previous completion results to ensure the temporal consistency. Our experiments sup-  port that the proposed framework is comparable to the optimization-based methods and outperform other deep learning based approaches. We extended our framework to restore over/under-exposed in videos and were able to significantly increase the lane detection accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?Figure 2 :</head><label>2</label><figDesc>Network Overview. Our framework consists of 3 sub-networks: alignment network, copy network, and paste network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Detailed illustration of the context matching module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>An 1-D example of masked softmax.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Synthesized training dataset example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative comparison of object removal results for the scenes elephant (left) and tennis (right) from DAVIS 2017 sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>User study for video object removal results (lower value is better)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Result using softmax (b) Result using masked softmax</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Ablation study for masked softmax.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Ablation study for reference update. (b), (c) and (d) show the temporal profile of the red line shown in input (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Using affine transformation for the alignment yields larger temporal search compared to the optical flow based alignment. More distant reference frame provides more valuable information as the overlap of the hole regions is smaller.</figDesc><table><row><cell></cell><cell></cell><cell>( 1? , 2? , ? ,</cell><cell>? )</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Reference</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>visibility map</cell><cell></cell><cell></cell></row><row><cell>Target</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2,</cell><cell>Masked</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Softmax</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>?</cell><cell>(Eq.4)</cell><cell>?</cell><cell>Weighted</cell></row><row><cell></cell><cell>,</cell><cell></cell><cell></cell><cell>sum</cell></row><row><cell>Reference features</cell><cell cols="4">Overlap of Target frame Figure 3: Global similarity</cell><cell>and aligned +2 Reference frame +2 Overlap of</cell><cell>and aligned +8 Reference frame +8</cell></row></table><note>? Context Matching module</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative Results (video restoration) for DAVIS 2017</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>User study average rank (lower value is better)</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Patchmatch: a randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mit saliency benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoya</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilke</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4778" to="4787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03798</idno>
		<title level="m">Deep image homography estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Background inpainting for videos with dynamic objects and a free-moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang</forename><forename type="middle">In</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporally coherent completion of dynamic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Globally and Locally Consistent Image Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno>107:1-107:14</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH 2017)</title>
		<meeting>of SIGGRAPH 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5792" to="5801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video inpainting of complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alasdair</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrs</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Fradet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Prez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences, Society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1993" to="2019" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised deep homography: A fast and robust homography estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ty</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shreyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo Jose</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2346" to="2353" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video inpainting by jointly learning temporal structure and spatial details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5232" to="5239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03589</idno>
		<title level="m">Free-form image inpainting with gated convolution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
