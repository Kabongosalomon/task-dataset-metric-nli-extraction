<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Event-Based Fusion for Motion Deblurring with Cross-modal Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaozu</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<settlement>Leuven</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Event-Based Fusion for Motion Deblurring with Cross-modal Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional frame-based cameras inevitably suffer from motion blur due to long exposure times. As a kind of bio-inspired camera, the event camera records the intensity changes in an asynchronous way with high temporal resolution, providing valid image degradation information within the exposure time. In this paper, we rethink the eventbased image deblurring problem and unfold it into an end-to-end twostage image restoration network. To effectively fuse event and image features, we design an event-image cross-modal attention module applied at multiple levels of our network, which allows to focus on relevant features from the event branch and filter out noise. We also introduce a novel symmetric cumulative event representation specifically for image deblurring as well as an event mask gated connection between the two stages of our network which helps avoid information loss. At the dataset level, to foster event-based motion deblurring and to facilitate evaluation on challenging real-world images, we introduce the Real Event Blur (REBlur) dataset, captured with an event camera in an illuminationcontrolled optical laboratory. Our Event Fusion Network (EFNet) sets the new state of the art in motion deblurring, surpassing both the prior best-performing image-based method and all event-based methods with public implementations on the GoPro dataset (by up to 2.47dB) and on our REBlur dataset, even in extreme blurry conditions. The code and our REBlur dataset will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Motion blur often occurs in images due to camera shake or object motion during the exposure time. The goal of deblurring is to recover a sharp image with clear edge structures and texture details from the blurry image. This is a highly ill-posed problem because of the infinitely many feasible solutions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b49">50]</ref>. Traditional methods explicitly utilize natural image priors and various constraints <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b45">46]</ref>. To better generalize when addressing the deblurring problem, modern learning-based methods choose to train Convolutional Neural Networks (CNNs) on large-scale data to learn the implicit relationships between blurry and sharp images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49]</ref>. Despite their high performance on existing public datasets, these learning-based methods often fail when facing extreme or real-world blur. Their performance heavily relies on the quality and scale of the training data, which creates the need for a more general and reliable deblurring method.</p><p>Event cameras <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref> are bio-inspired asynchronous sensors with high temporal resolution (in the order of ?s) and they operate well in environments with high dynamic range. Different from traditional frame-based cameras, event cameras capture the intensity change of each pixel (i.e. event information) independently, if the change surpasses a threshold. Event cameras encode the intensity change information within the exposure time of the image frame into an event stream, making it possible to deblur an image frame with events <ref type="bibr" target="#b26">[27]</ref>. However, because of sensor noise and uncertainty in the aforementioned threshold, it is difficult to use a physical model to deblur images based solely on events. Thus, some methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref> utilize CNNs to deal with noise corruption and threshold uncertainty. Nevertheless, these methods only achieve slight performance gains compared to image-only methods, due to rather ineffective event representations and fusion mechanisms between events and images.</p><p>In this paper, we first revisit the mechanism of motion blur and how event information is utilized in image reconstruction. To deal with the inherent defect of the event-based motion deblurring equation, we propose EFNet, an Event Fusion Network for image deblurring which effectively combines information from event and frame-based cameras for image deblurring. Motivated by the physical model of event-based image deblurring <ref type="bibr" target="#b26">[27]</ref>, we design a symmetric cumulative event representation (SCER) specifically for deblurring and formulate our network based on a two-stage image restoration model. Each stage of the model has a U-Net-like architecture <ref type="bibr" target="#b31">[32]</ref>. The first stage consists of two branches, an image branch and an event branch, the features of which are fused at multiple levels. In order to perform the fusion of the two modalities, we propose an Event-Image Cross-modal Attention (EICA) fusion module, which allows to attend to the event features that are relevant for deblurring via a channel-level attention mechanism. To the best of our knowledge, this is the first time that a multi-head attention mechanism is applied to event-based image deblurring. We also enable information exchange between the two stages of our network by applying Event Mask Gated Connections (EMGC), which selectively transfer feature maps from the encoder and decoder of the first stage to the second stage. A detailed ablation study shows the effectiveness of our novel fusion module using cross-modal attention, our gated connection module and our multi-level middle fusion design. Additionally, we record a real-world event blur dataset named Real Event Blur (REBlur) in an optical laboratory with stable illumination and a high-precision electronically controlled slide-rail which allows various types of motion. We conduct extensive comparisons against state-of-the-art deblurring methods on the GoPro dataset <ref type="bibr" target="#b25">[26]</ref> with synthetic events and on REBlur with real events and demonstrate the superiority of our event-based image deblurring method.</p><p>In summary, we make the following main contributions:</p><p>-We design a novel event-image fusion module which applies cross-modal channel-wise attention to adaptively fuse event features with image features, and incorporate it at multiple levels of a novel end-to-end deblurring network. -We introduce a novel symmetric cumulative event voxel representation for deblurring, which is inspired by the physical model that connects blurry image formation and event generation. -We present REBlur, a real-world dataset consisting of tuples of blurry images, sharp images and event streams from an event camera, which provides a challenging evaluation setting for deblurring methods. -Our deblurring network, equipped with our proposed modules and event representation, sets the new state of the art for image deblurring on the GoPro dataset and our REBlur dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image deblurring. Traditional approaches often formulate deblurring as an optimization problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b45">46]</ref>. Recently, with the success of deep learning, image deblurring has achieved impressive performance thanks to the usage of CNNs. CNN-based methods directly map the blurry image to the latent sharp image. Several novel components and techniques have been proposed, such as attention modules <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40]</ref>, multi-scale fusion <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref>, multi-stage networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47]</ref>, and coarse-to-fine strategies <ref type="bibr" target="#b7">[8]</ref>, improving the accuracy and robustness of deblurring. Despite the benefits they have shown for deblurring, all aforementioned deep networks operate solely on images, a modality which does not explicitly capture motion and thus inherently limits performance when facing real-world blurry images, especially in extreme conditions. Event-based deblurring. Recently, events have been used for motion deblurring, due to the strong connection they possess with motion information. Pan et al . <ref type="bibr" target="#b26">[27]</ref> proposed an Event Double Integral (EDI) deblurring model using the double integral of event data. They established a mathematical event-based model mapping blurry frames to sharp frames, which is a seminal approach to deblurring with events. However, limited by the sampling mechanism of event cameras, this method often introduces strong, accumulated noise. Jiang et al . <ref type="bibr" target="#b13">[14]</ref> extracted motion information and sharp edges from events to assist deblurring. However, their early fusion approach, which merely concatenates events into the main branch of the network, does not take full advantage of events. Lin et al . <ref type="bibr" target="#b22">[23]</ref> fused events with the image via dynamic filters from STFAN <ref type="bibr" target="#b50">[51]</ref>. In addition, Shang et al . <ref type="bibr" target="#b33">[34]</ref> fused event information into a weight matrix that can be applied to any state-of-the-art network. To sum up, most of the above event-based learning methods did not use event information effectively, achieving only minor improvements compared to image-only methods on standard benchmarks. Event representation. Different from synchronous signals such as images from frame-based cameras, events are asynchronous and sparse. A key point in how to extract information from events effectively is the representation of the events. Event representation is an application-dependent problem and different tasks  EFNet consists of two UNet-like backbones <ref type="bibr" target="#b31">[32]</ref> and an event extraction branch. After each residual convolution block ("Res Block"), feature maps from the event branch and the image branch are fused. The second UNet backbone refines the deblurred image further. "SCER": symmetric cumulative event representation, "EICA": event-image cross-modal attention, "SConv": 4?4 strided convolution with stride 2, "TConv": 2?2 transposed convolution with stride 2, "SAM": supervision attention module <ref type="bibr" target="#b46">[47]</ref>. admit different solutions. The event-by-event method is suitable for spiking neural networks owing to its asynchronous architecture <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44]</ref>. A Time Surface, which is a 2D map that stores the time value deriving from the timestamp of the last event, has proved suitable for event-based classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref>. Some modern learning-based methods convert events to a 2D frame by counting events or accumulating polarities <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. This approach is compatible with conventional computer vision tasks but loses temporal information. 3D space-time histograms of events, also called voxel grids, preserve the temporal information of events better by accumulating event polarity on a voxel <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52]</ref>. For image deblurring, most works utilized 2D event-image pairs <ref type="bibr" target="#b33">[34]</ref> or borrowed 3D voxel grids like Stacking Based on Time (SBT) from image reconstruction <ref type="bibr" target="#b40">[41]</ref>. However, there still is no event representation specifically designed for motion deblurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We first introduce the mathematical model for the formation of blurry images from sharp images that involves events in Sec. 3.1. Based on this model, we pose the event-based deblurring problem as a deblurring-denoising problem and base the high-level design of our network architecture on this formulation, as explained in Sec. 3.2. We present our symmetric cumulative representation for events, which constitutes a 3D voxel grid in which the temporal dimension is discretized, in Sec. 3.3. This event representation is provided as input together with the blurry image to our two-stage network. We then detail the two main novel components of our network: our novel event-image cross-modal attention fusion mechanism (Sec. 3.4), which adaptively fuses feature channels associated with events and images, and our event mask gated connection module between the two stages of our network (Sec. 3.5), which helps selectively forward to the second stage the features at sharp regions of the input from the encoder and the features at blurry regions from the decoder of the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>For an event camera, the i-th event e i is represented as a tuple e i = (x i , y i , t i , p i ), where x i , y i and t i represent the pixel coordinates and the timestamp of the event respectively, and p i ? {?1, +1} is the polarity of the event <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>. An event is triggered at time t only when the change in pixel intensity I surpasses the threshold compared to the pixel intensity at the time of the last trigger. This is formulated as</p><formula xml:id="formula_0">p i = ? ? ? +1, if log It(xi,yi) I t??t (xi,yi) &gt; c, ?1, if log It(xi,yi) I t??t (xi,yi) &lt; ?c,<label>(1)</label></formula><p>where c is the contrast threshold of intensity change, which may differ across the sensor plane.</p><p>Given the intensity of a latent sharp image L, according to <ref type="bibr" target="#b26">[27]</ref>, the corresponding blurred image B can be derived by the Event-based Double Integral (EDI) model:</p><formula xml:id="formula_1">B = 1 T f +T /2 f ?T /2 L(t)dt = L(f ) T f +T /2 f ?T /2 exp c t f p(s)ds dt,<label>(2)</label></formula><p>where f is the middle point of the exposure time T , p(s) is the polarity component of the event stream and L(f ) is the latent sharp image corresponding to the blurred image B. The discretized version of (2) can be expressed as</p><formula xml:id="formula_2">B = L(N ) 2N + 1 2N i=0 exp ? ? c sgn(i ? N ) j: m?t j ?M pj?x j y j ? ? ,<label>(3)</label></formula><p>where sgn is the signum function,</p><formula xml:id="formula_3">m = min{f + T /2(i/N ? 1), f }, M = max{f + T /2(i/N ? 1), f } and ? is the Kronecker delta, defined as ? kl (m, n) = 1, if k = m and l = n, 0, otherwise.<label>(4)</label></formula><p>In (3), we partition the exposure time T into 2N equal intervals. Rearranging (3) yields:</p><formula xml:id="formula_4">L(N ) = (2N + 1)B 2N i=0 exp c sgn(i ? N ) j: m?t j ?M pj?x j y j .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">General Architecture of EFNet</head><p>The formulation in <ref type="bibr" target="#b4">(5)</ref> indicates that the latent sharp image can be derived from the blurred image combined with the set of events</p><formula xml:id="formula_5">E = {e i = (x i , y i , t i , p i ) : f ? T /2 ? t i ? f + T /2} (i.e.</formula><p>, all the events which are triggered within the exposure time), when events in this set are accumulated over time. We propose to learn this relation with a deep neural network, named Event Fusion Network (EFNet), which admits as inputs the blurred image and the events and maps them to the sharp image. The generic form of the learned mapping is (3) is the ideal formulation for event-based motion image deblurring. However, in real-world settings, three factors make it impossible to restore the image simply based on this equation:</p><formula xml:id="formula_6">L initial = f 3 (f 1 (B; ? 1 ), f 2 (E; ? 2 ); ? 3 ) ,<label>(6)</label></formula><p>-Instead of being strictly equal to a fixed value, the values of threshold c for a given event camera are neither constant in time nor across the image <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43]</ref>. -Intensity changes that are lower than the threshold c do not trigger an event.</p><p>-Spurious events occur over the entire image.</p><p>Most of the restoration errors come from the first two factors, which result in degradation of the restored image in regions with events. We denote these regions as R e . Taking the above factors into account, we design our network so that it includes a final mapping of the initial deblurred image L initial to a denoised version of it, which can correct potential errors in the values of pixels inside R e :</p><p>L final = f 4 (L initial ; ? 4 ).</p><p>Two-stage backbone. Based on the generic architecture of the network presented in <ref type="formula" target="#formula_6">(6)</ref> and <ref type="formula" target="#formula_7">(7)</ref>, we design EFNet as a two-stage network to progressively restore sharp images from blurred images and event streams, where the first and second stage implement the mappings in <ref type="formula" target="#formula_6">(6)</ref> and <ref type="formula" target="#formula_7">(7)</ref> respectively. The detailed architecture of EFNet is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>  <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47]</ref>. Thus, we use SAM to connect the two stages of our network. In the first stage, we fuse features from the event branch and the image branch at multiple levels using a novel crossmodal attention-based block. Between the two stages, we design an Event Mask Gated Connection module to boost feature aggregation with blurring priors from events. The details of the two aforementioned components of EFNet are given in Sec. 3.4 and 3.5. To feed the asynchronous events corresponding to synchronized image frames to our network, we design a representation specifically suited for deblurring. In <ref type="formula" target="#formula_2">(3)</ref>, the accumulation of polarities via the inner sum on the right-hand side indicates the relative intensity changes between the target latent sharp image L(N ) and each of the rest of latent sharp images in the exposure time. The accumulation via the outer sum on the righthand side represents the sum of all latent sharp images. Based on this relationship, we propose the Symmetric Cumulative Event Representation (SCER). As <ref type="figure">Fig. 2</ref> shows, the exposure time T of the blurry image is divided equally into 2N intervals. Assuming 2N + 1 latent sharp images in T , the polarity accumulation from the central target latent image L(N ) to a single latent image turns into a 2D tensor with dimensions (H, W ):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Symmetric Cumulative Event Representation</head><formula xml:id="formula_8">SCER i = sgn(i ? N ) j: m?tj ?M p j ? xj yj .<label>(8)</label></formula><p>For i = N , SCER N = 0, so we discard this tensor. The remaining 2N tensors are concatenated together, forming a tensor which indicates intensity changes between the central latent sharp image L(N ) and each of the 2N other latent images. In this way, SCER ? R H?W ?2N includes all the relative intensity values corresponding to the center latent sharp frame and it becomes suitable for feature extraction with our image deblurring model. As the accumulation limits change, our SCER also contains both information about the area in which blur occurs (channel 0 and channel 2N ? 1) and information about sharp edges (channel N ? 1 and channel N ).</p><p>Our method discretizes T into 2N parts, quantizing temporal information of events within the time interval T 2N . However, SCER still holds temporal information, as the endpoints of the time interval in which events are accumulated is different across channels. The larger N is, the less temporal information is lost. In our implementation, we fix N = 3. How to jointly extract and fuse information from event streams and images is the key to event-based deblurring. Previous work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref> simply multiplies or concatenates lowresolution feature maps from the two modalities, but this simple fusion approach cannot fully model the relation between events and images. Other methods estimate optical flow with events and use that for deblurring <ref type="bibr" target="#b33">[34]</ref>. However, the estimation of optical flow itself introduces errors. To utilize event data, we instead include a novel cross-modal attention block at multiple levels of EFNet. Contrary to self-attention blocks, in which the queries (Q), keys (K) and values (V) all come from the same branch of the network, our Event-Image Cross-modal Attention (EICA) block admits as inputs the queries Q image from the image branch and the keys K event and values V event from the event branch, as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. In particular, the input features from the two branches are first fed to normalization and 1 ? 1 convolution layers, where the latter have c output channels. We then apply cross-modal attention between vectorized features from the two modalities via</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Event-Image Cross-modal Attention Fusion</head><formula xml:id="formula_9">Attention(Q image , K event , V event ) = V event softmax Q T image K event ? d k .<label>(9)</label></formula><p>The reason for introducing the 1 ? 1 convolution layer is to reduce the spatial complexity of the above attention operation. In particular, c is chosen to be much smaller than hw, where h and w are the height and width of the input feature maps, and the soft indexing of K event by Q image is performed at the channel dimension instead of the spatial dimensions. Thus, the resulting soft attention map from <ref type="formula" target="#formula_9">(9)</ref> is c ? c instead of hw ? hw, reducing the spatial complexity from O(h 2 w 2 ) to O(c 2 ) and making the operation feasible even for features with high spatial resolution, as in our case. Finally, the output of the attention operation is added to the input image features and the result of this addition is passed to a multi-layer perceptron (MLP) consisting of two fully connected layers with a Gaussian Error Linear Unit (GELU) <ref type="bibr" target="#b12">[13]</ref> in between. We use the EICA module at multiple levels of EFNEt to fuse event information aggregated across receptive fields of varying size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Event Mask Gated Connection Module</head><p>Previous work <ref type="bibr" target="#b29">[30]</ref> predicts a mask indicating which areas of an image are severely distorted, but this mask is not completely accurate. Apart from information about intensity changes, event data also contain spatial information about the blurred regions of the input image. Typically, regions in which events occur are more severely degraded in the blurry image. Motivated by this observation, we introduce an Event Mask Gated Connection (EMGC) between the two stages of our network to exploit the spatial information about blurred regions.</p><p>In particular, we binarize the sum of the first and last channel of SCER and thus obtain a binary event mask, in which pixels where an event has occurred are set to 0 and the rest are set to 1. As illustrated in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, EMGC masks out the feature maps of the encoder at regions where the event mask is 0, which are expected to be more blurry, and masks out the feature maps of the decoder at regions where the event mask is 1 (using the complement of the event mask), which are expected to be less blurry. A skip connection is added beside the mask operation. Feature maps with less artifacts in the encoder and better restored feature maps are combined through the event mask gate. In this way, we selectively connect feature maps from the encoder and the decoder of the first stage to the second stage. Besides, EMGC eases the flow of information through the network, as it creates a shortcut through which features can be transferred directly from the first to the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REBlur Dataset</head><p>Almost all event-based motion deblurring methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref> train models on blurred image datasets, such as GoPro <ref type="bibr" target="#b25">[26]</ref>, with synthetic events from ESIM <ref type="bibr" target="#b30">[31]</ref>. Although the contrast threshold c in the event simulator varies across pixels as in reality, a domain gap between synthetic and real events still exists because of the background activity noise, dark current noise, and false negatives in refractory period <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43]</ref>. Recently, Jiang et al . <ref type="bibr" target="#b13">[14]</ref> proposed BlurDVS by capturing an image plus events with slow motion, and then synthesizing motion blur by averaging multiple nearby frames. However, motion blur in the groundtruth images is inevitable in this setting and fast motion causes different events from slow motion because of the false negatives in the refractory period of event cameras <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45]</ref>. Thus, a large-scale real-world dataset with blurry images, reliable corresponding events, and ground-truth sharp images is missing.</p><p>We present a new event-based dataset for deblurring, Real Event Blur (REBlur), to provide ground truth for blurry images in a two-shot way. To collect REBlur, we built an image collection system in a high-precision optical laboratory with very stable illumination. We fixed an Insightness Seem 1 event camera and a Dynamic and Active Pixel Vision Sensor (DAVIS) to the optical table, outputting time-aligned event streams and 260 ? 360 gray images. To obtain blurry-sharp image pairs under high-speed motion, we also fixed a high-precision electronic-controlled slide-rail system to the optical table. In the first shot, we  <ref type="bibr" target="#b25">[26]</ref>. ? denotes event-based methods. SRN+ and HINet+ denote eventenhanced versions of SRN <ref type="bibr" target="#b38">[39]</ref> and HINet <ref type="bibr" target="#b6">[7]</ref>  captured images with motion blur for the pattern on the slide-rail and corresponding event streams. In the second shot, according to the timestamp t s of the blurry images, we selected events within the time range [t s ?125?s, t s +125?s] and visualized these events in the preview of the sharp image capture program.</p><p>Referring to the edge information from high-temporal-resolution events, we could relocate the slide-rail to the coordinate corresponding to the timestamp t s by an electronic-controlled stepping motor and then capture the latent sharp image. Between the two shots, the background was kept static.  To enhance the generalization of the network for different objects and moving processes, our dataset includes 12 kinds of linear and nonlinear motions for 3 different moving patterns and for the camera itself, as detailed in <ref type="figure" target="#fig_5">Fig. 4</ref>. The dataset consists of 36 sequences and 1469 groups of blurry-sharp image pairs with associated events, where 486 pairs are used for training and 983 for testing. We also include an additional set of 4 sequences including extreme blur, without ground truth. Please refer to the supplement for more details on REBlur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Settings</head><p>GoPro dataset. We use the GoPro dataset <ref type="bibr" target="#b25">[26]</ref>, which is widely used in motion deblurring, for training and evaluation. It consists of 3214 pairs of blurry and sharp images with a resolution of 1280?720 and the blurred images are produced by averaging several high-speed sharp images. We use 2103 pairs for training and 1111 pairs for testing, following standard practice <ref type="bibr" target="#b25">[26]</ref>. We use ESIM <ref type="bibr" target="#b30">[31]</ref>, an open-source event camera simulator, to generate simulated event data for GoPro. To make the results more realistic, we set the contrast threshold c randomly for each pixel, following a Gaussian distribution N (? = 0.2, ? = 0.03). REBlur dataset. In order to close the gap between simulated events and real events, before evaluating models that are trained on GoPro on REBlur, we finetune them on the training set of REBlur. We then evaluate the fine-tuned models on the test set of REBlur. More details on this fine-tuning follow. Implementation details. Our network requires no pre-training. We train it on 256 ? 256 crops of full images from GoPro. Full details about our network configuration (numbers of channels, kernel sizes etc.) are given in the supplement. For data augmentation, horizontal and vertical flipping, random noise and hot pixels in event voxels <ref type="bibr" target="#b35">[36]</ref> are applied. We use Adam <ref type="bibr" target="#b14">[15]</ref> with an initial learning rate of 2 ? 10 ?4 , and the cosine learning rate strategy with a minimum learning rate of 10 ?7 . The model is trained with a batch size of 8 for 300k iterations. Finetuning on REBlur involves 600 iterations, the initial learning rate is 2?10 ?5 and other configurations are kept the same as for GoPro. We use the same training and fine-tuning settings for our method and other methods for a fair comparison. Evaluation protocol. All quantitative comparisons are performed using PSNR and SSIM <ref type="bibr" target="#b41">[42]</ref>. Apart from these, we also report the relative reduction in error of the best-performing model for the GoPro benchmark. This is done by converting PSNR to RMSE (RMSE ? ? 10 ?PSNR/10 ) and translating SSIM to DSSIM (DSSIM = (1 ? SSIM)/2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparisons with State-of-the-Art Methods</head><p>We compare our method with state-of-the-art image-only and event-based deblurring methods on GoPro and REBlur. Since most learning-based methods using events do not have publicly available implementations, in the qualitative comparison part, apart from BHA <ref type="bibr" target="#b26">[27]</ref>, we compare our method with SRN <ref type="bibr" target="#b38">[39]</ref> and HINet <ref type="bibr" target="#b6">[7]</ref>, the latter being the current best model on the GoPro benchmark. To have a fair comparison, we also include event-enhanced versions of these two models by concatenating event voxel grids and images in the input. GoPro dataset. We report deblurring results in <ref type="table" target="#tab_3">Table 1</ref>. Compared to the best existing image-based method <ref type="bibr" target="#b6">[7]</ref> and event-based method <ref type="bibr" target="#b5">[6]</ref>, our method achieves 2.75 dB and 2.47dB improvement in PSNR and 0.013 and 0.037 improvement in SSIM, resp., with a low parameter count of 8.47M. Despite utilizing an extra modality, other learning-based methods using events such as D 2 Nets, Blurry Image SRN <ref type="bibr" target="#b38">[39]</ref> HINet <ref type="bibr" target="#b6">[7]</ref> BHA <ref type="bibr" target="#b26">[27]</ref> MPRNet <ref type="bibr" target="#b46">[47]</ref> SRN+ <ref type="bibr" target="#b38">[39]</ref> HINet+ <ref type="bibr" target="#b6">[7]</ref> EFNet (Ours) GT Blurry Image SRN <ref type="bibr" target="#b38">[39]</ref> HINet <ref type="bibr" target="#b6">[7]</ref> BHA <ref type="bibr" target="#b26">[27]</ref> MPRNet <ref type="bibr" target="#b46">[47]</ref> SRN+ <ref type="bibr" target="#b38">[39]</ref> HINet+ <ref type="bibr" target="#b6">[7]</ref> EFNet (Ours) GT LEMD, and ERDNet do not improve significantly upon image-only methods, indicating that they do not take full advantage of event features. Our model sets the new state of the art in image deblurring, showing that our principled two-stage architecture with multi-level attentive fusion leverages event information more effectively for this task. Note that by simply including our SCER to HINet <ref type="bibr" target="#b6">[7]</ref>, the resulting enhanced version of it also surpasses the best previous event-based method <ref type="bibr" target="#b5">[6]</ref>. We show qualitative results on GoPro in <ref type="figure" target="#fig_6">Fig. 5</ref>. Results from image-based methods are more blurry, losing sharp edge information. BHA <ref type="bibr" target="#b26">[27]</ref> restores edges better but suffers from noise around them because of the factors described in Sec. 3.1. Learning-based methods using events cannot fully exploit the motion information from events. By inputting the concatenation of our SCER with the image to SRN+ and HINet+, they both achieve large improvements. However, results from SRN+ include artifacts and noise due to the absence of a second stage in the network that would refine the result. HINet+ produces results with more artifacts, indicating that simply concatenating events and images in the input is not effective. Based on the physical model for event deblurring, EFNet achieves sharp and faithful results. Both dominant structures and fine details are restored well thanks to our attentive fusion at multiple levels. REBlur dataset. We report quantitative results on REBlur in <ref type="table" target="#tab_5">Table 2</ref>. Our model outperforms all other methods in this challenging real-world setting. <ref type="figure">Fig. 6</ref> depicts qualitative results from the test set and the additional set. Even the best image-based method, HINet, does not perform well on these severe cases of real-world motion blur. Event-based methods are more robust to such adverse conditions and less prone to overfitting on synthetic training data. Results from  <ref type="figure">Fig. 6</ref>: Visual comparison on the REBlur dataset. The first two columns are from the test set of the REBlur dataset, and the rest are from the additional set, for which ground truth is not available. Our method shows superior performance in cases with severe blur both due to object motion and due to camera motion.</p><p>Best viewed on a screen and zoomed in.</p><p>BHA are sharper, but accumulation noise still exists. Simply adding events with our SCER representation to the state-of-the-art image-based method <ref type="bibr" target="#b6">[7]</ref> improves performance significantly because of the physical basis of SCER, but still leads to artifacts and ghost structures. Our EFNet restores both smooth texture and sharp edges, demonstrating the utility of our two-stage architecture and our cross-modal attention for fusion. Thanks to the selective feature connection via EMGC, EFNet restores blurry regions well while also maintaining the content of sharp regions. Results on more images are provided in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>We conduct two ablation studies on GoPro to analyze the contribution of different components of our network <ref type="table">(Table 3</ref>) and our event representation <ref type="table" target="#tab_7">(Table 4)</ref>. First and foremost, our EICA fusion block fuses event and image features effectively, improving PSNR by 0.6 dB or more and SSIM by 0.4% compared to simple strategies for fusion such as multiplication or addition (rows 6-8 and <ref type="table">Table 3</ref>: Ablation study of various components of our method on Go-Pro <ref type="bibr" target="#b25">[26]</ref>. "Early": fusion by concatenation of event voxel grid and image, "Multilevel": fusion with our proposed architecture. SCER is used to represent events.  <ref type="table">Table 3</ref>). Second, simply introducing middle fusion at multiple levels and using simple strategies for fusion yields an improvement of ?1 dB in PSNR and 0.7% in SSIM over early fusion (rows 5-8), evidencing the benefit of using multi-level fusion in our EFNet. Third, simply adding events as input to the network via early fusion of our SCER voxel grids with the images improves PSNR by 1.53 dB and SSIM by 0.6% (rows 3-4), showcasing the informativeness of the event modality regarding motion, which leads to better deblurring results. Fourth, adding a second stage in our network to progressively restore the blurry image benefits deblurring significantly, both in the image-only case (rows 1 and 3) and in the case where our fully-fledged EFNet is used (rows 2 and 10). Fifth, connecting the two stages of EFNet with our EMGC improves the selective flow of information from the first stage to the second one, yielding an improvement of 0.15 dB (rows 9-10). Finally, all our contributions put together yield a substantial improvement of 6.4 dB in PSNR and 3.6% in SSIM over the image-only one-stage baseline, setting the new state of the art in motion deblurring. Event representation. Introducing events can improve performance due to the high temporal resolution of the event stream, which provides a vital signal for deblurring. <ref type="table" target="#tab_7">Table 4</ref> shows a comparison between SCER and other event representations, including SBT <ref type="bibr" target="#b40">[41]</ref>, which accumulates polarities in fixed time intervals. We use the same number of intervals (6) for SBT and SCER for a fair comparison. Based explicitly on physics, SCER utilizes event information for image deblurring (35.46dB) more effectively than SBT (35.12 dB). Note that simply accumulating all events across the exposure time ("Stack") deteriorates the performance compared to not using events at all, which demonstrates that finding a suitable event representation for deblurring, such as SCER, is non-trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have looked into single image motion deblurring from the perspective of event-based fusion. Based on the common physical model which describes both blurry image formation and event generation, we have introduced EFNet, an end-to-end motion deblurring network with an attention-based eventimage fusion module applied at multiple levels of the network. In addition, we have proposed a novel event voxel representation to best utilize events for deblurring. We have captured a new real-world dataset, REBlur, including several cases of severe motion blur, which provides a challenging evaluation setting. Our EFNet significantly surpasses the prior state of the art in image deblurring, both on the GoPro dataset and on our new REBlur dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(a): The architecture of our Event Fusion Network (EFNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b): The Event Mask Gated Connection module (EMGC) transfers features across stages guided by an event mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 Fig. 2 :</head><label>42</label><figDesc>The proposed Symmetric Cumulative Event Representation (SCER). Red and blue dots represent events with positive and negative polarity respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>The proposed Event-Image Cross-modal Attention fusion module. The size of the attention map is c ? c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Distribution of different motion categories in our REBlur dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Visual comparison on the GoPro dataset. SRN+ and HINet+ are event-enhanced versions of SRN and HINet using our SCER. Compared to image-based and event-based state-of-the-art methods, our method restores fine texture and structural patterns better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where the blurred image and the events are mapped individually to intermediate representations via f 1 and f 2 respectively and these intermediate representations are afterwards passed to a joint mapping f 3 . ? 1 , ? 2 and ? 3 denote the respective parameters of the three mappings. The main challenges we need to address given this generic formulation of our model are (i) how to represent the set of events E in a suitable way for inputting it to the network, and (ii) how and when to fuse the intermediate representations that are generated for the blurred image by f 1 and for the events by f 2 , i.e., how to design f 3 . We address the issue of how to represent the events in Sec. 3.3 and how to perform fusion in Sec. 3.4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Both stages of EFNet have an encoder-decoder structure, based on the UNet<ref type="bibr" target="#b31">[32]</ref> architecture, and each stage consists of two down-sampling and two up-sampling layers. Between the encoder and decoder, we add a skip connection with 3 ? 3 convolution. The residual convolution block in the UNet consists of two 3 ? 3 convolution layers and leaky ReLU functions with a 1 ? 1 convolution shortcut. Recently, the Supervised Attention Module (SAM) in multi-stage methods demonstrated superior capacity in transferring features between different sub-networks</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison of motion deblurring methods on the GoPro dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>using our SCER.</figDesc><table><row><cell>Method</cell><cell>PSNR ?</cell><cell>SSIM ?</cell></row><row><cell>DeblurGAN [18]</cell><cell>28.70 (54.1%)</cell><cell>0.858 (80.3%)</cell></row><row><cell>BHA  ? [27]</cell><cell>29.06 (52.1%)</cell><cell>0.940 (53.3%)</cell></row><row><cell>Nah et al . [26]</cell><cell>29.08 (52.0%)</cell><cell>0.914 (67.4%)</cell></row><row><cell>DeblurGAN-v2 [19]</cell><cell>29.55 (49.4%)</cell><cell>0.934 (57.6%)</cell></row><row><cell>SRN [39]</cell><cell>30.26 (45.1%)</cell><cell>0.934 (57.6%)</cell></row><row><cell>SRN+  ? [39]</cell><cell>31.02 (40.0%)</cell><cell>0.936 (56.3%)</cell></row><row><cell>DMPHN [48]</cell><cell>31.20 (38.8%)</cell><cell>0.940 (53.3%)</cell></row><row><cell>D 2 Nets  ? [34]</cell><cell>31.60 (35.9%)</cell><cell>0.940 (53.3%)</cell></row><row><cell>LEMD  ? [14]</cell><cell>31.79 (34.5%)</cell><cell>0.949 (45.1%)</cell></row><row><cell>Suin et al . [37]</cell><cell>31.85 (34.0%)</cell><cell>0.948 (46.2%)</cell></row><row><cell>SPAIR[30]</cell><cell>32.06 (32.4%)</cell><cell>0.953 (40.4%)</cell></row><row><cell>MPRNet [47]</cell><cell>32.66 (27.6%)</cell><cell>0.959 (31.7%)</cell></row><row><cell>HINet [7]</cell><cell>32.71 (27.1%)</cell><cell>0.959 (31.7%)</cell></row><row><cell>ERDNet  ? [6]</cell><cell>32.99 (24.8%)</cell><cell>0.935 (56.9%)</cell></row><row><cell>HINet+  ? [7]</cell><cell>33.69 (18.4%)</cell><cell>0.961 (28.2%)</cell></row><row><cell>EFNet (Ours)  ?</cell><cell>35.46</cell><cell>0.972</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparison of motion deblurring methods on our REBlur dataset. The notation is the same as inTable 1.</figDesc><table><row><cell>Method</cell><cell></cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell></cell><cell>Params (M) ?</cell></row><row><cell cols="2">SRN [39]</cell><cell>35.10 (29.4%)</cell><cell cols="2">0.961 (35.9%)</cell><cell>10.25</cell></row><row><cell cols="2">HINet [7]</cell><cell>35.58 (25.4%)</cell><cell cols="2">0.965 (28.6%)</cell><cell>88.67</cell></row><row><cell cols="2">BHA  ? [27]</cell><cell>36.52 (16.8%)</cell><cell cols="2">0.964 (30.6%)</cell><cell>0.51</cell></row><row><cell cols="2">SRN+  ? [39]</cell><cell>36.87 (13.4%)</cell><cell cols="2">0.970 (16.7%)</cell><cell>10.43</cell></row><row><cell cols="2">HINet+  ? [7]</cell><cell>37.68 (4.9%)</cell><cell cols="2">0.973 (7.4%)</cell><cell>88.85</cell></row><row><cell cols="2">EFNet (Ours)  ?</cell><cell>38.12</cell><cell>0.975</cell><cell></cell><cell>8.47</cell></row><row><cell>Blur</cell><cell>GT</cell><cell>Blur</cell><cell>Event</cell><cell>Blur</cell><cell>Event</cell></row><row><cell>SRN</cell><cell>SRN+</cell><cell>SRN</cell><cell>SRN+</cell><cell>SRN</cell><cell>SRN+</cell></row><row><cell>HINet</cell><cell>HINet+</cell><cell>HINet</cell><cell>HINet+</cell><cell>HINet</cell><cell>HINet+</cell></row><row><cell>BHA</cell><cell>Ours</cell><cell>BHA</cell><cell>Ours</cell><cell>BHA</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison between different event representations on the GoPro<ref type="bibr" target="#b25">[26]</ref> dataset. "Stack": temporal accumulation of events in a single channel.</figDesc><table><row><cell>Event representation</cell><cell>PSNR ?</cell><cell>SSIM ?</cell></row><row><cell>None (image-only)</cell><cell>32.15</cell><cell>0.954</cell></row><row><cell>Stack</cell><cell>31.90</cell><cell>0.950</cell></row><row><cell>SBT [41]</cell><cell>35.12</cell><cell>0.970</cell></row><row><cell>SCER (Ours)</cell><cell>35.46</cell><cell>0.972</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was partly supported by China Scholarship Council and Sunny Optical Technology (Group) Co., Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A R</forename><surname>Ahad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishikawa</surname></persName>
		</author>
		<title level="m">Motion history image: its variants and applications. Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-uniform blind deblurring by reblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bahat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Event probability mask (EPM) and event denoising convolutional neural network (EDnCNN) for neuromorphic cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Almatrafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirakawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Simultaneous optical flow and intensity estimation from an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bardow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A 240 ? 180 130 dB 3 ?s latency global shutter spatiotemporal vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brandli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning to deblur and generate high frame rate video with an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00847</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">HINet: Half instance normalization network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPRW</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking coarse-to-fine approach in single image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Removing camera shake from a single photograph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Event-based vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From motion blur to motion flow: A deep learning solution for removing heterogeneous motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning event-based motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2020) 2, 3, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Blind deconvolution using alternating maximum a posteriori estimation with heavy-tailed priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>?roubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAIP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Blind deconvolution using a normalized sparsity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">DeblurGAN: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">DeblurGAN-v2: Deblurring (ordersof-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HOTS: A hierarchy of event-based time-surfaces for pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding and evaluating blind deconvolution algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient marginal likelihood optimization in blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning event-driven video deblurring and interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020) 2, 3</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive time-slice block-matching optical flow algorithm for dynamic vision sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Eventbased vision meets deep learning on steering prediction for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Maqueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2017) 2, 3, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bringing a blurry frame alive at high frame-rate with an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scheerlinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning of a hierarchical spiking neural network for optical flow estimation: From events to global motion perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paredes-Vall?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y W</forename><surname>Scheper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C H E</forename><surname>De Croon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A 128?128 120 dB 15? s latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatially-adaptive image restoration using distortion-guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">ESIM: an open event camera simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoLR</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Continuous-time intensity estimation using event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scheerlinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bringing events into video deblurring with non-consecutively blurry frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>In: ICCV (2021) 2, 3, 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">HATS: Histograms of averaged time surfaces for robust event-based object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brambilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bourdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reducing the sim-to-real gap for event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stoffregen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scheerlinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kleeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Spatially-attentive patch-hierarchical network for adaptive motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07518</idno>
		<title level="m">BANet: Blur-aware attention networks for dynamic scene deblurring</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Event-based high dynamic range image and very high frame rate video generation using conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M M</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Event camera calibration of per-pixel biased contrast threshold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Goor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACRA</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Event-based particle filtering for robot selflocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weikersdorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ROBIO</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Motion deblurring with real events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unnatural L0 sparse representation for natural image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-stage progressive image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 3, 4</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using spatially variant recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Spatio-temporal filter adaptive network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unsupervised event-based learning of optical flow, depth, and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
