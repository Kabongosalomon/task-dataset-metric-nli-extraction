<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Interpretable Deep Metric Learning with Structural Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of Intelligent Technologies and Systems</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
							<email>raoyongming95@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of Intelligent Technologies and Systems</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of Intelligent Technologies and Systems</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<email>lujiwen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of Intelligent Technologies and Systems</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of Intelligent Technologies and Systems</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Interpretable Deep Metric Learning with Structural Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How do the neural networks distinguish two images? It is of critical importance to understand the matching mechanism of deep models for developing reliable intelligent systems for many risky visual applications such as surveillance and access control. However, most existing deep metric learning methods match the images by comparing feature vectors, which ignores the spatial structure of images and thus lacks interpretability. In this paper, we present a deep interpretable metric learning (DIML) method for more transparent embedding learning. Unlike conventional metric learning methods based on feature vector comparison, we propose a structural matching strategy that explicitly aligns the spatial embeddings by computing an optimal matching flow between feature maps of the two images. Our method enables deep models to learn metrics in a more human-friendly way, where the similarity of two images can be decomposed to several part-wise similarities and their contributions to the overall similarity. Our method is modelagnostic, which can be applied to off-the-shelf backbone networks and metric learning methods. We evaluate our method on three major benchmarks of deep metric learning including CUB200-2011, Cars196, and Stanford Online Products, and achieve substantial improvements over popular metric learning methods with better interpretability. Code is available at https://github.com/wl-zhao/DIML.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual similarity plays an important role in a range of vision tasks including image retrieval <ref type="bibr" target="#b33">[33]</ref>, person identification <ref type="bibr" target="#b4">[4]</ref> and image clustering <ref type="bibr" target="#b30">[30]</ref>. Recent advances in learning visual similarity are mostly driven by Deep Metric Learning (DML), which leverages deep neural networks to  <ref type="figure">Figure 1</ref>: The main idea of the proposed deep interpretable metric learning (DIML) method. Unlike most existing deep metric learning methods match the images by comparing feature vectors, we propose a structural matching strategy that explicitly aligns the spatial embeddings by computing an optimal matching flow between feature maps of the two images to improve the interpretability of visual similarity. learn an embedding space where the embedding similarity in this space can meaningfully reflect the semantic similarity between samples. A variety of deep metric learning methods have been proposed and have shown strong superiority in learning accurate and generalizable visual similarities on various tasks <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b16">16]</ref>. Despite the great progress in learning discriminative embeddings, deep metric learning methods with better interpretability have drawn limited attention from the community. Understanding the underlying matching mechanism of deep metric learning models is of critical importance for developing reliable intelligent systems for many risky visual applications such as surveillance <ref type="bibr" target="#b34">[34]</ref> and access control <ref type="bibr" target="#b21">[21]</ref>.</p><p>To improve the transparency of deep visual models, many efforts have been made recently by either explaining the existing models <ref type="bibr" target="#b53">[52,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref> or modifying models to achieve better interpretability <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b50">49]</ref>. For example, visual attribution methods leverage correlation or gradient to find the important regions that have high contributions to the final prediction. <ref type="bibr" target="#b49">[48]</ref> and <ref type="bibr" target="#b50">[49]</ref> propose to add part constraints and tree structures to construct interpretable CNN models respectively. However, these methods are only designed for explaining the reasoning process of how the output of a deep model is produced and did not consider the interaction between samples. Although they achieve promising results on image classification <ref type="bibr" target="#b53">[52,</ref><ref type="bibr" target="#b1">2]</ref>, visual question answering <ref type="bibr" target="#b31">[31]</ref>, and image generation <ref type="bibr" target="#b0">[1]</ref>, they cannot explain how visual similarity is composed. Therefore, how to improve the interpretability of deep metric learning methods is still an open problem that has barely been visited in previous works.</p><p>In this paper, we present a deep interpretable metric learning (DIML) framework as a first step towards more transparent embedding learning. Different from most existing deep metric learning methods that match the images by directly comparing feature vectors, we propose to leverage the spatial structure of images during matching to improve interpretability, as illustrated in <ref type="figure">Figure 1</ref>. More specifically, we measure the similarity of two images by computing an optimal matching flow between the feature maps using the optimal transport theory such that the similarity can be decomposed into several part-wise similarities with different contributions to the overall similarity. Our framework consists of three key components: 1) Structural Similarity (SS). Unlike most existing deep metric learning methods that match the images by comparing feature vectors, we propose a new similarity/distance metric by measuring the similarity of corresponding parts in the feature maps based on the optimal matching flow; 2) Spatial Cross-Correlation (CC). To handle the view variance in the image retrieval problem, we propose to use spatial cross-correlation as the initial marginal distribution to compute the optimal transport plan; 3) Multi-scale Matching (MM). We also devise a multi-scale matching strategy to better incorporate existing metric learning methods and enable us to adaptively adjust the extra computational cost in large-scale search problems. Since our method is model-agnostic and our contribution is orthogonal to previous deep metric learning methods on architectures <ref type="bibr" target="#b14">[14]</ref>, objective functions <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b16">16]</ref> and sampling strategies <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b52">51]</ref>, our method can be applied to off-the-shelf backbone networks and metric learning methods even without training. Extensive experimental study on three major benchmarks of deep metric learning including CUB200-2011 <ref type="bibr" target="#b40">[40]</ref>, Cars196 <ref type="bibr" target="#b17">[17]</ref> and Stanford Online Products (SOP) <ref type="bibr" target="#b24">[24]</ref> shows that our method enables us to achieve more interpretable metric learning while substantially improving various metric learning methods with or without re-training the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Metric Learning. Deep metric learning (DML) has drawn increasing attention recently and become one of the primary framework for a range of vision tasks including image retrieval <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b16">16]</ref>, image clustering <ref type="bibr" target="#b30">[30]</ref>, person reidentification <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b3">3]</ref> and face recognition <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b26">26]</ref>. Previous works on deep metric learning commonly focus on learning more accurate and robust embeddings to better reflect the semantic relations among samples. To achieve this goal, a variety of deep metric learning approaches are proposed to improve the architectures <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b14">14]</ref>, objective functions <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b16">16]</ref> and sampling strategies <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b28">28]</ref>. Different from these works, there is a line of deep metric learning research on developing more effective distance or similarity metrics. Except for the commonly used p distance and cosine similarity, signal-to-noise ratio (SNR) <ref type="bibr" target="#b47">[46]</ref> and hyperbolic geodesic distance <ref type="bibr" target="#b15">[15]</ref> have also proven to be effective to reflect the semantic relationships among samples. However, these deep metric learning methods only consider the distance or similarity between feature vectors, which ignores the spatial structure of images and thus lacks interpretability. In this work, we propose to measure the similarity of two images by explicitly leveraging the spatial structures of images such that more accurate and interpretable similarity of two samples can be obtained.</p><p>Explainable &amp; Interpretable Vision Models. Recent years have witnessed remarkable progress in various computer vision tasks driven by the success of deep learning <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b19">19]</ref>. Despite the impressive discriminative power, the interpretability is often viewed as an Achilles' heel of deep models. Improving the explainability and interpretability of deep models has attracted increasing attention in recent years. Existing works can be roughly divided into two groups: 1) explaining existing models through visualization and diagnosis of deep representations; 2) modifying deep models to learn disentangled and interpretable representations. For example, Zhou et al. <ref type="bibr" target="#b53">[52]</ref> proposes a method named Class Activation Mapping (CAM), which identifies discriminative regions in feature maps of CNNs by analyzing the effects on the final classification results. Grad-CAM <ref type="bibr" target="#b31">[31]</ref> improves the method by combining both the input features and the gradients of a model's layer. Apart from these methods focusing on explaining and analyzing trained models, interpretable vision models are developed by revising the architectures or training procedure of conventional deep models. Zhang et al. <ref type="bibr" target="#b49">[48]</ref> design interpretable CNNs by enforcing each filter in a high-level convolutional layer represents a specific object part. <ref type="bibr" target="#b50">[49]</ref> combine the CNNs and decision tree to inherit the advantages of the two types of models to construct power yet interpretable image classification models. However, these methods only explain the reasoning process of how the output of a deep model is produced and did not consider the interaction between samples. Therefore, they cannot analyze and explain how the similarity of the two samples is composed. Recently, Williford et al. <ref type="bibr" target="#b43">[43]</ref> present a study on explainable face recognition, which uses image editing techniques to gener-  <ref type="figure">Figure 2</ref>: The overall pipeline of our deep interpretable metric learning (DIML) framework. The feature maps extracted from the backbone CNN model are further fed into the cross-correlation module (CC) to compute the marginal distributions that represent the weights of each location. The optimal transport plan then is obtained using the marginal distributions and the similarity matrix. Our framework decomposes the visual similarity to part-wise similarities and their contributions, which enable us to interpret and analyze how a deep model distinguishes two images. ate a new dataset to evaluate what regions contribute to face matching. Their benchmark requires prior knowledge on face structures and thus is hard to generalize to other image matching problems. Different from these works, we propose to study a new and more generic problem of interpretable deep metric learning and provide a basic solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries: Deep Metric Learning</head><p>Deep metric learning aims to find a distance metric parameterized by deep neural networks to map the input image feature pairs to a distance in R that reflects the semantic similarity of the two images defined by labels. Formally, given a set of images X = {x k } N k=1 and the corresponding labels Y = {y k } N k=1 , deep metric learning introduces the deep neural networks f : X ? ? ? R C to map an image to a feature ? k = f (x k ), where the semantic patterns of the input image are extracted. The mainstreams of deep metric learning aim to learn Mahalanobis distance metrics d(?, ?), which can be formulated as:</p><formula xml:id="formula_0">d(x k , x l ) = M f (x k ) ? M f (x l ) 2 = g(? k ) ? g(? l ) 2 ,</formula><p>where g(? k ) = M ? k := ? k ? ? is an parametrized linear projection from the feature space ? to an embedding space ? ? R D . Following the configuration in the backbone networks like ResNet <ref type="bibr" target="#b12">[12]</ref> and Inception <ref type="bibr" target="#b35">[35]</ref>, f can be decomposed into f = GAP ?f 1 , where f 1 extracts a feature map ? k = f 1 (x k ) ? R H?W ?C and GAP is the global average pooling. The GAP operation abstracts the feature maps into vectors so as to enable fast similarity calculation.</p><p>However, the abstraction on deep features also loses the spatial structures of the images during the embedding process, which makes most deep metric learning methods lack interpretability-deep models can tell us whether the two images are similar but cannot show us the reason. Since it is of importance to understand the matching mechanism in many risky visual applications, developing a more interpretable deep metric learning method becomes a critical research topic but it has barely been visited in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Structural Matching via Optimal Transport</head><p>To exploit the spatial structures in images for more interpretable deep metric learning, we devise a new structural matching scheme to compute feature similarity based on optimal transport theory <ref type="bibr" target="#b39">[39]</ref>.</p><p>Our core algorithm is adopted from the optimal transport theory, which aims to seek the minimal cost transport plan between two distributions. Given a source distribution ? s and a target distribution ? t that are defined on probability space U and V respectively, the minimal cost transport plan can be obtain by minimizing the Wasserstein distance between the two distributions:</p><formula xml:id="formula_1">? * = arg inf ???(? s ,? t ) U ?V c(u, v)d?(u, v),<label>(1)</label></formula><p>where ? * is the optimal transport plan, ?(? s , ? t ) is the joint probability distribution with marginals ? s and ? t , and c : U ? V ? R + is the cost function of transportation. Different from the above generic formulation, here we only need to consider the discrete distribution matching for image feature maps. Consider two feature maps ? s , ? t ? R H?W ?C obtained by a backbone (e.g. ResNet50 <ref type="bibr" target="#b12">[12]</ref>). We first use the projection layer g to map each element in the feature maps ? k i into an embedding space of dimension D individually 1 :</p><formula xml:id="formula_2">z s i = g(? s i ) ? R D , z t j = g(? t j ) ? R D .<label>(2)</label></formula><p>The cost of transporting one unit of mass from i to j is:</p><formula xml:id="formula_3">C i,j = c(i, j) := d(z s i , z t j ),<label>(3)</label></formula><p>where we use the distance metric d(?, ?) for two vectors (e.g., Euclid distance or cosine distance) as the transport cost function c. In this discrete case, the transport plan ? matching the two distributions also becomes discrete. Given the two corresponding discrete distributions ? s and ? t , the original optimal transport problem is equivalent to:</p><formula xml:id="formula_4">T * = arg min T ?0 tr(CT ), subject to T 1 = ? s , T 1 = ? t .<label>(4)</label></formula><p>T * is the optimal matching flow between these two distributions, which can be also viewed as the structural matching plan of the two images. T * i,j is the amount of mass that needs to move from i to j in order to reach an overall minimum cost, which represents the contribution of location pair (i, j) to the overall matching.</p><p>To efficiently solve the optimization problem in (4), we adopt the Sinkhorn divergence algorithm <ref type="bibr" target="#b6">[6]</ref> by introducing an entropic regularizer to enable fast training and inference. More details about the algorithm can be found in Supplementary Material. Note that the iterative algorithm is fully differentiable, which can be easily implemented by using the automatic differentiation library like PyTorch <ref type="bibr" target="#b25">[25]</ref> and directly apply the matching process to any deep metric learning pipelines.</p><p>Discussions. Some closely related works of the proposed structural matching scheme include EMD metric learning <ref type="bibr" target="#b51">[50]</ref> and Wasserstein embedding learning <ref type="bibr" target="#b8">[8]</ref>. However, different from our method, they usually focus on learning better embeddings for set inputs, which can be naturally solved by the Wasserstein distance metric learning framework. Here our main contribution is not the matching algorithm itself but the introduction of structural matching for learning more interpretable visual similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deep Interpretable Metric Learning</head><p>In Section 3.2, we have already shown how to calculate the distance between two distributions using the optimal transport. In this section, we describe how to perform structural matching in metric learning. Specifically, our method consists of three components: 1) we use optimal transport to calculate the structural similarity (SS) of two images; 2) we propose to calculate the spatial cross-correlation (CC) to initialize the marginal distributions in Equation (1); 3) we propose multi-scale matching (MM) to improve the metric and reduce the computation cost.</p><p>Structural Similarity (SS). Given the marginal distributions ? s and ? t (which we will discuss in detail later) and the cost matrix C, we can then obtain the optimal transport T * by solving (4). Once we have T * , we can define the structural similarity of two feature maps z s , z t ? R HW ?D as follows:</p><formula xml:id="formula_5">s struct (z s , z t ) = 1?i,j?HW s(z s i , z t j )T * i,j ,<label>(5)</label></formula><p>where s(?, ?) is a function to calculate the similarity between two vectors. Our structural similarity enables us to investigate the composition of the overall similarity, thus we can easily decompose the similarity and understand how the similarity between different locations in the two images contribute to the overall similarity. Similarly, given any distance function d(?, ?), we can also derive our structural distance:</p><formula xml:id="formula_6">d struct (z s , z t ) = 1?i,j?HW d(z s i , z t j )T * i,j ,<label>(6)</label></formula><p>Cross-Correlation (CC). Another important part is the definition of the marginal distributions ? s and ? t . One trivial solution is to initialize them with uniform distributions, i.e.,</p><formula xml:id="formula_7">? s i = ? t i = 1 HW , ?1 ? i ? HW,<label>(7)</label></formula><p>which indicates similarity of each location has the identical weight to the overall similarity. In the structural matching algorithm, the marginal distributions should characterize the importance of each spatial location. Simply using uniform distributions implies that we want to match all the features with equal importance, which is not desired in some cases. For example, some image contains background information that may be less useful for matching thus we want to impose lower weights on the background. Another common circumstance is when we want to match two images with different views (e.g., the first image contains the whole object and the second one only contains a part of it), and similarly we only need to focus on the certain part of the first image and treat the rest as background. To find the areas that are most related to the similarity, we propose to calculate the cross-correlation between the two images as the marginal distributions for the matching algorithm. Specifically, we first perform global average pooling to z s , z t and obtain the global featurez s ,z t . We then slide the global feature of one image on the feature map of the other image and calculate point-wise correlation at each spatial location. Formally, the cross-correlation is calculated as:</p><formula xml:id="formula_8">? s i = z s , z t i z s z t i , ? t i = z t , z s i z t z s i ,<label>(8)</label></formula><p>where ?, ?, is the dot product and ? k i ? [?1, 1]. After obtaining the cross-correlation, we can use ? k i to reflect the importance of z k i in the matching problem. To further reduce the effects of low correlation regions, we discard the negative value of ? k i and normalize it to obtain the final marginal distributions:</p><formula xml:id="formula_9">? k i = max(0, ? k i ), ? k i = 1 i ? k i ? k i ?1 ? i ? HW, k ? {s, t}.<label>(9)</label></formula><p>Once we have the marginal distributions ? (k) , we can then apply the structural matching algorithm in Section 3.2 to calculate the similarity between two images. We will show in Section 4.3 that cross-correlation is an indispensable component to improve the power of DIML.</p><p>Multi-scale Matching (MM). Although DIML can capture the structural similarity of two images and can provide results easily understood by humans, it requires more computation (O(H 2 W 2 )) to solve the optimal transport problem. In the application of image retrieval, there are usually a great number of images in the gallery. Given an image as an anchor, calculate the structural similarity between the anchor and all the images in the gallery is inefficient. To reduce the computational cost, we propose a multi-scale matching method for image retrieval. Let z a ? R H?W ?D be the feature map of the anchor image and z k ? R H?W ?D , k = 1, . . . , N be the feature maps of all the images in the gallery. In the first scale (1 ? 1), we compute the global feature using global average pooling to getz a ,z k ? R D , and calculate cosine similarity betweenz a and eachz k as conventional DML methods. We can then define a truncation number K and select the images with top-K similarity score and denote the indices of them as I K to further enhance the similarity with our method. In the second scale (H ? W ), we calculate the structural similarity between z a and each z k , k ? I K . Since K is fixed, the extra computational cost of DIML can be controlled. By multi-scale matching, we can filter out the obvious dissimilar samples (1 ? 1 scale, cosine similarity) and focus on the hard ones (H ? W scale, structural similarity). Combining the similarity at two scales can also capture both semantic and spatial information, which is also helpful to improve retrieval precision. We will show later in Section 4.3 that a small K can yield a significant performance boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation</head><p>One of the major advantages of DIML is that we can apply DIML to any pre-train model to improve performance with no need of training. Besides, we can also incorporate DIML into the training objective. In this section, we will describe how to use DIML in these two scenarios.</p><p>Testing. Given a pre-trained model, we first calculate the feature maps ? s , ? t ? R H?W ?C (before the global pooling layer) of the image pair x s , x t . We can then use the algorithm describe in Section 3.3 to compute the structural similarity. However, HW may be sometimes large in practice (e.g., for</p><p>ResNet50 <ref type="bibr" target="#b12">[12]</ref>, H = W = 7). Therefore, we can use ROI Align <ref type="bibr" target="#b11">[11]</ref> to pool the feature maps to R H ?W ?C , where H &lt; H and W &lt; W . With smaller feature maps, we can then calculate the structural similarity with a relatively lower computational cost. In our implementation, we use H = W = G and we found G = 4 can achieve good trade-off between cost and performance.</p><p>Training. We can also combining DIML and existing metric learning methods to facilitate training. We now use Margin loss <ref type="bibr" target="#b45">[44]</ref> as an example to show how to incorporate DIML into the training objective. The Margin loss <ref type="bibr" target="#b45">[44]</ref> is defined as</p><formula xml:id="formula_10">L margin (k, l) = ? + (?1) I(y k =y l ) (D k,l ? ?) + ,<label>(10)</label></formula><p>where ? and ? are learnable parameters, and D kl is used to measure the distance between image k and l:</p><formula xml:id="formula_11">D k,l = 1 2 d struct (z k , z l ) + d(z k ,z l )<label>(11)</label></formula><p>For the implementation details of other metric learning methods, please refer to the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the performance of our proposed DIML, we conduct experiments on three widely used datasets in the image retrieval research field: CUB200-2011 <ref type="bibr" target="#b40">[40]</ref>, Cars196 <ref type="bibr" target="#b17">[17]</ref>, and Standard Online Products (SOP) <ref type="bibr" target="#b24">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setups</head><p>Datasets. We evaluate our method under a zero-shot image retrieval setting, where the training set and test set contain image classes with no intersection. We follow the training/test set splits in previous works <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b29">29]</ref>:</p><p>? CUB200-2011 <ref type="bibr" target="#b40">[40]</ref>  A Fair Evaluation Protocol. Although there are many previous metric learning methods, <ref type="bibr" target="#b23">[23]</ref> pointed out that the improvements over time are not significant, due to the unfair comparisons of different methods. Therefore, we try our best to provide fair results by implementing all the methods under the same evaluation protocol. For all the baseline methods, we use ResNet-50 <ref type="bibr" target="#b12">[12]</ref> pre-trained on ImageNet <ref type="bibr" target="#b18">[18]</ref> as the backbone. We freeze the BatchNorm layers during training and modify the output channel of the last linear layer to a fixed embedding dimension D. We use embedding size D = 128 and other implementation settings following <ref type="bibr" target="#b29">[29]</ref> for most experiments unless otherwise noted.</p><p>Evaluation Metrics. Most previous works use Recall@K, Normalized Mutual Information, and F1 score as accuracy metrics. However, as is pointed by <ref type="bibr" target="#b23">[23]</ref>, NMI and F1 scores sometimes give us wrong pictures of the embedding space.</p><p>To this end, we adopt the evaluation metrics used in <ref type="bibr" target="#b23">[23]</ref>: Precision@1, R-Precision, and MAP@R. For the formal definition of the metrics, see Supplementary Material.</p><p>Implementation. It is also worth noting that our proposed DIML does not require any training. Therefore, we aim to prove that our method can improve the performance given any trained model as the baseline. Therefore, we perform experiments on a wide range of loss functions (Margin <ref type="bibr" target="#b45">[44]</ref>, Arcface <ref type="bibr" target="#b7">[7]</ref>, etc.) and sampling methods (Distance <ref type="bibr" target="#b45">[44]</ref>, N-Pair <ref type="bibr" target="#b33">[33]</ref>, etc.) to prove the effectiveness and the generalization ability of our method. For most of the baseline methods, we follow the implementation from <ref type="bibr" target="#b29">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>We first evaluate our method by applying DIML to a wide range of metric learning methods. We measure the performance using the evaluation metrics aforementioned: Preci-sion@1 (P@1), R-Precision (RP) and MAP@R (M@R), and the results 2 are shown in <ref type="table" target="#tab_1">Table 1</ref>. For all the experiments, we set the truncation number K = 100 and feature map size G = 4. We observe that our method can improve the performance for all the models on all the three benchmarks, without any extra training. Especially, we find on Cars196 dataset, the performances of all the methods are enhanced profoundly after equipped with our DIML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study and Analysis</head><p>In this section, we will evaluate our DIML in various settings and provide detailed analyses through experiments and visualization. Effects of different components. The DIML consists of three components: structural similarity (SS), crosscorrelation (CC), and multi-scale matching (MM). We will analyze the effect of each one, as is shown in <ref type="table" target="#tab_2">Table 2</ref>. We start with two baseline methods Margin <ref type="bibr" target="#b45">[44]</ref> and Multi-Similarity <ref type="bibr" target="#b42">[42]</ref>, and add the three components gradually. First, we adopt structural similarity instead of standard cosine similarity (where we use uniform distribution in Equation <ref type="bibr" target="#b7">(7)</ref> for ? s and ? t ). We find that SS can improve the 2 For more results, please refer to the Supplementary Material. performance on all the datasets except for SOP (as is highlighted by underline). It is mainly because that the SS algorithm aims to match every part of a source image to a target image. However, the views vary a lot in SOP dataset, which hinders the application of SS. Second, we show that multiscale matching can make use of the semantic information and improve the performance on all three datasets. Finally, we replace the uniform distribution with the one calculated by cross-correlation. We find the marginal distributions obtained in this way are helpful to explore the important area of the images and can further improve the performance.</p><p>Effects of truncation number. To show how the truncation number K affect our DIML, we test our method on Margin <ref type="bibr" target="#b45">[44]</ref> and Multi-Similarity <ref type="bibr" target="#b42">[42]</ref> with K increasing from 0 to 500 <ref type="figure" target="#fig_2">(Figure 3</ref>). Note that K = 0 means no structural similarity is used, which is identical to the baseline. We find that even a small K will bring considerable improvement on the performance (especially for the P@1 metric). Generally, the retrieval accuracy grows with K increasing and saturates  We test for different truncation number K ranging from 0 to 500. Experimental results show that a small K can already bring considerable performance improvement. Effects of feature map size. We then perform an ablation study on the feature map size G. In our experiments, we use ResNet50 <ref type="bibr" target="#b12">[12]</ref> as our backbone, and the size of the feature map before the pooling layer is 7 ? 7. Hence, we need to pool the feature map to a smaller one (G ? G) to reduce computational complexity. Specifically, we let G ? 7 and evaluate for the cases where G = 1, 2, 4, 7. The results are shown in <ref type="table" target="#tab_4">Table 4</ref> and we observe that the performance of  our method is better with larger G in general. We can also find G = 4 is a good trade-off between performance and computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of training.</head><p>Besides the default setting where we use DIML to test on any pre-trained model, we can also incorporate the structural similarity into the training objectives (see Section 3.4 for details). In <ref type="table" target="#tab_3">Table 3</ref>, we compare the performance in three scenarios: (1) without DIML testing or training (same as baseline) (2) with DIML testing only (2) with DIML testing and training. The results are listed in <ref type="table" target="#tab_3">Table 3</ref>. We find that for most cases, using DIML to test a pre-trained model can already improve the baseline by a significant margin. Besides, it is also useful sometimes to apply DIML in the training stage.</p><p>Effects of embedding size. Our proposed DIML is also robust across different embedding sizes. Apart from the results in <ref type="table" target="#tab_1">Table 1</ref> where D = 128, we perform experiments with D = 64/512 for Margin <ref type="bibr" target="#b45">[44]</ref>, Multi-Similarity <ref type="bibr" target="#b42">[42]</ref> and Proxy Anchor <ref type="bibr" target="#b16">[16]</ref> and the results are summarized in <ref type="table" target="#tab_4">Table 4</ref>. We demonstrate that our method can boost the performance of the three methods consistently no matter how the embedding size D varies.  <ref type="figure">Figure 5</ref>: Visualization. We use heatmaps to show the marginal distributions obtained by cross-correlation (CC). We also illustrate two most representative part-wise similarity and their contributions to the overall similarity in the form of</p><formula xml:id="formula_12">[G 4 T * i,j ] ? [S i,j ],</formula><p>where G is the grid size, T * i,j and S i,j are the matching flow and similarity between location i and j respectively. We also show the overall similarity changes after applying our method to the baseline model (cosine similarity ? structural similarity). All image pairs are positive pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization</head><p>To better understand how our method works, we provide some visualizations for CUB200-2011 <ref type="bibr" target="#b40">[40]</ref> in <ref type="figure">Figure 5</ref>, where each pair of images is from the same category. First, we visualize the marginal distributions ? s and ? t (calculated by cross-correlation) through heatmaps and find that they can focus on some discriminative parts in the images (e.g., head, foot, etc.). Second, we show the optimal transport flow T * i,j and the similarity S i,j = s(z s i , z t j ) for some pair of spatial location (i, j). Since the sum of the values in T * equals 1 and each T * i,j is relatively small, we use a re-scaled versionT * i,j = G 4 T * i,j such that an uniform transport flow yieldsT * i,j = 1, ?i, j. We draw arrows between the location pairs that make a large (or small) contribution to the final structural similarity in red (or blue). The formula along with an arrow takes the form ofT * i,j ? S ij . We observe that our method can match similar parts and assign a higher T * i,j to the pair while enforcing lower T * i,j to the parts that are less informative to determine the similarity between the two images. Finally, we demonstrate that by re-weighting the similarity matrix S with the optimal transport matrix T * , our proposed structural similarity (shown in bold text) is higher than the standard cosine similarity (shown in light text) by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented the deep interpretable metric learning (DIML) method for more transparent embedding learning. We proposed a structural matching strategy that explicitly aligns the spatial embeddings by computing an optimal matching flow between feature maps of the two images. We evaluated our method on three major benchmarks of deep metric learning including CUB200-2011, Cars196 and Stanford Online Products, and achieved substantial improvements over popular metric learning methods with better interpretability. Our method enables deep models to learn metrics in a more human-friendly way, which can be used to inspect and understand the visual similarity of any two samples or applied to any deep metric learning methods with the proposed multi-scale matching strategy to improve image retrieval performance with controllable cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation of DIML</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. The Sinkhorn Algorithm</head><p>The Sinkhorn algorithm <ref type="bibr" target="#b6">[6]</ref> modifies the original optimal transport problem (Eq.4) into the following one:</p><formula xml:id="formula_13">T * = arg min T ?0 tr(CT ) + ?tr T (log(T ) ? 11 ) , subject to T 1 = ? s , T 1 = ? t ,<label>(12)</label></formula><p>where ? is a non-negative regularization parameter. By adding the entropic regularizer, the Equation <ref type="formula" target="#formula_1">(12)</ref> becomes a convex problem, which can be solved with Sinkhorn-Knopp algorithm <ref type="bibr" target="#b32">[32]</ref>. Starting from an initial matrix K = exp(?C/?), the problem can be solved by iteratively projecting onto the marginal constraints until convergence:</p><formula xml:id="formula_14">a ? ? s /Kb, b ? ? t /K a.<label>(13)</label></formula><p>After converged, we can obtain the optimal transport plan:</p><formula xml:id="formula_15">T * = diag(a)Kdiag(b).<label>(14)</label></formula><p>A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Testing</head><p>In all of our experiments, we use ResNet50 <ref type="bibr" target="#b12">[12]</ref> as our backbone. Therefore, the size of the feature map before the pooling layer is 7 ? 7. To reduce computational costs, we first use ROI Align <ref type="bibr" target="#b11">[11]</ref> to pool the feature map to G?G and G = 4 in most of our experiments unless otherwise noted. According to the multi-scale matching algorithm, for each image as a query, we first sort the images in the gallery using the standard cosine similarity to obtain the indices of top-K candidates I K (we use K = 100 in most of the experiments). We then calculate the proposed structural similarity of all the images in I K . To combine both global and structural information, we use the sum of the cosine similarity and the structural similarity for the top-K images to compute their ranks. The regularization parameter ? in Equation <ref type="formula" target="#formula_6">(6)</ref> is set to 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Training</head><p>Incorporating DIML into the training objectives is quite straightforward. Generally, the loss functions in metric learning can be roughly categorized into distance-based methods (e.g., Contrastive <ref type="bibr" target="#b10">[10]</ref>, Triplet <ref type="bibr" target="#b9">[9]</ref>, Margin <ref type="bibr" target="#b45">[44]</ref>) and similarity-based methods (e.g., Multi-Similarity <ref type="bibr" target="#b42">[42]</ref>, Arcface <ref type="bibr" target="#b7">[7]</ref>, N-Pair <ref type="bibr" target="#b33">[33]</ref>) For distance-based methods, we replace the original distance function d with the average of d and our structural distance d struct ; For similarity-based methods, we replace the original similarity function s with the average of s and our structural similarity s struct . In this section, we will use several loss functions as examples to demonstrate how to apply DIML during training.</p><p>Margin <ref type="bibr" target="#b45">[44]</ref> The Margin loss <ref type="bibr" target="#b45">[44]</ref> is defined as</p><formula xml:id="formula_16">L margin (k, l) = ? + (?1) I(y k =y l ) (D k,l ? ?) + ,<label>(15)</label></formula><p>where ? and ? are learnable parameters, and D kl is used to measure the distance between image k and l:</p><formula xml:id="formula_17">D k,l = 1 2 d struct (z k , z l ) + d(z k ,z l ) ,<label>(16)</label></formula><p>where d is Euclid distance and d struct is derived from d using Equation <ref type="formula" target="#formula_1">(10)</ref>.</p><p>Multi-Similarity <ref type="bibr" target="#b42">[42]</ref> The original Multi-Similarity is defined as:</p><formula xml:id="formula_18">s * (k, l) = ? ? ? ? ? s (k, l) , s (k, l) &gt; min p?P k s (k, p) ? s (k, l) , s (k, l) &lt; max n?N k s (k, n) + 0, otherwise ,<label>(17)</label></formula><formula xml:id="formula_19">L MS = 1 B k?B ? ? 1 ? log ? ? 1 + p?P k exp (?? (s * (k, p) ? ?)) ? ? + 1 ? log 1 + n?N k exp (? (s * (k, n) ? ?)) ,<label>(18)</label></formula><p>where s(k, l) = s(? k , ? l ) is the cosine similarity of the embeddings ? k , ? l of the two images. To utilize DIML, we can replace s with</p><formula xml:id="formula_20">s(k, l) ? 1 2 s(z k ,z l ) + s struct (z k , z l ) .<label>(19)</label></formula><p>Note that in our notation both ? k andz k represent the same embedding in R D .</p><p>ProxyNCA <ref type="bibr" target="#b22">[22]</ref> It is also worth mentioning there are slight difference when applying DIML to proxy-based methods during training. Taking ProxyNCA <ref type="bibr" target="#b22">[22]</ref> as example, the original objective is</p><formula xml:id="formula_21">L proxy = ? 1 B k?B log ? ? exp ?d ? k , ? y k c?C\{y k } exp (?d (? k , ? c ) ? ? ,<label>(20)</label></formula><p>where d is Euclid distance and ? c ? R D is the proxy for the c-th class. To use DIML, we need to use proxies with the size R H?W ?D , denoted as {? c , c ? C}. Then, we can replace the d(? k , ? c ) with</p><formula xml:id="formula_22">d(? k , ? c ) ? 1 2 d(? k , ? c ) + d struct (z k , ? c ) ,<label>(21)</label></formula><p>where we also note that GAP(? c ) = ? c . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Evaluation Metrics</head><p>We implement the same evaluation metrics as <ref type="bibr" target="#b23">[23]</ref>, including Precision at 1 (P@1), R-Precision (RP), and Mean Average Precision at R (MAP@R). P@1 is also known as Recall@1 in metric learning. Given a sample x q and feature encoder ?(?), the set of k nearest neighbors of x q is calculated as the precision of k nearest neighbors: </p><p>where d e (?, ?) is the euclidean distance. Then P@k can be measured as</p><formula xml:id="formula_24">P@k = 1 |X test | xq?Xtest 1 k x i ?N k q 1, y i = y q , 0, otherwise ,<label>(23)</label></formula><p>where y i is the class label of sample x i . We only report P@1 in our experiments, i.e. k = 1. R-precision is defined in <ref type="bibr" target="#b23">[23]</ref>. Specifically, for each sample x q , let R be the number of images that are the same class with x q and R-precision is simply defined as P@R (see <ref type="bibr">Equation 23</ref>). However, R-precision does not consider the ranking of correct retrievals, so it is not informative enough.</p><p>To tackle this problem, <ref type="bibr" target="#b23">[23]</ref> introduced Mean Average Precision at R. MAP@R is similar to mean average precision, but limit the number of nearest neighbors to R. So it replaces precision in MAP calculation with R-precision:</p><formula xml:id="formula_25">MAP@R = 1 R R i=1 P (i),<label>(24)</label></formula><p>where P (i) = P@i, if the i-th retrieval is correct; 0, otherwise.</p><p>MAP@R is more informative than P@1 and it can be computed directly from the embedding space without clustering as post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Experimental Setups</head><p>For most of the baseline methods, we follow the implementation and the hyper-parameters in <ref type="bibr" target="#b36">[36]</ref>. For Proxy Anchor <ref type="bibr" target="#b16">[16]</ref>, we use their original implementation but set the hyper-parameters as <ref type="bibr" target="#b36">[36]</ref> (batch size 112, embedding size 128, etc.). Besides various loss functions, we also experiment with different sampling methods. In <ref type="table" target="#tab_1">Table 1</ref> of the original paper, we use suffixes to represent the sampling methods (-R: Random; -D: Distance <ref type="bibr" target="#b45">[44]</ref>; -S Semihard <ref type="bibr" target="#b30">[30]</ref>; -H: Softhard <ref type="bibr" target="#b29">[29]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Detailed Results</head><p>In the original paper, we have demonstrated the effects of truncation number K and feature map size G using charts. In this section, we provide the original numerical results that were used to plot those charts in <ref type="table" target="#tab_5">Table 5 and Table 6</ref>. <ref type="table">Table 6</ref>: Effects of the size of feature map. Generally, the performance of our DIMLis better with higher G. DIML with G = 4 yields good results within relatively low computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head><p>G <ref type="table" target="#tab_1">CUB-200  Cars196  SOP   P@1  RP  M@R P@1  RP  M@R P@1  RP  M@R</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparisons of different truncation number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Effects of the size of feature map. Generally, the performance of our DIML is better with higher G. DIML with G = 4 yields good results within relatively low computational costs. before K reaches 100. This phenomenon indicates that with a fixed and relatively small K, we can already enjoy a significant performance boost with constant extra computational cost and no extra training cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>min N ?Xtest,|N |=k x f ?N d e (?(x q ), ?(x f ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Applying DIML to various deep metric learning methods. Experimental results show that our method can improve the performance of all the methods consistently. Contrasitive [10] 61.77 34.25 23.24 67.45 30.01 18.61 73.27 40.92 37.5 + DIML 64.43 35.16 24.29 73.35 30.76 20.13 74.47 41.58 38.29 ProxyNCA [22] 62.76 35.05 24.03 71.05 31.62 20.55 74.70 41.32 37.96 + DIML 64.75 36.02 25.10 74.86 32.43 22.00 76.17 42.65 39.36 Histogram [38] 59.96 33.07 22.15 69.49 31.62 19.76 71.15 38.06 34.70 + DIML 62.69 33.80 23.00 74.50 32.36 21.26 72.06 38.57 35.30 Quadruplet [5] 61.53 34.05 22.93 69.64 31.40 19.67 77.02 44.27 40.88 + DIML 62.80 34.65 23.62 75.66 32.35 21.69 78.08 45.16 41.79 SNR [46] 62.00 34.72 23.59 72.95 32.72 21.28 77.82 44.98 41.51 + DIML 64.55 35.25 24.27 77.57 33.54 23.02 78.50 45.65 42.24 Softmax [47] 61.06 32.7 21.55 72.61 31.17 19.88 77.02 43.47 40.25 + DIML 63.30 33.71 22.64 76.39 32.06 21.49 78.17 44.62 41.43 Margin [44] 62.47 34.12 23.14 72.18 32.00 20.82 78.39 45.64 42.34 + DIML 65.16 35.37 24.51 76.62 32.85 22.48 79.26 46.44 43.19 Arcface [7] 61.39 33.70 22.4 73.37 31.90 20.52 77.55 44.44 41.07 + DIML 64.72 34.88 23.72 77.24 32.88 22.34 78.52 45.45 42.10 MS [42] 62.56 32.74 21.99 74.81 32.72 21.60 77.90 44.97 41.54 + DIML 64.89 33.99 23.34 78.44 33.57 23.31 78.53 45.59 42.22 ProxyAnchor [16] 65.24 35.81 24.87 82.36 36.00 25.85 79.10 46.31 42.91 + DIML 66.46 36.49 25.58 86.13 37.90 28.11 79.22 46.43 43.04</figDesc><table><row><cell>Method</cell><cell>CUB200-2011</cell><cell>Cars196</cell><cell>SOP</cell></row><row><cell></cell><cell cols="3">P@1 RP M@R P@1 RP M@R P@1 RP M@R</cell></row><row><cell>Triplet-R [30]</cell><cell cols="3">58.34 32.00 20.93 62.73 26.95 15.24 66.60 33.62 30.26</cell></row><row><cell>+ DIML</cell><cell cols="3">60.60 32.63 21.74 67.92 27.65 16.72 68.73 35.04 31.79</cell></row><row><cell>Triplet-S [30]</cell><cell cols="3">59.28 32.77 21.79 67.00 30.0 18.23 73.67 40.45 37.14</cell></row><row><cell>+ DIML</cell><cell cols="3">62.85 33.87 23.04 72.06 30.89 20.04 75.14 41.68 38.42</cell></row><row><cell>Triplet-H [29]</cell><cell cols="3">61.39 33.21 22.15 71.76 32.53 20.83 73.28 39.98 36.56</cell></row><row><cell>+ DIML</cell><cell cols="3">62.02 33.50 22.49 74.75 32.94 21.76 73.62 40.14 36.79</cell></row><row><cell>Triplet-D [44]</cell><cell cols="3">61.99 33.92 22.90 73.07 32.18 20.81 77.34 44.25 40.80</cell></row><row><cell>+ DIML</cell><cell cols="3">63.40 34.49 23.59 77.31 33.05 22.61 77.81 44.82 41.39</cell></row><row><cell>NPair [33]</cell><cell cols="3">60.30 33.53 22.27 69.52 32.24 20.25 76.47 43.48 39.94</cell></row><row><cell>+ DIML</cell><cell cols="3">62.17 34.02 22.85 74.65 32.91 21.67 76.86 43.87 40.38</cell></row><row><cell>Angular [41]</cell><cell cols="3">61.36 34.17 23.00 70.93 32.97 21.31 73.79 41.42 37.90</cell></row><row><cell>+ DIML</cell><cell cols="3">63.77 35.09 24.06 74.72 33.80 22.83 74.91 42.17 38.73</cell></row><row><cell>GenLifted [13]</cell><cell cols="3">58.27 32.86 21.83 66.88 30.96 19.00 74.84 42.28 38.66</cell></row><row><cell>+ DIML</cell><cell cols="3">61.07 33.82 22.98 72.95 31.93 20.88 75.92 43.08 39.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effects of the three components in our DIML: Structural Similarity (SS), Multi-scale Matching (MM) and Cross Correlation (CC). We show that our method can enhance the performance of the baseline methods by combining the three components together.<ref type="bibr" target="#b23">23</ref>.14 72.18 20.82 78.39 42.34 63.64 22.52 74.86 21.24 77.30 41.02 64.96 23.87 76.02 22.02 78.53 42.45 65.16 24.51 76.62 22.48 79.26 43.19 MS [42] 62.56 21.99 74.81 21.60 77.90 41.54 63.52 21.66 75.63 21.10 75.81 39.38 64.40 22.83 77.39 22.77 77.87 41.55 64.89 23.34 78.44 23.31 78.53 42.22</figDesc><table><row><cell>Baseline</cell><cell cols="3">Components</cell><cell>CUB200-2011</cell><cell>Cars196</cell><cell>SOP</cell></row><row><cell></cell><cell>SS</cell><cell>MM</cell><cell>CC</cell><cell cols="2">P@1 M@R P@1 M@R P@1 M@R</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.47</cell></row><row><cell>Margin [44]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Effects of training. Our method can substantially improve the baseline model with or without training. 23.14 72.18 20.82 78.39 42.34 65.16 24.54 76.65 22.95 79.26 43.19 65.36 24.90 75.61 22.34 78.81 42.89 MS [42] 62.56 21.99 74.81 21.60 77.90 41.54 64.89 23.38 78.50 23.81 78.53 42.23 65.72 24.37 78.90 23.80 79.00 42.96</figDesc><table><row><cell>Baseline</cell><cell>Setting</cell><cell>CUB200-2011</cell><cell>Cars196</cell><cell>SOP</cell></row><row><cell></cell><cell cols="4">test train P@1 M@R P@1 M@R P@1 M@R</cell></row><row><cell></cell><cell></cell><cell>62.47</cell><cell></cell><cell></cell></row><row><cell>Margin [44]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effects of embedding size. Our DIML is robust to the changing of the embedding size D and can improve the performance of the baseline methods consistently. 37.40 26.38 84.75 37.56 27.66 ProxyAnchor + DIML 67.93 37.92 26.88 87.01 39.03 29.39</figDesc><table><row><cell>D</cell><cell>Method</cell><cell cols="3">CUB200-2011</cell><cell>Cars196</cell></row><row><cell></cell><cell></cell><cell>P@1</cell><cell>RP</cell><cell>M@R P@1</cell><cell>RP</cell><cell>M@R</cell></row><row><cell></cell><cell>Margin</cell><cell cols="4">59.39 32.59 21.53 69.31 30.98 19.67</cell></row><row><cell></cell><cell>Margin [44] + DIML</cell><cell cols="4">62.98 33.88 22.95 74.44 31.96 21.60</cell></row><row><cell>64</cell><cell>MS [42]</cell><cell cols="4">58.52 31.35 20.23 71.67 30.90 19.57</cell></row><row><cell></cell><cell>MS + DIML</cell><cell cols="4">61.73 32.61 21.62 76.94 31.95 21.72</cell></row><row><cell></cell><cell>ProxyAnchor [16]</cell><cell cols="4">62.56 34.61 23.45 78.08 34.35 23.95</cell></row><row><cell></cell><cell cols="5">ProxyAnchor + DIML 65.01 35.53 24.40 83.11 36.49 26.55</cell></row><row><cell></cell><cell>Margin [44]</cell><cell cols="4">64.92 35.94 24.92 73.68 32.03 21.09</cell></row><row><cell></cell><cell>Margin + DIML</cell><cell cols="4">66.91 36.82 25.89 76.67 32.62 22.20</cell></row><row><cell>512</cell><cell>MS [42]</cell><cell cols="4">65.92 35.14 24.17 76.85 33.93 22.78</cell></row><row><cell></cell><cell>MS + DIML</cell><cell cols="4">68.15 36.04 25.14 79.74 34.68 24.01</cell></row><row><cell></cell><cell>ProxyAnchor[16]</cell><cell>67.30</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of different truncation numbers. We test for different truncation number K ranging from 0 to 500. Experimental results show that a small K can already bring considerable performance improvement. 34.12 23.14 72.18 32.00 20.82 78.39 45.64 42.34 10 65.16 34.56 23.87 76.65 32.52 21.72 79.26 46.44 43.20 50 65.16 35.43 24.54 76.65 33.64 22.83 79.26 46.44 43.19 100 65.16 35.48 24.54 76.65 33.93 22.95 79.26 46.44 43.19 500 65.16 35.48 24.54 76.65 33.93 22.95 79.26 46.44 43.19 Multi-Similarity[42] 0 62.56 32.74 21.99 74.81 32.72 21.60 77.90 44.97 41.54 10 64.89 33.21 22.73 78.50 33.26 22.50 78.53 45.60 42.24 50 64.89 34.04 23.37 78.50 34.46 23.72 78.53 45.60 42.23 100 64.89 34.12 23.38 78.50 34.70 23.81 78.53 45.60 42.23 500 64.89 34.12 23.38 78.50 34.70 23.81 78.53 45.60 42.23</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CUB-200</cell><cell></cell><cell>Cars196</cell><cell></cell><cell>SOP</cell><cell></cell></row><row><cell>Baseline</cell><cell>K</cell><cell>P@1</cell><cell>RP</cell><cell>M@R P@1</cell><cell>RP</cell><cell>M@R P@1</cell><cell>RP</cell><cell>M@R</cell></row><row><cell></cell><cell>0</cell><cell>62.47</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Margin[44]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the sake of simplicity, we use a single subscript i ? [1, HW ] to index the spatial location. For pre-trained metric learning models, we can directly apply the original projection layer on the elements in the feature maps. Thus, our method does not need any modifications on the parameters.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61822603, Grant U1813218, and Grant U1713214, in part by a grant from the Beijing Academy of Artificial Intelligence (BAAI), and in part by a grant from the Institute for Guo Qiang, Tsinghua University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
		<title level="m">Gan dissection: Visualizing and understanding generative adversarial networks. ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Transformer interpretability beyond attention visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shir</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno>Margin[44] 1 62.47 34.12 23.14 72.18 32.00 20.82 78.39 45.64 42.34 2 64.15 34.79 23.83 75.04 32.59 21.85 79.06 46.29 43.03 4 65.16 35.48 24.54 76.65 33.93 22.95 79.26 46.44 43.19 7 65.58 35.58 24.79 76.96 32.93 22.66 79.59 46.83 43.62</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Multi-Similarity</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>42</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal coherence or temporal motion: Which is more critical for video-based person re-identification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning embeddings into entropic wasserstein spaces. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Frogner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzaneh</forename><surname>Mirzazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Dollar, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metric learning with horde: High-order regularizer for deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aymeric</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6539" to="6548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hyperbolic image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyla</forename><surname>Mirvakhabova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6418" to="6428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Proxy anchor loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep variational metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep face recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIBGRAPI</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention-aware deep reinforcement learning for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3931" to="3940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning discriminative aggregation network for video-based face recognition and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="701" to="718" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pads: Policyadapted sampling for visual similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6568" to="6577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting training strategies and generalization performance in deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Paul</forename><surname>Cohen</surname></persName>
		</author>
		<idno>PMLR, 2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Diagonal equivalence to matrices with prescribed row and column sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sinkhorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Mathematical Monthly</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="402" to="405" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Intelligent video surveillance: a review through deep learning techniques for crowd analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sreenu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma Saleem Durai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4170" to="4178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2593" to="2601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explainable face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jonathan R Williford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="248" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<idno>2020. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep randomized ensembles for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="723" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Signal-to-noise ratio: A robust distance metric for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Classification is a strong baseline for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">91</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">terpretable convolutional neural networks. In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Interpreting cnns via decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6261" to="6270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Emd metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hardness-aware deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaodong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
