<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Low-Light Image Enhancement via Histogram Equalization Prior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20221">AUGUST 2022 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Low-Light Image Enhancement via Histogram Equalization Prior</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20221">AUGUST 2022 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Low-Light Image Enhancement</term>
					<term>Unsupervised Learning</term>
					<term>Histogram Equalization Prior</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning-based methods for low-light image enhancement typically require enormous paired training data, which are impractical to capture in real-world scenarios. Recently, unsupervised approaches have been explored to eliminate the reliance on paired training data. However, they perform erratically in diverse real-world scenarios due to the absence of priors. To address this issue, we propose an unsupervised low-light image enhancement method based on an effective prior termed histogram equalization prior (HEP). Our work is inspired by the interesting observation that the feature maps of histogram equalization enhanced image and the ground truth are similar. Specifically, we formulate the HEP to provide abundant texture and luminance information. Embedded into a Light Up Module (LUM), it helps to decompose the low-light images into illumination and reflectance maps, and the reflectance maps can be regarded as restored images. However, the derivation based on Retinex theory reveals that the reflectance maps are contaminated by noise. We introduce a Noise Disentanglement Module (NDM) to disentangle the noise and content in the reflectance maps with the reliable aid of unpaired clean images. Guided by the histogram equalization prior and noise disentanglement, our method can recover finer details and is more capable to suppress noise in real-world low-light scenarios. Extensive experiments demonstrate that our method performs favorably against the state-of-the-art unsupervised low-light enhancement algorithms and even matches the state-of-the-art supervised algorithms. The code is available at https://github.com/fengzhang427/HEP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I MAGES captured under low-light conditions often suffer from poor visibility, unexpected noise, and color distortion. In order to take high-quality images in low-light conditions, several operations including setting long exposures, high ISO, and flash are commonly applied. However, solely turning up the brightness of dark regions will inevitably amplify image degradation. To further mitigate the degradation caused by low-light conditions, several traditional methods have been proposed. Histogram Equalization (HE) <ref type="bibr" target="#b0">[1]</ref> rearranges the pixels of the low-light image to improve the dynamic range of the image. Retinex-based methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> decompose the lowlight images into illumination and reflection maps and obtain the intensified image by fusing the enhanced reflection map and illumination map. Dehazing-based methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> regard <ref type="bibr">Feng</ref>  (a) Input (b) Zero-DCE <ref type="bibr" target="#b5">[6]</ref> (c) EnglightenGAN <ref type="bibr" target="#b6">[7]</ref> (d) Ours <ref type="figure">Fig. 1</ref>: Visual results of the proposed method compared with the state-of-the-art unsupervised low-light enhancement methods. The low-light image of (a) is from EnlightenGAN test set <ref type="bibr" target="#b6">[7]</ref>.</p><p>the inverted low-light image as a haze image and improve visibility by applying dehazing. Although these methods can improve brightness, especially for dark pixels, they barely consider realistic lighting factors, often making the enhanced results visually tenuous and inconsistent with the actual scene.</p><p>Recently, Deep Convolutional Neural Networks (CNNs) set the state-of-the-art in low-light image enhancement. Compared with traditional methods, the CNNs learn better feature representation to obtain enhanced results with superior visual quality, which benefits from the large dataset and powerful computational ability. However, most CNN-based methods require training examples with references, whereas it is extremely challenging to simultaneously capture low-light and normal-light images of the same visual scene. To eliminate the reliance on paired training data, several unsupervised deep learning-based methods <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> have been proposed. These algorithms are able to restore images with better illumination and contrast in some cases. However, most unsupervised methods heavily rely on carefully selected multi-exposure training data or unpaired training data, which makes these approaches not generalize well to handle various types of images. Therefore, it is of great interest to seek a novel strategy to deal with different scenarios in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2112.01766v1 [cs.CV] 3 Dec 2021</head><p>In this study, we propose an unsupervised low-light image enhancement algorithm based on an effective prior termed histogram equalization prior (HEP). Our work is motivated by an interesting observation on the pre-trained networks: the feature maps of histogram equalization enhanced image and the ground truth are similar. Intuitively, the feature maps of histogram equalization enhanced images can directly provide abundant texture and luminance information <ref type="bibr" target="#b8">[9]</ref>. We show theoretically and empirically that this generic property of the histogram equalization enhanced image holds for many low-light images; more details are shown in Section III. This inspires us to regularize the feature similarity between the histogram equalization enhanced images and the restored images.</p><p>Following <ref type="bibr" target="#b10">[10]</ref>, we split the low-light image enhancement process into two stages: image brightening and image denoising. The first stage decomposes the low-light images into illumination and reflectance maps, and the reflectance maps can be regarded as restored images. We formulate the histogram equalization prior to guiding the training process and add an illumination smoothness loss to suppress the texture and color information in the illumination map. However, according to the derivation based on Retinex theory <ref type="bibr" target="#b11">[11]</ref>, the reflectance maps are contaminated by noise. To improve the image quality, the second stage works as an enhancer to denoise the reflectance map. In this stage, we propose an unsupervised denoising model based on disentangled representation to remove the noise and generate the final enhanced image. The disentanglement is achieved by splitting the content and noise features in a reflectance map using content encoders and noise encoders. Inspired by <ref type="bibr" target="#b12">[12]</ref>, we add a KL divergence loss to regularize the distribution range of extracted noise features to suppress the contained content information. Moreover, we adopt the adversarial loss and the cycle-consistency loss as regularizers to assist the generator networks in yielding more realistic images and preserving the content of the original image. Extensive experiments demonstrate that our method performs favorably against the state-of-the-art unsupervised low-light enhancement algorithms and even matches the stateof-the-art supervised algorithms. <ref type="figure">Fig.1</ref> shows an example of enhancing the low-light image. In comparison to state-of-theart methods, our method delivers improved image brightness while preserving the details.</p><p>In summary, the main contributions of this work are as follows:</p><p>1. We propose an effective prior termed histogram equalization prior (HEP) for low-light image decomposition and add an illumination smoothness loss to suppress the texture and color information in the illumination map.</p><p>2. We introduce a noise disentanglement module to disentangle the noise and content in the reflectance maps with the reliable aid of unpaired clean images.</p><p>3. We build an unsupervised low-light image enhancement framework based on Retinex and disentangled representation, possessing more effective training and faster convergence speed. <ref type="bibr" target="#b3">4</ref>. We demonstrate that the proposed method achieves remarkable performance compared with the state-of-the-art unsupervised algorithms and even matches the state-of-the-art supervised algorithms.</p><p>The rest of this paper is organized as follows. Section II provides a brief review of some related works. Section III presents our proposed histogram equalization prior first, then introduces the decomposition network, finally, presents the proposed noise encoder. Section IV illustrated the experimental results. Section V provided the ablation studies on each component. Finally, concluding remarks are provided in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conventional Methods</head><p>The conventional methods for lowlight image enhancement can be roughly divided into three aspects: Gamma Correction (GC) <ref type="bibr" target="#b13">[13]</ref>, Histogram Equalization (HE) <ref type="bibr" target="#b0">[1]</ref>, and Retinex <ref type="bibr" target="#b14">[14]</ref>. Gamma correction edits the gamma curve of the image to perform nonlinear tone editing to detect the dark part and the light part in the image signal and increase the ratio of the two-part to improve the contrast. However, the global parameters lead to local over/under-exposure, and the value of the global parameter is very complicated to select. Rahman et al. <ref type="bibr" target="#b15">[15]</ref> proposed an adaptive gamma correction method that dynamically determines the intensity conversion function based on the statistical characteristics of the image.</p><p>Histogram Equalization stretches the image's dynamic range by evenly distributing the pixel values to improve the contrast and brightness of the image. However, it applies the adjustment globally, leads to unexpected local overexposure and amplifying the noise. Adaptive Histogram Equalization (AHE) <ref type="bibr" target="#b16">[16]</ref> has been proposed to map the histogram of the local region as a simple mathematical distribution. Pizer et al. <ref type="bibr" target="#b0">[1]</ref> proposed Contrast Limited Adaptive Histogram Equalization (CLAHE). This method sets a threshold and assumes that if a certain pixel value of the histogram exceeds the threshold, crop this pixel and evenly distribute the part that exceeds the threshold to each pixel.</p><p>Retinex theory is a calculation theory of color constancy. As a model of human visual perception, these methods decompose images into reflectance and illumination maps. MSR <ref type="bibr" target="#b17">[17]</ref> obtains enhanced results by fusing different single-scale Retinex outputs. MSRCR <ref type="bibr" target="#b17">[17]</ref> improves the color distortion problem of the previous methods. However, the Retinex methods lead to unreal or partially over-enhanced. Inspired by the Retinex theory, NPE <ref type="bibr" target="#b1">[2]</ref> was proposed for the enhancement of nonuniform illumination images. MF <ref type="bibr" target="#b18">[18]</ref> was proposed to apply multi-layer fusion to image enhancement under different light conditions. LIME <ref type="bibr" target="#b19">[19]</ref> evaluate the illumination map of the image and smooth the illumination map for enhancement. SRIE <ref type="bibr" target="#b20">[20]</ref> evaluate the illumination map and the reflectance map simultaneously through a weighted variational model.</p><p>Deep learning based Methods Deep learning-based methods have dominated the research of low-light image enhancement. Lore et al. <ref type="bibr" target="#b21">[21]</ref> proposed the first convolutional neural networks for low-light image enhancement termed LL-Net, perform contrast enhancement and denoising based on deep auto-encoder. Chen et al. <ref type="bibr" target="#b10">[10]</ref> proposed Retinex-Net, which includes a Decom-Net that splits the input images into reflectance and illumination maps, and an Enhance-Net that adjusts the illumination map for low-light enhancement. Zhang et al.proposed KinD <ref type="bibr" target="#b22">[22]</ref>, which is similar to Reinex-Net. It presented a new decomposition network, a reflection map enhancement network, and an illumination map enhancement network, which achieved outstanding performance in lowlight image enhancement. Zhang et al.proposed KinD++ <ref type="bibr" target="#b11">[11]</ref>, which improves the KinD method, and achieves state-of-theart performance. Guo et al. <ref type="bibr" target="#b5">[6]</ref> proposed a zero-shot learning method named Zero-DCE, which is achieved by an intuitive and straightforward nonlinear curve mapping. However, Zero-DCE heavily relies on the usage of multi-exposure training data. Zhang et al. <ref type="bibr" target="#b7">[8]</ref> proposed a self-supervised method that uses the max entropy loss for better image decomposition, but the restored image still suffers from noise contamination.</p><p>Image to Image Translation Generative Adversarial Net-work (GAN) is the most influential generative model in computer vision technology. Based on the powerful generative capabilities of GAN, image-to-image translation has become an important way to achieve image enhancement, which is achieved by converting corrupted images to sharp images. Zhu et al. <ref type="bibr" target="#b23">[23]</ref> proposed CylceGAN, which showed tremendous capacity in the field of the image domain transfer. Liu et al. <ref type="bibr" target="#b24">[24]</ref> proposed UNIT, which learned shared-latent representation for diverse image translation. Lee et al. <ref type="bibr" target="#b25">[25]</ref> proposed DRIT, which separated the latent space to content space and attribute space. The content space is shared, the attribute space is independent. Yuan et al. <ref type="bibr" target="#b26">[26]</ref> proposed a nested CycleGAN to achieve the unsupervised image super-resolution. Lu et al. <ref type="bibr" target="#b27">[27]</ref> extended DRIT and proposed to decompose the image into the image content domain and the noise domain to achieved unsupervised image deblurring. Based on Lu's work, Du et al. <ref type="bibr" target="#b28">[28]</ref> added Background Consistency Module and  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The main purpose of our method is to recover texture details, reduce noise and color bias, and maintain sharp edges for low-light image enhancement. As shown in <ref type="figure" target="#fig_0">Fig.2</ref>, the proposed method consists of two components: 1) Light Up Module (LUM); 2) Noise Disentanglement Module (LUM). The first stage is improving the brightness of the images, and the second stage is removing the noise of the images.</p><p>For low-light image enhancement, unsupervised learningbased methods are complicated to implement. The main reason is that texture and color information in low-light images is difficult to extract without the aid of paired ground truth data or prior information. Therefore, we investigate an effective prior information to guide the training process and maintain the texture and structure. In the following subsections, we first introduce the proposed histogram equalization prior in Section III-A. Then, we present the method to decompose the low-light images into reflectance maps and illumination maps in Section III-B. In Section III-C, we discuss the approach to disentangle the noise and content in reflectance maps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Histogram Equalization Prior</head><p>The histogram equalization prior is based on histogram equalization enhanced image. Traditional histogram equalization can make the dark images visible by stretching the dynamic range of dark images via manipulating the corresponding histogram. However, it is not flexible enough for visual property adjustment in local regions and leads to undesirable local appearances, e.g., under/over-exposure and amplified noise. Encouraging the pixels of output images to match the histogram equalization enhanced images will capture unpleasant local impressions contained in the enhanced image. Inspired by <ref type="bibr" target="#b29">[29]</ref>, we can adopt the VGG feature map to constrain the perceptual similarity between the low-light image and its histogram equalization enhanced version. As shown in <ref type="figure" target="#fig_1">Fig.3</ref>, we can observe that the feature map of the input lowlight image has less semantic information <ref type="bibr" target="#b30">[30]</ref>. In contrast, the feature map of histogram equalization enhanced image has rich semantic information, and it is remarkably similar to the feature map of ground truth.</p><p>To further verify the validity of the histogram equalization prior, we have selected 500 paired images from the LOL dataset <ref type="bibr" target="#b10">[10]</ref>. We calculate the cosine similarity between the feature maps of the histogram equalization enhanced image and the feature maps of the ground truth. <ref type="figure" target="#fig_2">Fig.4</ref> is the histogram of cosine similarities over all 500 low-light images. We can observe that about 80% of the cosine similarities are concentrated above 0.8. Compared with the cosine similarities between the feature maps of the input low-light images and the feature maps of the ground truth, the cosine similarities has been substantially improved. This statistic provides a strong support to our histogram equalization prior, and it indicates that we can adopt this prior instead of ground truth to guide the training process. <ref type="figure" target="#fig_3">Fig.5</ref> shows the different layer of VGG-19 <ref type="bibr" target="#b31">[31]</ref> networks pre-trained on ImageNet <ref type="bibr" target="#b32">[32]</ref> with a histogram equalization enhanced image. Feature maps closer to the input layer pay more attention to the specific details of texture information, and some feature maps can also show the shape of the toy's face. Feature maps farther away from the input layer are more concerned with semantic and abstract information, such as the toy's eye and nose characteristics. The feature maps of the deepest layers become more obscure and can no longer provide adequate information, while the features are similar between each group of feature maps. Based on this information, we select the feature map of conv4 1 layer as the feature similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Light Up</head><p>The first stage improves the brightness of the images based on the Retinex theory. According to the Retinex theory, the images can be decomposed into reflectance maps and illumination maps. Mathematically, a degraded low-light image can be naturally modeled as follows:</p><formula xml:id="formula_0">I = R ? L + N<label>(1)</label></formula><p>where I stands for input image, R stands for reflectance map, L is the illumination map, N represents the noise component,</p><p>? represents element-wise multiplication.</p><p>As the illumination map determines the dynamic range of images, it cannot be affected by noise. In contrast, the reflectance map represents the intrinsic properties of the images, which are often affected by noise during the imaging process. Hence, by taking simple algebra steps <ref type="bibr" target="#b11">[11]</ref>, we can have the following formula:</p><formula xml:id="formula_1">I = R ? L + N = R ? L +? ? L = (R +? ) ? L =R ? L (2)</formula><p>where? stands for the degradation having the illumination decoupled,R represents for the polluted reflectance map.</p><p>According to the above theory, the reflectance map can be regarded as a restored image with noise. Therefore, we design a neural network to decompose the low-light images into reflectance and illumination maps, and then send the reflectance maps to the NDM for further denoising. We follow the similar network architecture as the one used in <ref type="bibr" target="#b7">[8]</ref>, the module framework is shown in <ref type="figure" target="#fig_0">Fig.2(a)</ref>. It first uses a 9?9 convolutional layer to extract features from the input image. Secondly, three 3?3 convolutional+ReLU layers and one 3?3 deconvolutional+ReLU layer are followed. A residual feature from the conv2 layer concatenates with the feature from the deconv layer and feeds to a 3?3 convolutional+ReLU layer. The feature from this layer concatenates with the feature from a 3?3 convolutional+ReLU layer, which extracts features from the input image. Finally, two 3?3 convolutional layers project reflectance map and illumination map from feature space.</p><p>The sigmoid function constrains both reflectance map and illumination map in the range of [0,1].</p><p>Due to the lack of ground-truth data to guide the training process, it is tough to recover these two components from low-light images. We adopt the histogram equalization prior to constrain the reflectance map. We define an MSE loss between the feature map of the output reflectance map and the feature map of the input image, which we call the histogram equalization prior loss. The loss function can be formulated as follows:</p><formula xml:id="formula_2">L hep = F (R) ? F (I) 2 2<label>(3)</label></formula><p>where F (?) denotes the feature map extracted from a VGG-19 model pre-trained on ImageNet.</p><p>Since the network decomposes the image into an illumination map and a reflectance map, the decomposed two maps should reproduce the input image. We introduce reconstruction loss to ensure the quality of the generated image. The formula is as follows:</p><formula xml:id="formula_3">L recon = R ? L ? I 1<label>(4)</label></formula><p>As the reflectance map should preserve more texture and color details. In other words, the illumination map should be smooth in textural information while still preserving the structural boundaries. To make the illumination map aware of the image structure boundary, we modify the illumination smoothness loss proposed in <ref type="bibr" target="#b11">[11]</ref>. Different from the previous loss, our illumination smoothness loss only takes the low-light input image as the reference. This term constrains the relative structure of the illumination map to be consistent with the input image, which can reduce the risk of over-smoothing on the structure boundary. The illumination smoothness loss is formulated as:</p><formula xml:id="formula_4">L is = ?L max(| ?I |, ) 1<label>(5)</label></formula><p>where | ? | means the absolute value operator, is a small positive constant for avoiding zero denominators, ? denotes the gradient including ?h (horizontal) and ?v (vertical).</p><p>As a result, the loss function of the LUM is as follows:</p><formula xml:id="formula_5">L = L recon + ? rs L hep + ? is L is<label>(6)</label></formula><p>In our experiment, these parameters are set to ? hep = ? is = 0.1, = 0.01. Due to the careful settings of these loss terms, the light-up module can perform sufficiently well. Still, the light-up image is constrained by histogram equalization, as the method often causes noise and blur. Although the images generated by the network have been enhanced, however, compared with the normal-light images, the noise level cannot meet the visual quality. Therefore, they need to be further denoised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Noise Disentanglement</head><p>Although the content information of the low-light image appears after being decomposed into a reflectance map, the noise contained in it seriously interferes with the clarity of the image. Therefore, we adopt the domain transfer method to eliminate the noise and retain the content information.</p><p>As shown in <ref type="figure" target="#fig_0">Fig.2(b)</ref>. The noise disentanglement module consists of six parts: 1) content encoder E C X and E C Y (due to the shared parameter, we regard the content encoder of two domains are the same); 2) noise encoder E N ; 3) noise domain image generator G X ; 4) clean domain image generator G Y ; 5) noise domain image discriminator D X ; 6) clean domain image discriminator D Y . Given a training image sample I n from the noise domain and a training image sample I c from the clean domain, the content encoder E C X and E C Y extract the content feature from corresponding image samples, the noise encoder E N extract the noise feature from noise image samples. G X then takes the content feature of the clean domain and noise feature of the noise domain to generate a noise image I gn while G Y then takes the content feature of the noise domain to generate a clean image I gc . The discriminators D X and D Y distinguish between the real and generated examples.</p><p>Due to the unpaired setting, it is not trivial to disentangle the content information from a noise image. To restrain the noise encoder to only encode the noise information, we add the KL distance to constrain the distribution of noise feature extracted by the noise encoder, forcing the distribution of the noise feature to be closer to the standard normal distribution. The KL distance formula is as follows:</p><formula xml:id="formula_6">KL(q(z n ) p(z)) = q(z n ) log p(z) q(z n ) dz<label>(7)</label></formula><p>where q(z n ) stands for the distribution of the noise features z n , p(z) stands for the distribution of the standard normal distribution N (0, 1). As proved in <ref type="bibr" target="#b33">[33]</ref>, the KL divergence loss will suppress the content information contained in the noise feature z n , and minimizing the KL divergence is equivalent to minimizing the following loss function, which has been proved in <ref type="bibr" target="#b12">[12]</ref>.</p><formula xml:id="formula_7">L KL = 1 2 d i=1 (? log(? 2 i ) + ? 2 i + ? 2 u ? 1)<label>(8)</label></formula><p>where d is the dimension of noise feature, ? and ? are the mean and standard deviation of noise feature. In order to make the enhanced images look like realistic normal-light images, we adopt the adversarial loss to minimize the distance between the real image and output distributions. We modified the discriminator slightly to replace the loss function with the least-square GAN (LSGAN) loss. The adversarial loss function is as follows:</p><formula xml:id="formula_8">L adv = 1 2 E x?pr [(D(x)?b) 2 ]+ 1 2 E z?pz [(D(G(z))?a) 2 ] (9)</formula><p>where a is the label for the generated samples, b is the label for the real samples, and z is the latent vector. Without pairwise supervision, the denoised image may lose some content information. Similar to <ref type="bibr" target="#b23">[23]</ref>, we introduce the cycle-consistency loss to guarantee that the generated corrupted image I gn translates back to the original clean image domain, and the denoised image I gc reconstructed to the original corrupted sample. We define the cycle-consistency loss on both domains as:</p><formula xml:id="formula_9">L cc = I ?? 1<label>(10)</label></formula><p>where I is the input samples,? is the the backward translation of the input samples. In addition to the cycle-consistency loss, we introduce selfreconstruction loss to facilitate the better-perceived quality of the generated image. The formula of the loss function is as follows:</p><p>L rec = I rec ? I 1</p><p>Following the observations from <ref type="bibr" target="#b34">[34]</ref> that features extracted from the deep layer of pre-trained model contain rich semantic information, we add perceptual loss between the denoised images and the original corrupted images to recover finer image texture details. It could be formulated as:</p><formula xml:id="formula_11">L per = ? l (I g ) ? ? l (I) 2 2<label>(12)</label></formula><p>where ? l (?) represents the feature extracted from l-th layer of the pre-trained VGG network, I g is the generated samples. In our experiments, we use the conv3 2 layer of the VGG-19 pre-trained network on ImageNet.</p><p>To eliminate the potential color deviations in the denoised image, we adopt the color constancy loss proposed in <ref type="bibr" target="#b5">[6]</ref>, it follows the Gray-World color constancy hypothesis that color in each sensor channel averages to gray over the entire image. The loss function can be expressed as:</p><formula xml:id="formula_12">L col = ?(p,q)? (J p ? J q ) 2 , = {(R, G), (R, B), (G, B)}<label>(13)</label></formula><p>where J p represents the the average intensity value of p channel in the denoised image, (p, q) represents a pair of channels.</p><p>From our preliminary experiments, we find that the generated denoised samples often over-smooth in the background, then we adopt background consistency loss proposed by <ref type="bibr" target="#b28">[28]</ref>, which uses a multi-scale Gaussian-Blur operator to obtain multi-scale features respectively. The loss function is formulated as:</p><formula xml:id="formula_13">L bc = ?=i,j,k ? ? B ? (I) ? B ? (I g ) 1<label>(14)</label></formula><p>where ? ? is the hyper-parameter to balance the errors at different Gaussian-Blur levels, B ? (?) represents the Gaussian-Blur operator with blur kernel ?. In our experiments, we set ? ? = 0.25, 0.5, 1.0 for ? = 5, 9, 15 respectively. The entire loss function for the NDM is summarized as follows:</p><formula xml:id="formula_14">L = L adv + ? KL L KL + ? cc L cc + ? col L col + ? per L per + ? bc L bc + ? rec L rec<label>(15)</label></formula><p>We empirically set these parameters to ? KL = 0.01, ? per = 0.1, ? col = 0.5, ? bc = 5, ? cc = ? rec = 10. At test time, given a test corrupted sample, E N and E C X extract the noise and content features map respectively. Then G Y takes the latent vector and generates the denoised image as the outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL VALIDATION</head><p>In this section, we first introduce the implementation details of the proposed method for low-light image enhancement. Then we qualitatively and quantitatively compare the proposed (a) Input (b) HE <ref type="bibr" target="#b0">[1]</ref> (c) LIME <ref type="bibr" target="#b19">[19]</ref> (d) Retinex-Net <ref type="bibr" target="#b10">[10]</ref> (e) KinD++ <ref type="bibr" target="#b11">[11]</ref> (f) Zero-DCE <ref type="bibr" target="#b5">[6]</ref> (g) EnlightenGAN <ref type="bibr" target="#b6">[7]</ref> (h) Self-Supervised <ref type="bibr" target="#b7">[8]</ref> (i) Ours (j) Ground-Truth <ref type="figure">Fig. 6</ref>: Visual comparison with other state-of-the-art methods on LOL dataset <ref type="bibr" target="#b10">[10]</ref>. Best viewed in color and by zooming in.</p><p>method with the state-of-the-art methods (include supervised and unsupervised methods), we use traditional metrics to evaluate, such as Peak-Signal-Noise-Rate (PSNR), Structural Similarity (SSIM) <ref type="bibr" target="#b35">[35]</ref>, and Natural Image Quality Evaluator (NIQE) <ref type="bibr" target="#b36">[36]</ref>. Furthermore, we test the proposed method on some real-world datasets while comparing them with the stateof-the-art methods in terms of visual performance and NIQE metrics. Finally, we conduct ablation studies to demonstrate the effectiveness of each component or loss in the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Since the proposed method is a two-stage model, we need to train the model separately. In the first stage, our training dataset is selected from the low-light part of the LOL dataset <ref type="bibr" target="#b10">[10]</ref>, which includes 500 low/normal-light image pairs. During the training, we use Adam <ref type="bibr" target="#b37">[37]</ref> optimizer to perform optimization with the weight decay equal to 0.0001. The initial learning rate is set to 10 ?4 , which decreases to 10 ?5 after 20 epochs and then to 10 ?6 after 40 epochs. The batch size is set to 16 and the patch size to 48?48. In the second stage, we assemble a mixture of 481 low-light images from the LOL dataset and 481 normal-light images from the EnlightenGAN dataset <ref type="bibr" target="#b6">[7]</ref>. The Adam method is adopted to optimize the parameters with the momentum equal to 0.9 and the weight decay equal to 0.0001. The learning rate is initially set to 10 ?4 and exponential decay over the 10K iterators. The batch size is set to 16 and the patch size to 64?64. All experiments are conducted using PyTorch <ref type="bibr" target="#b38">[38]</ref> framework on an Nvidia 2080Ti GTX GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Evaluation</head><p>We first visually evaluate our proposed networks on the classical low-light image datasets: LOL datasets, and compare it with other state-of-the-art approaches with available codes, including HE <ref type="bibr" target="#b0">[1]</ref>, LIME <ref type="bibr" target="#b19">[19]</ref>, Retinex-Net <ref type="bibr" target="#b10">[10]</ref>, KinD++ <ref type="bibr" target="#b11">[11]</ref>, Zero-DCE <ref type="bibr" target="#b5">[6]</ref>, EnlightenGAN <ref type="bibr" target="#b6">[7]</ref>, and Self-Supervised <ref type="bibr" target="#b7">[8]</ref>.</p><p>We have fine-tuned all models on the LOL train set and then evaluated it on the LOL test set. <ref type="figure">Fig.6</ref> shows some representative results for visual comparison. The enhanced results show that the EnlightenGAN and Zero-DCE fail to recover the images. HE significantly improves the brightness of the low-light image. However, it applies a contrast pullup to each channel of RGB separately, which leads to color (a) Input (b) HE <ref type="bibr" target="#b0">[1]</ref> (c) LIME <ref type="bibr" target="#b19">[19]</ref> (d) Retinex-Net <ref type="bibr" target="#b10">[10]</ref> (e) KinD++ <ref type="bibr" target="#b11">[11]</ref> (f) Zero-DCE <ref type="bibr" target="#b5">[6]</ref> (g) EnlightenGAN <ref type="bibr" target="#b6">[7]</ref> (h) Self-Supervised <ref type="bibr" target="#b7">[8]</ref> (i) Ours <ref type="figure">Fig. 7</ref>: Visual comparison with state-of-the-art methods on the SCIE dataset <ref type="bibr" target="#b39">[39]</ref>. Best viewed in color and by zooming in.</p><p>distortion (for example, the wall in <ref type="figure">Fig.6(b)</ref>). LIME enhances the images by directly estimating the illumination map, but this approach enhances both details and noise. Retinex-Net notably improves the visual quality of the low-light images, but it oversmoothes details, amplifies noise, and even produces color bias. It seems that the results of Self-Supervised, KinD++, and ours have better visual quality among all the methods. To further investigate the differences between these three methods, we have zoomed in the details inside the red and green bounding boxes. We can find from <ref type="figure">Fig.6</ref>(h) that Self-Supervised produces blurred results for the rotation switch in the red rectangle, while the results of KinD++ and ours show a better reconstruction. For the platform area in the green rectangle, we can see that the image estimated by Self-Supervised is corrupted, while KinD++ and our results are clearer. In summary, the best visual quality can be obtained with our proposed method and KinD++. Considering that KinD++ is a supervised method, this shows that our proposed unsupervised method is very effective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative Evaluation</head><p>We have also quantitatively compared our method to the other state-of-the-art methods. We have fine-tuned all models on the LOL train set and then evaluated it on the LOL test set. As shown in <ref type="table" target="#tab_1">Table I</ref>, the proposed method achieves the best performance with an average PSNR score of 20.23 dB, SSIM score of 0.79, and NIEQ score of 3.78 in unsupervised methods, which exceed the second-best unsupervised method (Self-Supervised) by 1.1 dB on PSNR, 0.139 on SSIM, and 0.922 on NIQE. It demonstrates that the proposed method possesses the highest capability among all unsupervised methods and its performance is approximating the level of the state-ofthe-art supervised methods. Recently, NIQE has been used to evaluate the image quality of low-light image enhancement, which evaluating real image restoration without ground truth. A smaller NIQE score indicates better visual quality. We can see from <ref type="table" target="#tab_1">Table I</ref> that our method obtains the best NIQE scores in all unsupervised methods and even surpasses the state-ofthe-art supervised method KinD++. It indicates that the lowlight images enhanced with our method have the best visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Generalization Ability on Real-World Images</head><p>To further demonstrate the generalization ability of the proposed method, we have tested the proposed method on some real-world low-light image sets, including MEF <ref type="bibr" target="#b40">[40]</ref>(17 images), LIME <ref type="bibr" target="#b19">[19]</ref>(10 images), NPE <ref type="bibr" target="#b1">[2]</ref>(84 images), VV 1 (24 images), DICM <ref type="bibr" target="#b41">[41]</ref>(64 images), EnlightenGAN <ref type="bibr" target="#b6">[7]</ref>(148 images), SCIE <ref type="bibr" target="#b39">[39]</ref>(select 100 low-light images from the datasets). Furthermore, in order to showcase this unique advantage of our method in practice, we also conduct experiments using low-light images from other datasets, which are built for object detection and recognition. We selected 216 lowlight images from ExDark <ref type="bibr" target="#b42">[42]</ref> and 100 nighttime images from COCO <ref type="bibr" target="#b43">[43]</ref>. We have fine-tuned all models on the EnlightenGAN train set 2 and then evaluated it on all the lowlight image sets. As all these datasets are unpaired, we employ the NIQE metric to provide quantitative comparisons with the state-of-the-art methods, which are used for evaluating real image restoration without ground truth. The NIQE results on nine publicly available image sets used by previous works are reported in <ref type="table" target="#tab_1">Table II</ref>. Our method achieved the best performance in eight of these nine datasets and achieved the first place in the average score. <ref type="figure">Fig.7</ref> shows the results of a challenging image on the SCIE dataset. From the results, we can observe that our proposed method and KinD++ enhance dark regions and simultaneously preserve the color. The result is visually pleasing without obvious noise and color casts. In contrast, HE, LIME, and EnlightenGAN generate visually good results, but it contains some undesired artifacts (e.g., the white wall). Zero-DCE fails to recover the image. Retinex-Net and Self-Supervised over-smooth the details, amplify noise, and even produce color deviation. Our proposed method and KinD++ enhance dark regions and preserve the color of the input image simultaneously. The result is visually pleasing without obvious noise and color casts. It demonstrates that our method has great generalization ability in real-world images with more naturalistic quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ABLATION STUDY</head><p>To demonstrate the effectiveness of each component proposed in Section III, we conduct several ablation experiments. We primarily analyze the components in our Light Up Module (LUM), which are the core contribution and play critical roles in this work.</p><p>A. Contribution of Light Up 1) Effect of Histogram Equalization Prior: Since histogram equalization prior is the main contribution in our work, a comparative assessment of its validity has been carried out. We employ the histogram equalization enhanced image instead as the reference image. We have evaluated different loss functions with the histogram equalization enhanced image: L1 loss L L1 , MSE loss L M SE , and SSIM loss L SSIM , and max information entropy loss L max <ref type="bibr" target="#b7">[8]</ref>. The formulas of these losses are as follows:</p><formula xml:id="formula_15">L L1 = R ? H(I) 1<label>(16)</label></formula><formula xml:id="formula_16">L M SE = R ? H(I) 2 2<label>(17)</label></formula><formula xml:id="formula_17">L SSIM = 1 ? SSIM (R, H(I))<label>(18)</label></formula><formula xml:id="formula_18">L max = max c?R,G,B (R c ) ? H( max c?R,G,B (I c )) 1<label>(19)</label></formula><p>where H(?) stands for histogram equalization operation, R represents the relfectance map, I denotes the input low-light image, R c represents the max channel of relfectance map, I c represents the max channel of input low-light image.</p><p>The comparison results are shown in <ref type="table" target="#tab_1">Table III</ref>. Using L L1 or L M SE achieves similar SSIM and NIQE scores. Nevertheless, for the PSNR values, the estimation from L M SE exceeds those from L L1 with 0.33dB. The L SSIM improves the NIQE score   by a large margin. L SSIM surpasses HEP in NIQE score, but HEP outperformed by 1.58dB in PSNR and 0.047 in SSIM. L max achieves similar SSIM scores with HEP, but it failed in NIQE and PSNR by a large margin. <ref type="figure" target="#fig_4">Fig.8</ref> shows a visual comparison of these loss functions. L L1 and L M SE significantly improves the brightness of the low-light images. However, they have obvious color deviations (e.g., the color of the floor) and undesired artifacts (e.g., the dark region of the wall). L SSIM reveals the color and texture, but with the blurry mask. L max has color distortion. Both quantitative and qualitative results demonstrate the effectiveness of the proposed prior.</p><p>2) Effect of Loss functions: We present the results of LUM trained by various combinations of losses in <ref type="figure" target="#fig_5">Fig. 9</ref>. Removing the reconstruction loss L recon fails to brighten the image, and this shows the importance of reconstruction loss in enhancing the quality of the generated image. The results with illumination smoothness loss L is haves relatively lower contrast than the full results, and it shows smooth illumination map can somehow brighten the reflectance map. Finally, removing the histogram equalization prior loss L hep hampers the correlations between neighboring regions leading to obvious artifacts. To further demonstrate the effectiveness of  <ref type="table" target="#tab_1">Table IV</ref>. The results show that without the histogram equalization prior loss, the PSNR decrease from 19.52 to 9.0, the SSIM decrease from 0.701 to 0.54. It demonstrates the importance of perceptual loss. To better prove the role of perceptual loss, we conduct an ablation study on this prior in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contribution of Noise Disentanglement</head><p>1) Effect of Network Architecture: In this part, we compare three different denoise manners, including a traditional denoising tool BM3D <ref type="bibr" target="#b44">[44]</ref>, a GAN-based denoise method <ref type="bibr" target="#b28">[28]</ref>, which has a similar architecture to ours, and our proposed NDM. <ref type="figure" target="#fig_6">Fig.10</ref> shows the comparison results of these three methods. The BM3D and the GAN-based method are the stateof-the-art denoising methods. However, the results show that the BM3D can handle noise, but it blurs the image. The GANbased method is visually similar to our proposed NDM, but the image is overexposed compared to the ground truth. The result of our proposed NDM contains more delicate details and more vivid colors than other methods. As the quantitative results are shown in <ref type="table" target="#tab_5">Table V</ref>, the NDM improves the GANbased denoise method by a large margin in terms of PSNR   and outperforms the BM3D by about 0.66dB in PSNR, 0.014 in SSIM, and 2.497 in NIQE. The new design of the NDM proves its effectiveness by the best results in this comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Effect of Loss functions:</head><p>We evaluate the loss functions of the NDM and the evaluation results are shown in table VI. From the results, we can conclude that removing self-reconstruction loss L recon can significantly reduce PSNR and NIQE scores. Without the KL divergence loss L KL , background consistency loss L bc , and perceptual loss L per , all metrics have dropped a lot. Removing the adversarial loss L adv cause SSIM and NIQE scores to drop a lot. Finally, when removing the cycle-consistency loss L cc , the NIQE scores have risen by 0.028, but at the same time, PSNR and SSIM have dropped by 0.32dB and 0.01. The entire loss function of NDM is designed to transfer noise image to clean image, and it performs stronger noise suppression on regions where the brightness is significantly promoted after the image brightness enhancement guided by the histogram equalization prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we propose an unsupervised network for low-light image enhancement. Inspired by Retinex theory, we design a two-stage network to enhance the low-light image. The first stage is an image decomposition network termed light up module (LUM), and the second stage is an image denoising network termed noise disentanglement module (NDM). The LUM brightens the image by decomposing the images into reflectance and illumination maps. In the absence of ground truth, we introduce an effective prior termed histogram equalization prior to guiding the training process, which is an extension of histogram equalization that investigates the spatial correlation between feature maps. Benefiting from the abundant information of the histogram equalization prior, the reflectance maps generated by LUM simultaneously improve brightness and preserve texture and color information. The NDM further denoises the reflectance maps to obtain the final images while preserving more natural color and texture details. Both qualitative and quantitative experiments demonstrate the advantages of our model over state-of-the-art algorithms.</p><p>In the future work, we intend to explore more effective prior for low-light image enhancement and investigate some GAN-based methods for low-light and normal-light image transfer. Due to the limited application value of low light enhancement, we also expect to integrate enhancement algorithms with some high-level tasks, such as object detection and semantic segmentation, which can be used for autonomous driving to provide reliable visual aids for dark and challenging environments. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the framework. The proposed method consists of two stages: (a) light-up and (b) noise disentanglement. The light-up module first decomposes the low-light image into an illumination map and reflectance map. Then the noise disentanglement module denoises the reflectance map to yield the final enhanced image. In (a), the bright channel is a 1channel image, which is obtained by calculating the maximum channel value of the input RGB image. Then the bright channel and the input image are concatenated together to form a 4-channel image as the input of the network. In (b), blue arrows represent the data flow of the noise domain, orange arrows represent the data flow of the clean domain. E N is the noise encoder for noise images; E C Y and E C X are the content encoders for noise and clean images; G X and G Y are noise image and clean image generators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Feature maps on the conv4 1 layer of VGG-19 networks pre-trained on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Histogram of the cosine similarities. Green: cosine similarities between the feature maps of the input low-light images and the ground truth. Blue: cosine similarities between the histogram equalization prior and the ground truth. Semantic Consistency Module to the networks, learning robust representation under dual-domain constraints, such as feature and image domains. Jiang et al. [7] proposed a backbone model EnlightenGAN for low-light image enhancement based on adversarial learning. However, EnlightenGAN relies on large number of parameters for good performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Feature maps on different layers of VGG-19 networks pre-trained on ImageNet with a histogram equalization enhanced image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>with L L1 (c) with L M SE (d) with L SSIM (e)with Lmax (f) with HEP Ablation study of the contribution of histogram equalization prior in LUM (replace reflectance similarity loss L rs with L1 loss L L1 , MSE loss L M SE , SSIM loss L SSIM , and max information entropy loss L max ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>(a) Input (b) w/o Lrecon (c) w/o L is (d) w/o L hep (e)full loss Ablation study of the contribution of loss functions in LUM (reconstruction loss L recon , illumination smoothness loss L is ), histogram equalization prior loss L hep ). Red rectangle indicate the obvious differences and amplified details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Ablation study of the contribution of noise encoder in NDM (compare with BM3D and a GAN-based denoise mode).Red rectangle indicate the obvious differences and amplified details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Feng</head><label></label><figDesc>Zhang is currently a Ph.D. candidate in the School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, supervised by Prof. Nong Sang. He received his M.S. degree from the School of Materials Science and Engineering, Huazhong University of Science and Technology in 2015. His research interests include computer vision and deep learning. Now he mainly works on the area of low-light image enhancement. Yuanjie Shao received the B.S. and M.S degree in college of mechanical and electronic information, China University of Geosciences in 2010 and 2013, Wuhan, China, and PhD degree in Control science and Engineering from Huazhong University of Science and Technology in 2018. He is currently a postdoctoral with the School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China. His research interests include pattern recognition, computer vision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Zhang, Yuanjie Shao, Yishi Sun, Kai Zhu, Changxin Gao, and Nong Sang are with the National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan 430074, China (e-mail: fengzhangaia@hust.edu.cn; cgao@hust.edu.cn; nsang@hust.edu.cn).</figDesc><table /><note>Correspondence addressed to: Nong Sang (nsang@hust.edu.cn) Manuscript received April 9, 2021; revised August 26, 2021.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Quantitative comparisons on the LOL test set in terms of PSNR, SSIM, and NIQE. The best result is in red, whereas the second-best results are in blue, respectively. T, SL, and UL represent the traditional method, supervised learning method, and unsupervised learning method, respectively.</figDesc><table><row><cell>Learning</cell><cell>Method</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>NIQE?</cell></row><row><cell></cell><cell>Input</cell><cell>7.77</cell><cell>0.191</cell><cell>6.749</cell></row><row><cell>T</cell><cell>HE [1] LIME [19]</cell><cell>14.95 17.18</cell><cell>0.409 0.484</cell><cell>8.427 8.221</cell></row><row><cell>SL</cell><cell>Retinex-Net [10] KinD++ [11]</cell><cell>16.77 21.32</cell><cell>0.425 0.829</cell><cell>8.879 5.120</cell></row><row><cell></cell><cell>Zero-DCE [6]</cell><cell>14.86</cell><cell>0.562</cell><cell>7.767</cell></row><row><cell>UL</cell><cell>EnlightenGAN [7] Self-Supervised [8]</cell><cell>17.48 19.13</cell><cell>0.652 0.651</cell><cell>4.684 4.702</cell></row><row><cell></cell><cell>Ours</cell><cell>20.23</cell><cell>0.790</cell><cell>3.780</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>NIQE scores on low-light image sets(MEF, LIME, NPE, VV, DICM, SCIE, ExDark, EnlightenGAN, COCO). The best result is in red whereas the second best results are in blue, respectively. Smaller NIQE scores indicate a better quality of perceptual tendency.</figDesc><table><row><cell>Learning</cell><cell>Method</cell><cell>MEF</cell><cell>LIME</cell><cell>NPE</cell><cell>VV</cell><cell>DICM</cell><cell>EnlightenGAN</cell><cell>SCIE</cell><cell>ExDark</cell><cell>COCO</cell><cell>Avg</cell></row><row><cell>T</cell><cell>HE [1] LIME [19]</cell><cell>3.472 3.56</cell><cell>4.125 4.138</cell><cell>4.289 4.194</cell><cell>3.202 2.456</cell><cell>3.643 3.818</cell><cell>6.993 6.956</cell><cell>3.373 3.222</cell><cell>4.135 4.759</cell><cell>4.206 4.24</cell><cell>4.530 4.667</cell></row><row><cell>SL</cell><cell>Retinex-Net [10] KinD++ [11]</cell><cell>4.386 3.734</cell><cell>4.68 4.81</cell><cell>4.567 4.381</cell><cell>2.461 2.352</cell><cell>4.451 3.787</cell><cell>8.063 4.572</cell><cell>3.705 3.143</cell><cell>5.274 4.074</cell><cell>4.89 3.896</cell><cell>5.296 3.926</cell></row><row><cell></cell><cell>Zero-DCE [6]</cell><cell>3.283</cell><cell>3.782</cell><cell>4.273</cell><cell>3.217</cell><cell>3.56</cell><cell>6.582</cell><cell>3.284</cell><cell>4.149</cell><cell>3.903</cell><cell>4.386</cell></row><row><cell>UL</cell><cell>EnlightenGAN [7] Self-Supervised [8]</cell><cell>3.221 4.477</cell><cell>3.678 4.966</cell><cell>4.125 4.743</cell><cell>2.251 3.364</cell><cell>3.546 4.588</cell><cell>4.609 4.872</cell><cell>2.939 3.978</cell><cell>4.357 5.176</cell><cell>3.953 4.947</cell><cell>3.973 4.758</cell></row><row><cell></cell><cell>Ours</cell><cell>3.188</cell><cell>3.484</cell><cell>3.504</cell><cell>2.336</cell><cell>3.425</cell><cell>3.711</cell><cell>2.864</cell><cell>3.422</cell><cell>3.037</cell><cell>3.325</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Ablation study of the contribution of histogram equalization prior in LUM in terms of PSNR, SSIM and NIQE.</figDesc><table><row><cell>Loss Function</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>NIQE?</cell></row><row><cell>Input</cell><cell>7.77</cell><cell>0.191</cell><cell>6.749</cell></row><row><cell>with L L1</cell><cell>17.51</cell><cell>0.687</cell><cell>6.343</cell></row><row><cell>with L M SE</cell><cell>17.84</cell><cell>0.698</cell><cell>6.649</cell></row><row><cell>with L SSIM</cell><cell>17.94</cell><cell>0.654</cell><cell>4.869</cell></row><row><cell>with Lmax</cell><cell>18.29</cell><cell>0.690</cell><cell>7.294</cell></row><row><cell>with HEP</cell><cell>19.52</cell><cell>0.701</cell><cell>5.480</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Ablation study of the contribution of loss functions in LUM in terms of PSNR, SSIM and NIQE.</figDesc><table><row><cell>Loss Function</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>NIQE?</cell></row><row><cell>Input</cell><cell>7.77</cell><cell>0.191</cell><cell>6.749</cell></row><row><cell>w/o L hep</cell><cell>9.00</cell><cell>0.540</cell><cell>4.539</cell></row><row><cell>w/o Lrecon</cell><cell>17.06</cell><cell>0.675</cell><cell>6.782</cell></row><row><cell>w/o L is</cell><cell>17.93</cell><cell>0.621</cell><cell>6.350</cell></row><row><cell>full loss</cell><cell>19.52</cell><cell>0.701</cell><cell>5.480</cell></row><row><cell cols="4">each loss, we conduct several experiments on the LOL dataset.</cell></row><row><cell cols="4">The evaluation results of each loss show in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Ablation study of the contribution of noise encoder in NDM in terms of PSNR, SSIM, and NIQE.</figDesc><table><row><cell>Denoise Model</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>NIQE?</cell></row><row><cell>LUM</cell><cell>19.52</cell><cell>0.701</cell><cell>5.480</cell></row><row><cell>LUM + BM3D [44]</cell><cell>19.57</cell><cell>0.776</cell><cell>6.277</cell></row><row><cell>LUM + Du et al. [28]</cell><cell>18.74</cell><cell>0.791</cell><cell>4.539</cell></row><row><cell>LUM + NDM</cell><cell>20.23</cell><cell>0.790</cell><cell>3.780</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Ablation study of the contribution of loss functions in NDM in terms of PSNR, SSIM, and NIQE.</figDesc><table><row><cell>Loss Function</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>NIQE?</cell></row><row><cell>w/o L adv</cell><cell>19.66</cell><cell>0.705</cell><cell>5.299</cell></row><row><cell>w/o L KL</cell><cell>19.68</cell><cell>0.778</cell><cell>4.394</cell></row><row><cell>w/o Lper</cell><cell>19.83</cell><cell>0.781</cell><cell>4.389</cell></row><row><cell>w/o Lcc</cell><cell>19.91</cell><cell>0.780</cell><cell>3.752</cell></row><row><cell>w/o L bc</cell><cell>19.92</cell><cell>0.785</cell><cell>4.143</cell></row><row><cell>w/o Lrecon</cell><cell>19.96</cell><cell>0.783</cell><cell>4.234</cell></row><row><cell>full loss</cell><cell>20.23</cell><cell>0.790</cell><cell>3.780</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/site/vonikakis/datasets 2 Since EnlightenGAN dataset is unpaired and cannot be used as the train set for supervised method, so we use LOL dataset as the train set for the supervised method</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contrast-limited adaptive histogram equalization: Speed and effectiveness stephen m. pizer, r. eugene johnston, james p. ericksen, bonnie c. yankaskas, keith e. muller medical image display research group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Visualization in Biomedical Computing</title>
		<meeting>the First Conference on Visualization in Biomedical Computing<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">337</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Naturalness preserved enhancement algorithm for non-uniform illumination images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3538" to="3548" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational bayesian method for retinex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3381" to="3396" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast efficient algorithm for enhancement of low lighting video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A low-light image enhancement method for both denoising and contrast enlarging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zeroreference deep curve estimation for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1780" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enlightengan: Deep light enhancement without paired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2340" to="2349" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Self-supervised image enhancement network: Training with low light images only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11300</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Imagenet-trained cnns are biased towards texture</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep retinex decomposition for low-light enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference. British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond brightening low-light images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1013" to="1037" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards open-set identity preserving face synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6713" to="6722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Blind inverse gamma correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1428" to="1433" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lightness and retinex theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josa</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An adaptive gamma correction for image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdullah-Al-Wadud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Al-Quaderi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shoyaib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive histogram equalization and its variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Amburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cromartie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geselowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision, graphics, and image processing</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A multiscale retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A fusion-based enhancing method for weakly illuminated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="82" to="96" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lime: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="982" to="993" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A weighted variational model for simultaneous reflectance and illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2782" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Llnet: A deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Lore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akintayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="650" to="662" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kindling the darkness: A practical low-light image enhancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on multimedia</title>
		<meeting>the 27th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1632" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised domain-specific deblurring via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="225" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning invariant representation for unsupervised image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking and improving the robustness of image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Workshop on Autodiff</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a deep single image contrast enhancer from multi-exposure images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2049" to="2062" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Power-constrained contrast enhancement for emissive displays based on histogram equalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="93" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation of 2d histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5372" to="5384" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Getting to know low-light images with the exclusively dark dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
