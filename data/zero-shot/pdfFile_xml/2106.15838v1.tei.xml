<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliang</forename><surname>Ren</surname></persName>
							<email>liliang3@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenkai</forename><surname>Sun</surname></persName>
							<email>chenkai5@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<email>hengji@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
							<email>juliahmr@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-to-Graph extraction aims to automatically extract information graphs consisting of mentions and types from natural language texts. Existing approaches, such as table filling and pairwise scoring, have shown impressive performance on various information extraction tasks, but they are difficult to scale to datasets with longer input texts because of their secondorder space/time complexities with respect to the input length. In this work, we propose a Hybrid SPan GenerAtor (HySPA) that invertibly maps the information graph to an alternating sequence of nodes and edge types, and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities. Extensive experiments on the ACE05 dataset show that our approach also significantly outperforms state-ofthe-art on the joint entity and relation extraction task. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information Extraction (IE) can be viewed as a Text-to-Graph extraction task that aims to extract an information graph <ref type="bibr" target="#b28">Shi et al., 2017)</ref> consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) <ref type="bibr" target="#b7">(Florian et al., 2006</ref><ref type="bibr" target="#b8">(Florian et al., , 2010</ref> and Relation Extraction (RE) <ref type="bibr" target="#b30">(Sun et al., 2011;</ref><ref type="bibr" target="#b13">Jiang and Zhai, 2007)</ref>, and either perform them separately <ref type="bibr" target="#b2">(Chan and Roth, 2011)</ref> or jointly <ref type="bibr" target="#b6">Eberts and Ulges, 2019)</ref>.</p><p>Recent joint IE models <ref type="bibr" target="#b34">Wang and Lu, 2020;</ref><ref type="bibr" target="#b19">Lin et al., 2020)</ref> have shown impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this approach is computationally inefficient because it needs to enumerate all possible entity pairs in a document, and the relation type is a null value for most of the cases due to the sparsity of relations between entities. Also, pairwise scoring techniques evaluate each relation type independently and thus fail to capture interrelations between relation types for different pairs of mentions.</p><p>Another approach is to treat the joint information extraction task as a table filling problem <ref type="bibr" target="#b34">Wang and Lu, 2020)</ref>, and generate twodimensional tables with a Multi-Dimensional Recurrent Neural Network <ref type="bibr" target="#b9">(Graves et al., 2007)</ref>. This can capture interrelations among entities and relations, but the space complexity grows quadratically with respect to the length of the input text, making this approach impractical for long sequences.</p><p>Some attempts, such as Seq2RDF  and IMoJIE <ref type="bibr" target="#b15">(Kolluru et al., 2020)</ref>, leverage the power of Seq2seq models <ref type="bibr" target="#b3">(Cho et al., 2014)</ref> to capture the interrelations among mentions and types with first-order complexity, but they all use a pre-defined vocabulary for mention prediction, which largely depends on the distribution of the target words and will not be able to handle unseen out-of-vocabulary words.</p><p>To solve these problems, we propose a first-order approach that invertibly maps the target graph to an alternating sequence of nodes and edges, and applies a hybrid span generator that directly learns to generate such alternating sequences. Our main contributions are three-fold:</p><p>? We propose a general technique to invertibly map between an information graph and an alternating sequence (assuming a given graph traversal algorithm). Generating an alternating sequence is equivalent to generating the original information graph.</p><p>? We propose a novel neural decoder that is enforced to only generate alternating sequences by decoding spans and types in a hybrid manner. For each decoding step, our decoder only has linear space and time complexity with respect to the length of the input sequence, and it can capture inter-dependencies among mentions and types due to its nature as a sequential decision process.</p><p>? We conduct extensive experiments on the Automatic Content Extraction (ACE) dataset which show that our model achieves state-ofthe-art performance on the joint entity and relation extraction task which aims to extract a knowledge graph from a piece of unstructured text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modeling Information Graphs as Alternating Sequences</head><p>An information graph can be viewed as a heterogeneous multigraph <ref type="bibr" target="#b28">Shi et al., 2017)</ref>  Representing information graphs as sequences Instead of directly modeling the space of heterogeneous multigraphs, G, we build a mapping s ? = f s (G, ?) from G, to a sequence space S ? . f s depends on a (given) ordering ? of nodes and their edges in G, constructed by a graph traversal algorithm like Breadth First Search (BFS) or Depth First Search (DFS), and an internal ordering of nodes and edge types. We assume that the elements s ? i of the resultant sequences s ? are drawn from finite sets of node representations V (defined below), node types Q, edge types R (incl. [TYPE]), and "virtual" edge types U :</p><formula xml:id="formula_0">G = (V, E), where V is</formula><formula xml:id="formula_1">? s ? i ? s ? , s ? i ? V ? Q ? R ? U . Virtual edge types U = {[SOS], [EOS],</formula><p>[SEP]} do not represent edges in G, but serve to control the generation of the sequence, indicating the start/end of sequences and the separation of levels in the graph.</p><p>We furthermore assume that s ? = s ? 0 , ..., s ? n that represent graphs have an alternating structure, where s ? 0 , s ? 2 , s ? 4 , ... represent nodes V , and s ? 1 , s ? 3 , ... represent actual or virtual edges. In the case of BFS, we exploit the fact that it visits nodes level by level, i.e., in the order p i , c i1 , ..., c ik , p j (where c ik is the k-th child of parent p i , connected by edge e ik , and p j may or may not be equal to one of the children of p i ), which we turn into a sequence,</p><formula xml:id="formula_2">s ? = p i ,?(e i1 ), c i1 , ..., ?(e ik ), c ik , [SEP], p j , ...</formula><p>where we use the special edge type [SEP] to delineate the levels in the graph. This representation allows us to unambiguously recover the original graph, if we know which type of graph traversal is assumed (BFS or DFS). <ref type="bibr">3</ref> Algorithm 1 (which we use to translate graphs in the training data to sequences) shows how an alternating sequence for a given graph can be constructed with BFS traversal. <ref type="figure" target="#fig_0">Figure 1</ref> shows the alternating sequence for an information multigraph. The length |s ? | is bounded linearly by the size of the graph O(|s ? |) = O(|V | + |E|) (which is also the complexity of typical graph traversal algorithms like BFS/DFS).</p><p>Algorithm 1 Alternating sequence construction algorithm with BFS Input :Ordered adjacency dictionary of an information graph G, positions of nodes in the input text p q , frequency of edge types in the training set p r Output :An alternating sequence y ? Sort the nodes in G according to p q For each node v in G, sort the neighbors and the edges of v according to p q and p r respectively Instantiate y ? as an empty list for u in G do if u is not visited then Initialize an empty queue q Mark u as visited and enqueue u to q while q is not empty do Dequeue a node w from q if w in G then Append w and all the neighbors of w with their edge types to y ? Append the separation edge type, [SEP], to y ? Mark all unvisited neighbors of w as visited and enqueue them to q end end end end Return y ? Node and Edge Representations Our node and edge representations (explained below) rely on the observation that there are only two kinds of objects in an information graph: spans (as addresses to pieces of input texts) and types (as representations of abstract concepts). Since we can view types as special spans of length 1 grounded on the vocabulary of all types, Q ? R ? U , we only need O(nm + |Q ? R ? U |) number of indices to unambiguously represent the spans grounded on a concatenated representation of the type vocabulary and the input text, where n is the maximum input length, m is the maximum span length, and m n. We denote these indices as hybrid spans because they consist of both the spans of texts and the length-1 spans of types. These indices can be invertibly mapped back to types or text spans depending on their magnitudes (details of this mapping are explained in Section 3.2). With this joint indexing of spans and types, the task of generating an information graph is thus converted to generat-ing an alternating sequence of hybrid spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating sequences</head><p>We model the distribution p(s ? ) by a sequence generator h with parameters ? (|s ? | is the length of the s ? ):</p><formula xml:id="formula_3">p(s ? i |s ? 0 , ..., s ? i?1 ) = h(s ? 0 , ..., s ? i?1 , ?), p(s ? ) = |s ? | i=1 p(s ? i |s ? 0 , ..., s ? i?1 ),</formula><p>We will address in the following sections how to enforce the sequence generator, h, to only generate sequences in the space S ? , since we do not want h to assign non-zero probabilities to arbitrary sequences that do not have a corresponding graph.</p><p>3 HySPA: Hybrid Span Generation for Alternating Sequences</p><p>In order to directly generate a target sequence that alternates between nodes that represent spans in the input and a set of node/edge types that depend on our extraction task, we first build a hybrid representation H that is a concatenation of the hidden representations from edge types, node types and the input text. This representation functions as both the context space and the output space for our decoder. Then we invertibly map both the spans of input text and the indices of the types to the hybrid spans grounded on the representation H. Finally, hybrid spans are generated auto-regressively through a hybrid span decoder to form the alternating sequence y ? ? S ? . By translating the graph extraction task to a sequence generation task, we can easily use beam-search decoding to reduce possible exposure bias <ref type="bibr" target="#b36">(Wiseman and Rush, 2016)</ref> of the sequential decision process and thus find globally better graph representation.</p><p>High-level overview of HySPA: The HySPA model takes a piece of text (e.g. a sentence or passage), and the pre-defined node and edge types as input, and outputs an alternating sequence representation of an information graph. We enforce the generation of this sequence to be alternated by applying an alternating mask to the output probabilities. The detailed architecture is described in the following subsections. we arrange the type list, v as a concatenation of the label names of the edge types, virtual edge types and node types, i.e., v =R ?? ?Q</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text and Types Encoder</head><formula xml:id="formula_4">R = [R 1 , ..., R |R| ] U = [U 1 , ..., U |U | ] Q = [Q 1 , ..., Q |Q| ]</formula><p>where ? means the concatenation operator between two lists, andR,? ,Q are the lists of the type names in the sets R, U, Q, respectively (e.g. Q = ["Geopolitics", "Person", ...]). Note that the concatenation order between the lists of type names can be arbitrary as long as it is kept consistent throughout the whole model. Then, as in the embedding part of the table-sequence encoder <ref type="bibr" target="#b34">(Wang and Lu, 2020)</ref>, for each type, v i , we embed the label tokens of the types with the contextualized word embedding from a pre-trained language model, the GloVe embedding <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref> and the character embedding,</p><formula xml:id="formula_5">E 1 = ContextualizedEmbed(v), ? R lp?dc E 2 = GloveEmbed(v), ? R lp?dg E 3 = CharacterEmbed(v), ? R lp?d k E 4 = E 1 ? E 2 ? E 3 ? R lp?de , E v = E 4 W T 0 ? R lp?dm ,</formula><p>where l p = |R| + |U | + |Q| is the number of all kinds of types, W 0 ? R de?dm is the weight matrix of the linear projection layer, d e = d c + d g + d k is the total embedding dimension and d m is the hidden size of our model. After we obtain the contextualized embedding of the tokens of each type v i ? v, we take the average of these token vectors as the representation of v i and freeze its update during training. More details of the embedding pipeline can be found in Appendix A. This embedding pipeline is also used to embed the words in the input text, x. Unlike the pipeline for the type embedding, we represent the word as the contextualized embedding of its first sub-token from the pre-trained Language Model (LM, e.g. BERT <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>), and finetune the LM in an end-to-end fashion.</p><p>After obtaining the type embedding E v , and the text embedding E x respectively, we concatenate them along the sequence length dimension to form the hybrid representation H 0 . Since H 0 is a concatenation of word vectors from four different types of tokens, i.e., edge types, virtual edge types, node types and text, a meta-type embedding is applied to indicate this type difference between the blocks of vectors from the representation H 0 , as shown in <ref type="figure">Figure 2</ref>. The final context representation H is obtained by element-wise addition of the meta-type embedding and H 0 ,</p><formula xml:id="formula_6">H 0 = E v ? E x ? R l h ?dm , H s = MetaTypeEmbed(H 0 ) ? R l h ?dm , H = H 0 + H s ? R l h ?dm ,</formula><p>where l h = l p + |x| is the height of our hybrid representation matrix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Invertible Mapping between Spans &amp; Types and Hybrid Spans</head><p>Given a span in the text, t = (t s , t e ) ? N 2 , t s &lt; t e , we convert the span t to an index k, k ? l p , in the representation H via the mapping g k , We take m = 16 and l p = 19 for this example. "19" in the alternating sequence is the index for the span (0,1) of "He", "83" is the index for the span (4,5) of "Baghdad", and "10" is the index of the virtual edge type, <ref type="bibr">[SEP]</ref>. The input text (on top) for this graph is "He was captured in Baghdad late Monday night".</p><formula xml:id="formula_7">k = g k (t s , t e ) = t s m + t e ? t s ? 1 + l p ? N,</formula><p>where m is the maximum length of spans, and l p = |R| + |U | + |Q|. We keep the type indices in the graph unchanged because they are smaller than l p and k ? l p . Since, for an information graph, the maximum span length, m, of a mention is often far smaller than the length of the text, i.e., m n, we can then reduce the bound of the maximum magnitude of k from O(n 2 ) to O(nm) by only considering spans of length smaller than m, and thus maintain linear space complexity for our decoder with respect to the length of the input text, n. <ref type="figure" target="#fig_2">Figure 3</ref> shows a concrete example of our alternating sequence for a knowledge graph in the ACE05 dataset.</p><p>Since t s , t e , k are all natural numbers, we can construct an inverse mapping g t that converts the index k in H back to t = (t s , t e ),</p><formula xml:id="formula_8">t s = g ts (k) = ?max(0, ?k + l p )+ max(0, k ? l p )/m + l p , t e = g te (k) = g ts (k) + max(0, k ? l p ) mod m,</formula><p>where ? is the integer floor function and mod is the modulus operator. Note that g t (k) can be directly applied to the indices from the types segment of H and remain their values unchanged, i.e., g t (k) = (k, k), ?k &lt; l p , k ? N.</p><p>With this property, we can easily incorporate the mapping g t into our decoder to map the alternating sequence y ? back to the spans in the hybrid representation H. <ref type="figure">Figure 4</ref> shows the general model architecture of our hybrid span decoder. Our decoder takes the context representation H as input, and recurrently decodes the alternating sequence y ? given a startof-sequence token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hybrid Span Decoder</head><p>Hybrid Span Encoding via Attention Given the alternating sequence y ? , and the mapping g t (section 3.2), our decoder first maps each index in y ? to a span, (t s i , t e i ) = g t (y ? i ), grounded on the representation H and then converts the span to an attention mask, M 0 , to allow the model to learn to represent a span as a weighted sum of a segment of the contextualized word representations referred by the span,</p><formula xml:id="formula_9">Q = W T 1 H [CLS] + b 1 ? R |y ? |?dm , K = W T 2 H + b 2 ? R l h ?dm , H y = softmax QK T ? d m + M 0 H ? R |y ? |?dm , M 0 (i, j) = 0, t s i ? j ? t e i ??, otherwise</formula><p>where H [CLS] ? R |y ? |?dm is the |y ? |-times repeated hidden representation of the start of the sequence token, [CLS], from the text segment of H, and H y is our final representation of the hybrid spans in y ? . W 1 , W 2 , b 1 , b 2 are learnable parameters, and t s i , t e i are the start and the end position of the span thatwe are encoding. Note that for the type spans whose length is 1, the result of the softmax calculation will always be 1, which leads to its span representation to be exactly its embedding vector as we desired.</p><p>Traversal Embedding In order to distinguish the hybrid spans at different position in y ? , a naive way is to add a sinusoidal position embedding <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> to H y . However, this approach treats the alternating sequence as an ordinary sequence and ignores the underlying graph structure it encodes. To alleviate this issue, we propose a novel traversal embedding approach which captures the traversal level information, the parentchild information and the intra-level connection information as a substitution of the naive position embedding. Our traversal embedding can either encode the BFS or DFS traversal pattern. As an example, we assume BFS traversal here and leave the details of DFS traversal embedding in Appendix D. <ref type="figure">Figure 4</ref>: The architecture of our hybrid span decoder. N is the number of the decoder layers. ? before the softmax function means the concatenation operator. H N y is the hidden representation of the sequence y ? from the last decoder layer. Our hybrid span decoder can be understood as an auto-regressive model that operates in a closed context space and output space defined by H. Our BFS traversal embedding is a pointwise sum of the level embedding, L, the parent-child embedding, P , and the tree embedding, T of a given alternating sequence, y, TravEmbed(y) = L(y)+P (y)+T (y) ? R |y|?dm where the level embedding assigns the same embedding vector L i for each position at the BFS traversal level i, and the value of the embedding vector is filled according to the non-parametric sinusoidal position embedding since we want our embedding to extrapolate to the sequence that is longer than any sequences in the training set. The parent-child embedding assigns different random initialized embedding vectors at the positions of the parent nodes and the child nodes in the BFS traversal levels to help model distinguish between these two kinds of nodes. For encoding the intra-level connection information, our insight is that the connection between each nodes in a BFS level can be viewed as a depth-3 tree, where the first depth takes the parent node, the second depth is filled with the edge types and the third depth consists of the corresponding child nodes for each of the edge types. Our tree embedding is then formed by encoding the position information of the depth-3 tree with a tree positional embedding <ref type="bibr" target="#b29">(Shiv and Quirk, 2019)</ref> for each BFS level. <ref type="figure" target="#fig_3">Figure 5</ref> shows a concrete example of how these embeddings function for a given alternating sequence. The obtained traversal embedding is then pointwisely added to the hidden representation of the alternating sequence H y for injecting the traversal information of the graph structure.</p><p>Inner blocks With the input text representation H text sliced from the hybrid representation H and the target sequence representation H y , we apply an N -layer transformer structure with mixed-attention  to allow our model to utilize features from different attention layers when decoding the edges or the nodes of an alternating sequence. Note that our hybrid span decoder is perpendicular to the actual choice of the neural structures of the inner blocks, and we choose the design of mixed-attention transformer  because its layerwise coordination property is empirically more suitable for our heterogeneous decoding of two different kinds of sequence elements. The detailed structure of the inner blocks is explained in Appendix E.</p><p>Hybrid span decoding For the hybrid span decoding module, we first slice off the hidden rep-resentation of the alternating sequence y ? from the output of the N -layer inner blocks and denote it as H N y . Then for each hidden representation h N y i ? H N y , 0 ? i &lt; |y ? |, we apply two different linear layers to obtain the start position representation, s y i , and the end position representation, e y i ,</p><formula xml:id="formula_10">s y i = W T 5 h y i + b 5 ? R dm , e y i = W T 6 h y i + b 6 ? R dm ,</formula><p>where W 5 , W 6 ? R dm?dm and b 5 , b 6 ? R dm are learnable parameters. Then we calculate the scores of the target spans separately for the types segment and the text segment of H, and concatenate them together before the final softmax operator for a joint estimation of the probabilities of text spans and type spans,</p><formula xml:id="formula_11">h s i = H types s y i + m a ? R lp , h e i = H types e y i + m a ? R lp , h i = h s i + h e i ? R lp , t s i = H text s y i + m a ? R n , t e i = H text e y i + m a ? R n , t i = unfold(t e i , m) + t s i ? R nm , p(y ? i+1 ) = softmax(h i ? t i ) ? R nm+lp ,</formula><p>where h i is the score vector of possible spans in the type segment of H, and t i is the score vector of possible spans in the text segment of H. Since the type spans always have a span length 1, we only need an element-wise addition between the start position scores, h s i and the end position scores h e i to calculate h i . The entries of t i contain the scores for the text spans, t s i ,j + t e i ,k , ?j ? k, k ? j &lt; m, which are calculated with the help of an unfold function which converts the vector t e i ? R n to a stack of n sliding windows of size m, the maximum span length, with stride 1. The alternating masks m a ? R lp , m a ? R n are defined as:</p><formula xml:id="formula_12">m a (j) = 0, y ? i &gt; l e ? j &lt; l e ??, otherwise m a (j) = ??, y ? i &gt; l e 0, otherwise</formula><p>where l e = |R| + |U | is the total number of edge types. In this way, while we have a joint model of nodes and edge types, the output distribution is enforced by the alternating masks to produce an alternating decoding of nodes and edge types, and this is the main reason why we call this decoder a hybrid span decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>We test our model on the ACE 2005 dataset distributed by LDC 4 , which includes 14.5k sentences, 38.3k entities (with 7 types), and 7.1k relations (with 6 types), derived from the general news domain. More details can be found in Appendix C.</p><p>Following previous work, we use F1 as an evaluation metric for both NER and RE. For the NER task, a prediction is marked correct when both the type and the boundary span match those of the gold entity. For the RE task, a prediction is correct when both the relation type and the boundaries of the two entities are correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>When training our model, we apply the crossentropy loss with a label smoothing factor of 0.1. The model is trained with 2048 tokens per batch (roughly a batch size of 28) for 25000 steps using an AdamW optimizer <ref type="bibr" target="#b22">(Loshchilov and Hutter, 2018)</ref> with a learning rate of 2e ?4 , a weight decay of 0.01, and an inverse square root scheduler with 2000 warm-up steps. Following the TabSeq model <ref type="bibr" target="#b34">(Wang and Lu, 2020)</ref>, we use RoBERTa-large <ref type="bibr" target="#b20">(Liu et al., 2019)</ref> or ALBERT-xxlarge-v1 <ref type="bibr" target="#b16">(Lan et al., 2020)</ref> for the pretrained language model and slow its learning rate by a factor of 0.1 during training. A hidden state dropout rate of 0.2 is applied to RoBERTa-large while the rate of 0.1 for ALBERTxxlarge-v1. A dropout rate of 0.1 is also applied to our hybrid span decoder during training. We set the maximum span length, m = 16, the hidden size of our model, d m = 256, and the number of the decoder blocks, N = 12. Even though theoretically the beam-search should help us reduce the exposure bias, we do not observe any performance gain during grid search of the beam size and the length penalty on the validation set (detailed grid search setting is in Appendix A). Thus we set a vanilla beam size of 1 and the length penalty of 1, and leave this theory-experiment contradiction for future research. Our model is built with the FAIRSEQ toolkit  for efficient distributed training and all the experiments are conducted on two NVIDIA TITAN X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IE Models</head><p>Space Complexity Time Complexity NER RE PointerNet <ref type="bibr" target="#b14">(Katiyar and Cardie, 2017</ref>) O(n) O(n 2 ) 82.6 55.9 SpanRE <ref type="bibr" target="#b5">(Dixit and Al-Onaizan, 2019</ref>) O(n) O(n 2 ) 86.0 62.8 Dygie++  O(n) O(n 2 ) 88.6 63.4 OneIE <ref type="bibr" target="#b19">(Lin et al., 2020)</ref> O(n) O(n 2 ) 88.8 67.5 TabSeq <ref type="bibr" target="#b34">(Wang and Lu, 2020)</ref> O(n 2 ) O(n) 89.5 67.6 HySPA (ours) w/ RoBERTa O(n) O(n) 88.9 68.2 w/ ALBERT 89.9 68.0  <ref type="table">Table 2</ref>: Ablation study on the ACE05 test set. "-Traversal-embedding": we remove the traversal embedding and instead use sinusoidal position embedding, and the following ablations are based on the model after this ablation. "-Masking": we remove the alternating mask from the hybrid span decoder. "-BFS": we use DFS instead of BFS as traversal. "-Mixedattention": we remove the mixed-attention layer and use a standard transformer encoder decoder structure. "-Span-attention": we remove the span attention in the span encoding module and instead average the words in the span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Table 1 compares our model with the previous stateof-the-art results on the ACE05 test set. Compared with the previous SOTA, TabSeq <ref type="bibr" target="#b34">(Wang and Lu, 2020)</ref> with ALBERT pretrained language model, our model with ALBERT has significantly better performance for both NER score and RE score, while maintaining a linear space complexity which is an order smaller than TabSeq. Our model is the first joint model that has both linear space and time complexities compared with all previous joint IE models, and thus has the best scalability for largescale real world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To prove the effectiveness of our approach, we conduct ablation experiments on the ACE05 dataset. As shown in <ref type="table">Table 2</ref>, after we remove the traversal embedding the RE F1 scores drop significantly, which indicates that our traversal embedding can help encode the graph structure and improve relation predictions. Also if the alternating masking is dropped, the NER F1 and RE F1 scores both drop significantly, which proves the importance of enforcing the alternating pattern. We can observe that the mixed-attention layer contributes significantly for relation extraction. This is because the layerwise coordination can help the decoder to disentangle the source features and utilize different layer features between the entity and the relation prediction. We can also observe that the DFS traversal has worse performance than BFS. We suspect that this is because the resultant alternating sequence from DFS is often longer than the one from BFS due to the nature of the knowledge graphs, and thus increases the learning difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Error Analysis</head><p>After analyzing 80 remaining errors, we categorize and discuss common cases below ( <ref type="figure" target="#fig_4">Figure 6</ref> plots the distribution of error types). These may require additional features and strategies to address.</p><p>Insufficient context. In many examples, the answer entity is a pronoun that cannot be accurately typed given the limited context: in "We notice they said they did not want to use the word destroyed, in fact, they said let others do that", it's difficult to correctly classify We as an organization. This could be mitigated by using entire documents as input, leveraging cross-sentence context. Rare words. The rare word issue is when the word in test set rarely appeared in the training set and often not termed in the dictionary. In the sentence "There are also Marine FA-18s and Marine Heriers at this base", the term Heriers (a vehicle incorrectly classified as person by the model) neither appeared in the training set, nor understood well by pre-trained language model; the model, in this case, can only rely on subword-level representation.</p><p>Background knowledge required Often the sentence mentions entities that are difficult to infer from the context, but are easily identified by consulting a knowledge base: in "but critics say Airbus should have sounded a stronger alarm after a similar incident occurred in 1997", our model incorrectly predicts the Airbus to be a vehicle while the Airbus here refers to the European aerospace corporation. Our system also separated United Nations Security Council into two entities United Nations and Security Council, generating a non-existing relation triple (Security Council part-of United Nations). Such mistakes could be avoided by consulting a knowledge base such as DBpedia <ref type="bibr" target="#b1">(Bizer et al., 2009)</ref> or by performing entity linking. Inherent ambiguity Many examples have inherent ambiguity, e.g. European Union can be typed as organization or political entity, while some entities (e.g., military bases) can be both locations and organizations, or facilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>NER is often done jointly with RE in order to mitigate error propagation and learn inter-relation between tasks. One line of approaches is to treat the joint task as a squared table filling problem <ref type="bibr" target="#b25">(Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b11">Gupta et al., 2016;</ref><ref type="bibr" target="#b34">Wang and Lu, 2020)</ref>, where the i-th column or row represents the i-th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by <ref type="bibr" target="#b24">Miwa and Bansal (2016)</ref>, the authors used BiLSTM <ref type="bibr" target="#b10">(Graves et al., 2013)</ref> for NER and conse-quently a Tree-LSTM <ref type="bibr" target="#b31">(Tai et al., 2015)</ref> based on dependency graph for RE.  and , on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relations. Extending on , <ref type="bibr" target="#b19">Lin et al. (2020)</ref> introduced ONEIE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a graph. Note that our model differs from ONEIE <ref type="bibr" target="#b19">(Lin et al., 2020)</ref> in that our model captures global relationships automatically through autoregressive generation while ONEIE uses feature engineered templates; Moreover, ONEIE needs to do pairwise classification for relation extraction, while our method efficiently generates existing relations and entities. While several Seq2Seq-based models <ref type="bibr" target="#b38">Zeng et al., 2018</ref><ref type="bibr" target="#b35">Wei et al., 2019;</ref><ref type="bibr" target="#b41">Zhang et al., 2019)</ref> have been proposed to generate triples (i.e., node-edge-node), our model is fundamentally different from them in that: (1) it is generating a BFS/DFS traversal of the target graph, which captures dependencies between nodes and edges and has a shorter target sequence, (2) we model the nodes as the spans in the text, which is independent of the vocabulary, so even if the tokens of the nodes are rare or unseen words, we can still generate spans on them based on the context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose the Hybrid Span Generation (HySPA) model, the first end-to-end text-tograph extraction model that has a linear space and time complexity at the graph decoding stage. Besides its scalability, the model also achieves stateof-the-art performance on the ACE05 joint entity and relation extraction task. Given the flexibility of the structure of our hybrid span generator, abundant future research directions remain, e.g. incorporating the external knowledge for hybrid span generation, applying more efficient sparse self-attention, and developing better search methods to find more globally plausible graphs represented by the alternating sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head><p>We use 100-dimensional GloVe word embeddings trained on 6B tokens as intialization 5 , and freeze its update during training. The character embedding has 30-dimension with LSTM encoding 6 and the Glove Embeddings for the out of vocabulary tokens are replaced with randomly initialized vectors following <ref type="bibr" target="#b34">Wang and Lu (2020)</ref>. We use gradient clipping of 0.25 during training. The number of heads for our mixed attention is set to 8. The beam size and length penalty is decided by a grid-search on the validation set of the ACE05 dataset, and the range for the beam size is from 1 to 7 with a step size of 1 and the length penalty is from 0.7 to 1.2 with a step size of 0.1. We choose the best beam size and length penalty based on the metric of relation extraction F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training Details</head><p>Our model has 236 million parameters with the ALBERT-xxlarge pretrained language model. On average, our best performing model with ALBERT-xxlarge can be trained distributedly on two NVIDIA TITAN X GPUs for 20 hours.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>We represent directed multigraphs as alternating sequences of nodes (blue) and edges (orange).Here, the graph is traversed by Breadth First Search (BFS) with an ascending ordering of nodes and edge types. "[s]" or [SEP] is a virtual edge type, representing the end of each BFS level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 Figure 2 :</head><label>22</label><figDesc>shows the encoder architecture of our proposed model. For the set of node types, Q, and the set of edge types, R, and the virtual edge types, U , The encoder architecture of our model, where the ? symbol is the concatenation operator, k is the index of the word vectors in H 0 , and l e = |R| + |U |. The colored table on the right indicates the assignment of the meta-types for different blocks of the concatenated word vectors from H 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>An example of the alternating sequence representation (in the middle) of a knowledge graph (at bottom) from the ACE05 training set, where A 1 means the Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>An example of BFS traversal embedding for an alternating sequence, ["He", Type, PER, [SEP], "Baghdad", Type, GPE, PHYS, "He"]. Our traversal embedding is the sum of the level embedding, the parent-child embedding and the tree embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Distribution of remaining errors on the ACE05 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Joint NER and RE F1 scores of the IE models on the ACE05 test set. Complexities are calculated for the entity and relation decoding part of the models (n is the length of the input text). The performance of the TabSeq model reported here is based on the same ALBERT-xxlarge<ref type="bibr" target="#b16">(Lan et al., 2020)</ref> pretrained language model as ours.</figDesc><table><row><cell>Model</cell><cell cols="2">NER F1 RE F1</cell></row><row><cell>HySPA w/ RoBERTa</cell><cell>88.9</cell><cell>68.2</cell></row><row><cell>-Traversal-embedding</cell><cell>88.9</cell><cell>66.7</cell></row><row><cell>-Masking</cell><cell>88.1</cell><cell>64.8</cell></row><row><cell>-BFS</cell><cell>88.7</cell><cell>66.2</cell></row><row><cell>-Mixed-attention</cell><cell>88.6</cell><cell>64.7</cell></row><row><cell>-Span-attention</cell><cell>88.5</cell><cell>66.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is publicly available at https://github. com/renll/HySPA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Q includes a [NULL] node type for the case when the input text does not have an information graph.3  In the case of DFS, [SEP] tokens appear after leaf nodes. Parents appear once for each child.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://catalog.ldc.upenn.edu/ LDC2006T06</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Data</head><p>The Automatic Content Extraction (ACE) 2005 7 dataset contains English, Arabic and Chinese training data for the 2005 Automatic Content Extraction (ACE) technology evaluation, providing entity, relation, and event annotations. We follow   <ref type="bibr">8</ref> for preprocessing and data splits. The preprocessed data contains 7.1k relations, 38k entities, and 14.5k sentences. The split contains 10051 samples for training, 2424 samples for development, and 2050 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DFS Traversal Embedding</head><p>Since the parent-child information is already contained in the intra-level connections of DFS traversal, we only have the sum of the level embedding and the connection embedding for DFS traversal 5 https://nlp.stanford.edu/projects/ glove/ 6 https://github.com/LorrinWWW/ two-are-better-than-one/blob/master/ layers/encodings/embeddings.py 7 https://www.ldc.upenn.edu/ collaborations/past-projects/ace 8 https://github.com/dwadden/dygiepp/ tree/master/scripts/data/ace05/ preprocess embedding. Similar to BFS embedding, the DFS level embedding assigns the same embedding vector L i for each position at the DFS traversal level i, but the value of the embedding vector is randomly initialized instead of filled with the non-parametric sinusoidal position embedding, since the proximity information does not exist between the traversal levels of DFS. However, we do have clear distance information for the elements in a DFS level, i,e., for a DFS level D = [A, B, C, ..., <ref type="bibr">[sep]</ref>], the distance from A to the elements [A, B, C, ..., <ref type="bibr">[sep]</ref>] is [0, 1, 2, 3, ..., |D| ? 1]. We encode this distance information with the sinusoidal position embedding which becomes our connection embedding that captures the intra-level connection information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Transformer with Mixed-attention</head><p>We first slice off the hidden representation of the input text from the hybrid representation H, and denote it as H text , then the input text representation H text and the output from the Hybrid Span Encoding H y gets fed into a stack of N mixedattention/feedforward blocks that have the following structure (as shown in <ref type="figure">Figure 7)</ref>:</p><p>Since generating the node and edge types may need features from different layers, we use mixed attention , which allows our model to utilize the features from different attention layers when encoding the text segment, H text , and the target features, H y ,</p><p>where n = |x| is the length of the input text, l m = |x| + |y ? | is the total length of the source and the target features. Denoting the concatenation of the source features, H text , and the target features, H y , as H 0 , a source/target embedding  is also added to H 0 before the first layer of the mixed attention to allow the model to distinguish the features from the source and the target sequences. The mixed-attention layer is combined with a feed-forward layer to form a decoder block:</p><p>are the learnable parameters, and LayerNorm is the Layer Normalization layer <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>. The decoder block is stacked N times to obtain the final hidden representation H N , and output the final representation of the target sequence, H N y . The mixed-attention has a time complexity of O(n 2 ) when encoding the source features, but we can cache the hidden representation of this part when generating the target tokens due to the causal masking of the target features, and thus maintain a time complexity of O(n) for each decoding step.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dbpedia-a crystallization point for the web of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of web semantics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
	<note>Portland</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Span-level model for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpit</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1525</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5308" to="5314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07755</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Factorizing complex models: A case study in mention detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
		<idno type="DOI">10.3115/1220175.1220235</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving mention detection robustness to noisy input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pitrelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="335" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-dimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Layer-wise coordination between encoder and decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="7944" to="7954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A systematic exploration of the feature space for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Going out on a limb: Joint extraction of entity mentions and relations without dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1085</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="917" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Imojie: Iterative memory-based joint open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshav</forename><surname>Kolluru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipul</forename><surname>Rathore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08178</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1038</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Constructing information networks using one single model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1198</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1846" to="1851" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A joint neural model for information extraction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.713</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7999" to="8009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach. arxiv 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><forename type="middle">L</forename><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01763</idno>
		<title level="m">Seq2rdf: An endto-end application for deriving triples from natural language text</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03296</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00770</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey of heterogeneous information network analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2016.2598561</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="37" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Novel positional encodings to enable tree-based transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Shiv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised relation extraction with large-scale word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="521" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<title level="m">Improved semantic representations from tree-structured long short-term memory networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Two are better than one: Joint entity and relation extraction with table-sequence encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03851</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A novel cascade binary tagging framework for relational triple extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03227</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Copymtl: Copy mechanism for joint extraction of entities and relations with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9507" to="9514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Minimize exposure bias of seq2seq models in joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysa</forename><surname>Xuemo Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end neural relation extraction with global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1182</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1730" to="1740" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Broad-coverage semantic parsing as transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1392</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3786" to="3798" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
